Title,Arxiv_ID,Article_link,Pdf_link,Author,Abstract,Date,Tag,Doi_number
              Biodegradable Polymeric Micro/Nano-Structures with Intrinsic Antifouling/Antimicrobial Properties:Relevance in Damaged Skin and Other Biomedical Applications          ,2010.08250,https://arxiv.org/abs/2010.08250,https://arxiv.org/pdf/2010.08250,"Authors:MarioMilazzo,GiuseppeGallone,ElenaMarcello,MariaDonatellaMariniello,LucaBruschini,IpsitaRoy,SerenaDanti","        Bacterial colonization ofimplanted biomedical devicesis themain cause of healthcare-associated infections, estimated to be 8.8 million per year in Europe. Many infections originate from damaged skin, which lets microorganisms exploit injuries and surgical accesses as passageways to reach the implant site and inner organs. Therefore, an effective treatment of skin damage is highly desirable for the success of many biomaterial-related surgical procedures. Due to gained resistance to antibiotics, new antibacterial treatments are becoming vital to control nosocomial infections arising as surgical and post-surgical complications. Surface coatings can avoid biofouling and bacterial colonization thanks to biomaterial inherent properties (e.g., super hydrophobicity), specifically without using drugs, which may cause bacterial resistance. The focus of this review is to highlight the emerging role of degradable polymeric micro- and nano-structures that show intrinsic antifouling and antimicrobial properties, with a special outlook towards biomedical applications dealing with skin and skin damage. The intrinsic properties owned by the biomaterials encompass three main categories: (1) physical-mechanical, (2) chemical, and (3) electrostatic. Clinical relevance in ear prostheses and breast implants is reported. Collecting and discussing the updated outcomes in this field would help the development of better performing biomaterial-based antimicrobial strategies, which are useful to prevent infections.        △ Less","16 October, 2020","physics.bio-ph,physics.med-ph",10.3390/jfb11030060 
              Risk-Aware Decision Making in Service Robots to Minimize Risk of Patient Falls in Hospitals          ,2010.08124,https://arxiv.org/abs/2010.08124,https://arxiv.org/pdf/2010.08124,"Authors:RoyaSabbaghNovin,AmirYazdani,AndrewMerryweather,TuckerHermans","        Planning under uncertainty is a crucial capability for autonomous systems to operate reliably in uncertain and dynamic environments. The concern of patient safety becomes even more critical in healthcare settings where robots interact with humans. In this paper, we propose a novel risk-aware planning framework to minimize the risk of patient falls by providing a patient with an assistive device. Our approach combines learning-based prediction with model-based control to plan for the fall prevention tasks. This provides advantages compared to end-to-end learning methods in which the robot's performance is limited to specific scenarios, or purely model-based approaches that use relatively simple function approximators and are prone to high modeling errors. We compare two different risk metrics and the combination of them and report the results from various simulated scenarios. The results show that using the proposed cost function, the robot can plan interventions to avoid high fall score events.        △ Less","15 October, 2020",cs.RO,
              When Virtual Therapy and Art Meet: A Case Study of Creative Drawing Game in Virtual Environments          ,2010.08100,https://arxiv.org/abs/2010.08100,https://arxiv.org/pdf/2010.08100,"Authors:LaurenBaron,BrianCohn,RoghayehBarmaki","        There have been a resurge lately on virtual therapy and other virtual- and tele-medicine services due to the new normal of practicing 'shelter at home'. In this paper, we propose a creative drawing game for virtual therapy and investigate user's comfort and movement freedom in a pilot study. In a mixed-design study, healthy participants (N=16, 8 females) completed one of the easy or hard trajectories of the virtual therapy game in standing and seated arrangements using a virtual-reality headset. The results from participants' movement accuracy, task completion time, and usability questionnaires indicate that participants had significant performance differences on two levels of the game based on its difficulty (between-subjects factor), but no difference in seated and standing configurations (within-subjects factor). Also, the hard mode was more favorable among participants. This work offers implications on virtual reality and 3D-interactive systems, with specific contributions to virtual therapy, and serious games for healthcare applications.        △ Less","15 October, 2020",cs.HC,
              IoT Platform for COVID-19 Prevention and Control: A Survey          ,2010.08056,https://arxiv.org/abs/2010.08056,https://arxiv.org/pdf/2010.08056,"Authors:YudiDong,Yu-DongYao","        As a result of the worldwide transmission of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), coronavirus disease 2019 (COVID-19) has evolved into an unprecedented pandemic. Currently, with unavailable pharmaceutical treatments and vaccines, this novel coronavirus results in a great impact on public health, human society, and global economy, which is likely to last for many years. One of the lessons learned from the COVID-19 pandemic is that a long-term system with non-pharmaceutical interventions for preventing and controlling new infectious diseases is desirable to be implemented. Internet of things (IoT) platform is preferred to be utilized to achieve this goal, due to its ubiquitous sensing ability and seamless connectivity. IoT technology is changing our lives through smart healthcare, smart home, and smart city, which aims to build a more convenient and intelligent community. This paper presents how the IoT could be incorporated into the epidemic prevention and control system. Specifically, we demonstrate a potential fog-cloud combined IoT platform that can be used in the systematic and intelligent COVID-19 prevention and control, which involves five interventions including COVID-19 Symptom Diagnosis, Quarantine Monitoring, Contact Tracing \& Social Distancing, COVID-19 Outbreak Forecasting, and SARS-CoV-2 Mutation Tracking. We investigate and review the state-of-the-art literatures of these five interventions to present the capabilities of IoT in countering against the current COVID-19 pandemic or future infectious disease epidemics.        △ Less","15 October, 2020","cs.HC,cs.AI,eess.SY",
              QReLU and m-QReLU: Two novel quantum activation functions to aid medical diagnostics          ,2010.08031,https://arxiv.org/abs/2010.08031,https://arxiv.org/pdf/2010.08031,"Authors:L.Parisi,D.Neagu,R.Ma,F.Campean","        The ReLU activation function (AF) has been extensively applied in deep neural networks, in particular Convolutional Neural Networks (CNN), for image classification despite its unresolved dying ReLU problem, which poses challenges to reliable applications. This issue has obvious important implications for critical applications, such as those in healthcare. Recent approaches are just proposing variations of the activation function within the same unresolved dying ReLU challenge. This contribution reports a different research direction by investigating the development of an innovative quantum approach to the ReLU AF that avoids the dying ReLU problem by disruptive design. The Leaky ReLU was leveraged as a baseline on which the two quantum principles of entanglement and superposition were applied to derive the proposed Quantum ReLU (QReLU) and the modified-QReLU (m-QReLU) activation functions. Both QReLU and m-QReLU are implemented and made freely available in TensorFlow and Keras. This original approach is effective and validated extensively in case studies that facilitate the detection of COVID-19 and Parkinson Disease (PD) from medical images. The two novel AFs were evaluated in a two-layered CNN against nine ReLU-based AFs on seven benchmark datasets, including images of spiral drawings taken via graphic tablets from patients with Parkinson Disease and healthy subjects, and point-of-care ultrasound images on the lungs of patients with COVID-19, those with pneumonia and healthy controls. Despite a higher computational cost, results indicated an overall higher classification accuracy, precision, recall and F1-score brought about by either quantum AFs on five of the seven bench-mark datasets, thus demonstrating its potential to be the new benchmark or gold standard AF in CNNs and aid image classification tasks involved in critical applications, such as medical diagnoses of COVID-19 and PD.        △ Less","15 October, 2020","cs.CV,cs.LG,cs.NE,eess.IV",
              Double Robust Representation Learning for Counterfactual Prediction          ,2010.07866,https://arxiv.org/abs/2010.07866,https://arxiv.org/pdf/2010.07866,"Authors:ShuxiZeng,SergeAssaad,ChenyangTao,ShounakDatta,LawrenceCarin,FanLi","        Causal inference, or counterfactual prediction, is central to decision making in healthcare, policy and social sciences. To de-bias causal estimators with high-dimensional data in observational studies, recent advances suggest the importance of combining machine learning models for both the propensity score and the outcome function. We propose a novel scalable method to learn double-robust representations for counterfactual predictions, leading to consistent causal estimation if the model for either the propensity score or the outcome, but not necessarily both, is correctly specified. Specifically, we use the entropy balancing method to learn the weights that minimize the Jensen-Shannon divergence of the representation between the treated and control groups, based on which we make robust and efficient counterfactual predictions for both individual and average treatment effects. We provide theoretical justifications for the proposed method. The algorithm shows competitive performance with the state-of-the-art on real world and synthetic data.        △ Less","15 October, 2020","stat.ML,cs.LG",
              Tracking Results and Utilization of Artificial Intelligence (tru-AI) in Radiology: Early-Stage COVID-19 Pandemic Observations          ,2010.07437,https://arxiv.org/abs/2010.07437,https://arxiv.org/pdf/2010.07437,"Authors:AxelWismüller,LarryStockmaster","        Objective: To introduce a method for tracking results and utilization of Artificial Intelligence (tru-AI) in radiology. By tracking both large-scale utilization and AI results data, the tru-AI approach is designed to calculate surrogates for measuring important disease-related observational quantities over time, such as the prevalence of intracranial hemorrhage during the COVID-19 pandemic outbreak. Methods: To quantitatively investigate the clinical applicability of the tru-AI approach, we analyzed service requests for automatically identifying intracranial hemorrhage (ICH) on head CT using a commercial AI solution. This software is typically used for AI-based prioritization of radiologists' reading lists for reducing turnaround times in patients with emergent clinical findings, such as ICH or pulmonary embolism.We analyzed data of N=9,421 emergency-setting non-contrast head CT studies at a major US healthcare system acquired from November 1, 2019 through June 2, 2020, and compared two observation periods, namely (i) a pre-pandemic epoch from November 1, 2019 through February 29, 2020, and (ii) a period during the COVID-19 pandemic outbreak, April 1-30, 2020. Results: Although daily CT scan counts were significantly lower during (40.1 +/- 7.9) than before (44.4 +/- 7.6) the COVID-19 outbreak, we found that ICH was more likely to be observed by AI during than before the COVID-19 outbreak (p<0.05), with approximately one daily ICH+ case more than statistically expected. Conclusion: Our results suggest that, by tracking both large-scale utilization and AI results data in radiology, the tru-AI approach can contribute clinical value as a versatile exploratory tool, aiming at a better understanding of pandemic-related effects on healthcare.        △ Less","14 October, 2020","q-bio.QM,cs.LG,q-bio.PE",
              Keys from the Sky: A First Exploration of Physical-Layer Security Using Satellite Links          ,2010.07194,https://arxiv.org/abs/2010.07194,https://arxiv.org/pdf/2010.07194,"Authors:PascalZimmer,RolandWeinreich,ChristianT.Zenger,AydinSezgin,ChristofPaar","        In this paper, we investigate physical-layer security (PLS) methods for proximity-based group-key establishment and proof of location. Fields of application include secure car-to-car communication, privacy-preserving and secure distance evidence for healthcare or location-based feature activation. Existing technologies do not solve the problem satisfactorily, due to communication restrictions, e.g., ultra-wide band (UWB) based time of flight measurements, or trusted hardware, e.g., using global navigation satellite system (GNSS) positioning data.  We introduce PLS as a solution candidate. It is information theoretically secure, which also means post-quantum resistant, and has the potential to run on resource constrained devices with low latency. Furthermore, we use wireless channel properties of satellite-to-Earth links, demonstrate the first feasibility study using off-the-shelf hardware testbeds and present first evaluation results and future directions for research.        △ Less","14 October, 2020",cs.CR,
"              Machine Learning Research Towards Combating COVID-19: Virus Detection, Spread Prevention, and Medical Assistance          ",2010.07036,https://arxiv.org/abs/2010.07036,https://arxiv.org/pdf/2010.07036,"Authors:OsamaShahid,MohammadNasajpour,SeyedaminPouriyeh,RezaM.Parizi,MengHan,MariaValero,FangyuLi,MohammedAledhari,QuanZ.Sheng","        COVID-19 was first discovered in December 2019 and has continued to rapidly spread across countries worldwide infecting thousands and millions of people. The virus is deadly, and people who are suffering from prior illnesses or are older than the age of 60 are at a higher risk of mortality. Medicine and Healthcare industries have surged towards finding a cure, and different policies have been amended to mitigate the spread of the virus. While Machine Learning (ML) methods have been widely used in other domains, there is now a high demand for ML-aided diagnosis systems for screening, tracking, and predicting the spread of COVID-19 and finding a cure against it. In this paper, we present a journey of what role ML has played so far in combating the virus, mainly looking at it from a screening, forecasting, and vaccine perspectives. We present a comprehensive survey of the ML algorithms and models that can be used on this expedition and aid with battling the virus.        △ Less","29 September, 2020",cs.CY,
              Basic principles and concept design of a real-time clinical decision support system for autonomous medical care on missions to Mars based on adaptive deep learning          ,2010.07029,https://arxiv.org/abs/2010.07029,https://arxiv.org/pdf/2010.07029,Authors:JuanMGarcia-Gomez,"        Space agencies and private companies prepare the beginning of the human space exploration for the 2030s with missions to put the first human on the Mars surface. The absence of gravity and radiation, along with distance, isolation and hostile environment are expected to increase medical events with unidentified manifestations along the crewmembers. The current healthcare strategy based on telemedicine and the possibility to stabilise and transport the injured crewmember to a terrestrial definitive medical facility is not applicable in exploration class missions. Therefore, full autonomous capability to solve medical situations will guide design of future healthcare systems on-board.  This study presents ten basic principles and the concept design of MEDEA, an on-board clinical decision support system to help crewmembers to deal with medical conditions, with special attention to emergency care situations and critical monitoring. Therefore, MEDEA is conceptually designed as a software suite of four interconnected modules. The main of them is responsible to give direct advice to the crew by means of a deep learning multitask neural network to predict the characters of the medical event, a classifier of the tertiary medical intervention and an optimiser of medical action plans. This module is continuously evaluate and re-trained with changing physiological data from the crew by an adaptive deep learning module, ensuring fairness, interpretability and traceability of decision making during the full operational time of MEDEA. Finally, MEDEA would be semantically interoperable with health information systems on-board by a FHIR module.  The deployment of MEDEA on-board of future missions to Mars will facilitate the deployment of a comprehensive preventive medical strategy, future quantitative medicine on Earth and on the expansion of humans throughout the solar system.        △ Less","29 September, 2020","cs.CY,cs.LG,physics.med-ph",
              Review the Enterprise Resource Planning in Moroccan Healthcare Organizations          ,2010.06989,https://arxiv.org/abs/2010.06989,https://arxiv.org/pdf/2010.06989,"Authors:FatimaZahraYamani,MohamedElMerouani","        The Hospital Information Systems (HIS) in Morocco take a central place in the process of patient care. An approach is made to analyze the current situation of the HIS within the institutions in order to bring an integral and generic vision, allowing the judicious articulation of the business and IT layers. Currently, the Enterprise Resource Planning (ERP) implemented remains a system consisting of several applications dedicated to specific areas. These systems have become an indispensable element within any hospital. The goal of our study is to discover how the ERP has been used in Moroccan healthcare sector and how these software should be implemented and used to improve healthcare services.        △ Less","22 September, 2020",cs.CY,10.5281/zenodo.3987129 
              Equitable Allocation of Healthcare Resources with Fair Cox Models          ,2010.06820,https://arxiv.org/abs/2010.06820,https://arxiv.org/pdf/2010.06820,"Authors:KamrunNaherKeya,RashidulIslam,ShimeiPan,IanStockwell,JamesR.Foulds","Healthcare programs such as Medicaid provide crucial services to vulnerable populations, but due to limited resources, many of the individuals who need these services the most languish on waiting lists. Survival models, e.g. the Cox proportional hazards model, can potentially improve this situation by predicting individuals' levels of need, which can then be used to prioritize the waiting lists. Providing care to those in need can prevent institutionalization for those individuals, which both improves quality of life and reduces overall costs. While the benefits of such an approach are clear, care must be taken to ensure that the prioritization process is fair or independent of demographic information-based harmful stereotypes. In this work, we develop multiple fairness definitions for survival models and corresponding fair Cox proportional hazards models to ensure equitable allocation of healthcare resources. We demonstrate the utility of our methods in terms of fairness and predictive accuracy on two publicly available survival datasets.        △ Less","14 October, 2020","cs.LG,cs.AI,cs.CY",
              A standardized framework for risk-based assessment of treatment effect heterogeneity in observational healthcare databases          ,2010.06430,https://arxiv.org/abs/2010.06430,https://arxiv.org/pdf/2010.06430,"Authors:AlexandrosRekkas,DavidvanKlaveren,PatrickB.Ryan,EwoutW.Steyerberg,DavidM.Kent,PeterR.Rijnbeek","        Aim: One of the aims of the Observation Health Data Sciences and Informatics (OHDSI) initiative is population-level treatment effect estimation in large observational databases. Since treatment effects are well-known to vary across groups of patients with different baseline risk, we aimed to extend the OHDSI methods library with a framework for risk-based assessment of treatment effect heterogeneity.  Materials and Methods: The proposed framework consists of five steps: 1) definition of the problem, i.e. the population, the treatment, the comparator and the outcome(s) of interest; 2) identification of relevant databases; 3) development of a prediction model for the outcome(s) of interest; 4) estimation of propensity scores within strata of predicted risk and estimation of relative and absolute treatment effect within strata of predicted risk; 5) evaluation and presentation of results.  Results: We demonstrate our framework by evaluating heterogeneity of the effect of angiotensin-converting enzyme (ACE) inhibitors versus beta blockers on a set of 9 outcomes of interest across three observational databases. With increasing risk of acute myocardial infarction we observed increasing absolute benefits, i.e. from -0.03% to 0.54% in the lowest to highest risk groups. Cough-related absolute harms decreased from 4.1% to 2.6%.  Conclusions: The proposed framework may be useful for the evaluation of heterogeneity of treatment effect on observational data that are mapped to the OMOP Common Data Model. The proof of concept study demonstrates its feasibility in large observational data. Further insights may arise by application to safety and effectiveness questions across the global data network.        △ Less","13 October, 2020","stat.ME,stat.ML",
              RANDGAN: Randomized Generative Adversarial Network for Detection of COVID-19 in Chest X-ray          ,2010.06418,https://arxiv.org/abs/2010.06418,https://arxiv.org/pdf/2010.06418,"Authors:SamanMotamed,PatrikRogalla,FarzadKhalvati","        COVID-19 spread across the globe at an immense rate has left healthcare systems incapacitated to diagnose and test patients at the needed rate. Studies have shown promising results for detection of COVID-19 from viral bacterial pneumonia in chest X-rays. Automation of COVID-19 testing using medical images can speed up the testing process of patients where health care systems lack sufficient numbers of the reverse-transcription polymerase chain reaction (RT-PCR) tests. Supervised deep learning models such as convolutional neural networks (CNN) need enough labeled data for all classes to correctly learn the task of detection. Gathering labeled data is a cumbersome task and requires time and resources which could further strain health care systems and radiologists at the early stages of a pandemic such as COVID-19. In this study, we propose a randomized generative adversarial network (RANDGAN) that detects images of an unknown class (COVID-19) from known and labelled classes (Normal and Viral Pneumonia) without the need for labels and training data from the unknown class of images (COVID-19). We used the largest publicly available COVID-19 chest X-ray dataset, COVIDx, which is comprised of Normal, Pneumonia, and COVID-19 images from multiple public databases. In this work, we use transfer learning to segment the lungs in the COVIDx dataset. Next, we show why segmentation of the region of interest (lungs) is vital to correctly learn the task of classification, specifically in datasets that contain images from different resources as it is the case for the COVIDx dataset. Finally, we show improved results in detection of COVID-19 cases using our generative model (RANDGAN) compared to conventional generative adversarial networks (GANs) for anomaly detection in medical images, improving the area under the ROC curve from 0.71 to 0.77.        △ Less","6 October, 2020","eess.IV,cs.CV,cs.LG",
              Humane Visual AI: Telling the Stories Behind a Medical Condition          ,2010.06296,https://arxiv.org/abs/2010.06296,https://arxiv.org/pdf/2010.06296,"Authors:WonyoungSo,EdytaP.Bogucka,SanjaŠćepanović,SagarJoglekar,KeZhou,DanieleQuercia","        A biological understanding is key for managing medical conditions, yet psychological and social aspects matter too. The main problem is that these two aspects are hard to quantify and inherently difficult to communicate. To quantify psychological aspects, this work mined around half a million Reddit posts in the sub-communities specialised in 14 medical conditions, and it did so with a new deep-learning framework. In so doing, it was able to associate mentions of medical conditions with those of emotions. To then quantify social aspects, this work designed a probabilistic approach that mines open prescription data from the National Health Service in England to compute the prevalence of drug prescriptions, and to relate such a prevalence to census data. To finally visually communicate each medical condition's biological, psychological, and social aspects through storytelling, we designed a narrative-style layered Martini Glass visualization. In a user study involving 52 participants, after interacting with our visualization, a considerable number of them changed their mind on previously held opinions: 10% gave more importance to the psychological aspects of medical conditions, and 27% were more favourable to the use of social media data in healthcare, suggesting the importance of persuasive elements in interactive visualizations.        △ Less","13 October, 2020",cs.HC,
              COVID-19 Imaging Data Privacy by Federated Learning Design: A Theoretical Framework          ,2010.06177,https://arxiv.org/abs/2010.06177,https://arxiv.org/pdf/2010.06177,"Authors:AnwaarUlhaq,OliverBurmeister","        To address COVID-19 healthcare challenges, we need frequent sharing of health data, knowledge and resources at a global scale. However, in this digital age, data privacy is a big concern that requires the secure embedding of privacy assurance into the design of all technological solutions that use health data. In this paper, we introduce differential privacy by design (dPbD) framework and discuss its embedding into the federated machine learning system. To limit the scope of our paper, we focus on the problem scenario of COVID-19 imaging data privacy for disease diagnosis by computer vision and deep learning approaches. We discuss the evaluation of the proposed design of federated machine learning systems and discuss how differential privacy by design (dPbD) framework can enhance data privacy in federated learning systems with scalability and robustness. We argue that scalable differentially private federated learning design is a promising solution for building a secure, private and collaborative machine learning model such as required to combat COVID19 challenge.        △ Less","13 October, 2020","cs.CR,cs.CV,cs.LG",
              A Framework for Addressing the Risks and Opportunities In AI-Supported Virtual Health Coaches          ,2010.06059,https://arxiv.org/abs/2010.06059,https://arxiv.org/pdf/2010.06059,"Authors:SoniaBaee,MarkRucker,AnnaBaglione,MawuloloK.Ameko,LauraBarnes","        Virtual coaching has rapidly evolved into a foundational component of modern clinical practice. At a time when healthcare professionals are in short supply and the demand for low-cost treatments is ever-increasing, virtual health coaches (VHCs) offer intervention-on-demand for those limited by finances or geographic access to care. More recently, AI-powered virtual coaches have become a viable complement to human coaches. However, the push for AI-powered coaching systems raises several important issues for researchers, designers, clinicians, and patients. In this paper, we present a novel framework to guide the design and development of virtual coaching systems. This framework augments a traditional data science pipeline with four key guiding goals: reliability, fairness, engagement, and ethics.        △ Less","12 October, 2020",cs.AI,10.1145/3421937.3421971 
              Fully Automatic Wound Segmentation with Deep Convolutional Neural Networks          ,2010.05855,https://arxiv.org/abs/2010.05855,https://arxiv.org/pdf/2010.05855,"Authors:ChuanboWang,DMAnisuzzaman,VictorWilliamson,MrinalKantiDhar,BehrouzRostami,JeffreyNiezgoda,SandeepGopalakrishnan,ZeyunYu","        Acute and chronic wounds have varying etiologies and are an economic burden to healthcare systems around the world. The advanced wound care market is expected to exceed $22 billion by 2024. Wound care professionals rely heavily on images and image documentation for proper diagnosis and treatment. Unfortunately lack of expertise can lead to improper diagnosis of wound etiology and inaccurate wound management and documentation. Fully automatic segmentation of wound areas in natural images is an important part of the diagnosis and care protocol since it is crucial to measure the area of the wound and provide quantitative parameters in the treatment. Various deep learning models have gained success in image analysis including semantic segmentation. Particularly, MobileNetV2 stands out among others due to its lightweight architecture and uncompromised performance. This manuscript proposes a novel convolutional framework based on MobileNetV2 and connected component labelling to segment wound regions from natural images. We build an annotated wound image dataset consisting of 1,109 foot ulcer images from 889 patients to train and test the deep learning models. We demonstrate the effectiveness and mobility of our method by conducting comprehensive experiments and analyses on various segmentation neural networks.        △ Less","12 October, 2020",cs.CV,
              Inferring Causal Direction from Observational Data: A Complexity Approach          ,2010.05635,https://arxiv.org/abs/2010.05635,https://arxiv.org/pdf/2010.05635,"Authors:NikolaosNikolaou,KonstantinosSechidis","        At the heart of causal structure learning from observational data lies a deceivingly simple question: given two statistically dependent random variables, which one has a causal effect on the other? This is impossible to answer using statistical dependence testing alone and requires that we make additional assumptions. We propose several fast and simple criteria for distinguishing cause and effect in pairs of discrete or continuous random variables. The intuition behind them is that predicting the effect variable using the cause variable should be `simpler' than the reverse -- different notions of `simplicity' giving rise to different criteria. We demonstrate the accuracy of the criteria on synthetic data generated under a broad family of causal mechanisms and types of noise.        △ Less","12 October, 2020","cs.LG,cs.AI,stat.ML",
              ComStreamClust: A communicative text clustering approach to topic detection in streaming data          ,2010.05349,https://arxiv.org/abs/2010.05349,https://arxiv.org/pdf/2010.05349,"Authors:AliNajafi,ArazGholipour-Shilabin,RahimDehkharghani,AliMohammadpur-Fard,MeysamAsgari-Chenaghlu","        Topic detection is the task of determining and tracking hot topics in social media. Twitter is arguably the most popular platform for people to share their ideas with others about different issues. One such prevalent issue is the COVID-19 pandemic. Detecting and tracking topics on these kinds of issues would help governments and healthcare companies deal with this phenomenon. In this paper, we propose a novel communicative clustering approach, so-called ComStreamClust for clustering sub-topics inside a broader topic, e.g. COVID-19. The proposed approach was evaluated on two datasets: the COVID-19 and the FA CUP. The results obtained from ComStreamClust approve the effectiveness of the proposed approach when compared to existing methods such as LDA.        △ Less","11 October, 2020","cs.IR,cs.CL",
              Telerobotic Operation of Intensive Care Unit Ventilators          ,2010.05247,https://arxiv.org/abs/2010.05247,https://arxiv.org/pdf/2010.05247,"Authors:BalazsP.Vagvolgyi,MikhailKhrenov,JonathanCope,AntonDeguet,PeterKazanzides,SajidManzoor,RussellH.Taylor,AxelKrieger","        Since the first reports of a novel coronavirus (SARS-CoV-2) in December 2019, over 33 million people have been infected worldwide and approximately 1 million people worldwide have died from the disease caused by this virus, COVID-19. In the US alone, there have been approximately 7 million cases and over 200,000 deaths. This outbreak has placed an enormous strain on healthcare systems and workers. Severe cases require hospital care, and 8.5\% of patients require mechanical ventilation in an intensive care unit (ICU). One major challenge is the necessity for clinical care personnel to don and doff cumbersome personal protective equipment (PPE) in order to enter an ICU unit to make simple adjustments to ventilator settings. Although future ventilators and other ICU equipment may be controllable remotely through computer networks, the enormous installed base of existing ventilators do not have this capability. This paper reports the development of a simple, low cost telerobotic system that permits adjustment of ventilator settings from outside the ICU. The system consists of a small Cartesian robot capable of operating a ventilator touch screen with camera vision control via a wirelessly connected tablet master device located outside the room. Engineering system tests demonstrated that the open-loop mechanical repeatability of the device was 7.5\,mm, and that the average positioning error of the robotic finger under visual servoing control was 5.94\,mm. Successful usability tests in a simulated ICU environment were carried out and are reported. In addition to enabling a significant reduction in PPE consumption, the prototype system has been shown in a preliminary evaluation to significantly reduce the total time required for a respiratory therapist to perform typical setting adjustments on a commercial ventilator, including donning and doffing PPE, from 271 seconds to 109 seconds.        △ Less","11 October, 2020",cs.RO,
              A General Model of Conversational Dynamics and an Example Application in Serious Illness Communication          ,2010.05164,https://arxiv.org/abs/2010.05164,https://arxiv.org/pdf/2010.05164,"Authors:LaurenceA.Clarfeld,RobertGramling,DonnaM.Rizzo,MargaretJ.Eppstein","        Conversation has been a primary means for the exchange of information since ancient times. Understanding patterns of information flow in conversations is a critical step in assessing and improving communication quality. In this paper, we describe COnversational DYnamics Model (CODYM) analysis, a novel approach for studying patterns of information flow in conversations. CODYMs are Markov Models that capture sequential dependencies in the lengths of speaker turns. The proposed method is automated and scalable, and preserves the privacy of the conversational participants. The primary function of CODYM analysis is to quantify and visualize patterns of information flow, concisely summarized over sequential turns from one or more conversations. Our approach is general and complements existing methods, providing a new tool for use in the analysis of any type of conversation. As an important first application, we demonstrate the model on transcribed conversations between palliative care clinicians and seriously ill patients. These conversations are dynamic and complex, taking place amidst heavy emotions, and include difficult topics such as end-of-life preferences and patient values. We perform a versatile set of CODYM analyses that (a) establish the validity of the model by confirming known patterns of conversational turn-taking and word usage, (b) identify normative patterns of information flow in serious illness conversations, and (c) show how these patterns vary across narrative time and differ under expressions of anger, fear and sadness. Potential applications of CODYMs range from assessment and training of effective healthcare communication to comparing conversational dynamics across language and culture, with the prospect of identifying universal similarities and unique ""fingerprints"" of information flow.        △ Less","11 October, 2020",cs.CL,
"              Nowcasting of COVID-19 confirmed cases: Foundations, trends, and challenges          ",2010.05079,https://arxiv.org/abs/2010.05079,https://arxiv.org/pdf/2010.05079,"Authors:TanujitChakraborty,IndrajitGhosh,TirnaMahajan,TejasviArora","        The coronavirus disease 2019 (COVID-19) has become a public health emergency of international concern affecting more than 200 countries and territories worldwide. As of September 30, 2020, it has caused a pandemic outbreak with more than 33 million confirmed infections and more than 1 million reported deaths worldwide. Several statistical, machine learning, and hybrid models have previously tried to forecast COVID-19 confirmed cases for profoundly affected countries. Due to extreme uncertainty and nonstationarity in the time series data, forecasting of COVID-19 confirmed cases has become a very challenging job. For univariate time series forecasting, there are various statistical and machine learning models available in the literature. But, epidemic forecasting has a dubious track record. Its failures became more prominent due to insufficient data input, flaws in modeling assumptions, high sensitivity of estimates, lack of incorporation of epidemiological features, inadequate past evidence on effects of available interventions, lack of transparency, errors, lack of determinacy, and lack of expertise in crucial disciplines. This chapter focuses on assessing different short-term forecasting models that can forecast the daily COVID-19 cases for various countries. In the form of an empirical study on forecasting accuracy, this chapter provides evidence to show that there is no universal method available that can accurately forecast pandemic data. Still, forecasters' predictions are useful for the effective allocation of healthcare resources and will act as an early-warning system for government policymakers.        △ Less","10 October, 2020","q-bio.PE,stat.AP",
              Towards Social HRI for Improving Children's Healthcare Experiences          ,2010.04652,https://arxiv.org/abs/2010.04652,https://arxiv.org/pdf/2010.04652,"Authors:MaryEllenFoster,RonaldP.A.Petrick","        This paper describes a new research project that aims to develop a social robot designed to help children cope with painful and distressing medical procedures in a clinical setting. While robots have previously been trialled for this task, with promising initial results, the systems have tended to be teleoperated, limiting their flexibility and robustness. This project will use epistemic planning techniques as a core component for action selection in the robot system, in order to generate plans that include physical, sensory, and social actions for interacting with humans. The robot will operate in a task environment where appropriate and safe interaction with children, parents/caregivers, and healthcare professionals is required. In addition to addressing the core technical challenge of building an autonomous social robot, the project will incorporate co-design techniques involving all participant groups, and the final robot system will be evaluated in a two-site clinical trial.        △ Less","9 October, 2020","cs.RO,cs.AI",
              Prognosis Prediction in Covid-19 Patients from Lab Tests and X-ray Data through Randomized Decision Trees          ,2010.04420,https://arxiv.org/abs/2010.04420,https://arxiv.org/pdf/2010.04420,"Authors:AlfonsoEmilioGerevini,RobertoMaroldi,MatteoOlivato,LucaPutelli,IvanSerina","        AI and Machine Learning can offer powerful tools to help in the fight against Covid-19. In this paper we present a study and a concrete tool based on machine learning to predict the prognosis of hospitalised patients with Covid-19. In particular we address the task of predicting the risk of death of a patient at different times of the hospitalisation, on the base of some demographic information, chest X-ray scores and several laboratory findings. Our machine learning models use ensembles of decision trees trained and tested using data from more than 2000 patients. An experimental evaluation of the models shows good performance in solving the addressed task.        △ Less","9 October, 2020",cs.LG,
              Cascaded WLAN-FWA Networking and Computing Architecture for Pervasive In-Home Healthcare,2010.03805,https://arxiv.org/abs/2010.03805,https://arxiv.org/pdf/2010.03805,"Authors:SergioMartiradonna,GiuliaCisotto,GennaroBoggia,GiuseppePiro,LorenzoVangelista,StefanoTomasin","        Pervasive healthcare is a promising assisted-living solution for chronic patients. However, current cutting-edge communication technologies are not able to strictly meet the requirements of these applications, especially in the case of life-threatening events. To bridge this gap, this paper proposes a new architecture to support indoor healthcare monitoring, with a focus on epileptic patients. Several novel elements are introduced. The first element is the cascading of a WLAN and a cellular network, where IEEE 802.11ax is used for the wireless local area network to collect physiological and environmental data in-home and 5G-enabled Fixed Wireless Access links transfer them to a remote hospital. The second element is the extension of the network slicing concept to the WLAN, and the introduction of two new slice types to support both regular monitoring and emergency handling. Moreover, the inclusion of local computing capabilities at the WLAN router, together with a mobile edge computing resource, represents a further architectural enhancement. Local computation is required to trigger not only health-related alarms, but also the network slicing change in case of emergency: in fact, proper radio resource scheduling is necessary for the cascaded networks to handle healthcare traffic together with other promiscuous everyday communication services. Numerical results demonstrate the effectiveness of the proposed approach while highlighting the performance gain achieved with respect to baseline solutions.        △ Less","8 October, 2020","cs.NI,cs.CY,cs.DC,eess.SP",
              Adversarial Attacks to Machine Learning-Based Smart Healthcare Systems          ,2010.03671,https://arxiv.org/abs/2010.03671,https://arxiv.org/pdf/2010.03671,"Authors:AKMIqtidarNewaz,NurImtiazulHaque,AmitKumarSikder,MohammadAshiqurRahman,A.SelcukUluagac","        The increasing availability of healthcare data requires accurate analysis of disease diagnosis, progression, and realtime monitoring to provide improved treatments to the patients. In this context, Machine Learning (ML) models are used to extract valuable features and insights from high-dimensional and heterogeneous healthcare data to detect different diseases and patient activities in a Smart Healthcare System (SHS). However, recent researches show that ML models used in different application domains are vulnerable to adversarial attacks. In this paper, we introduce a new type of adversarial attacks to exploit the ML classifiers used in a SHS. We consider an adversary who has partial knowledge of data distribution, SHS model, and ML algorithm to perform both targeted and untargeted attacks. Employing these adversarial capabilities, we manipulate medical device readings to alter patient status (disease-affected, normal condition, activities, etc.) in the outcome of the SHS. Our attack utilizes five different adversarial ML algorithms (HopSkipJump, Fast Gradient Method, Crafting Decision Tree, Carlini & Wagner, Zeroth Order Optimization) to perform different malicious activities (e.g., data poisoning, misclassify outputs, etc.) on a SHS. Moreover, based on the training and testing phase capabilities of an adversary, we perform white box and black box attacks on a SHS. We evaluate the performance of our work in different SHS settings and medical devices. Our extensive evaluation shows that our proposed adversarial attack can significantly degrade the performance of a ML-based SHS in detecting diseases and normal activities of the patients correctly, which eventually leads to erroneous treatment.        △ Less","7 October, 2020","cs.LG,cs.CR",
              Infant-ID: Fingerprints for Global Good          ,2010.03624,https://arxiv.org/abs/2010.03624,https://arxiv.org/pdf/2010.03624,"Authors:JoshuaJ.Engelsma,DebayanDeb,KaiCao,AnjooBhatnagar,PremS.Sudhish,AnilK.Jain","        In many of the least developed and developing countries, a multitude of infants continue to suffer and die from vaccine-preventable diseases and malnutrition. Lamentably, the lack of official identification documentation makes it exceedingly difficult to track which infants have been vaccinated and which infants have received nutritional supplements. Answering these questions could prevent this infant suffering and premature death around the world. To that end, we propose Infant-Prints, an end-to-end, low-cost, infant fingerprint recognition system. Infant-Prints is comprised of our (i) custom built, compact, low-cost (85 USD), high-resolution (1,900 ppi), ergonomic fingerprint reader, and (ii) high-resolution infant fingerprint matcher. To evaluate the efficacy of Infant-Prints, we collected a longitudinal infant fingerprint database captured in 4 different sessions over a 12-month time span (December 2018 to January 2020), from 315 infants at the Saran Ashram Hospital, a charitable hospital in Dayalbagh, Agra, India. Our experimental results demonstrate, for the first time, that Infant-Prints can deliver accurate and reliable recognition (over time) of infants enrolled between the ages of 2-3 months, in time for effective delivery of vaccinations, healthcare, and nutritional supplements (TAR=95.2% @ FAR = 1.0% for infants aged 8-16 weeks at enrollment and authenticated 3 months later).        △ Less","7 October, 2020",cs.CV,
              Automated Human Activity Recognition by Colliding Bodies Optimization-based Optimal Feature Selection with Recurrent Neural Network          ,2010.03324,https://arxiv.org/abs/2010.03324,https://arxiv.org/pdf/2010.03324,"Authors:PankajKhatiwada,MatrikaSubedi,AyanChatterjee,MartinWulfGerdes","        In smart healthcare, Human Activity Recognition (HAR) is considered to be an efficient model in pervasive computation from sensor readings. The Ambient Assisted Living (AAL) in the home or community helps the people in providing independent care and enhanced living quality. However, many AAL models were restricted using many factors that include computational cost and system complexity. Moreover, the HAR concept has more relevance because of its applications. Hence, this paper tempts to implement the HAR system using deep learning with the data collected from smart sensors that are publicly available in the UC Irvine Machine Learning Repository (UCI). The proposed model involves three processes: (1) Data collection, (b) Optimal feature selection, (c) Recognition. The data gathered from the benchmark repository is initially subjected to optimal feature selection that helps to select the most significant features. The proposed optimal feature selection is based on a new meta-heuristic algorithm called Colliding Bodies Optimization (CBO). An objective function derived by the recognition accuracy is used for accomplishing the optimal feature selection. Here, the deep learning model called Recurrent Neural Network (RNN) is used for activity recognition. The proposed model on the concerned benchmark dataset outperforms existing learning methods, providing high performance compared to the conventional models.        △ Less","7 October, 2020","cs.LG,eess.SP",
"              HIV-prevalence mapping using Small Area Estimation in Kenya, Tanzania, and Mozambique at the first sub-national level          ",2010.03114,https://arxiv.org/abs/2010.03114,https://arxiv.org/pdf/2010.03114,Authors:EnriqueM.Saldarriaga,"        Local estimates of HIV-prevalence provide information that can be used to target interventions and consequently increase the efficiency of the resources. This closer-to-optimal allocation can lead to better health outcomes, including the control of the disease spread, and for more people. Producing reliable estimates at smaller geographical levels can be challenging and careful consideration of the nature of the data and the epidemiologic rational is needed. In this paper, we use the DHS data phase V to estimate HIV prevalence at the first-subnational level in Kenya, Tanzania, and Mozambique. We fit the data to a spatial random effect intrinsic conditional autoregressive (ICAR) model to smooth the outcome. We also use a sampling specification from a multistage cluster design. We found that Nyanza (P=14.2%) and Nairobi (P=7.8%) in Kenya, Iringa (P=16.2%) and Dar es Salaam (P=10.1%) in Tanzania, and Gaza (P=13.7%) and Maputo City (P=12.7%) in Mozambique are the regions with the highest prevalence of HIV, within country. Our results are based on statistically rigorous methods that allowed us to obtain an accurate visual representation of the HIV prevalence in the subset of African countries we chose. These results can help in identification and targeting of high-prevalent regions to increase the supply of healthcare services to reduce the spread of the disease and increase the health quality of people living with HIV.        △ Less","6 October, 2020",stat.AP,
              Interpretable Sequence Classification via Discrete Optimization          ,2010.02819,https://arxiv.org/abs/2010.02819,https://arxiv.org/pdf/2010.02819,"Authors:MaayanShvo,AndrewC.Li,RodrigoToroIcarte,SheilaA.McIlraith","        Sequence classification is the task of predicting a class label given a sequence of observations. In many applications such as healthcare monitoring or intrusion detection, early classification is crucial to prompt intervention. In this work, we learn sequence classifiers that favour early classification from an evolving observation trace. While many state-of-the-art sequence classifiers are neural networks, and in particular LSTMs, our classifiers take the form of finite state automata and are learned via discrete optimization. Our automata-based classifiers are interpretable---supporting explanation, counterfactual reasoning, and human-in-the-loop modification---and have strong empirical performance. Experiments over a suite of goal recognition and behaviour classification datasets show our learned automata-based classifiers to have comparable test performance to LSTM-based classifiers, with the added advantage of being interpretable.        △ Less","6 October, 2020","cs.LG,cs.AI",
              Wound and episode level readmission risk or weeks to readmit: Why do patients get readmitted? How long does it take for a patient to get readmitted?          ,2010.02742,https://arxiv.org/abs/2010.02742,https://arxiv.org/pdf/2010.02742,"Authors:SubbaReddyOota,NafisurRahman,ShahidSaleemMohammed,JeffreyGalitz,MingLiu","        The Affordable care Act of 2010 had introduced Readmission reduction program in 2012 to reduce avoidable re-admissions to control rising healthcare costs. Wound care impacts 15 of medicare beneficiaries making it one of the major contributors of medicare health care cost. Health plans have been exploring proactive health care services that can focus on preventing wound recurrences and re-admissions to control the wound care costs. With rising costs of Wound care industry, it has become of paramount importance to reduce wound recurrences & patient re-admissions. What factors are responsible for a Wound to recur which ultimately lead to hospitalization or re-admission? Is there a way to identify the patients at risk of re-admission before the occurrence using data driven analysis? Patient re-admission risk management has become critical for patients suffering from chronic wounds such as diabetic ulcers, pressure ulcers, and vascular ulcers. Understanding the risk & the factors that cause patient readmission can help care providers and patients avoid wound recurrences. Our work focuses on identifying patients who are at high risk of re-admission & determining the time period with in which a patient might get re-admitted. Frequent re-admissions add financial stress to the patient & Health plan and deteriorate the quality of life of the patient. Having this information can allow a provider to set up preventive measures that can delay, if not prevent, patients' re-admission. On a combined wound & episode-level data set of patient's wound care information, our extended autoprognosis achieves a recall of 92 and a precision of 92 for the predicting a patient's re-admission risk. For new patient class, precision and recall are as high as 91 and 98, respectively. We are also able to predict the patient's discharge event for a re-admission event to occur through our model with a MAE of 2.3 weeks.        △ Less","5 October, 2020","cs.LG,stat.ML",
              Assessing Automated Machine Learning service to detect COVID-19 from X-Ray and CT images: A Real-time Smartphone Application case study          ,2010.02715,https://arxiv.org/abs/2010.02715,https://arxiv.org/pdf/2010.02715,"Authors:RazibMustafiz,KhaledMohsin","        The recent outbreak of SARS COV-2 gave us a unique opportunity to study for a non interventional and sustainable AI solution. Lung disease remains a major healthcare challenge with high morbidity and mortality worldwide. The predominant lung disease was lung cancer. Until recently, the world has witnessed the global pandemic of COVID19, the Novel coronavirus outbreak. We have experienced how viral infection of lung and heart claimed thousands of lives worldwide. With the unprecedented advancement of Artificial Intelligence in recent years, Machine learning can be used to easily detect and classify medical imagery. It is much faster and most of the time more accurate than human radiologists. Once implemented, it is more cost-effective and time-saving. In our study, we evaluated the efficacy of Microsoft Cognitive Service to detect and classify COVID19 induced pneumonia from other Viral/Bacterial pneumonia based on X-Ray and CT images. We wanted to assess the implication and accuracy of the Automated ML-based Rapid Application Development (RAD) environment in the field of Medical Image diagnosis. This study will better equip us to respond with an ML-based diagnostic Decision Support System(DSS) for a Pandemic situation like COVID19. After optimization, the trained network achieved 96.8% Average Precision which was implemented as a Web Application for consumption. However, the same trained network did not perform the same like Web Application when ported to Smartphone for Real-time inference. Which was our main interest of study. The authors believe, there is scope for further study on this issue. One of the main goal of this study was to develop and evaluate the performance of AI-powered Smartphone-based Real-time Application. Facilitating primary diagnostic services in less equipped and understaffed rural healthcare centers of the world with unreliable internet service.        △ Less","3 October, 2020","eess.IV,cs.CV",10.20944/preprints202009.0647.v1 
              An Ensemble Approach for Automatic Structuring of Radiology Reports          ,2010.02256,https://arxiv.org/abs/2010.02256,https://arxiv.org/pdf/2010.02256,"Authors:MortezaPourrezaShahri,AmirTahmasebi,BingyangYe,HenghuiZhu,JavedAslam,TimothyFerris","        Automatic structuring of electronic medical records is of high demand for clinical workflow solutions to facilitate extraction, storage, and querying of patient care information. However, developing a scalable solution is extremely challenging, specifically for radiology reports, as most healthcare institutes use either no template or department/institute specific templates. Moreover, radiologists' reporting style varies from one to another as sentences are telegraphic and do not follow general English grammar rules. We present an ensemble method that consolidates the predictions of three models, capturing various attributes of textual information for automatic labeling of sentences with section labels. These three models are: 1) Focus Sentence model, capturing context of the target sentence; 2) Surrounding Context model, capturing the neighboring context of the target sentence; and finally, 3) Formatting/Layout model, aimed at learning report formatting cues. We utilize Bi-directional LSTMs, followed by sentence encoders, to acquire the context. Furthermore, we define several features that incorporate the structure of reports. We compare our proposed approach against multiple baselines and state-of-the-art approaches on a proprietary dataset as well as 100 manually annotated radiology notes from the MIMIC-III dataset, which we are making publicly available. Our proposed approach significantly outperforms other approaches by achieving 97.1% accuracy.        △ Less","10 October, 2020","cs.CL,cs.LG",
              AdaLead: A simple and robust adaptive greedy search algorithm for sequence design          ,2010.02141,https://arxiv.org/abs/2010.02141,https://arxiv.org/pdf/2010.02141,"Authors:SamSinai,RichardWang,AlexanderWhatley,StewartSlocum,ElinaLocane,EricD.Kelsic","        Efficient design of biological sequences will have a great impact across many industrial and healthcare domains. However, discovering improved sequences requires solving a difficult optimization problem. Traditionally, this challenge was approached by biologists through a model-free method known as ""directed evolution"", the iterative process of random mutation and selection. As the ability to build models that capture the sequence-to-function map improves, such models can be used as oracles to screen sequences before running experiments. In recent years, interest in better algorithms that effectively use such oracles to outperform model-free approaches has intensified. These span from approaches based on Bayesian Optimization, to regularized generative models and adaptations of reinforcement learning. In this work, we implement an open-source Fitness Landscape EXploration Sandbox (FLEXS: github.com/samsinai/FLEXS) environment to test and evaluate these algorithms based on their optimality, consistency, and robustness. Using FLEXS, we develop an easy-to-implement, scalable, and robust evolutionary greedy algorithm (AdaLead). Despite its simplicity, we show that AdaLead is a remarkably strong benchmark that out-competes more complex state of the art approaches in a variety of biologically motivated sequence design challenges.        △ Less","5 October, 2020","cs.LG,math.OC,q-bio.BM,q-bio.QM",
              Explaining Deep Neural Networks          ,2010.01496,https://arxiv.org/abs/2010.01496,https://arxiv.org/pdf/2010.01496,Authors:Oana-MariaCamburu,"        Deep neural networks are becoming more and more popular due to their revolutionary success in diverse areas, such as computer vision, natural language processing, and speech recognition. However, the decision-making processes of these models are generally not interpretable to users. In various domains, such as healthcare, finance, or law, it is critical to know the reasons behind a decision made by an artificial intelligence system. Therefore, several directions for explaining neural models have recently been explored.  In this thesis, I investigate two major directions for explaining deep neural networks. The first direction consists of feature-based post-hoc explanatory methods, that is, methods that aim to explain an already trained and fixed model (post-hoc), and that provide explanations in terms of input features, such as tokens for text and superpixels for images (feature-based). The second direction consists of self-explanatory neural models that generate natural language explanations, that is, models that have a built-in module that generates explanations for the predictions of the model.        △ Less","4 October, 2020","cs.CL,cs.AI",
              Explanation Ontology: A Model of Explanations for User-Centered AI          ,2010.01479,https://arxiv.org/abs/2010.01479,https://arxiv.org/pdf/2010.01479,"Authors:ShruthiChari,OshaniSeneviratne,DanielM.Gruen,MorganA.Foreman,AmarK.Das,DeborahL.McGuinness","        Explainability has been a goal for Artificial Intelligence (AI) systems since their conception, with the need for explainability growing as more complex AI models are increasingly used in critical, high-stakes settings such as healthcare. Explanations have often added to an AI system in a non-principled, post-hoc manner. With greater adoption of these systems and emphasis on user-centric explainability, there is a need for a structured representation that treats explainability as a primary consideration, mapping end user needs to specific explanation types and the system's AI capabilities. We design an explanation ontology to model both the role of explanations, accounting for the system and user attributes in the process, and the range of different literature-derived explanation types. We indicate how the ontology can support user requirements for explanations in the domain of healthcare. We evaluate our ontology with a set of competency questions geared towards a system designer who might use our ontology to decide which explanation types to include, given a combination of users' needs and a system's capabilities, both in system design settings and in real-time operations. Through the use of this ontology, system designers will be able to make informed choices on which explanations AI systems can and should provide.        △ Less","3 October, 2020","cs.AI,cs.HC,cs.LG",
              Uncertainty-Aware Multi-Modal Ensembling for Severity Prediction of Alzheimer's Dementia          ,2010.01440,https://arxiv.org/abs/2010.01440,https://arxiv.org/pdf/2010.01440,"Authors:UtkarshSarawgi,WazeerZulfikar,RishabKhincha,PattieMaes","        Reliability in Neural Networks (NNs) is crucial in safety-critical applications like healthcare, and uncertainty estimation is a widely researched method to highlight the confidence of NNs in deployment. In this work, we propose an uncertainty-aware boosting technique for multi-modal ensembling to predict Alzheimer's Dementia Severity. The propagation of uncertainty across acoustic, cognitive, and linguistic features produces an ensemble system robust to heteroscedasticity in the data. Weighing the different modalities based on the uncertainty estimates, we experiment on the benchmark ADReSS dataset, a subject-independent and balanced dataset, to show that our method outperforms the state-of-the-art methods while also reducing the overall entropy of the system. This work aims to encourage fair and aware models. The source code is available at https://github.com/wazeerzulfikar/alzheimers-dementia        △ Less","3 October, 2020","cs.LG,cs.SD,eess.AS,q-bio.QM",
              A Pattern Sequence for Designing Blockchain-Based Healthcare Information Technology Systems          ,2010.01172,https://arxiv.org/abs/2010.01172,https://arxiv.org/pdf/2010.01172,"Authors:PengZhang,DouglasC.Schmidt,JulesWhite","        Known for its decentralized and tamper-aware properties, blockchain is attractive to enhance the infrastructure of systems that have been constrained by traditionally centralized and vendor-locked environments. Although blockchain has commonly been used as the operational model behind cryptocurrency, it has far more foreseeable utilities in domains like healthcare, where efficient data flow is highly demanded. Particularly, blockchain and related technologies have been touted as foundational technologies for addressing healthcare interoperability challenges, such as promoting effective communications and securing data exchanges across various healthcare systems. Despite the increasing interests in leveraging blockchain technology to improve healthcare infrastructures, a major gap in literature is the lack of available recommendations for concrete architectural styles and design considerations for creating blockchain-based apps and systems with a healthcare focus. This research provides two contributions to bridge the gap in existing research. First, we introduce a pattern sequence for designing blockchain-based healthcare systems focused on secure and at-scale data exchange. Our approach adapts traditional software patterns and proposes novel patterns that take into account both the technical requirements specific to healthcare systems and the implications of these requirements on naive blockchain-based solutions. Second, we provide a pattern-oriented reference architecture using an example application of the pattern sequence for guiding software developers to design interoperable (on the technical level) healthcare IT systems atop blockchain-based infrastructures. The reference architecture focuses on minimizing storage requirements on-chain, preserving the privacy of sensitive information, facilitating scalable communications, and maximizing evolvability of the system.        △ Less","2 October, 2020",cs.CR,
              Evaluating Progress on Machine Learning for Longitudinal Electronic Healthcare Data          ,2010.01149,https://arxiv.org/abs/2010.01149,https://arxiv.org/pdf/2010.01149,"Authors:DavidBellamy,LeoCeli,AndrewL.Beam","        The Large Scale Visual Recognition Challenge based on the well-known Imagenet dataset catalyzed an intense flurry of progress in computer vision. Benchmark tasks have propelled other sub-fields of machine learning forward at an equally impressive pace, but in healthcare it has primarily been image processing tasks, such as in dermatology and radiology, that have experienced similar benchmark-driven progress. In the present study, we performed a comprehensive review of benchmarks in medical machine learning for structured data, identifying one based on the Medical Information Mart for Intensive Care (MIMIC-III) that allows the first direct comparison of predictive performance and thus the evaluation of progress on four clinical prediction tasks: mortality, length of stay, phenotyping, and patient decompensation. We find that little meaningful progress has been made over a 3 year period on these tasks, despite significant community engagement. Through our meta-analysis, we find that the performance of deep recurrent models is only superior to logistic regression on certain tasks. We conclude with a synthesis of these results, possible explanations, and a list of desirable qualities for future benchmarks in medical machine learning.        △ Less","2 October, 2020","cs.LG,stat.ML",
              Cardea: An Open Automated Machine Learning Framework for Electronic Health Records          ,2010.00509,https://arxiv.org/abs/2010.00509,https://arxiv.org/pdf/2010.00509,"Authors:SarahAlnegheimish,NajatAlrashed,FaisalAleissa,ShahadAlthobaiti,DongyuLiu,MansourAlsaleh,KalyanVeeramachaneni","        An estimated 180 papers focusing on deep learning and EHR were published between 2010 and 2018. Despite the common workflow structure appearing in these publications, no trusted and verified software framework exists, forcing researchers to arduously repeat previous work. In this paper, we propose Cardea, an extensible open-source automated machine learning framework encapsulating common prediction problems in the health domain and allows users to build predictive models with their own data. This system relies on two components: Fast Healthcare Interoperability Resources (FHIR) -- a standardized data structure for electronic health systems -- and several AUTOML frameworks for automated feature engineering, model selection, and tuning. We augment these components with an adaptive data assembler and comprehensive data- and model- auditing capabilities. We demonstrate our framework via 5 prediction tasks on MIMIC-III and Kaggle datasets, which highlight Cardea's human competitiveness, flexibility in problem definition, extensive feature generation capability, adaptable automatic data assembler, and its usability.        △ Less","1 October, 2020","cs.LG,stat.ML",
              Developing Effective Community Network Analysis Tools According to Visualization Psychology          ,2010.00488,https://arxiv.org/abs/2010.00488,https://arxiv.org/pdf/2010.00488,"Authors:DarrenJ.Edwards,MinChen","        Visualization is a useful technology in health science, and especially for community network analysis. Because visualization applications in healthcare are typically risk-averse, health psychologists can play a significant role in ensuring appropriate and effective uses of visualization techniques in healthcare. In this paper, we examine the role of health psychologists in the triangle of ""health science"", ""visualization technology"", and ""visualization psychology"". We conclude that health psychologists can use visualization to aid data intelligence workflows in healthcare and health psychology, while researching into visualization psychology to aid the improvement and optimization of data visualization processes.        △ Less","1 October, 2020","cs.HC,cs.SI",
              Physical Exercise Recommendation and Success Prediction Using Interconnected Recurrent Neural Networks          ,2010.00482,https://arxiv.org/abs/2010.00482,https://arxiv.org/pdf/2010.00482,"Authors:ArashMahyari,PeterPirolli","        Unhealthy behaviors, e.g., physical inactivity and unhealthful food choice, are the primary healthcare cost drivers in developed countries. Pervasive computational, sensing, and communication technology provided by smartphones and smartwatches have made it possible to support individuals in their everyday lives to develop healthier lifestyles. In this paper, we propose an exercise recommendation system that also predicts individual success rates . The system, consisting of two inter-connected recurrent neural networks (RNNs), uses the history of workouts to recommend the next workout activity for each individual. The system then predicts the probability of successful completion of the predicted activity by the individual. The prediction accuracy of this interconnected-RNN model is assessed on previously published data from a four-week mobile health experiment and is shown to improve upon previous predictions from a computational cognitive model.        △ Less","1 October, 2020","cs.LG,cs.AI,cs.CV,cs.IR,cs.IT,stat.ML",
              COVID-CT-MD: COVID-19 Computed Tomography (CT) Scan Dataset Applicable in Machine Learning and Deep Learning          ,2009.14623,https://arxiv.org/abs/2009.14623,https://arxiv.org/pdf/2009.14623,"Authors:ParnianAfshar,ShahinHeidarian,NastaranEnshaei,FarnooshNaderkhani,MoezedinJavadRafiee,AnastasiaOikonomou,FaranakBabakiFard,KavehSamimi,KonstantinosN.Plataniotis,ArashMohammadi","        Novel Coronavirus (COVID-19) has drastically overwhelmed more than 200 countries affecting millions and claiming almost 1 million lives, since its emergence in late 2019. This highly contagious disease can easily spread, and if not controlled in a timely fashion, can rapidly incapacitate healthcare systems. The current standard diagnosis method, the Reverse Transcription Polymerase Chain Reaction (RT- PCR), is time consuming, and subject to low sensitivity. Chest Radiograph (CXR), the first imaging modality to be used, is readily available and gives immediate results. However, it has notoriously lower sensitivity than Computed Tomography (CT), which can be used efficiently to complement other diagnostic methods. This paper introduces a new COVID-19 CT scan dataset, referred to as COVID-CT-MD, consisting of not only COVID-19 cases, but also healthy and subjects infected by Community Acquired Pneumonia (CAP). COVID-CT-MD dataset, which is accompanied with lobe-level, slice-level and patient-level labels, has the potential to facilitate the COVID-19 research, in particular COVID-CT-MD can assist in development of advanced Machine Learning (ML) and Deep Neural Network (DNN) based solutions.        △ Less","28 September, 2020","eess.IV,cs.CV,cs.LG",
              Network memory in the movement of hospital patients carrying drug-resistant bacteria          ,2009.14480,https://arxiv.org/abs/2009.14480,https://arxiv.org/pdf/2009.14480,"Authors:AshleighC.Myall,RobertL.Peach,AndreaY.Weiße,FrancesDavies,SiddharthMookerjee,AlisonHolmes,MauricioBarahona","        Hospitals constitute highly interconnected systems that bring into contact an abundance of infectious pathogens and susceptible individuals, thus making infection outbreaks both common and challenging. In recent years, there has been a sharp incidence of antimicrobial-resistance amongst healthcare-associated infections, a situation now considered endemic in many countries. Here we present network-based analyses of a data set capturing the movement of patients harbouring drug-resistant bacteria across three large London hospitals. We show that there are substantial memory effects in the movement of hospital patients colonised with drug-resistant bacteria. Such memory effects break first-order Markovian transitive assumptions and substantially alter the conclusions from the analysis, specifically on node rankings and the evolution of diffusive processes. We capture variable length memory effects by constructing a lumped-state memory network, which we then use to identify overlapping communities of wards. We find that these communities of wards display a quasi-hierarchical structure at different levels of granularity which is consistent with different aspects of patient flows related to hospital locations and medical specialties.        △ Less","7 October, 2020",physics.soc-ph,
              Artificial Intelligence in Surgery: Neural Networks and Deep Learning          ,2009.13411,https://arxiv.org/abs/2009.13411,https://arxiv.org/pdf/2009.13411,"Authors:DeepakAlapatt,PietroMascagni,VinkleSrivastav,NicolasPadoy","        Deep neural networks power most recent successes of artificial intelligence, spanning from self-driving cars to computer aided diagnosis in radiology and pathology. The high-stake data intensive process of surgery could highly benefit from such computational methods. However, surgeons and computer scientists should partner to develop and assess deep learning applications of value to patients and healthcare systems. This chapter and the accompanying hands-on material were designed for surgeons willing to understand the intuitions behind neural networks, become familiar with deep learning concepts and tasks, grasp what implementing a deep learning model in surgery means, and finally appreciate the specific challenges and limitations of deep neural networks in surgery. For the associated hands-on material, please see https://github.com/CAMMA-public/ai4surgery.        △ Less","28 September, 2020",cs.NE,
              BiteNet: Bidirectional Temporal Encoder Network to Predict Medical Outcomes          ,2009.13252,https://arxiv.org/abs/2009.13252,https://arxiv.org/pdf/2009.13252,"Authors:XuepingPeng,GuodongLong,TaoShen,SenWang,JingJiang,ChengqiZhang","        Electronic health records (EHRs) are longitudinal records of a patient's interactions with healthcare systems. A patient's EHR data is organized as a three-level hierarchy from top to bottom: patient journey - all the experiences of diagnoses and treatments over a period of time; individual visit - a set of medical codes in a particular visit; and medical code - a specific record in the form of medical codes. As EHRs begin to amass in millions, the potential benefits, which these data might hold for medical research and medical outcome prediction, are staggering - including, for example, predicting future admissions to hospitals, diagnosing illnesses or determining the efficacy of medical treatments. Each of these analytics tasks requires a domain knowledge extraction method to transform the hierarchical patient journey into a vector representation for further prediction procedure. The representations should embed a sequence of visits and a set of medical codes with a specific timestamp, which are crucial to any downstream prediction tasks. Hence, expressively powerful representations are appealing to boost learning performance. To this end, we propose a novel self-attention mechanism that captures the contextual dependency and temporal relationships within a patient's healthcare journey. An end-to-end bidirectional temporal encoder network (BiteNet) then learns representations of the patient's journeys, based solely on the proposed attention mechanism. We have evaluated the effectiveness of our methods on two supervised prediction and two unsupervised clustering tasks with a real-world EHR dataset. The empirical results demonstrate the proposed BiteNet model produces higher-quality representations than state-of-the-art baseline methods.        △ Less","23 September, 2020","cs.LG,cs.AI,cs.CL",
              GDPR Compliance for Blockchain Applications in Healthcare,2009.12913,https://arxiv.org/abs/2009.12913,https://arxiv.org/pdf/2009.12913,"Authors:AntonHasselgren,PaulKengfaiWan,MargarethHorn,KatinaKralevska,DaniloGligoroski,ArildFaxvaag","        The transparent and decentralized characteristics associated with blockchain can be both appealing and problematic when applied to a healthcare use-case. As health data is highly sensitive, it is also highly regulated to ensure the privacy of patients. At the same time, access to health data and interoperability is in high demand. Regulatory frameworks such as GDPR and HIPAA are, amongst other objectives, meant to contribute to mitigating the risk of privacy violations in health data. Blockchain features can likely improve interoperability and access control to health data, and at the same time, preserve or even increase, the privacy of patients. Blockchain applications should address compliance with the current regulatory framework to increase real-world feasibility. This exploratory work indicates that published proof-of-concepts in the health domain comply with GDRP, to an extent. Blockchain developers need to make design choices to be compliant with GDPR since currently, none available blockchain platform can show compliance out of the box.        △ Less","27 September, 2020","cs.CR,cs.CY,cs.SI",
              Self-Organizing Software Models for the Internet of Things          ,2009.12844,https://arxiv.org/abs/2009.12844,https://arxiv.org/pdf/2009.12844,Authors:DamianArellanes,"        The Internet of Things (IoT) envisions the integration of physical objects into software systems for automating crucial aspects of our lives, such as healthcare, security, agriculture, and city management. Although the vision is promising, with the rapid advancement of hardware and communication technologies, IoT systems are becoming increasingly dynamic, large, and complex to the extent that manual management becomes infeasible. Thus, it is of paramount importance to provide software engineering foundations for constructing autonomic IoT systems. In this paper, I introduce a novel paradigm referred to as self-organizing software models in which IoT software systems are not explicitly programmed, but emerge in a decentralized manner during system operation, with minimal or without human intervention. I particularly present an overview of those models by including their definition, motivation, research challenges, and potential directions.        △ Less","27 September, 2020",cs.SE,
              Multi-task Causal Learning with Gaussian Processes          ,2009.12821,https://arxiv.org/abs/2009.12821,https://arxiv.org/pdf/2009.12821,"Authors:VirginiaAglietti,TheodorosDamoulas,MauricioÁlvarez,JavierGonzález","        This paper studies the problem of learning the correlation structure of a set of intervention functions defined on the directed acyclic graph (DAG) of a causal model. This is useful when we are interested in jointly learning the causal effects of interventions on different subsets of variables in a DAG, which is common in field such as healthcare or operations research. We propose the first multi-task causal Gaussian process (GP) model, which we call DAG-GP, that allows for information sharing across continuous interventions and across experiments on different variables. DAG-GP accommodates different assumptions in terms of data availability and captures the correlation between functions lying in input spaces of different dimensionality via a well-defined integral operator. We give theoretical results detailing when and how the DAG-GP model can be formulated depending on the DAG. We test both the quality of its predictions and its calibrated uncertainties. Compared to single-task models, DAG-GP achieves the best fitting performance in a variety of real and synthetic settings. In addition, it helps to select optimal interventions faster than competing approaches when used within sequential decision making frameworks, like active learning or Bayesian optimization.        △ Less","27 September, 2020","stat.ML,cs.LG",
              Democratizing Artificial Intelligence in Healthcare: A Study of Model Development Across Two Institutions Incorporating Transfer Learning          ,2009.12437,https://arxiv.org/abs/2009.12437,https://arxiv.org/pdf/2009.12437,"Authors:VikashGupta1,HolgerRoth,VarunBuch3,MarcioA.B.C.Rockenbach,RichardDWhite,DongYang,OlgaLaur,BrianGhoshhajra,IttaiDayan,DaguangXu,MonaG.Flores,BarbarosSelnurErdal","        The training of deep learning models typically requires extensive data, which are not readily available as large well-curated medical-image datasets for development of artificial intelligence (AI) models applied in Radiology. Recognizing the potential for transfer learning (TL) to allow a fully trained model from one institution to be fine-tuned by another institution using a much small local dataset, this report describes the challenges, methodology, and benefits of TL within the context of developing an AI model for a basic use-case, segmentation of Left Ventricular Myocardium (LVM) on images from 4-dimensional coronary computed tomography angiography. Ultimately, our results from comparisons of LVM segmentation predicted by a model locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance. This process reduces the time required to build a new model in the clinical environment at a different institution.        △ Less","25 September, 2020","eess.IV,cs.CV",
              Why have a Unified Predictive Uncertainty? Disentangling it using Deep Split Ensembles          ,2009.12406,https://arxiv.org/abs/2009.12406,https://arxiv.org/pdf/2009.12406,"Authors:UtkarshSarawgi,WazeerZulfikar,RishabKhincha,PattieMaes","        Understanding and quantifying uncertainty in black box Neural Networks (NNs) is critical when deployed in real-world settings such as healthcare. Recent works using Bayesian and non-Bayesian methods have shown how a unified predictive uncertainty can be modelled for NNs. Decomposing this uncertainty to disentangle the granular sources of heteroscedasticity in data provides rich information about its underlying causes. We propose a conceptually simple non-Bayesian approach, deep split ensemble, to disentangle the predictive uncertainties using a multivariate Gaussian mixture model. The NNs are trained with clusters of input features, for uncertainty estimates per cluster. We evaluate our approach on a series of benchmark regression datasets, while also comparing with unified uncertainty methods. Extensive analyses using dataset shits and empirical rule highlight our inherently well-calibrated models. Our work further demonstrates its applicability in a multi-modal setting using a benchmark Alzheimer's dataset and also shows how deep split ensembles can highlight hidden modality-specific biases. The minimal changes required to NNs and the training procedure, and the high flexibility to group features into clusters makes it readily deployable and useful. The source code is available at https://github.com/wazeerzulfikar/deep-split-ensembles        △ Less","25 September, 2020","cs.LG,stat.ML",
              Latent Causal Socioeconomic Health Index          ,2009.12217,https://arxiv.org/abs/2009.12217,https://arxiv.org/pdf/2009.12217,"Authors:F.SwenKuh,GraceS.Chiu,AntonH.Westveld","        This research develops a model-based LAtent Causal Socioeconomic Health (LACSH) index at the national level. We build upon the latent health factor index (LHFI) approach that has been used to assess the unobservable ecological/ecosystem health. This framework integratively models the relationship between metrics, the latent health, and the covariates that drive the notion of health. In this paper, the LHFI structure is integrated with spatial modeling and statistical causal modeling, so as to evaluate the impact of a continuous policy variable (mandatory maternity leave days and government's expenditure on healthcare, respectively) on a nation's socioeconomic health, while formally accounting for spatial dependency among the nations. A novel visualization technique for evaluating covariate balance is also introduced for the case of a continuous policy (treatment) variable. We apply our LACSH model to countries around the world using data on various metrics and potential covariates pertaining to different aspects of societal health. The approach is structured in a Bayesian hierarchical framework and results are obtained by Markov chain Monte Carlo techniques.        △ Less","24 September, 2020","stat.ME,econ.GN,stat.AP",
              COVID-19 Pandemic Prediction using Time Series Forecasting Models          ,2009.12176,https://arxiv.org/abs/2009.12176,https://arxiv.org/pdf/2009.12176,"Authors:NareshKumar,SebaSusan","        Millions of people have been infected and lakhs of people have lost their lives due to the worldwide ongoing novel Coronavirus (COVID-19) pandemic. It is of utmost importance to identify the future infected cases and the virus spread rate for advance preparation in the healthcare services to avoid deaths. Accurately forecasting the spread of COVID-19 is an analytical and challenging real-world problem to the research community. Therefore, we use day level information of COVID-19 spread for cumulative cases from whole world and 10 mostly affected countries; US, Spain, Italy, France, Germany, Russia, Iran, United Kingdom, Turkey, and India. We utilize the temporal data of coronavirus spread from January 22, 2020 to May 20, 2020. We model the evolution of the COVID-19 outbreak, and perform prediction using ARIMA and Prophet time series forecasting models. Effectiveness of the models are evaluated based on the mean absolute error, root mean square error, root relative squared error, and mean absolute percentage error. Our analysis can help in understanding the trends of the disease outbreak, and provide epidemiological stage information of adopted countries. Our investigations show that ARIMA model is more effective for forecasting COVID-19 prevalence. The forecasting results have potential to assist governments to plan policies to contain the spread of the virus.        △ Less","22 July, 2020",physics.soc-ph,
              Intelligent Risk Alarm for Asthma Patients using Artificial Neural Networks          ,2009.12175,https://arxiv.org/abs/2009.12175,https://arxiv.org/pdf/2009.12175,"Authors:RawabiA.Aroud,AnasH.Blasi,MohammedA.Alsuwaiket","        Asthma is a chronic disease of the airways of the lungs. It results in inflammation and narrowing of the respiratory passages, which prevents air flow into the airways and leads to frequent bouts of shortness of breath with wheezing accompanied by coughing and phlegm after exposure to inhalation of substances that provoke allergic reactions or irritation of the respiratory system. Data mining in healthcare system is very important in diagnosing and understanding data, so data mining aims to solve basic problems in diagnosing diseases due to the complexity of diagnosing asthma. Predicting chemicals in the atmosphere is very important and one of the most difficult problems since the last century. In this paper, the impact of chemicals on asthma patient will be presented and discussed. Sensor system called MQ5 will be used to examine the smoke and nitrogen content in the atmosphere. MQ5 will be inserted in a wristwatch that checks the smoke and nitrogen content in the patients place, the system shall issue a warning alarm if this gas affects the person with asthma. It will be based on the Artificial Neural Networks (ANN) algorithm that has been built using data that containing a set of chemicals such as carbon monoxide, NMHC (GT) acid gas, C6H6 (GT) Gasoline, NOx (GT) Nitrogen Oxide, and NO2 (GT) Nitrogen Dioxide. The temperature and humidity will be also used as they can negatively affect asthma patient. Finally, the rating model was evaluated and achieved 99.58% classification accuracy.        △ Less","24 September, 2020","eess.SP,cs.CY",10.14569/IJACSA.2020.0110612 
              Adversarial Examples in Deep Learning for Multivariate Time Series Regression          ,2009.11911,https://arxiv.org/abs/2009.11911,https://arxiv.org/pdf/2009.11911,"Authors:GautamRajMode,KhazaAnuarulHoque","        Multivariate time series (MTS) regression tasks are common in many real-world data mining applications including finance, cybersecurity, energy, healthcare, prognostics, and many others. Due to the tremendous success of deep learning (DL) algorithms in various domains including image recognition and computer vision, researchers started adopting these techniques for solving MTS data mining problems, many of which are targeted for safety-critical and cost-critical applications. Unfortunately, DL algorithms are known for their susceptibility to adversarial examples which also makes the DL regression models for MTS forecasting also vulnerable to those attacks. To the best of our knowledge, no previous work has explored the vulnerability of DL MTS regression models to adversarial time series examples, which is an important step, specifically when the forecasting from such models is used in safety-critical and cost-critical applications. In this work, we leverage existing adversarial attack generation techniques from the image classification domain and craft adversarial multivariate time series examples for three state-of-the-art deep learning regression models, specifically Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU). We evaluate our study using Google stock and household power consumption dataset. The obtained results show that all the evaluated DL regression models are vulnerable to adversarial attacks, transferable, and thus can lead to catastrophic consequences in safety-critical and cost-critical domains, such as energy and finance.        △ Less","24 September, 2020","cs.LG,stat.ML",
              Game theory to enhance stock management of personal protective equipment (PPE) during the COVID-19 outbreak          ,2009.11838,https://arxiv.org/abs/2009.11838,https://arxiv.org/pdf/2009.11838,"Authors:KhaledAbedrabboh,MatthiasPilz,ZaidAl-Fagih,OthmanS.Al-Fagih,Jean-ChristopheNebel,LuluwahAl-Fagih","        Since the outbreak of the COVID-19 pandemic, many healthcare facilities have suffered from shortages in medical resources, particularly in Personal Protective Equipment (PPE). In this paper, we propose a game-theoretic approach to schedule PPE orders among healthcare facilities. In this PPE game, each independent healthcare facility optimises its own storage utilisation in order to keep its PPE cost at a minimum. Such a model can reduce peak demand considerably when applied to a variable PPE consumption profile. Experiments conducted for NHS England regions using actual data confirm that the challenge of securing PPE supply during disasters such as COVID-19 can be eased if proper stock management procedures are adopted. These procedures can include early stockpiling, increasing storage capacities and implementing measures that can prolong the time period between successive infection waves, such as social distancing measures. Simulation results suggest that the provision of PPE dedicated storage space can be a viable solution to avoid straining PPE supply chains in case a second wave of COVID-19 infections occurs.        △ Less","25 September, 2020",cs.CY,
              Steering a Historical Disease Forecasting Model Under a Pandemic: Case of Flu and COVID-19          ,2009.11407,https://arxiv.org/abs/2009.11407,https://arxiv.org/pdf/2009.11407,"Authors:AlexanderRodriguez,NikhilMuralidhar,BijayaAdhikari,AnikaTabassum,NarenRamakrishnan,B.AdityaPrakash","        Forecasting influenza in a timely manner aids health organizations and policymakers in adequate preparation and decision making. However, effective influenza forecasting still remains a challenge despite increasing research interest. It is even more challenging amidst the COVID pandemic, when the influenza-like illness (ILI) counts is affected by various factors such as symptomatic similarities with COVID-19 and shift in healthcare seeking patterns of the general population. We term the ILI values observed when it is potentially affected as COVID-ILI. Under the current pandemic, historical influenza models carry valuable expertise about the disease dynamics but face difficulties adapting. Therefore, we propose CALI-NET, a neural transfer learning architecture which allows us to 'steer' a historical disease forecasting model to new scenarios where flu and COVID co-exist. Our framework enables this adaptation by automatically learning when it is should emphasize learning from COVID-related signals and when from the historical model. In such way, we exploit representations learned from historical ILI data as well as the limited COVID-related signals. Our experiments demonstrate that our approach is successful in adapting a historical forecasting model to the current pandemic. In addition, we show that success in our primary goal, adaptation, does not sacrifice overall performance as compared with state-of-the-art influenza forecasting approaches.        △ Less","23 September, 2020","cs.LG,stat.AP",
"              Novel Computational Linguistic Measures, Dialogue System and the Development of SOPHIE: Standardized Online Patient for Healthcare Interaction Education          ",2009.11247,https://arxiv.org/abs/2009.11247,https://arxiv.org/pdf/2009.11247,"Authors:MohammadRafayetAli,TaylanSen,BenjaminKane,ShagunBose,ThomasMCarroll,RonaldEpstein,LenhartSchubert,EhsanHoque","        In this paper, we describe the iterative participatory design of SOPHIE, an online virtual patient for feedback-based practice of sensitive patient-physician conversations, and discuss an initial qualitative evaluation of the system by professional end users. The design of SOPHIE was motivated from a computational linguistic analysis of the transcripts of 383 patient-physician conversations from an essential office visit of late stage cancer patients with their oncologists. We developed methods for the automatic detection of two behavioral paradigms, lecturing and positive language usage patterns (sentiment trajectory of conversation), that are shown to be significantly associated with patient prognosis understanding. These automated metrics associated with effective communication were incorporated into SOPHIE, and a pilot user study identified that SOPHIE was favorably reviewed by a user group of practicing physicians.        △ Less","23 September, 2020",cs.HC,
              Using Under-trained Deep Ensembles to Learn Under Extreme Label Noise          ,2009.11128,https://arxiv.org/abs/2009.11128,https://arxiv.org/pdf/2009.11128,"Authors:KonstantinosNikolaidis,ThomasPlagemann,SteinKristiansen,VeraGoebel,MohanKankanhalli","        Improper or erroneous labelling can pose a hindrance to reliable generalization for supervised learning. This can have negative consequences, especially for critical fields such as healthcare. We propose an effective new approach for learning under extreme label noise, based on under-trained deep ensembles. Each ensemble member is trained with a subset of the training data, to acquire a general overview of the decision boundary separation, without focusing on potentially erroneous details. The accumulated knowledge of the ensemble is combined to form new labels, that determine a better class separation than the original labels. A new model is trained with these labels to generalize reliably despite the label noise. We focus on a healthcare setting and extensively evaluate our approach on the task of sleep apnea detection. For comparison with related work, we additionally evaluate on the task of digit recognition. In our experiments, we observed performance improvement in accuracy from 6.7\% up-to 49.3\% for the task of digit classification and in kappa from 0.02 up-to 0.55 for the task of sleep apnea detection.        △ Less","23 September, 2020","cs.LG,stat.ML",
              Probabilistic Machine Learning for Healthcare,2009.11087,https://arxiv.org/abs/2009.11087,https://arxiv.org/pdf/2009.11087,"Authors:IreneY.Chen,ShalmaliJoshi,MarzyehGhassemi,RajeshRanganath","        Machine learning can be used to make sense of healthcare data. Probabilistic machine learning models help provide a complete picture of observed data in healthcare. In this review, we examine how probabilistic machine learning can advance healthcare. We consider challenges in the predictive model building pipeline where probabilistic models can be beneficial including calibration and missing data. Beyond predictive models, we also investigate the utility of probabilistic machine learning models in phenotyping, in generative models for clinical use cases, and in reinforcement learning.        △ Less","23 September, 2020","stat.ML,cs.CY,cs.LG",
              Accurate and Interpretable Machine Learning for Transparent Pricing of Health Insurance Plans          ,2009.10990,https://arxiv.org/abs/2009.10990,https://arxiv.org/pdf/2009.10990,"Authors:RohunKshirsagar,Li-YenHsu,CharlesH.Greenberg,MatthewMcClelland,AnushadeviMohan,WideetShende,NicolasP.Tilmans,MinGuo,AnkitChheda,MeredithTrotter,ShonketRay,MiguelAlvarado","        Health insurance companies cover half of the United States population through commercial employer-sponsored health plans and pay 1.2 trillion US dollars every year to cover medical expenses for their members. The actuary and underwriter roles at a health insurance company serve to assess which risks to take on and how to price those risks to ensure profitability of the organization. While Bayesian hierarchical models are the current standard in the industry to estimate risk, interest in machine learning as a way to improve upon these existing methods is increasing. Lumiata, a healthcare analytics company, ran a study with a large health insurance company in the United States. We evaluated the ability of machine learning models to predict the per member per month cost of employer groups in their next renewal period, especially those groups who will cost less than 95\% of what an actuarial model predicts (groups with ""concession opportunities""). We developed a sequence of two models, an individual patient-level and an employer-group-level model, to predict the annual per member per month allowed amount for employer groups, based on a population of 14 million patients. Our models performed 20\% better than the insurance carrier's existing pricing model, and identified 84\% of the concession opportunities. This study demonstrates the application of a machine learning system to compute an accurate and fair price for health insurance products and analyzes how explainable machine learning models can exceed actuarial models' predictive accuracy while maintaining interpretability.        △ Less","23 September, 2020","cs.CY,cs.LG,stat.ML",
              A Model-Driven Architecture Approach for Developing Healthcare ERP: Case study in Morocco          ,2009.10807,https://arxiv.org/abs/2009.10807,https://arxiv.org/pdf/2009.10807,"Authors:FatimaZahraYamani,MohamedElMerouani","        Nowadays, there are many problems in the Enterprise Resource Planning (ERP) implemented in the majority of hospitals in Morocco such as the difficulty of adaptation by the different users, the lack of several functionalities, errors that block the daily work, etc. All these problems require frequent modifications in the code, which implies a high effort to develop healthcare ERP as one of complex systems. In this paper, we are going to present a model-driven approach for developing healthcare ERP based on class diagram. First, we constitute the independent model using UML, define the transformation rules then apply them on our source model class to generate at the end an XML file that will be necessary for the ERP code. Our approach will not only resolve the above problems, but also improve the efficiency of software development through the automatically generated code.        △ Less","22 September, 2020",cs.SE,10.5281/zenodo.3987098 
              Public Health Informatics: Proposing Causal Sequence of Death Using Neural Machine Translation          ,2009.10318,https://arxiv.org/abs/2009.10318,https://arxiv.org/pdf/2009.10318,"Authors:YuandaZhu,YingSha,HangWu,MaiLi,RyanA.Hoffman,MayD.Wang","        Each year there are nearly 57 million deaths around the world, with over 2.7 million in the United States. Timely, accurate and complete death reporting is critical in public health, as institutions and government agencies rely on death reports to analyze vital statistics and to formulate responses to communicable diseases. Inaccurate death reporting may result in potential misdirection of public health policies. Determining the causes of death is, nevertheless, challenging even for experienced physicians. To facilitate physicians in accurately reporting causes of death, we present an advanced AI approach to determine a chronically ordered sequence of clinical conditions that lead to death, based on decedent's last hospital admission discharge record. The sequence of clinical codes on the death report is named as causal chain of death, coded in the tenth revision of International Statistical Classification of Diseases (ICD-10); the priority-ordered clinical conditions on the discharge record are coded in ICD-9. We identify three challenges in proposing the causal chain of death: two versions of coding system in clinical codes, medical domain knowledge conflict, and data interoperability. To overcome the first challenge in this sequence-to-sequence problem, we apply neural machine translation models to generate target sequence. We evaluate the quality of generated sequences with the BLEU (BiLingual Evaluation Understudy) score and achieve 16.44 out of 100. To address the second challenge, we incorporate expert-verified medical domain knowledge as constraint in generating output sequence to exclude infeasible causal chains. Lastly, we demonstrate the usability of our work in a Fast Healthcare Interoperability Resources (FHIR) interface to address the third challenge.        △ Less","22 September, 2020","cs.LG,stat.ML",
              Detailed Review of Cloud based Mobile application for the stroke patient          ,2009.09837,https://arxiv.org/abs/2009.09837,https://arxiv.org/pdf/2009.09837,Authors:BalagopalRamdurai,"        In the current years, due to the significant developments in technologies in almost every domain, the standard of living has been improved. Emergence of latest innovations, advanced machinery and equipment especially in the healthcare domain, have simplified the diagonalizing process to a wide extent.        △ Less","17 September, 2020",cs.CY,10.14445/22312803 
              When Healthcare Meets Off-the-Shelf WiFi: A Non-Wearable and Low-Costs Approach for In-Home Monitoring          ,2009.09715,https://arxiv.org/abs/2009.09715,https://arxiv.org/pdf/2009.09715,"Authors:LingchaoGuo,ZhaomingLu,ShuangZhou,XiangmingWen,ZhihongHe","        As elderly population grows, social and health care begin to face validation challenges, in-home monitoring is becoming a focus for professionals in the field. Governments urgently need to improve the quality of healthcare services at lower costs while ensuring the comfort and independence of the elderly. This work presents an in-home monitoring approach based on off-the-shelf WiFi, which is low-costs, non-wearable and makes all-round daily healthcare information available to caregivers. The proposed approach can capture fine-grained human pose figures even through a wall and track detailed respiration status simultaneously by off-the-shelf WiFi devices. Based on them, behavioral data, physiological data and the derived information (e.g., abnormal events and underlying diseases), of the elderly could be seen by caregivers directly. We design a series of signal processing methods and a neural network to capture human pose figures and extract respiration status curves from WiFi Channel State Information (CSI). Extensive experiments are conducted and according to the results, off-the-shelf WiFi devices are capable of capturing fine-grained human pose figures, similar to cameras, even through a wall and track accurate respiration status, thus demonstrating the effectiveness and feasibility of our approach for in-home monitoring.        △ Less","21 September, 2020","eess.SP,cs.CV,cs.LG",
              Focused Clinical Query Understanding and Retrieval of Medical Snippets powered through a Healthcare Knowledge Graph          ,2009.09086,https://arxiv.org/abs/2009.09086,https://arxiv.org/pdf/2009.09086,"Authors:MaulikR.Kamdar,MichaelCarroll,WillDowling,LindaWogulis,CaileyFitzgerald,MattCorkum,DanielleWalsh,DavidConrad,CraigE.Stanley,Jr.,SteveRoss,DruHenke,MevanSamarasinghe","        Clinicians face several significant barriers to search and synthesize accurate, succinct, updated, and trustworthy medical information from several literature sources during the practice of medicine and patient care. In this talk, we will be presenting our research behind the development of a Focused Clinical Search Service, powered by a Healthcare Knowledge Graph, to interpret the query intent behind clinical search queries and retrieve relevant medical snippets from a diverse corpus of medical literature.        △ Less","17 September, 2020","cs.CY,cs.AI",
              Automated Stroke Rehabilitation Assessment using Wearable Accelerometers in Free-Living Environments          ,2009.08798,https://arxiv.org/abs/2009.08798,https://arxiv.org/pdf/2009.08798,"Authors:XiChen,YuGuan,Jian-QingShi,Xiu-LiDu,JanetEyre","        Stroke is known as a major global health problem, and for stroke survivors it is key to monitor the recovery levels. However, traditional stroke rehabilitation assessment methods (such as the popular clinical assessment) can be subjective and expensive, and it is also less convenient for patients to visit clinics in a high frequency. To address this issue, in this work based on wearable sensing and machine learning techniques, we developed an automated system that can predict the assessment score in an objective and continues manner. With wrist-worn sensors, accelerometer data was collected from 59 stroke survivors in free-living environments for a duration of 8 weeks, and we aim to map the week-wise accelerometer data (3 days per week) to the assessment score by developing signal processing and predictive model pipeline. To achieve this, we proposed two new features, which can encode the rehabilitation information from both paralysed/non-paralysed sides while suppressing the high-level noises such as irrelevant daily activities. We further developed the longitudinal mixed-effects model with Gaussian process prior (LMGP), which can model the random effects caused by different subjects and time slots (during the 8 weeks). Comprehensive experiments were conducted to evaluate our system on both acute and chronic patients, and the results suggested its effectiveness.        △ Less","16 September, 2020","eess.SP,cs.HC,cs.LG,stat.AP",
              Cough Against COVID: Evidence of COVID-19 Signature in Cough Sounds          ,2009.08790,https://arxiv.org/abs/2009.08790,https://arxiv.org/pdf/2009.08790,"Authors:PiyushBagad,AmanDalmia,JigarDoshi,ArshaNagrani,ParagBhamare,AmritaMahale,SaurabhRane,NeerajAgarwal,RahulPanicker","        Testing capacity for COVID-19 remains a challenge globally due to the lack of adequate supplies, trained personnel, and sample-processing equipment. These problems are even more acute in rural and underdeveloped regions. We demonstrate that solicited-cough sounds collected over a phone, when analysed by our AI model, have statistically significant signal indicative of COVID-19 status (AUC 0.72, t-test,p <0.01,95% CI 0.61-0.83). This holds true for asymptomatic patients as well. Towards this, we collect the largest known(to date) dataset of microbiologically confirmed COVID-19 cough sounds from 3,621 individuals. When used in a triaging step within an overall testing protocol, by enabling risk-stratification of individuals before confirmatory tests, our tool can increase the testing capacity of a healthcare system by 43% at disease prevalence of 5%, without additional supplies, trained personnel, or physical infrastructure        △ Less","23 September, 2020","cs.SD,cs.LG,eess.AS",
              Variational Disentanglement for Rare Event Modeling          ,2009.08541,https://arxiv.org/abs/2009.08541,https://arxiv.org/pdf/2009.08541,"Authors:ZidiXiu,ChenyangTao,MichaelGao,ConnorDavis,BenjaminGoldstein,RicardoHenao","        Combining the increasing availability and abundance of healthcare data and the current advances in machine learning methods have created renewed opportunities to improve clinical decision support systems. However, in healthcare risk prediction applications, the proportion of cases with the condition (label) of interest is often very low relative to the available sample size. Though very prevalent in healthcare, such imbalanced classification settings are also common and challenging in many other scenarios. So motivated, we propose a variational disentanglement approach to semi-parametrically learn from rare events in heavily imbalanced classification problems. Specifically, we leverage the imposed extreme-distribution behavior on a latent space to extract information from low-prevalence events, and develop a robust prediction arm that joins the merits of the generalized additive model and isotonic neural nets. Results on synthetic studies and diverse real-world datasets, including mortality prediction on a COVID-19 cohort, demonstrate that the proposed approach outperforms existing alternatives.        △ Less","20 September, 2020","stat.ML,cs.LG",
              United We Stand: Transfer Graph Neural Networks for Pandemic Forecasting          ,2009.08388,https://arxiv.org/abs/2009.08388,https://arxiv.org/pdf/2009.08388,"Authors:GeorgePanagopoulos,GiannisNikolentzos,MichalisVazirgiannis","        The recent outbreak of COVID-19 has affected millions of individuals around the world and has posed a significant challenge to global healthcare. From the early days of the pandemic, it became clear that it is highly contagious and that human mobility contributes significantly to its spread. In this paper, we study the impact of population movement on the spread of COVID-19, and we capitalize on recent advances in the field of representation learning on graphs to capture the underlying dynamics. Specifically, we create a graph where nodes correspond to a country's regions and the edge weights denote human mobility from one region to another. Then, we employ graph neural networks to predict the number of future cases, encoding the underlying diffusion patterns that govern the spread into our learning model. Furthermore, to account for the limited amount of training data, we capitalize on the pandemic's asynchronous outbreaks across countries and use a model-agnostic meta-learning based method to transfer knowledge from one country's model to another's. We compare the proposed approach against simple baselines and more traditional forecasting techniques in 3 European countries. Experimental results demonstrate the superiority of our method, highlighting the usefulness of GNNs in epidemiological prediction. Transfer learning provides the best model, highlighting its potential to improve the accuracy of the predictions in case of secondary waves, if data from past/parallel outbreaks is utilized.        △ Less","10 September, 2020","cs.SI,cs.LG,stat.ML",
              Computational models in Electroencephalography          ,2009.08385,https://arxiv.org/abs/2009.08385,https://arxiv.org/pdf/2009.08385,"Authors:KatharinaGlomb,JoanaCabral,AnnaCattani,AlbertoMazzoni,AshishRaj,BenedettaFranceschiello","        Computational models lie at the intersection of basic neuroscience and healthcare applications because they allow researchers to test hypotheses \textit{in silico} and predict the outcome of experiments and interactions that are very hard to test in reality. Yet, what is meant by ""computational model"" is understood in many different ways by researchers in different fields of neuroscience and psychology, hindering communication and collaboration. In this review, we point out the state of the art of computational modeling in Electroencephalography (EEG) and outline how these models can be used to integrate findings from electrophysiology, network-level models, and behavior. On the one hand, computational models serve to investigate the mechanisms that generate brain activity, for example measured with EEG, such as the transient emergence of oscillations at different frequency bands and/or with different spatial topographies. On the other hand, computational models serve to design experiments and test hypotheses \emph{in silico}. The final purpose of computational models of EEG is to obtain a comprehensive understanding of the mechanisms that underlie the EEG signal. This is crucial for an accurate interpretation of EEG measurements that may ultimately serve in the development of novel clinical applications.        △ Less","17 September, 2020","q-bio.NC,cs.CE",
              Graph representation forecasting of patient's medical conditions: towards a digital twin          ,2009.08299,https://arxiv.org/abs/2009.08299,https://arxiv.org/pdf/2009.08299,"Authors:PietroBarbiero,RamonViñasTorné,PietroLió","        Objective: Modern medicine needs to shift from a wait and react, curative discipline to a preventative, interdisciplinary science aiming at providing personalised, systemic and precise treatment plans to patients. The aim of this work is to present how the integration of machine learning approaches with mechanistic computational modelling could yield a reliable infrastructure to run probabilistic simulations where the entire organism is considered as a whole. Methods: We propose a general framework that composes advanced AI approaches and integrates mathematical modelling in order to provide a panoramic view over current and future physiological conditions. The proposed architecture is based on a graph neural network (GNNs) forecasting clinically relevant endpoints (such as blood pressure) and a generative adversarial network (GANs) providing a proof of concept of transcriptomic integrability. Results: We show the results of the investigation of pathological effects of overexpression of ACE2 across different signalling pathways in multiple tissues on cardiovascular functions. We provide a proof of concept of integrating a large set of composable clinical models using molecular data to drive local and global clinical parameters and derive future trajectories representing the evolution of the physiological state of the patient. Significance: We argue that the graph representation of a computational patient has potential to solve important technological challenges in integrating multiscale computational modelling with AI. We believe that this work represents a step forward towards a healthcare digital twin.        △ Less","17 September, 2020","stat.ML,cs.LG",
              Robust Aggregation for Adaptive Privacy Preserving Federated Learning in Healthcare,2009.08294,https://arxiv.org/abs/2009.08294,https://arxiv.org/pdf/2009.08294,"Authors:MateiGrama,MariaMusat,LuisMuñoz-González,JonathanPasserat-Palmbach,DanielRueckert,AmirAlansary","        Federated learning (FL) has enabled training models collaboratively from multiple data owning parties without sharing their data. Given the privacy regulations of patient's healthcare data, learning-based systems in healthcare can greatly benefit from privacy-preserving FL approaches. However, typical model aggregation methods in FL are sensitive to local model updates, which may lead to failure in learning a robust and accurate global model. In this work, we implement and evaluate different robust aggregation methods in FL applied to healthcare data. Furthermore, we show that such methods can detect and discard faulty or malicious local clients during training. We run two sets of experiments using two real-world healthcare datasets for training medical diagnosis classification tasks. Each dataset is used to simulate the performance of three different robust FL aggregation strategies when facing different poisoning attacks. The results show that privacy preserving methods can be successfully applied alongside Byzantine-robust aggregation techniques. We observed in particular how using differential privacy (DP) did not significantly impact the final learning convergence of the different aggregation strategies.        △ Less","17 September, 2020",cs.CR,
              Deep Transparent Prediction through Latent Representation Analysis          ,2009.07044,https://arxiv.org/abs/2009.07044,https://arxiv.org/pdf/2009.07044,"Authors:D.Kollias,N.Bouas,Y.Vlaxos,V.Brillakis,M.Seferis,I.Kollia,L.Sukissian,J.Wingate,S.Kollias","        The paper presents a novel deep learning approach, which extracts latent information from trained Deep Neural Networks (DNNs) and derives concise representations that are analyzed in an effective, unified way for prediction purposes. It is well known that DNNs are capable of analyzing complex data; however, they lack transparency in their decision making, in the sense that it is not straightforward to justify their prediction, or to visualize the features on which the decision was based. Moreover, they generally require large amounts of data in order to learn and become able to adapt to different environments. This makes their use difficult in healthcare, where trust and personalization are key issues. Transparency combined with high prediction accuracy are the targeted goals of the proposed approach. It includes both supervised DNN training and unsupervised learning of latent variables extracted from the trained DNNs. Domain Adaptation from multiple sources is also presented as an extension, where the extracted latent variable representations are used to generate predictions in other, non-annotated, environments. Successful application is illustrated through a large experimental study in various fields: prediction of Parkinson's disease from MRI and DaTScans; prediction of COVID-19 and pneumonia from CT scans and X-rays; optical character verification in retail food packaging.        △ Less","20 September, 2020","cs.LG,cs.CV,eess.IV,stat.ML",
              Learning Hidden Patterns from Patient Multivariate Time Series Data Using Convolutional Neural Networks: A Case Study of Healthcare Cost Prediction          ,2009.06783,https://arxiv.org/abs/2009.06783,https://arxiv.org/pdf/2009.06783,"Authors:MohammadAminMorid,OliviaR.LiuSheng,KensakuKawamoto,SamirAbdelrahman","        Objective: To develop an effective and scalable individual-level patient cost prediction method by automatically learning hidden temporal patterns from multivariate time series data in patient insurance claims using a convolutional neural network (CNN) architecture.  Methods: We used three years of medical and pharmacy claims data from 2013 to 2016 from a healthcare insurer, where data from the first two years were used to build the model to predict costs in the third year. The data consisted of the multivariate time series of cost, visit and medical features that were shaped as images of patients' health status (i.e., matrices with time windows on one dimension and the medical, visit and cost features on the other dimension). Patients' multivariate time series images were given to a CNN method with a proposed architecture. After hyper-parameter tuning, the proposed architecture consisted of three building blocks of convolution and pooling layers with an LReLU activation function and a customized kernel size at each layer for healthcare data. The proposed CNN learned temporal patterns became inputs to a fully connected layer.  Conclusions: Feature learning through the proposed CNN configuration significantly improved individual-level healthcare cost prediction. The proposed CNN was able to outperform temporal pattern detection methods that look for a pre-defined set of pattern shapes, since it is capable of extracting a variable number of patterns with various shapes. Temporal patterns learned from medical, visit and cost data made significant contributions to the prediction performance. Hyper-parameter tuning showed that considering three-month data patterns has the highest prediction accuracy. Our results showed that patients' images extracted from multivariate time series data are different from regular images, and hence require unique designs of CNN architectures.        △ Less","14 September, 2020",cs.LG,
Healthcare Cost Prediction: Leveraging Fine-grain Temporal Patterns          ,2009.06780,https://arxiv.org/abs/2009.06780,https://arxiv.org/pdf/2009.06780,"Authors:MohammadAminMorid,OliviaR.LiuSheng,KensakuKawamoto,TravisAult,JosetteDorius,SamirAbdelrahman","        Objective: To design and assess a method to leverage individuals' temporal data for predicting their healthcare cost. To achieve this goal, we first used patients' temporal data in their fine-grain form as opposed to coarse-grain form. Second, we devised novel spike detection features to extract temporal patterns that improve the performance of cost prediction. Third, we evaluated the effectiveness of different types of temporal features based on cost information, visit information and medical information for the prediction task.  Materials and methods: We used three years of medical and pharmacy claims data from 2013 to 2016 from a healthcare insurer, where the first two years were used to build the model to predict the costs in the third year. To prepare the data for modeling and prediction, the time series data of cost, visit and medical information were extracted in the form of fine-grain features (i.e., segmenting each time series into a sequence of consecutive windows and representing each window by various statistics such as sum). Then, temporal patterns of the time series were extracted and added to fine-grain features using a novel set of spike detection features (i.e., the fluctuation of data points). Gradient Boosting was applied on the final set of extracted features. Moreover, the contribution of each type of data (i.e., cost, visit and medical) was assessed.  Conclusions: Leveraging fine-grain temporal patterns for healthcare cost prediction significantly improves prediction performance. Enhancing fine-grain features with extraction of temporal cost and visit patterns significantly improved the performance. However, medical features did not have a significant effect on prediction performance. Gradient Boosting outperformed all other prediction models.        △ Less","14 September, 2020",cs.LG,10.1016/j.jbi.2019.103113 
"              Organic Electronics Picks Up the Pace: Mask-Less, Solution Processed Organic Transistors Operating at 160 MHz          ",2009.06685,https://arxiv.org/abs/2009.06685,https://arxiv.org/pdf/2009.06685,"Authors:AndreaPerinot,MicheleGiorgio,VirgilioMattoli,DarioNatali,MarioCaironi","        Organic printed electronics has proven its potential as an essential enabler for applications related to healthcare, entertainment, energy and distributed intelligent objects. The possibility of exploiting solution-based and direct-writing production schemes further boosts the benefits offered by such technology, facilitating the implementation of cheap, conformable, bio-compatible electronic applications. The result shown in this work challenges the widespread assumption that such class of electronic devices is relegated to low-frequency operation, owing to the limited charge mobility of the materials and to the low spatial resolution achievable with conventional printing techniques. Here, it is shown that solution-processed and direct-written organic field-effect transistors can be carefully designed and fabricated so to achieve a maximum transition frequency of 160 MHz, unlocking an operational range that was not available before for organics. Such range was believed to be only accessible with more performing classes of semiconductor materials and/or more expensive fabrication schemes. The present achievement opens a route for cost- and energy-efficient manufacturability of flexible and conformable electronics with wireless-communication capabilities.        △ Less","14 September, 2020","physics.app-ph,cond-mat.mtrl-sci",
              Aligning Subjective Ratings in Clinical Decision Making          ,2009.06403,https://arxiv.org/abs/2009.06403,https://arxiv.org/pdf/2009.06403,"Authors:AnnikaPick,SebastianGinzel,StefanRüping,JilSander,AnnChristinaFoldenauer,MichaelaKöhm","        In addition to objective indicators (e.g. laboratory values), clinical data often contain subjective evaluations by experts (e.g. disease severity assessments). While objective indicators are more transparent and robust, the subjective evaluation contains a wealth of expert knowledge and intuition. In this work, we demonstrate the potential of pairwise ranking methods to align the subjective evaluation with objective indicators, creating a new score that combines their advantages and facilitates diagnosis. In a case study on patients at risk for developing Psoriatic Arthritis, we illustrate that the resulting score (1) increases classification accuracy when detecting disease presence/absence, (2) is sparse and (3) provides a nuanced assessment of severity for subsequent analysis.        △ Less","11 September, 2020","stat.AP,cs.LG,stat.ML",
              Spoiled for Choice? Personalized Recommendation for Healthcare Decisions: A Multi-Armed Bandit Approach          ,2009.06108,https://arxiv.org/abs/2009.06108,https://arxiv.org/pdf/2009.06108,"Authors:TongxinZhou,YingfeiWang,Lu,Yan,YongTan","        Online healthcare communities provide users with various healthcare interventions to promote healthy behavior and improve adherence. When faced with too many intervention choices, however, individuals may find it difficult to decide which option to take, especially when they lack the experience or knowledge to evaluate different options. The choice overload issue may negatively affect users' engagement in health management. In this study, we take a design-science perspective to propose a recommendation framework that helps users to select healthcare interventions. Taking into account that users' health behaviors can be highly dynamic and diverse, we propose a multi-armed bandit (MAB)-driven recommendation framework, which enables us to adaptively learn users' preference variations while promoting recommendation diversity in the meantime. To better adapt an MAB to the healthcare context, we synthesize two innovative model components based on prominent health theories. The first component is a deep-learning-based feature engineering procedure, which is designed to learn crucial recommendation contexts in regard to users' sequential health histories, health-management experiences, preferences, and intrinsic attributes of healthcare interventions. The second component is a diversity constraint, which structurally diversifies recommendations in different dimensions to provide users with well-rounded support. We apply our approach to an online weight management context and evaluate it rigorously through a series of experiments. Our results demonstrate that each of the design components is effective and that our recommendation design outperforms a wide range of state-of-the-art recommendation systems. Our study contributes to the research on the application of business intelligence and has implications for multiple stakeholders, including online healthcare platforms, policymakers, and users.        △ Less","13 September, 2020","cs.LG,cs.AI,cs.IR,stat.ML",
              Recent Advances in Wearable Sensors with Application in Rehabilitation Motion Analysis          ,2009.06062,https://arxiv.org/abs/2009.06062,https://arxiv.org/pdf/2009.06062,"Authors:S.Shokri,S.Ward,P.M.Anton,P.Siffredi,G.Papetti","        The increase in world elderly population has significantly underlined the need for continuous health care measurement, specifically in rehabilitation monitoring. The new technologies has enabled people to have in home healthcare services, meanwhile, motion analysis methods are widely used for human activity monitoring as a remote healthcare service. Wearable sensors have indicated promising results both in convenience and technical performance. These sensors are extensively used in human motion analysis and advancement of wireless communications has intensively contributed to this field. Exploiting wireless technology and wearable sensors contributes to more effective help in emergency cases and has significantly decreased the hospitalization time. This paper reviews the most recent advances in wearable sensors used in motion analysis, specifically in the field of rehabilitation. Firstly, common wearable sensor technologies are introduced and then wearable sensors deploying Carbon Nano Tubes (CNT) are specifically reviewed. The next section is dedicated to sensor fusion in which possibility and performance of integration of new technologies are reviewed. This technique has been widely exploited to bring forth certainty in clinical results. Lastly, the challenges and future possibilities for advancement in motion analysis sensors is discussed.        △ Less","13 September, 2020","cs.HC,eess.SY",
              Machine Learning Against Cancer: Accurate Diagnosis of Cancer by Machine Learning Classification of the Whole Genome Sequencing Data          ,2009.05847,https://arxiv.org/abs/2009.05847,https://arxiv.org/pdf/2009.05847,Authors:ArashHooshmand,"        Machine learning can precisely identify different cancer tumors at any stage by classifying cancerous and healthy samples based on their genomic profile. We have developed novel methods of MLAC (Machine Learning Against Cancer) achieving perfect results with perfect precision, sensitivity, and specificity. We have used the whole genome sequencing data acquired by next-generation RNA sequencing techniques in The Cancer Genome Atlas and Genotype-Tissue Expression projects for cancerous and healthy tissues respectively. Moreover, we have shown that unsupervised machine learning clustering has great potential to be used for cancer diagnosis. Indeed, a creative way to work with data and general algorithms has resulted in perfect classification i.e. all precision, sensitivity, and specificity are equal to 1 for most of the different tumor types even with a modest amount of data, and the same method works well on a series of cancers and results in great clustering of cancerous and healthy samples too. Our system can be used in practice because once the classifier is trained, it can be used to classify any new sample of new potential patients. One advantage of our work is that the aforementioned perfect precision and recall are obtained on samples of all stages including very early stages of cancer; therefore, it is a promising tool for diagnosis of cancers in early stages. Another advantage of our novel model is that it works with normalized values of RNA sequencing data, hence people's private sensitive medical data will remain hidden, protected, and safe. This type of analysis will be widespread and economical in the future and people can even learn to receive their RNA sequencing data and do their own preliminary cancer studies themselves which have the potential to help the healthcare systems. It is a great step forward toward good health that is the main base of sustainable societies.        △ Less","12 September, 2020","q-bio.GN,cs.LG,stat.ML",
"              COVID-19 what have we learned? The rise of social machines and connected devices in pandemic management following the concepts of predictive, preventive and personalised medicine          ",2009.05791,https://arxiv.org/abs/2009.05791,https://arxiv.org/pdf/2009.05791,"Authors:PetarRadanliev,DavidDeRoure,RobWalton,MaxVanKleek,RafaelMantillaMontalvo,OmarSantos,LaTreallMaddox,StacyCannady","        This review paper investigates the integration of predictive, preventive and personalised interoperable digital healthcare systems. The second point of interest is the use of new mass surveillance technologies to feed personal data from health professionals to governments, without any comprehensive studies that determine if such new technologies and data policies would address the pandemic crisis.Two approaches were used: A comprehensive bibliographic review with R statistical methods of the COVID-19 pandemic in PubMed literature and Web of Science Core Collection, supported with Google Scholar search. In addition, a case study review of emerging new approaches in different regions, using medical literature, academic literature, news articles, and other reliable data sources. We present solutions that would enable much greater speed in future responses. These solutions are enabled by the social aspect of human-computer interactions (social machines) and the increased connectivity of humans and devices (internet of things).        △ Less","12 September, 2020","cs.CY,cs.HC",10.1007/s13167-020-00218-x 
              The Adoption of ICT Powered Healthcare Technologies towards Managing Global Pandemics          ,2009.05716,https://arxiv.org/abs/2009.05716,https://arxiv.org/pdf/2009.05716,"Authors:NavodNeranjanThilakarathne,MohanKrishnaKagita,ThippaReddyGadekallu,PraveenKumarReddyMaddikunta","        Pandemic is an outbreak that happens over a large geographic area affecting a greater portion of the population as new pathogens appear for which people have less immune and no vaccines are available. It can spread from person to person in a very short time, and in fact, the health workers are at greater risk of infection because of the patients who carry the disease. In the 21st century, where everyone is connected through digital technologies, Information and Communication Technology (ICT) plays a critical role in improving health care for individuals and larger communities. ICT has currently been severed in a variety of application domains which signifies its importance as a major technological paradigm, and it has drawn higher attention for its potential to alleviate the burden on healthcare systems caused by a rise in chronic diseases, aging and increased population and pandemic situations. This paper surveys and offers substantial knowledge about how effective ICT Healthcare strategy can be used to manage global pandemics by presenting a four-phased framework, which can be deployed to alleviate the strain on healthcare during a pandemic. In addition, we discuss how ICT powered technologies can be used towards managing a pandemic during the transformation of simple disease outbreak into a global pandemic.        △ Less","11 September, 2020",cs.CY,
              A Selective Review of Negative Control Methods in Epidemiology          ,2009.05641,https://arxiv.org/abs/2009.05641,https://arxiv.org/pdf/2009.05641,"Authors:XuShi,WangMiao,EricTchetgenTchetgen","        Purpose of Review: Negative controls are a powerful tool to detect and adjust for bias in epidemiological research. This paper introduces negative controls to a broader audience and provides guidance on principled design and causal analysis based on a formal negative control framework.  Recent Findings: We review and summarize causal and statistical assumptions, practical strategies, and validation criteria that can be combined with subject matter knowledge to perform negative control analyses. We also review existing statistical methodologies for detection, reduction, and correction of confounding bias, and briefly discuss recent advances towards nonparametric identification of causal effects in a double negative control design.  Summary: There is great potential for valid and accurate causal inference leveraging contemporary healthcare data in which negative controls are routinely available. Design and analysis of observational data leveraging negative controls is an area of growing interest in health and social sciences. Despite these developments, further effort is needed to disseminate these novel methods to ensure they are adopted by practicing epidemiologists.        △ Less","11 September, 2020",stat.ME,
              A Review on Security and Privacy of Internet of Medical Things          ,2009.05394,https://arxiv.org/abs/2009.05394,https://arxiv.org/pdf/2009.05394,"Authors:MohanKrishnaKagita,NavodThilakarathne,ThippaReddyGadekallu,PraveenKumarReddyMaddikunta","        The Internet of Medical Things (IoMT) are increasing the accuracy, reliability, and the production capability of electronic devices by playing a very important part in the industry of healthcare. The available medical resources and services related to healthcare are working to get an interconnection with each other by the digital healthcare system by the contribution of the researchers. Sensors, wearable devices, medical devices, and clinical devices are all connected to form an ecosystem of the Internet of Medical Things. The different applications of healthcare are enabled by the Internet of Medical Things to reduce the healthcare costs, to attend the medical responses on time and it also helps in increasing the quality of the medical treatment. The healthcare industry is transformed by the Internet of Medical Things as it delivers targeted and personalized medical care and it also seamlessly enables the communication of medical data. Devices used in the medical field and their application are connected to the system of healthcare of Information technology with the help of the digital world.        △ Less","11 September, 2020","cs.CY,cs.NI",
              COVIDNet-CT: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest CT Images          ,2009.05383,https://arxiv.org/abs/2009.05383,https://arxiv.org/pdf/2009.05383,"Authors:HaydenGunraj,LindaWang,AlexanderWong","        The coronavirus disease 2019 (COVID-19) pandemic continues to have a tremendous impact on patients and healthcare systems around the world. In the fight against this novel disease, there is a pressing need for rapid and effective screening tools to identify patients infected with COVID-19, and to this end CT imaging has been proposed as one of the key screening methods which may be used as a complement to RT-PCR testing, particularly in situations where patients undergo routine CT scans for non-COVID-19 related reasons, patients with worsening respiratory status or developing complications that require expedited care, and patients suspected to be COVID-19-positive but have negative RT-PCR test results. Motivated by this, in this study we introduce COVIDNet-CT, a deep convolutional neural network architecture that is tailored for detection of COVID-19 cases from chest CT images via a machine-driven design exploration approach. Additionally, we introduce COVIDx-CT, a benchmark CT image dataset derived from CT imaging data collected by the China National Center for Bioinformation comprising 104,009 images across 1,489 patient cases. Furthermore, in the interest of reliability and transparency, we leverage an explainability-driven performance validation strategy to investigate the decision-making behaviour of COVIDNet-CT, and in doing so ensure that COVIDNet-CT makes predictions based on relevant indicators in CT images. Both COVIDNet-CT and the COVIDx-CT dataset are available to the general public in an open-source and open access manner as part of the COVID-Net initiative. While COVIDNet-CT is not yet a production-ready screening solution, we hope that releasing the model and dataset will encourage researchers, clinicians, and citizen data scientists alike to leverage and build upon them.        △ Less","8 September, 2020","eess.IV,cs.CV,cs.LG",
              A Framework for Evaluating Dashboards in Healthcare,2009.04792,https://arxiv.org/abs/2009.04792,https://arxiv.org/pdf/2009.04792,"Authors:MengdieZhuang,DaveConcannon,EdManley","        In the era of ""information overload"", effective information provision is essential for enabling rapid response and critical decision making. In making sense of diverse information sources, data dashboards have become an indispensable tool, providing fast, effective, adaptable, and personalized access to information for professionals and the general public alike. However, these objectives place a heavy requirement on dashboards as information systems, resulting in poor usability and ineffective design. Understanding these shortfalls is a challenge given the absence of a consistent and comprehensive approach to dashboard evaluation. In this paper we systematically review literature on dashboard implementation in the healthcare domain, a field where dashboards have been employed widely, and in which there is widespread interest for improving the current state of the art, and subsequently analyse approaches taken towards evaluation. We draw upon consolidated dashboard literature and our own observations to introduce a general definition of dashboards which is more relevant to current trends, together with a dashboard task-based classification, which underpin our subsequent analysis. From a total of 81 papers we derive seven evaluation scenarios - task performance, behaviour change, interaction workflow, perceived engagement, potential utility, algorithm performance and system implementation. These scenarios distinguish different evaluation purposes which we illustrate through measurements, example studies, and common challenges in evaluation study design. We provide a breakdown of each evaluation scenario, and highlight some of the subtle and less well posed questions. We conclude by outlining a number of active discussion points and a set of dashboard evaluation best practices for the academic, clinical and software development communities alike.        △ Less","10 September, 2020","cs.HC,cs.GR",
              Segmentation-free Estimation of Aortic Diameters from MRI Using Deep Learning          ,2009.04507,https://arxiv.org/abs/2009.04507,https://arxiv.org/pdf/2009.04507,"Authors:AxelAguerreberry,EzequieldelaRosa,AlainLalande,ElmerFernandez","        Accurate and reproducible measurements of the aortic diameters are crucial for the diagnosis of cardiovascular diseases and for therapeutic decision making. Currently, these measurements are manually performed by healthcare professionals, being time consuming, highly variable, and suffering from lack of reproducibility. In this work we propose a supervised deep learning method for the direct estimation of aortic diameters. The approach is devised and tested over 100 magnetic resonance angiography scans without contrast agent. All data was expert-annotated at six aortic locations typically used in clinical practice. Our approach makes use of a 3D+2D convolutional neural network (CNN) that takes as input a 3D scan and outputs the aortic diameter at a given location. In a 5-fold cross-validation comparison against a fully 3D CNN and against a 3D multiresolution CNN, our approach was consistently superior in predicting the aortic diameters. Overall, the 3D+2D CNN achieved a mean absolute error between 2.2-2.4 mm depending on the considered aortic location. These errors are less than 1 mm higher than the inter-observer variability. Thus, suggesting that our method makes predictions almost reaching the expert's performance. We conclude that the work allows to further explore automatic algorithms for direct estimation of anatomical structures without the necessity of a segmentation step. It also opens possibilities for the automation of cardiovascular measurements in clinical settings.        △ Less","9 September, 2020","eess.IV,cs.CV",
              Enhancing the Interpretability of Deep Models in Heathcare Through Attention: Application to Glucose Forecasting for Diabetic People          ,2009.03732,https://arxiv.org/abs/2009.03732,https://arxiv.org/pdf/2009.03732,"Authors:MaximeDeBois,MounîmA.ElYacoubi,MehdiAmmi","        The adoption of deep learning in healthcare is hindered by their ""black box"" nature. In this paper, we explore the RETAIN architecture for the task of glusose forecasting for diabetic people. By using a two-level attention mechanism, the recurrent-neural-network-based RETAIN model is interpretable. We evaluate the RETAIN model on the type-2 IDIAB and the type-1 OhioT1DM datasets by comparing its statistical and clinical performances against two deep models and three models based on decision trees. We show that the RETAIN model offers a very good compromise between accuracy and interpretability, being almost as accurate as the LSTM and FCN models while remaining interpretable. We show the usefulness of its interpretable nature by analyzing the contribution of each variable to the final prediction. It revealed that signal values older than one hour are not used by the RETAIN model for the 30-minutes ahead of time prediction of glucose. Also, we show how the RETAIN model changes its behavior upon the arrival of an event such as carbohydrate intakes or insulin infusions. In particular, it showed that the patient's state before the event is particularily important for the prediction. Overall the RETAIN model, thanks to its interpretability, seems to be a very promissing model for regression or classification tasks in healthcare.        △ Less","8 September, 2020","cs.LG,cs.AI,stat.ML",
              High-throughput relation extraction algorithm development associating knowledge articles and electronic health records          ,2009.03506,https://arxiv.org/abs/2009.03506,https://arxiv.org/pdf/2009.03506,"Authors:YucongLin,KemingLu,YulinChen,ChuanHong,ShengYu","        Objective: Medical relations are the core components of medical knowledge graphs that are needed for healthcare artificial intelligence. However, the requirement of expert annotation by conventional algorithm development processes creates a major bottleneck for mining new relations. In this paper, we present Hi-RES, a framework for high-throughput relation extraction algorithm development. We also show that combining knowledge articles with electronic health records (EHRs) significantly increases the classification accuracy. Methods: We use relation triplets obtained from structured databases and semistructured webpages to label sentences from target corpora as positive training samples. Two methods are also provided for creating improved negative samples by combining positive samples with naïve negative samples. We propose a common model that summarizes sentence information using large-scale pretrained language models and multi-instance attention, which then joins with the concept embeddings trained from the EHRs for relation prediction. Results: We apply the Hi-RES framework to develop classification algorithms for disorder-disorder relations and disorder-location relations. Millions of sentences are created as training data. Using pretrained language models and EHR-based embeddings individually provides considerable accuracy increases over those of previous models. Joining them together further tremendously increases the accuracy to 0.947 and 0.998 for the two sets of relations, respectively, which are 10-17 percentage points higher than those of previous models. Conclusion: Hi-RES is an efficient framework for achieving high-throughput and accurate relation extraction algorithm development.        △ Less","7 September, 2020","cs.LG,stat.ML",
"              Is Everything Fine, Grandma? Acoustic and Linguistic Modeling for Robust Elderly Speech Emotion Recognition          ",2009.03432,https://arxiv.org/abs/2009.03432,https://arxiv.org/pdf/2009.03432,"Authors:GizemSoğancıoğlu,OxanaVerkholyak,HeysemKaya,DmitriiFedotov,TobiasCadèe,AlbertAliSalah,AlexeyKarpov","        Acoustic and linguistic analysis for elderly emotion recognition is an under-studied and challenging research direction, but essential for the creation of digital assistants for the elderly, as well as unobtrusive telemonitoring of elderly in their residences for mental healthcare purposes. This paper presents our contribution to the INTERSPEECH 2020 Computational Paralinguistics Challenge (ComParE) - Elderly Emotion Sub-Challenge, which is comprised of two ternary classification tasks for arousal and valence recognition. We propose a bi-modal framework, where these tasks are modeled using state-of-the-art acoustic and linguistic features, respectively. In this study, we demonstrate that exploiting task-specific dictionaries and resources can boost the performance of linguistic models, when the amount of labeled data is small. Observing a high mismatch between development and test set performances of various models, we also propose alternative training and decision fusion strategies to better estimate and improve the generalization performance.        △ Less","7 September, 2020","cs.CL,cs.HC,cs.LG",
              Conquery: an open source application to analyze high content healthcare data          ,2009.03304,https://arxiv.org/abs/2009.03304,https://arxiv.org/pdf/2009.03304,"Authors:FabianKovacs,MaxThonagel,MarionLudwig,AlexanderAlbrecht,HannesPriehn,ManuelHegner,DirkEnders,LennartHickstein,MaximilianvonKnobloch,AnneRothhardt,JochenWalker","        Background: Big data in healthcare must be exploited to achieve a substantial increase in efficiency and competitiveness. Especially the analysis of patient-related data possesses huge potential to considerably improve decision-making processes in the healthcare sector. Most analytical approaches used today are highly time- and resource-consuming. The presented software solution Conquery is an open source software tool providing advanced, but intuitive data analysis without the need for specialized statistical training. Results: Conquery is a column-oriented distributed timeseries database and analysis platform. Its main application is the analysis of per-person medical records by non-technical medical professionals. Complex analyses are realized in the Conquery frontend by dragging tree nodes into the query editor. Queries are evaluated by a bespoke distributed query-engine for medical records in a column-oriented fashion. We present a custom compression scheme to facilitate low response times that uses online calculated as well as precomputed metadata and data statistics. Conclusions: Conquery is an efficient and intuitive open source software for performant and secure data analysis and aims at supporting decision-making processes in the healthcare sector.        △ Less","7 September, 2020",cs.DB,
              Effect of lockdown interventions to control the COVID-19 epidemic in India          ,2009.03168,https://arxiv.org/abs/2009.03168,https://arxiv.org/pdf/2009.03168,"Authors:AnkitSharma,ShreyashArya,ShasheeKumari,ArnabChatterjee","        The pandemic caused by the novel Coronavirus SARS-CoV2 has been responsible for life threatening health complications, and extreme pressure on healthcare systems. While preventive and definite curative medical interventions are yet to arrive, Non-Pharmaceutical Interventions (NPIs) like physical isolation, quarantine and drastic social measures imposed by governing agencies are effective in arresting the spread of infections in a population. In densely populated countries like India, lockdown interventions are partially effective due to social and administrative complexities. Using detailed demographic data, we present an agent based model to imitate the behavior of the population and its mobility features, even under intervention. We demonstrate the effectiveness of contact tracing policies and how our model efficiently relates to empirical findings on testing efficiency. We also present various lockdown intervention strategies for mitigation - using the bare number of infections, the effective reproduction rate, as well as using reinforcement learning. Our analysis can help assess the socio-economic consequences of such interventions, and provide useful ideas and insights to policy makers for better decision making.        △ Less","7 September, 2020","physics.soc-ph,cs.MA,q-bio.PE",
              QualDash: Adaptable Generation of Visualisation Dashboards for Healthcare Quality Improvement          ,2009.03002,https://arxiv.org/abs/2009.03002,https://arxiv.org/pdf/2009.03002,"Authors:MaiElshehaly,RebeccaRandell,MatthewBrehmer,LynnMcVey,NatashaAlvarado,ChrisP.Gale,RoyA.Ruddle","        Adapting dashboard design to different contexts of use is an open question in visualisation research. Dashboard designers often seek to strike a balance between dashboard adaptability and ease-of-use, and in hospitals challenges arise from the vast diversity of key metrics, data models and users involved at different organizational levels. In this design study, we present QualDash, a dashboard generation engine that allows for the dynamic configuration and deployment of visualisation dashboards for healthcare quality improvement (QI). We present a rigorous task analysis based on interviews with healthcare professionals, a co-design workshop and a series of one-on-one meetings with front line analysts. From these activities we define a metric card metaphor as a unit of visual analysis in healthcare QI, using this concept as a building block for generating highly adaptable dashboards, and leading to the design of a Metric Specification Structure (MSS). Each MSS is a JSON structure which enables dashboard authors to concisely configure unit-specific variants of a metric card, while offloading common patterns that are shared across cards to be preset by the engine. We reflect on deploying and iterating the design of QualDash in cardiology wards and pediatric intensive care units of five NHS hospitals. Finally, we report evaluation results that demonstrate the adaptability, ease-of-use and usefulness of QualDash in a real-world scenario.        △ Less","7 September, 2020",cs.HC,
              WS2-QDs Decorated RGO Lattice on e-textile: Development of Ultrasensitive Wearable Quantum Thermometer          ,2009.02681,https://arxiv.org/abs/2009.02681,https://arxiv.org/pdf/2009.02681,"Authors:Abid,PoonamSehrawat,C.M.Julien,S.S.Islam","        We report the fabrication and human trial of a novel wearable temperature sensor based on WS2-QDs/RGO; which performs instant measurement like thermometer in a wide temperature range: 77K-398 K, in both static- and instant mode. The device is simple, scalable, flexible and cost-effective, where nanoscience and technology played a vital role behind its concept and realization. The WS2-QDs/RGO heterostructure is developed by decorating WS2-QDs on pre-RGO coated cotton textile. In static mode, the crucial parameters such as temperature coefficient of resistance (TCR) and thermal hysteresis (Hth) were analyzed in depth to get the intricate mechanism behind the working of a temperature sensor; and check its worthiness to be a better candidate in the field of temperature sensor. Temperature sensing data at both high- and low temperatures are very much encouraging; and endorses its viability. Human trial is conducted to make reliable and hassle free temperature monitoring like thermometer where the sensor device is found capable to measure accurate body temperature with exceptional resolution i.e. the minimum change in temperature the device can measure is ~0.06K in addition to, fast response- and recovery time ~1.4 s and 1.7 s respectively. In every sense, the developed sensor has exhibited highest degree of superiority vis-a-vis its counterpart commercial thermometer used in healthcare. Besides, the device has passed through all deformation test successfully and proved its mettle. This sensor device proved its flexibility and stability under various mechanical deformation(s), showing its promising potential for future generation wearable health monitoring devices. To the best of our knowledge, this is the first report on WS2 in general, and WS2-QDs, in specific, based temperature sensing device and its operational demonstration as of now.        △ Less","6 September, 2020","cond-mat.mtrl-sci,cond-mat.mes-hall",
              Online Disease Self-diagnosis with Inductive Heterogeneous Graph Convolutional Networks          ,2009.02625,https://arxiv.org/abs/2009.02625,https://arxiv.org/pdf/2009.02625,"Authors:ZifengWang,RuiWen,XiChen,ShileiCao,Shao-LunHuang,BuyueQian,YefengZheng","        We propose a Healthcare Graph Convolutional Network (HealGCN) to offer disease self-diagnosis service for online users, based on the Electronic Healthcare Records (EHRs). Two main challenges are focused in this paper for online disease self-diagnosis: (1) serving cold-start users via graph convolutional networks and (2) handling scarce clinical description via a symptom retrieval system. To this end, we first organize the EHR data into a heterogeneous graph that is capable of modeling complex interactions among users, symptoms and diseases, and tailor the graph representation learning towards disease diagnosis with an inductive learning paradigm. Then, we build a disease self-diagnosis system with a corresponding EHR Graph-based Symptom Retrieval System (GraphRet) that can search and provide a list of relevant alternative symptoms by tracing the predefined meta-paths. GraphRet helps enrich the seed symptom set through the EHR graph, resulting in better reasoning ability of our HealGCN model, when confronting users with scarce descriptions. At last, we validate our model on a large-scale EHR dataset, the superior performance does confirm our model's effectiveness in practice.        △ Less","5 September, 2020","cs.LG,cs.AI,cs.IR",
              Suicide Risk Modeling with Uncertain Diagnostic Records          ,2009.02597,https://arxiv.org/abs/2009.02597,https://arxiv.org/pdf/2009.02597,"Authors:WenjieWang,ChongliangLuo,RobertH.Aseltine,FeiWang,JunYan,KunChen","        Motivated by the pressing need for suicide prevention through improving behavioral healthcare, we use medical claims data to study the risk of subsequent suicide attempts for patients who were hospitalized due to suicide attempts and later discharged. Understanding the risk behaviors of such patients at elevated suicide risk is an important step towards the goal of ""Zero Suicide"". An immediate and unconventional challenge is that the identification of suicide attempts from medical claims contains substantial uncertainty: almost 20\% of ""suspected"" suicide attempts are identified from diagnostic codes indicating external causes of injury and poisoning with undermined intent. It is thus of great interest to learn which of these undetermined events are more likely actual suicide attempts and how to properly utilize them in survival analysis with severe censoring. To tackle these interrelated problems, we develop an integrative Cox cure model with regularization to perform survival regression with uncertain events and a latent cure fraction. We apply the proposed approach to study the risk of subsequent suicide attempt after suicide-related hospitalization for adolescent and young adult population, using medical claims data from Connecticut. The identified risk factors are highly interpretable; more intriguingly, our method distinguishes the risk factors that are most helpful in assessing either susceptibility or timing of subsequent attempt. The predicted statuses of the uncertain attempts are further investigated, leading to several new insights on suicide event identification.        △ Less","5 September, 2020","stat.AP,stat.ME,stat.ML",
              Technological Platform for the Prevention and Management of Healthcare Associated Infections and Outbreaks          ,2009.02502,https://arxiv.org/abs/2009.02502,https://arxiv.org/pdf/2009.02502,"Authors:MariaIulianaBocicor,MariaDascălu,AgnieszkaGaczowska,SorinHostiuc,AlinMoldoveanu,AntonioMolina,Arthur-JozsefMolnar,IonuţNegoi,VladRacoviţă","        Hospital acquired infections are infections that occur in patients during hospitalization, which were not present at the time of admission. They are among the most common adverse events in healthcare around the world, leading to increased mortality and morbidity rates, prolonged hospitalization periods and considerable financial burden on both hospitals and patients. Preventive guidelines and regulations have been devised, however compliance to these is frequently poor and there is much room for improvement. This paper presents the prototype of an extensible, configurable cyber-physical system, developed under European Union funding, that will assist in the prevention of hospital infections and outbreaks. Integrating a wireless sensor network for the surveillance of clinical processes with configurable monitoring software built around a workflow engine as key component, our solution detects deviations from established hygiene practices and provides real-time information and alerts whenever an infection risk is discovered. The platform is described from both hardware and software perspective, with emphasis on the wireless network's elements as well as the most important software components. Furthermore, two clinical workflows of different complexity, which are included in the system prototype are detailed. The finalized system is expected to facilitate the creation and automated monitoring of clinical workflows that are associated with over 90% of hospital infections.        △ Less","5 September, 2020",cs.CY,10.1007/978-3-319-94135-6_4 
              Intelligent Luminaire based Real-time Indoor Positioning for Assisted Living          ,2009.02483,https://arxiv.org/abs/2009.02483,https://arxiv.org/pdf/2009.02483,"Authors:IulianaMarin,MariaIulianaBocicor,Arthur-JozsefMolnar","        This paper presents an experimental evaluation on the accuracy of indoor localisation. The research was carried out as part of a European Union project targeting the creation of ICT solutions for older adult care. Current expectation is that advances in technology will supplement the human workforce required for older adult care, improve their quality of life and decrease healthcare expenditure. The proposed approach is implemented in the form of a configurable cyber-physical system that enables indoor localization and monitoring of older adults living at home or in residential buildings. Hardware consists of custom developed luminaires with sensing, communication and processing capabilities. They replace the existing lighting infrastructure, do not look out of place and are cost effective. The luminaires record the strength of a Bluetooth signal emitted by a wearable device equipped by the monitored user. The system's software server uses trilateration to calculate the person's location based on known luminaire placement and recorded signal strengths. However, multipath fading caused by the presence of walls, furniture and other objects introduces localisation errors. Our previous experiments showed that room-level accuracy can be achieved using software-based filtering for a stationary subject. Our current objective is to assess system accuracy in the context of a moving subject, and ascertain whether room-level localization is feasible in real time.        △ Less","5 September, 2020",cs.CY,10.5220/0009578705480555 
              A free web service for fast COVID-19 classification of chest X-Ray images          ,2009.01657,https://arxiv.org/abs/2009.01657,https://arxiv.org/pdf/2009.01657,"Authors:JoseDavidBermudezCastro,RicardoRei,JoseE.Ruiz,PedroAchanccarayDiaz,SmithAraucoCanchumuni,CristianMuñozVillalobos,FelipeBorgesCoelho,LeonardoForeroMendoza,MarcoAurelioC.Pacheco","        The coronavirus outbreak became a major concern for society worldwide. Technological innovation and ingenuity are essential to fight COVID-19 pandemic and bring us one step closer to overcome it. Researchers over the world are working actively to find available alternatives in different fields, such as the Healthcare System, pharmaceutic, health prevention, among others. With the rise of artificial intelligence (AI) in the last 10 years, IA-based applications have become the prevalent solution in different areas because of its higher capability, being now adopted to help combat against COVID-19. This work provides a fast detection system of COVID-19 characteristics in X-Ray images based on deep learning (DL) techniques. This system is available as a free web deployed service for fast patient classification, alleviating the high demand for standards method for COVID-19 diagnosis. It is constituted of two deep learning models, one to differentiate between X-Ray and non-X-Ray images based on Mobile-Net architecture, and another one to identify chest X-Ray images with characteristics of COVID-19 based on the DenseNet architecture. For real-time inference, it is provided a pair of dedicated GPUs, which reduce the computational time. The whole system can filter out non-chest X-Ray images, and detect whether the X-Ray presents characteristics of COVID-19, highlighting the most sensitive regions.        △ Less","27 August, 2020","eess.IV,cs.LG",
              Evaluation of Software Product Quality Metrics          ,2009.01557,https://arxiv.org/abs/2009.01557,https://arxiv.org/pdf/2009.01557,"Authors:Arthur-JozsefMolnar,AlexandraNeamţu,SimonaMotogna","        Computing devices and associated software govern everyday life, and form the backbone of safety critical systems in banking, healthcare, automotive and other fields. Increasing system complexity, quickly evolving technologies and paradigm shifts have kept software quality research at the forefront. Standards such as ISO's 25010 express it in terms of sub-characteristics such as maintainability, reliability and security. A significant body of literature attempts to link these subcharacteristics with software metric values, with the end goal of creating a metric-based model of software product quality. However, research also identifies the most important existing barriers. Among them we mention the diversity of software application types, development platforms and languages. Additionally, unified definitions to make software metrics truly language-agnostic do not exist, and would be difficult to implement given programming language levels of variety. This is compounded by the fact that many existing studies do not detail their methodology and tooling, which precludes researchers from creating surveys to enable data analysis on a larger scale. In our paper, we propose a comprehensive study of metric values in the context of three complex, open-source applications. We align our methodology and tooling with that of existing research, and present it in detail in order to facilitate comparative evaluation. We study metric values during the entire 18-year development history of our target applications, in order to capture the longitudinal view that we found lacking in existing literature. We identify metric dependencies and check their consistency across applications and their versions. At each step, we carry out comparative evaluation with existing research and present our results.        △ Less","3 September, 2020",cs.SE,10.1007/978-3-030-40223-5_8 
"              A Survey on Blockchain for Big Data: Approaches, Opportunities, and Future Directions          ",2009.00858,https://arxiv.org/abs/2009.00858,https://arxiv.org/pdf/2009.00858,"Authors:NatarajanDeepa,Quoc-VietPham,DinhC.Nguyen,SwetaBhattacharya,PrabadeviB,ThippaReddyGadekallu,PraveenKumarReddyMaddikunta,FangFang,PubuduN.Pathirana","        Big data has generated strong interest in various scientific and engineering domains over the last few years. Despite many advantages and applications, there are many challenges in big data to be tackled for better quality of service, e.g., big data analytics, big data management, and big data privacy and security. Blockchain with its decentralization and security nature has the great potential to improve big data services and applications. In this article, we provide a comprehensive survey on blockchain for big data, focusing on up-to-date approaches, opportunities, and future directions. First, we present a brief overview of blockchain and big data as well as the motivation behind their integration. Next, we survey various blockchain services for big data, including blockchain for secure big data acquisition, data storage, data analytics, and data privacy preservation. Then, we review the state-of-the-art studies on the use of blockchain for big data applications in different vertical domains such as smart city, smart healthcare, smart transportation, and smart grid. For a better understanding, some representative blockchain-big data projects are also presented and analyzed. Finally, challenges and future directions are discussed to further drive research in this promising area.        △ Less","2 September, 2020","cs.CR,cs.DC",
"              A survey on Blockchain-based applications for reforming data protection, privacy and security          ",2009.00530,https://arxiv.org/abs/2009.00530,https://arxiv.org/pdf/2009.00530,"Authors:PhanTheDuy,DoThiThuHien,Van-HauPham","        The modern society, economy and industry have been changed remarkably by many cutting-edge technologies over the last years, and many more are in development and early implementation that will in turn led even wider spread of adoptions and greater alteration. Blockchain technology along with other rising ones is expected to transform virtually every aspect of global business and individuals' lifestyle in some areas. It has been spreading with multi-sector applications from financial services to healthcare, supply chain, and cybersecurity emerging every passing day. Simultaneously, in the digital world, data protection and privacy are the most enormous issues which customers, companies and policymakers also take seriously into consideration due to the recent increase of security breaches and surveillance in reported incidents. In this case, blockchain has the capability and potential to revolutionize trust, security and privacy of individual data in the online world. Hence, the purpose of this paper is to study the actual cases of Blockchain applied in the reformation of privacy and security field by discussing its impacts as well as the opportunities and challenges.        △ Less","1 September, 2020",cs.CR,
              Evaluation of machine learning algorithms for Health and Wellness applications: a tutorial          ,2008.13690,https://arxiv.org/abs/2008.13690,https://arxiv.org/pdf/2008.13690,"Authors:JussiTohka,MarkvanGils","        Research on decision support applications in healthcare, such as those related to diagnosis, prediction, treatment planning, etc., have seen enormously increased interest recently. This development is thanks to the increase in data availability as well as advances in artificial intelligence and machine learning research. Highly promising research examples are published daily. However, at the same time, there are some unrealistic expectations with regards to the requirements for reliable development and objective validation that is needed in healthcare settings. These expectations may lead to unmet schedules and disappointments (or non-uptake) at the end-user side. It is the aim of this tutorial to provide practical guidance on how to assess performance reliably and efficiently and avoid common traps. Instead of giving a list of do's and don't s, this tutorial tries to build a better understanding behind these do's and don't s and presents both the most relevant performance evaluation criteria as well as how to compute them. Along the way, we will indicate common mistakes and provide references discussing various topics more in-depth.        △ Less","31 August, 2020","cs.LG,stat.ML",
              Estimating Individual Treatment Effects with Time-Varying Confounders          ,2008.13620,https://arxiv.org/abs/2008.13620,https://arxiv.org/pdf/2008.13620,"Authors:RuoqiLiu,ChangchangYin,PingZhang","        Estimating the individual treatment effect (ITE) from observational data is meaningful and practical in healthcare. Existing work mainly relies on the strong ignorability assumption that no hidden confounders exist, which may lead to bias in estimating causal effects. Some studies consider the hidden confounders are designed for static environment and not easily adaptable to a dynamic setting. In fact, most observational data (e.g., electronic medical records) is naturally dynamic and consists of sequential information. In this paper, we propose Deep Sequential Weighting (DSW) for estimating ITE with time-varying confounders. Specifically, DSW infers the hidden confounders by incorporating the current treatment assignments and historical information using a deep recurrent weighting neural network. The learned representations of hidden confounders combined with current observed data are leveraged for potential outcome and treatment predictions. We compute the time-varying inverse probabilities of treatment for re-weighting the population. We conduct comprehensive comparison experiments on fully-synthetic, semi-synthetic and real-world datasets to evaluate the performance of our model and baselines. Results demonstrate that our model can generate unbiased and accurate treatment effect by conditioning both time-varying observed and hidden confounders, paving the way for personalized medicine.        △ Less","26 August, 2020","stat.ME,stat.AP,stat.ML",
              Effective Transfer Learning for Identifying Similar Questions: Matching User Questions to COVID-19 FAQs          ,2008.13546,https://arxiv.org/abs/2008.13546,https://arxiv.org/pdf/2008.13546,"Authors:ClaraH.McCreery,NamitKatariya,AnithaKannan,ManishChablani,XavierAmatriain","        People increasingly search online for answers to their medical questions but the rate at which medical questions are asked online significantly exceeds the capacity of qualified people to answer them. This leaves many questions unanswered or inadequately answered. Many of these questions are not unique, and reliable identification of similar questions would enable more efficient and effective question answering schema. COVID-19 has only exacerbated this problem. Almost every government agency and healthcare organization has tried to meet the informational need of users by building online FAQs, but there is no way for people to ask their question and know if it is answered on one of these pages. While many research efforts have focused on the problem of general question similarity, these approaches do not generalize well to domains that require expert knowledge to determine semantic similarity, such as the medical domain. In this paper, we show how a double fine-tuning approach of pretraining a neural network on medical question-answer pairs followed by fine-tuning on medical question-question pairs is a particularly useful intermediate task for the ultimate goal of determining medical question similarity. While other pretraining tasks yield an accuracy below 78.7% on this task, our model achieves an accuracy of 82.6% with the same number of training examples, an accuracy of 80.0% with a much smaller training set, and an accuracy of 84.5% when the full corpus of medical question-answer data is used. We also describe a currently live system that uses the trained model to match user questions to COVID-related FAQs.        △ Less","4 August, 2020","cs.IR,cs.CL,cs.LG",
              Real-time Prediction of COVID-19 related Mortality using Electronic Health Records          ,2008.13412,https://arxiv.org/abs/2008.13412,https://arxiv.org/pdf/2008.13412,"Authors:PatrickSchwab,ArashMehrjou,SonaliParbhoo,LeoAnthonyCeli,JürgenHetzel,MarkusHofer,BernhardSchölkopf,StefanBauer","        Coronavirus Disease 2019 (COVID-19) is an emerging respiratory disease caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) with rapid human-to-human transmission and a high case fatality rate particularly in older patients. Due to the exponential growth of infections, many healthcare systems across the world are under pressure to care for increasing amounts of at-risk patients. Given the high number of infected patients, identifying patients with the highest mortality risk early is critical to enable effective intervention and optimal prioritisation of care. Here, we present the COVID-19 Early Warning System (CovEWS), a clinical risk scoring system for assessing COVID-19 related mortality risk. CovEWS provides continuous real-time risk scores for individual patients with clinically meaningful predictive performance up to 192 hours (8 days) in advance, and is automatically derived from patients' electronic health records (EHRs) using machine learning. We trained and evaluated CovEWS using de-identified data from a cohort of 66430 COVID-19 positive patients seen at over 69 healthcare institutions in the United States (US), Australia, Malaysia and India amounting to an aggregated total of over 2863 years of patient observation time. On an external test cohort of 5005 patients, CovEWS predicts COVID-19 related mortality from 78.8%78.8\% (95%95\% confidence interval [CI]: 76.076.0, 84.7%84.7\%) to 69.4%69.4\% (95%95\% CI: 57.6,75.2%57.6, 75.2\%) specificity at a sensitivity greater than 95%95\% between respectively 1 and 192 hours prior to observed mortality events - significantly outperforming existing generic and COVID-19 specific clinical risk scores. CovEWS could enable clinicians to intervene at an earlier stage, and may therefore help in preventing or mitigating COVID-19 related mortality.        △ Less","31 August, 2020","stat.AP,cs.LG,q-bio.QM",
              The Battle of Infinity: Explosive Demand Surge vs Gigantic Service Providers          ,2008.13360,https://arxiv.org/abs/2008.13360,https://arxiv.org/pdf/2008.13360,Authors:HiroshiToyoizumi,"        Infinite server queues have ultimate processing power to accommodate explosive demand surges. We provide a new stability criterion based on the Borel-Cantelli lemma to judge whether the infinite server safely accommodates heavy-tailed demands. We illustrate the battles between heavy-tailed demand and infinite servers in detail. In particular, we show some cases where the explosive demand overwhelms the infinite server queue. The medical demand caused by pandemics such as the COVID- 19 creates huge stress to the healthcare system. This framework indicates that healthcare systems need to account for the tail behavior of the cluster size and hospital stay length distributions to check the stability of their systems during pandemics.        △ Less","31 August, 2020",math.PR,
              Security Awareness of End-Users of Mobile Health Applications: An Empirical Study          ,2008.13009,https://arxiv.org/abs/2008.13009,https://arxiv.org/pdf/2008.13009,"Authors:BakheetAljedaani,AakashAhmad,MansoorehZahedi,M.AliBabar","        Mobile systems offer portable and interactive computing, empowering users, to exploit a multitude of context-sensitive services, including mobile healthcare. Mobile health applications (i.e., mHealth apps) are revolutionizing the healthcare sector by enabling stakeholders to produce and consume healthcare services. A widespread adoption of mHealth technologies and rapid increase in mHealth apps entail a critical challenge, i.e., lack of security awareness by end-users regarding health-critical data. This paper presents an empirical study aimed at exploring the security awareness of end-users of mHealth apps. We collaborated with two mHealth providers in Saudi Arabia to gather data from 101 end-users. The results reveal that despite having the required knowledge, end-users lack appropriate behaviour , i.e., reluctance or lack of understanding to adopt security practices, compromising health-critical data with social, legal, and financial consequences. The results emphasize that mHealth providers should ensure security training of end-users (e.g., threat analysis workshops), promote best practices to enforce security (e.g., multi-step authentication), and adopt suitable mHealth apps (e.g., trade-offs for security vs usability). The study provides empirical evidence and a set of guidelines about security awareness of mHealth apps.        △ Less","29 August, 2020",cs.SE,
              Hierarchical Deep Learning Ensemble to Automate the Classification of Breast Cancer Pathology Reports by ICD-O Topography          ,2008.12571,https://arxiv.org/abs/2008.12571,https://arxiv.org/pdf/2008.12571,"Authors:WaheedaSaib,DavidSengeh,GcininweDlamini,ElviraSingh","        Like most global cancer registries, the National Cancer Registry in South Africa employs expert human coders to label pathology reports using appropriate International Classification of Disease for Oncology (ICD-O) codes spanning 42 different cancer types. The annotation is extensive for the large volume of cancer pathology reports the registry receives annually from public and private sector institutions. This manual process, coupled with other challenges results in a significant 4-year lag in reporting of annual cancer statistics in South Africa. We present a hierarchical deep learning ensemble method incorporating state of the art convolutional neural network models for the automatic labelling of 2201 de-identified, free text pathology reports, with appropriate ICD-O breast cancer topography codes across 8 classes. Our results show an improvement in primary site classification over the state of the art CNN model by greater than 14% for F1 micro and 55% for F1 macro scores. We demonstrate that the hierarchical deep learning ensemble improves on state-of-the-art models for ICD-O topography classification in comparison to a flat multiclass model for predicting ICD-O topography codes for pathology reports.        △ Less","28 August, 2020","cs.LG,cs.CV,eess.IV",
              Exact and Approximation Algorithms for Sparse PCA          ,2008.12438,https://arxiv.org/abs/2008.12438,https://arxiv.org/pdf/2008.12438,"Authors:YongchunLi,WeijunXie","        Sparse PCA (SPCA) is a fundamental model in machine learning and data analytics, which has witnessed a variety of application areas such as finance, manufacturing, biology, healthcare. To select a prespecified-size principal submatrix from a covariance matrix to maximize its largest eigenvalue for the better interpretability purpose, SPCA advances the conventional PCA with both feature selection and dimensionality reduction. This paper proposes two exact mixed-integer SDPs (MISDPs) by exploiting the spectral decomposition of the covariance matrix and the properties of the largest eigenvalues. We then analyze the theoretical optimality gaps of their continuous relaxation values and prove that they are stronger than that of the state-of-art one. We further show that the continuous relaxations of two MISDPs can be recast as saddle point problems without involving semi-definite cones, and thus can be effectively solved by first-order methods such as the subgradient method. Since off-the-shelf solvers, in general, have difficulty in solving MISDPs, we approximate SPCA with arbitrary accuracy by a mixed-integer linear program (MILP) of a similar size as MISDPs. To be more scalable, we also analyze greedy and local search algorithms, prove their first-known approximation ratios, and show that the approximation ratios are tight. Our numerical study demonstrates that the continuous relaxation values of the proposed MISDPs are quite close to optimality, the proposed MILP model can solve small and medium-size instances to optimality, and the approximation algorithms work very well for all the instances. Finally, we extend the analyses to Rank-one Sparse SVD (R1-SSVD) with non-symmetric matrices and Sparse Fair PCA (SFPCA) when there are multiple covariance matrices, each corresponding to a protected group.        △ Less","27 August, 2020","stat.ML,cs.LG,math.OC",
              A New Dynamic Model to Predict the Effects of Governmental Decisions on the Progress of the CoViD-19 Epidemic          ,2008.11716,https://arxiv.org/abs/2008.11716,https://arxiv.org/pdf/2008.11716,"Authors:KamranSoltani,GhaderRezazadeh","        We have established a novel mathematical model that considers various aspects of the spreading of the virus, including, the transmission based on being in the latent period, environment to human transmission, governmental decisions, and control measures. To accomplish this, a compartmental model with eight batches (sub-population groups) has been proposed and the simulation of the set of differential equations has been conducted to show the effects of the various involved parameters. Also, to achieve more accurate results and closer to reality, the coefficients of a system of differential equations containing transmission rates, death rates, recovery rates and etc. have been proposed by some new step-functions viewpoint. Results: First of all, the efficiency of the proposed model has been shown for Iran and Italy, which completely denoted the flexibility of our model for predicting the epidemic progress and its moment behavior. The model has shown that the reopening plans and governmental measures directly affect the number of active cases of the disease. Also, it has specified that even releasing a small portion of the population (about 2-3 percent) can lead to a severe increase in active patients and consequently multiple waves in the disease progress. The effects of the healthcare capacities of the country have been obtained (quantitatively), which clearly specify the importance of this context. Control strategies including strict implementation of mitigation (reducing the transmission rates) and re-quarantine of some portion of population have been investigated and their efficiency has been shown.        △ Less","15 August, 2020","q-bio.QM,physics.soc-ph",
              Effectiveness of Common Fabrics to Block Aqueous Aerosols of COVID Virus-like Nanoparticles          ,2008.11622,https://arxiv.org/abs/2008.11622,https://arxiv.org/pdf/2008.11622,"Authors:StevenR.Lustig,JohnJ.S.Biswakarma,DevyeshRana,SusanH.Tilford,WeikeHu,MingSu,MichaelS.Rosenblatt","        Layered systems of commonly available fabric materials can be used by the public and healthcare providers in face masks to reduce the risk of inhaling viruses with protection about equivalent or better than the filtration and adsorption offered by 5-layer N95 respirators. Over 70 different common fabric combinations and masks were evaluated under steady state, forced convection air flux with pulsed aerosols that simulate forceful respiration. The aerosols contain fluorescent virus-like nanoparticles to track transmission through materials that greatly assist the accuracy of detection, thus avoiding artifacts including pore flooding and the loss of aerosol due to evaporation and droplet break-up. Effective materials comprise both absorbent, hydrophilic layers and barrier, hydrophobic layers. Although the hydrophobic layers can adhere virus-like nanoparticles, they may also repel droplets from adjacent absorbent layers and prevent wicking transport across the fabric system. Effective designs are noted with absorbent layers comprising terry cloth towel, quilting cotton and flannel. Effective designs are noted with barrier layers comprising non-woven polypropylene, polyester and polyaramid.        △ Less","26 August, 2020","physics.app-ph,q-bio.QM",10.1021/acsnano.0c03972 
              Smart Healthcare for Diabetes: A COVID-19 Perspective          ,2008.11153,https://arxiv.org/abs/2008.11153,https://arxiv.org/pdf/2008.11153,"Authors:AmitM.Joshi,UrvashiP.Shukla,SarajuP.Mohanty","        Diabetes is considered as an critical comorbidity linked with the latest coronavirus disease 2019 (COVID-19) which spreads through Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-Cov-2). The diabetic patients have higher threat of infection from novel corona virus. Depending on the region in the globe, 20% to 50% of patients infected with COVID-19 pandemic had diabetes. The current article discussed the risk associated with diabetic patients and also recommendation for controlling diabetes during this pandemic situation. The article also discusses the case study of COVID-19 at various regions around the globe and the preventive actions taken by various countries to control the effect from the virus. The article presents several smart healthcare solutions for the diabetes patients to have glucose insulin control for the protection against COVID-19.        △ Less","29 July, 2020",q-bio.OT,
              Using Deep Networks for Scientific Discovery in Physiological Signals          ,2008.10936,https://arxiv.org/abs/2008.10936,https://arxiv.org/pdf/2008.10936,"Authors:TomBeer,BarEini-Porat,SebastianGoodfellow,DannyEytan,UriShalit","        Deep neural networks (DNN) have shown remarkable success in the classification of physiological signals. In this study we propose a method for examining to what extent does a DNN's performance rely on rediscovering existing features of the signals, as opposed to discovering genuinely new features. Moreover, we offer a novel method of ""removing"" a hand-engineered feature from the network's hypothesis space, thus forcing it to try and learn representations which are different from known ones, as a method of scientific exploration. We then build on existing work in the field of interpretability, specifically class activation maps, to try and infer what new features the network has learned. We demonstrate this approach using ECG and EEG signals. With respect to ECG signals we show that for the specific task of classifying atrial fibrillation, DNNs are likely rediscovering known features. We also show how our method could be used to discover new features, by selectively removing some ECG features and ""rediscovering"" them. We further examine how could our method be used as a tool for examining scientific hypotheses. We simulate this scenario by looking into the importance of eye movements in classifying sleep from EEG. We show that our tool can successfully focus a researcher's attention by bringing to light patterns in the data that would be hidden otherwise.        △ Less","25 August, 2020","stat.ML,cs.LG",
"              Precision Health Data: Requirements, Challenges and Existing Techniques for Data Security and Privacy          ",2008.10733,https://arxiv.org/abs/2008.10733,https://arxiv.org/pdf/2008.10733,"Authors:ChandraThapa,SeyitCamtepe","        Precision health leverages information from various sources, including omics, lifestyle, environment, social media, medical records, and medical insurance claims to enable personalized care, prevent and predict illness, and precise treatments. It extensively uses sensing technologies (e.g., electronic health monitoring devices), computations (e.g., machine learning), and communication (e.g., interaction between the health data centers). As health data contain sensitive private information, including the identity of patient and carer and medical conditions of the patient, proper care is required at all times. Leakage of these private information affects the personal life, including bullying, high insurance premium, and loss of job due to the medical history. Thus, the security, privacy of and trust on the information are of utmost importance. Moreover, government legislation and ethics committees demand the security and privacy of healthcare data. Herein, in the light of precision health data security, privacy, ethical and regulatory requirements, finding the best methods and techniques for the utilization of the health data, and thus precision health is essential. In this regard, firstly, this paper explores the regulations, ethical guidelines around the world, and domain-specific needs. Then it presents the requirements and investigates the associated challenges. Secondly, this paper investigates secure and privacy-preserving machine learning methods suitable for the computation of precision health data along with their usage in relevant health projects. Finally, it illustrates the best available techniques for precision health data security and privacy with a conceptual system model that enables compliance, ethics clearance, consent management, medical innovations, and developments in the health domain.        △ Less","24 August, 2020","cs.CR,cs.AI",
              A Context Aware Framework for IoT Based Healthcare Monitoring Systems          ,2008.10341,https://arxiv.org/abs/2008.10341,https://arxiv.org/pdf/2008.10341,Authors:YousefAbuseta,"        This paper introduces an investigation of the healthcare monitoring systems and their provisioning in the IoT platform. The different roles that exist in healthcare systems are specified and modeled here. This paper also attempts to introduce and propose a generic framework for the design and development of context aware healthcare monitoring systems in the IoT platform. In such a framework, the fundamental components of the healthcare monitoring systems are identified and modelled as well as the relationship between these components. The paper also stresses on the crucial role played by the AI field in addressing resilient context aware healthcare monitoring systems. Architecturally, this framework is based on a distributed layered architecture where the different components are deployed over the physical layer, fog platform and the cloud platform.        △ Less","21 August, 2020","cs.SE,cs.CY",
              A systematic literature review on machine learning applications for consumer sentiment analysis using online reviews          ,2008.10282,https://arxiv.org/abs/2008.10282,https://arxiv.org/pdf/2008.10282,"Authors:PraphulaKumarJain,RajendraPamula","        Consumer sentiment analysis is a recent fad for social media related applications such as healthcare, crime, finance, travel, and academics. Disentangling consumer perception to gain insight into the desired objective and reviews is significant. With the advancement of technology, a massive amount of social web-data increasing in terms of volume, subjectivity, and heterogeneity, becomes challenging to process it manually. Machine learning techniques have been utilized to handle this difficulty in real-life applications. This paper presents the study to find out the usefulness, scope, and applicability of this alliance of Machine Learning techniques for consumer sentiment analysis on online reviews in the domain of hospitality and tourism. We have shown a systematic literature review to compare, analyze, explore, and understand the attempts and direction in a proper way to find research gaps to illustrating the future scope of this pairing. This work is contributing to the extant literature in two ways; firstly, the primary objective is to read and analyze the use of machine learning techniques for consumer sentiment analysis on online reviews in the domain of hospitality and tourism. Secondly, in this work, we presented a systematic approach to identify, collect observational evidence, results from the analysis, and assimilate observations of all related high-quality research to address particular research queries referring to the described research area.        △ Less","24 August, 2020",cs.HC,
              Handling of uncertainty in medical data using machine learning and probability theory techniques: A review of 30 years (1991-2020)          ,2008.10114,https://arxiv.org/abs/2008.10114,https://arxiv.org/pdf/2008.10114,"Authors:RoohallahAlizadehsani,MohamadRoshanzamir,SadiqHussain,AbbasKhosravi,AfsanehKoohestani,MohammadHosseinZangooei,MoloudAbdar,AdhamBeykikhoshk,AfshinShoeibi,AssefZare,MaryamPanahiazar,SaeidNahavandi,DiptiSrinivasan,AmirF.Atiya,U.RajendraAcharya","        Understanding data and reaching valid conclusions are of paramount importance in the present era of big data. Machine learning and probability theory methods have widespread application for this purpose in different fields. One critically important yet less explored aspect is how data and model uncertainties are captured and analyzed. Proper quantification of uncertainty provides valuable information for optimal decision making. This paper reviewed related studies conducted in the last 30 years (from 1991 to 2020) in handling uncertainties in medical data using probability theory and machine learning techniques. Medical data is more prone to uncertainty due to the presence of noise in the data. So, it is very important to have clean medical data without any noise to get accurate diagnosis. The sources of noise in the medical data need to be known to address this issue. Based on the medical data obtained by the physician, diagnosis of disease, and treatment plan are prescribed. Hence, the uncertainty is growing in healthcare and there is limited knowledge to address these problems. We have little knowledge about the optimal treatment methods as there are many sources of uncertainty in medical science. Our findings indicate that there are few challenges to be addressed in handling the uncertainty in medical raw data and new models. In this work, we have summarized various methods employed to overcome this problem. Nowadays, application of novel deep learning techniques to deal such uncertainties have significantly increased.        △ Less","23 August, 2020",cs.AI,
              Blockchain-enabled Internet of Medical Things to Combat COVID-19          ,2008.09933,https://arxiv.org/abs/2008.09933,https://arxiv.org/pdf/2008.09933,"Authors:Hong-NingDai,MuhammadImran,NomanHaider","        We are experiencing an unprecedented healthcare crisis caused by newly-discovered corona-virus disease (COVID-19). The outbreaks of COVID-19 reveal the frailties of existing healthcare systems. Therefore, the digital transformation of healthcare systems becomes an inevitable trend. During this process, the Internet of Medical Things (IoMT) plays a crucial role while intrinsic vulnerabilities of security and privacy deter the wide adoption of IoMT. In this article, we present a blockchain-enabled IoMT to address the security and privacy concerns of IoMT systems. We also discuss the solutions brought by blockchain-enabled IoMT to COVID-19 from five different perspectives. Moreover, we outline the open challenges and future directions of blockchain-enabled IoMT.        △ Less","22 August, 2020","cs.CY,cs.CR,cs.NI",
              A(DP)2^2SGD: Asynchronous Decentralized Parallel Stochastic Gradient Descent with Differential Privacy          ,2008.09246,https://arxiv.org/abs/2008.09246,https://arxiv.org/pdf/2008.09246,"Authors:JieXu,WeiZhang,FeiWang","        As deep learning models are usually massive and complex, distributed learning is essential for increasing training efficiency. Moreover, in many real-world application scenarios like healthcare, distributed learning can also keep the data local and protect privacy. A popular distributed learning strategy is federated learning, where there is a central server storing the global model and a set of local computing nodes updating the model parameters with their corresponding data. The updated model parameters will be processed and transmitted to the central server, which leads to heavy communication costs. Recently, asynchronous decentralized distributed learning has been proposed and demonstrated to be a more efficient and practical strategy where there is no central server, so that each computing node only communicates with its neighbors. Although no raw data will be transmitted across different local nodes, there is still a risk of information leak during the communication process for malicious participants to make attacks. In this paper, we present a differentially private version of asynchronous decentralized parallel SGD (ADPSGD) framework, or A(DP)2^2SGD for short, which maintains communication efficiency of ADPSGD and prevents the inference from malicious participants. Specifically, R{é}nyi differential privacy is used to provide tighter privacy analysis for our composite Gaussian mechanisms while the convergence rate is consistent with the non-private version. Theoretical analysis shows A(DP)2^2SGD also converges at the optimal O(1/T−−√)\mathcal{O}(1/\sqrt{T}) rate as SGD. Empirically, A(DP)2^2SGD achieves comparable model accuracy as the differentially private version of Synchronous SGD (SSGD) but runs much faster than SSGD in heterogeneous computing environments.        △ Less","20 August, 2020","cs.LG,cs.DC,stat.ML",
              Development of a Novel Computational Model for Evaluating Fall Risk in Patient Room Design          ,2008.09169,https://arxiv.org/abs/2008.09169,https://arxiv.org/pdf/2008.09169,"Authors:RoyaSabbaghNovin,EllenTaylor,TuckerHermans,AndrewMerryweather","        Objectives: The aims of this study are to identify factors in physical environments that contribute to patient falls in hospitals and to propose a computational model to evaluate patient room designs.  Background: The existing fall risk assessment tools have an acceptable level of sensitivity and specificity, however, they only consider intrinsic factors and medications, making the prediction very limited in terms of how the physical environment contributes to fall risk.  Methods: We provide a computational model for risk of fall based on physical-environment and patient-motion factors. We use a trajectory optimization approach for patient motion prediction.  Results: We run the proposed model on four room designs as examples of various room design categories. Results show the capabilities of the proposed model in identifying risky locations within the room.  Conclusions: Our study shows the potential capabilities of the proposed model. Due to lack of enough evidence for the examined factors, it is not possible at this point to gain robust confidence in the final evaluations. More studies using quantitative, relational, or causal designs are recommended to inform the proposed model for patient falls.  Application: Developing a comprehensive fall risk model is a significant step in understanding and solving the problem of patient falls in hospitals. It can provide guidance for healthcare decision makers to optimize effective interventions to reduce risk of falls while promoting safe patient mobility in the hospital room environment. We can also use it in healthcare technologies such as assistive robots to provide informed assistance.        △ Less","28 August, 2020",cs.CE,
              A Data-Efficient Deep Learning Based Smartphone Application For Detection Of Pulmonary Diseases Using Chest X-rays          ,2008.08912,https://arxiv.org/abs/2008.08912,https://arxiv.org/pdf/2008.08912,"Authors:HrithwikShalu,HarikrishnanP,AkashDas,MegdutMandal,HarshavardhanMSali,JunedKadiwala","        This paper introduces a paradigm of smartphone application based disease diagnostics that may completely revolutionise the way healthcare services are being provided. Although primarily aimed to assist the problems in rendering the healthcare services during the coronavirus pandemic, the model can also be extended to identify the exact disease that the patient is caught with from a broad spectrum of pulmonary diseases. The app inputs Chest X-Ray images captured from the mobile camera which is then relayed to the AI architecture in a cloud platform, and diagnoses the disease with state of the art accuracy. Doctors with a smartphone can leverage the application to save the considerable time that standard COVID-19 tests take for preliminary diagnosis. The scarcity of training data and class imbalance issues were effectively tackled in our approach by the use of Data Augmentation Generative Adversarial Network (DAGAN) and model architecture based as a Convolutional Siamese Network with attention mechanism. The backend model was tested for robustness us-ing publicly available datasets under two different classification scenarios(Binary/Multiclass) with minimal and noisy data. The model achieved pinnacle testing accuracy of 99.30% and 98.40% on the two respective scenarios, making it completely reliable for its users. On top of that a semi-live training scenario was introduced, which helps improve the app performance over time as data accumulates. Overall, the problems of generalisability of complex models and data inefficiency is tackled through the model architecture. The app based setting with semi live training helps in ease of access to reliable healthcare in the society, as well as help ineffective research of rare diseases in a minimal data setting.        △ Less","19 August, 2020","eess.IV,cs.LG,stat.ML",
              Deep learning-based transformation of the H&E stain into special stains improves kidney disease diagnosis          ,2008.08871,https://arxiv.org/abs/2008.08871,https://arxiv.org/pdf/2008.08871,"Authors:KevindeHaan,YijieZhang,TairanLiu,AnthonyE.Sisk,MiguelF.P.Diaz,JonathanE.Zuckerman,YairRivenson,W.DeanWallace,AydoganOzcan","        Pathology is practiced by visual inspection of histochemically stained slides. Most commonly, the hematoxylin and eosin (H&E) stain is used in the diagnostic workflow and it is the gold standard for cancer diagnosis. However, in many cases, especially for non-neoplastic diseases, additional ""special stains"" are used to provide different levels of contrast and color to tissue components and allow pathologists to get a clearer diagnostic picture. In this study, we demonstrate the utility of supervised learning-based computational stain transformation from H&E to different special stains (Masson's Trichrome, periodic acid-Schiff and Jones silver stain) using tissue sections from kidney needle core biopsies. Based on evaluation by three renal pathologists, followed by adjudication by a fourth renal pathologist, we show that the generation of virtual special stains from existing H&E images improves the diagnosis in several non-neoplastic kidney diseases, sampled from 16 unique subjects. Adjudication of N=48 diagnoses from the three pathologists revealed that the virtually generated special stains yielded 22 improvements (45.8%), 23 concordances (47.9%) and 3 discordances (6.3%), when compared against the use of H&E stained tissue only. As the virtual transformation of H&E images into special stains can be achieved in less than 1 min per patient core specimen slide, this stain-to-stain transformation framework can improve the quality of the preliminary diagnosis when additional special stains are needed, along with significant savings in time and cost, reducing the burden on healthcare system and patients.        △ Less","20 August, 2020","eess.IV,cs.CV,cs.LG,physics.med-ph",
              Simultaneously-Collected Multimodal Lying Pose Dataset: Towards In-Bed Human Pose Monitoring under Adverse Vision Conditions          ,2008.08735,https://arxiv.org/abs/2008.08735,https://arxiv.org/pdf/2008.08735,"Authors:ShuangjunLiu,XiaofeiHuang,NihangFu,ChengLi,ZhongnanSu,SarahOstadabbas","        Computer vision (CV) has achieved great success in interpreting semantic meanings from images, yet CV algorithms can be brittle for tasks with adverse vision conditions and the ones suffering from data/label pair limitation. One of this tasks is in-bed human pose estimation, which has significant values in many healthcare applications. In-bed pose monitoring in natural settings could involve complete darkness or full occlusion. Furthermore, the lack of publicly available in-bed pose datasets hinders the use of many successful pose estimation algorithms for this task. In this paper, we introduce our Simultaneously-collected multimodal Lying Pose (SLP) dataset, which includes in-bed pose images from 109 participants captured using multiple imaging modalities including RGB, long wave infrared, depth, and pressure map. We also present a physical hyper parameter tuning strategy for ground truth pose label generation under extreme conditions such as lights off and being fully covered by a sheet/blanket. SLP design is compatible with the mainstream human pose datasets, therefore, the state-of-the-art 2D pose estimation models can be trained effectively with SLP data with promising performance as high as 95% at PCKh@0.5 on a single modality. The pose estimation performance can be further improved by including additional modalities through collaboration.        △ Less","19 August, 2020",cs.CV,
              SegCodeNet: Color-Coded Segmentation Masks for Activity Detection from Wearable Cameras          ,2008.08452,https://arxiv.org/abs/2008.08452,https://arxiv.org/pdf/2008.08452,"Authors:AsifShahriyarSushmit,ParthoGhosh,Md.AbrarIstiak,NayeebRashid,AhsanHabibAkash,TaufiqHasan","        Activity detection from first-person videos (FPV) captured using a wearable camera is an active research field with potential applications in many sectors, including healthcare, law enforcement, and rehabilitation. State-of-the-art methods use optical flow-based hybrid techniques that rely on features derived from the motion of objects from consecutive frames. In this work, we developed a two-stream network, the \emph{SegCodeNet}, that uses a network branch containing video-streams with color-coded semantic segmentation masks of relevant objects in addition to the original RGB video-stream. We also include a stream-wise attention gating that prioritizes between the two streams and a frame-wise attention module that prioritizes the video frames that contain relevant features. Experiments are conducted on an FPV dataset containing 1818 activity classes in office environments. In comparison to a single-stream network, the proposed two-stream method achieves an absolute improvement of 14.366%14.366\% and 10.324%10.324\% for averaged F1 score and accuracy, respectively, when average results are compared for three different frame sizes 224×224224\times224, 112×112112\times112, and 64×6464\times64. The proposed method provides significant performance gains for lower-resolution images with absolute improvements of 17%17\% and 26%26\% in F1 score for input dimensions of 112×112112\times112 and 64×6464\times64, respectively. The best performance is achieved for a frame size of 224×224224\times224 yielding an F1 score and accuracy of 90.176%90.176\% and 90.799%90.799\% which outperforms the state-of-the-art Inflated 3D ConvNet (I3D) \cite{carreira2017quo} method by an absolute margin of 4.529%4.529\% and 2.419%2.419\%, respectively.        △ Less","19 August, 2020",cs.CV,
              Experts and authorities receive disproportionate attention on Twitter during the COVID-19 crisis          ,2008.08364,https://arxiv.org/abs/2008.08364,https://arxiv.org/pdf/2008.08364,"Authors:KristinaGligorić,ManoelHortaRibeiro,MartinMüller,OlesiaAltunina,MaximePeyrard,MarcelSalathé,GiovanniColavizza,RobertWest","        Timely access to accurate information is crucial during the COVID-19 pandemic. Prompted by key stakeholders' cautioning against an ""infodemic"", we study information sharing on Twitter from January through May 2020. We observe an overall surge in the volume of general as well as COVID-19-related tweets around peak lockdown in March/April 2020. With respect to engagement (retweets and likes), accounts related to healthcare, science, government and politics received by far the largest boosts, whereas accounts related to religion and sports saw a relative decrease in engagement. While the threat of an ""infodemic"" remains, our results show that social media also provide a platform for experts and public authorities to be widely heard during a global crisis.        △ Less","19 August, 2020",cs.SI,
"              The determinants of COVID-19 case fatality rate (CFR) in the Italian regions and provinces: an analysis of environmental, demographic, and healthcare factors          ",2008.08133,https://arxiv.org/abs/2008.08133,https://arxiv.org/pdf/2008.08133,Authors:GaetanoPerone,"        The Italian government has been one of the most responsive to COVID-19 emergency, through the adoption of quick and increasingly stringent measures to contain the outbreak. Despite this, Italy has suffered a huge human and social cost, especially in Lombardy. The aim of this paper is dual: i) first, to investigate the reasons of the case fatality rate (CFR) differences across Italian 20 regions and 107 provinces, using a multivariate OLS regression approach; and ii) second, to build a taxonomy of provinces with similar mortality risk of COVID-19, by using the Ward hierarchical agglomerative clustering method. I considered health system metrics, environmental pollution, climatic conditions, demographic variables, and three ad hoc indexes that represent the health system saturation. The results showed that overall health care efficiency, physician density, and average temperature helped to reduce the CFR. By the contrary, population aged 70 and above, car and firm density, level of air pollutants (NO2, O3, PM10, and PM2.5), relative average humidity, COVID-19 prevalence, and all three indexes of health system saturation were positively associated with the CFR. Population density, social vertical integration, and altitude were not statistically significant. In particular, the risk of dying increases with age, as 90 years old and above had a three-fold greater risk than the 80 to 89 years old and four-fold greater risk than 70 to 79 years old. Moreover, the cluster analysis showed that the highest mortality risk was concentrated in the north of the country, while the lowest risk was associated with southern provinces. Finally, since prevalence and health system saturation indexes played the most important role in explaining the CFR variability, a significant part of the latter may have been caused by the massive stress of the Italian health system.        △ Less","1 August, 2020","physics.soc-ph,q-bio.PE",10.1016/j.scitotenv.2020.142523 
              Trust and Medical AI: The challenges we face and the expertise needed to overcome them          ,2008.07734,https://arxiv.org/abs/2008.07734,https://arxiv.org/pdf/2008.07734,"Authors:ThomasP.Quinn,ManishaSenadeera,StephanJacobs,SimonCoghlan,VuongLe","        Artificial intelligence (AI) is increasingly of tremendous interest in the medical field. However, failures of medical AI could have serious consequences for both clinical outcomes and the patient experience. These consequences could erode public trust in AI, which could in turn undermine trust in our healthcare institutions. This article makes two contributions. First, it describes the major conceptual, technical, and humanistic challenges in medical AI. Second, it proposes a solution that hinges on the education and accreditation of new expert groups who specialize in the development, verification, and operation of medical AI technologies. These groups will be required to maintain trust in our healthcare institutions.        △ Less","18 August, 2020","cs.AI,cs.CY",
              RTFN: Robust Temporal Feature Network          ,2008.07707,https://arxiv.org/abs/2008.07707,https://arxiv.org/pdf/2008.07707,"Authors:ZhiwenXiao,XinXu,HuanlaiXing,JuanChen","        Time series analysis plays a vital role in various applications, for instance, healthcare, weather prediction, disaster forecast, etc. However, to obtain sufficient shapelets by a feature network is still challenging. To this end, we propose a novel robust temporal feature network (RTFN) that contains temporal feature networks and attentional LSTM networks. The temporal feature networks are built to extract basic features from input data while the attentional LSTM networks are devised to capture complicated shapelets and relationships to enrich features. In experiments, we embed RTFN into supervised structure as a feature extraction network and into unsupervised clustering as an encoder, respectively. The results show that the RTFN-based supervised structure is a winner of 40 out of 85 datasets and the RTFN-based unsupervised clustering performs the best on 4 out of 11 datasets in the UCR2018 archive.        △ Less","17 August, 2020","cs.LG,stat.ML",
              Estimation of causal effects of multiple treatments in healthcare database studies with rare outcomes          ,2008.07687,https://arxiv.org/abs/2008.07687,https://arxiv.org/pdf/2008.07687,"Authors:LiangyuanHu,ChenyangGu","        The preponderance of large-scale healthcare databases provide abundant opportunities for comparative effectiveness research. Evidence necessary to making informed treatment decisions often relies on comparing effectiveness of multiple treatment options on outcomes of interest observed in a small number of individuals. Causal inference with multiple treatments and rare outcomes is a subject that has been treated sparingly in the literature. This paper designs three sets of simulations, representative of the structure of our healthcare database study, and propose causal analysis strategies for such settings. We investigate and compare the operating characteristics of three types of methods and their variants: Bayesian Additive Regression Trees (BART), regression adjustment on multivariate spline of generalized propensity scores (RAMS) and inverse probability of treatment weighting (IPTW) with multinomial logistic regression or generalized boosted models. Our results suggest that BART and RAMS provide lower bias and mean squared error, and the widely used IPTW methods deliver unfavorable operating characteristics. We illustrate the methods using a case study evaluating the comparative effectiveness of robotic-assisted surgery, video-assisted thoracoscopic surgery and open thoracotomy for treating non-small cell lung cancer.        △ Less","2 October, 2020","stat.ME,cs.CY,stat.ML",
              Learning from Irregularly-Sampled Time Series: A Missing Data Perspective          ,2008.07599,https://arxiv.org/abs/2008.07599,https://arxiv.org/pdf/2008.07599,"Authors:StevenCheng-XianLi,BenjaminM.Marlin","        Irregularly-sampled time series occur in many domains including healthcare. They can be challenging to model because they do not naturally yield a fixed-dimensional representation as required by many standard machine learning models. In this paper, we consider irregular sampling from the perspective of missing data. We model observed irregularly-sampled time series data as a sequence of index-value pairs sampled from a continuous but unobserved function. We introduce an encoder-decoder framework for learning from such generic indexed sequences. We propose learning methods for this framework based on variational autoencoders and generative adversarial networks. For continuous irregularly-sampled time series, we introduce continuous convolutional layers that can efficiently interface with existing neural network architectures. Experiments show that our models are able to achieve competitive or better classification results on irregularly-sampled multivariate time series compared to recent RNN models while offering significantly faster training times.        △ Less","17 August, 2020","cs.LG,stat.ML",
              A decision integration strategy for short-term demand forecasting and ordering for red blood cell components          ,2008.07486,https://arxiv.org/abs/2008.07486,https://arxiv.org/pdf/2008.07486,"Authors:NaLi,FeiChiang,DouglasG.Down,NancyM.Heddle","        Blood transfusion is one of the most crucial and commonly administered therapeutics worldwide. The need for more accurate and efficient ways to manage blood demand and supply is an increasing concern. Building a technology-based, robust blood demand and supply chain that can achieve the goals of reducing ordering frequency, inventory level, wastage and shortage, while maintaining the safety of blood usage, is essential in modern healthcare systems. In this study, we summarize the key challenges in current demand and supply management for red blood cells (RBCs). We combine ideas from statistical time series modeling, machine learning, and operations research in developing an ordering decision strategy for RBCs, through integrating a hybrid demand forecasting model using clinical predictors and a data-driven multi-period inventory problem considering inventory and reorder constraints. We have applied the integrated ordering strategy to the blood inventory management system in Hamilton, Ontario using a large clinical database from 2008 to 2018. The proposed hybrid demand forecasting model provides robust and accurate predictions, and identifies important clinical predictors for short-term RBC demand forecasting. Compared with the actual historical data, our integrated ordering strategy reduces the inventory level by 40% and decreases the ordering frequency by 60%, with low incidence of shortages and wastage due to expiration. If implemented successfully, our proposed strategy can achieve significant cost savings for healthcare systems and blood suppliers. The proposed ordering strategy is generalizable to other blood products or even other perishable products.        △ Less","17 August, 2020","stat.AP,math.OC",
              Emotion Carrier Recognition from Personal Narratives          ,2008.07481,https://arxiv.org/abs/2008.07481,https://arxiv.org/pdf/2008.07481,"Authors:AniruddhaTammewar,AlessandraCervone,GiuseppeRiccardi","        Personal Narratives (PN) - recollections of facts, events, and thoughts from one's own experience - are often used in everyday conversations. So far, PNs have mainly been explored for tasks such as valence prediction or emotion classification (i.e. happy, sad). However, these tasks might overlook more fine-grained information that could nevertheless prove relevant for understanding PNs. In this work, we propose a novel task for Narrative Understanding: Emotion Carrier Recognition (ECR). We argue that automatic recognition of emotion carriers, the text fragments that carry the emotions of the narrator (i.e. 'loss of a grandpa', 'high school reunion'), from PNs, provides a deeper level of emotion analysis needed, for instance, in the mental healthcare domain. In this work, we explore the task of ECR using a corpus of PNs manually annotated with emotion carriers and investigate different baseline models for the task. Furthermore, we propose several evaluation strategies for the task. Based on the inter-annotator agreement, the task in itself was found to be complex and subjective for humans. Nevertheless, we discuss evaluation metrics that could be suitable for applications based on ECR.        △ Less","17 August, 2020",cs.CL,
              Binarised Regression with Instance-Varying Costs: Evaluation using Impact Curves          ,2008.07349,https://arxiv.org/abs/2008.07349,https://arxiv.org/pdf/2008.07349,"Authors:MatthewDirks,DavidPoole","        Many evaluation methods exist, each for a particular prediction task, and there are a number of prediction tasks commonly performed including classification and regression. In binarised regression, binary decisions are generated from a learned regression model (or real-valued dependent variable), which is useful when the division between instances that should be predicted positive or negative depends on the utility. For example, in mining, the boundary between a valuable rock and a waste rock depends on the market price of various metals, which varies with time. This paper proposes impact curves to evaluate binarised regression with instance-varying costs, where some instances are much worse to be classified as positive (or negative) than other instances; e.g., it is much worse to throw away a high-grade gold rock than a medium-grade copper-ore rock, even if the mine wishes to keep both because both are profitable. We show how to construct an impact curve for a variety of domains, including examples from healthcare, mining, and entertainment. Impact curves optimize binary decisions across all utilities of the chosen utility function, identify the conditions where one model may be favoured over another, and quantitatively assess improvement between competing models.        △ Less","14 August, 2020","cs.LG,stat.ML",
              Personality in Healthcare Human Robot Interaction (H-HRI): A Literature Review and Brief Critique          ,2008.06723,https://arxiv.org/abs/2008.06723,https://arxiv.org/pdf/2008.06723,"Authors:ConnorEsterwood,LionelP.Robert","        Robots are becoming an important way to deliver health care, and personality is vital to understanding their effectiveness. Despite this, there is a lack of a systematic overarching understanding of personality in health care human robot interaction (H-HRI). To address this, the authors conducted a review that identified 18 studies on personality in H-HRI. This paper presents the results of that systematic literature review. Insights are derived from this review regarding the methodologies, outcomes, and samples utilized. The authors of this review discuss findings across this literature while identifying several gaps worthy of attention. Overall, this paper is an important starting point in understanding personality in H-HRI.        △ Less","15 August, 2020","cs.CY,cs.AI,cs.HC,cs.RO",10.1145/3406499.3415075 
              Experimental investigations of acoustic curtains for hospital environment noise mitigations          ,2008.06690,https://arxiv.org/abs/2008.06690,https://arxiv.org/pdf/2008.06690,"Authors:SanjayKumar,RuiQinNg,HeowPuehLee","        The continuous increase of hospital noise levels has become a vital challenge for society. The complex soundscapes in the hospital produce unpleasant noise, which may exceed the prescribed noise level for the patients and healthcare professionals. Previous studies have reported that extended exposure to loud noise may cause auditory and nonauditory disorders in healthcare professionals, medical staff, and patients. Therefore, there is an increased interest for the design and fabrication of effective noise barriers for the hospital premises. Herein, we have performed the thorough experimental investigations on the acoustical performances for PVC coated polyester fabrics and 100 % pure PVC sheets. The performances of these potential acoustic curtains have found to be superior to that of existing acoustic curtains for hospitals. Also, the results showed that the sound transmission class rating of PVC curtains are much higher than the existing commercial acoustic curtains.        △ Less","15 August, 2020",physics.app-ph,
              Quantification of BERT Diagnosis Generalizability Across Medical Specialties Using Semantic Dataset Distance          ,2008.06606,https://arxiv.org/abs/2008.06606,https://arxiv.org/pdf/2008.06606,"Authors:MihirP.Khambete,WilliamSu,JuanGarcia,JosephLehar,MartinKang,MarcusA.Badgeley","        Deep learning models in healthcare may fail to generalize on data from unseen corpora. Additionally, no quantitative metric exists to tell how existing models will perform on new data. Previous studies demonstrated that NLP models of medical notes generalize variably between institutions, but ignored other levels of healthcare organization. We measured SciBERT diagnosis sentiment classifier generalizability between medical specialties using EHR sentences from MIMIC-III. Models trained on one specialty performed better on internal test sets than mixed or external test sets (mean AUCs 0.92, 0.87, and 0.83, respectively; p = 0.016). When models are trained on more specialties, they have better test performances (p < 1e-4). Model performance on new corpora is directly correlated to the similarity between train and test sentence content (p < 1e-4). Future studies should assess additional axes of generalization to ensure deep learning models fulfil their intended purpose across institutions, specialties, and practices.        △ Less","20 August, 2020",cs.CL,
              COVID-Robot: Monitoring Social Distancing Constraints in Crowded Scenarios          ,2008.06585,https://arxiv.org/abs/2008.06585,https://arxiv.org/pdf/2008.06585,"Authors:AdarshJaganSathyamoorthy,UtsavPatel,YashAjaySavle,MoumitaPaul,DineshManocha","        Maintaining social distancing norms between humans has become an indispensable precaution to slow down the transmission of COVID-19. We present a novel method to automatically detect pairs of humans in a crowded scenario who are not adhering to the social distance constraint, i.e. about 6 feet of space between them. Our approach makes no assumption about the crowd density or pedestrian walking directions. We use a mobile robot with commodity sensors, namely an RGB-D camera and a 2-D lidar to perform collision-free navigation in a crowd and estimate the distance between all detected individuals in the camera's field of view. In addition, we also equip the robot with a thermal camera that wirelessly transmits thermal images to a security/healthcare personnel who monitors if any individual exhibits a higher than normal temperature. In indoor scenarios, our mobile robot can also be combined with static mounted CCTV cameras to further improve the performance in terms of number of social distancing breaches detected, accurately pursuing walking pedestrians etc. We highlight the performance benefits of our approach in different static and dynamic indoor scenarios.        △ Less","21 August, 2020",cs.RO,
              Performance characterization of a novel deep learning-based MR image reconstruction pipeline          ,2008.06559,https://arxiv.org/abs/2008.06559,https://arxiv.org/pdf/2008.06559,Authors:R.MarcLebel,"        A novel deep learning-based magnetic resonance imaging reconstruction pipeline was designed to address fundamental image quality limitations of conventional reconstruction to provide high-resolution, low-noise MR images. This pipeline's unique aims were to convert truncation artifact into improved image sharpness while jointly denoising images to improve image quality. This new approach, now commercially available at AIR Recon DL (GE Healthcare, Waukesha, WI), includes a deep convolutional neural network (CNN) to aid in the reconstruction of raw data, ultimately producing clean, sharp images. Here we describe key features of this pipeline and its CNN, characterize its performance in digital reference objects, phantoms, and in-vivo, and present sample images and protocol optimization strategies that leverage image quality improvement for reduced scan time. This new deep learning-based reconstruction pipeline represents a powerful new tool to increase the diagnostic and operational performance of an MRI scanner.        △ Less","14 August, 2020","eess.IV,cs.CV",
              Bounding the local average treatment effect in an instrumental variable analysis of engagement with a mobile intervention          ,2008.06473,https://arxiv.org/abs/2008.06473,https://arxiv.org/pdf/2008.06473,"Authors:AndrewJ.Spieker,RobertA.Greevy,LyndsayA.Nelson,LindsayS.Mayberry","        Estimation of local average treatment effects in randomized trials typically requires an assumption known as the exclusion restriction in cases where we are unwilling to rule out unmeasured confounding. Under this assumption, any benefit from treatment would be mediated through the post-randomization variable being conditioned upon, and would be directly attributable to neither the randomization itself nor its latent descendants. Recently, there has been interest in mobile health interventions to provide healthcare support; such studies can feature one-way content and/or two-way content, the latter of which allowing subjects to engage with the intervention in a way that can be objectively measured on a subject-specific level (e.g., proportion of text messages receiving a response). It is hence highly likely that a benefit achieved by the intervention could be explained in part by receipt of the intervention content and in part by engaging with/responding to it. When seeking to characterize average causal effects conditional on post-randomization engagement, the exclusion restriction is therefore all but surely violated. In this paper, we propose a conceptually intuitive sensitivity analysis procedure for this setting that gives rise to sharp bounds on local average treatment effects. A wide array of simulation studies reveal this approach to have very good finite-sample behavior and to recover local average treatment effects under correct specification of the sensitivity parameter. We apply our methodology to a randomized trial evaluating a text message-delivered intervention for Type 2 diabetes self-care.        △ Less","14 August, 2020",stat.ME,
              Properties of materials considered for improvised masks          ,2008.06001,https://arxiv.org/abs/2008.06001,https://arxiv.org/pdf/2008.06001,"Authors:Steven.N.Rogak,TimothyA.Sipkens,MangGuan,HamedNikookar,DanielaVargasFigueroa,JingWang","        During a pandemic in which aerosol and droplet transmission is possible, the demand for masks that meet medical or workplace standards can prevent most individuals or organizations from obtaining suitable protection. Cloth masks are widely believed to impede droplet and aerosol transmission but most are constructed from materials with unknown filtration efficiency, airflow resistance and water resistance. Further, there has been no clear guidance on the most important performance metrics for the materials used by the general public (as opposed to high-risk healthcare settings). Here we provide data on a range of common fabrics that might be used to construct masks. None of the materials were suitable for masks meeting the N95 NIOSH standard, but many could provide useful filtration (>90%) of 3 micron particles (a plausible challenge size for human generated aerosols), with low pressure drop. These were: nonwoven sterile wraps, dried baby wipes and some double-knit cotton materials. Decontamination of N95 masks using isopropyl alcohol produces the expected increase in particle penetration, but for 3 micron particles, filtration efficiency is still well above 95%. Tightly woven thin fabrics, despite having the visual appearance of a good particle barrier, had remarkably low filtration efficiency and high pressure drop. These differences in filtration performance can be partly explained by the material structure; the better structures expose individual fibers to the flow while the poor materials may have small fundamental fibers but these are in tightly bundled yarns. The fit and use of the whole mask are critical factors not addressed in this work. Despite the complexity of the design of a very good mask, it is clear that for the larger aerosol particles, any mask will provide substantial protection to the wearer and those around them.        △ Less","13 August, 2020","physics.app-ph,physics.flu-dyn",
              An epidemiological model with voluntary quarantine strategies governed by evolutionary game dynamics          ,2008.05979,https://arxiv.org/abs/2008.05979,https://arxiv.org/pdf/2008.05979,"Authors:MarcoA.Amaral,MarceloM.deOliveira,MarcoA.Javarone","        During pandemic events, strategies such as social distancing can be fundamental to curb viral spreading. Such actions can reduce the number of simultaneous infections and mitigate the disease spreading, which is relevant to the risk of a healthcare system collapse. Although these strategies can be suggested, their actual implementation may depend on the population perception of the disease risk. The current COVID-19 crisis, for instance, is showing that some individuals are much more prone than others to remain isolated, avoiding unnecessary contacts. With this motivation, we propose an epidemiological SIR model that uses evolutionary game theory to take into account dynamic individual quarantine strategies, intending to combine in a single process social strategies, individual risk perception, and viral spreading. The disease spreads in a population whose agents can choose between self-isolation and a lifestyle careless of any epidemic risk. The strategy adoption is individual and depends on the perceived disease risk compared to the quarantine cost. The game payoff governs the strategy adoption, while the epidemic process governs the agent's health state. At the same time, the infection rate depends on the agent's strategy while the perceived disease risk depends on the fraction of infected agents. Results show recurrent infection waves, which were seen in previous epidemic scenarios with quarantine. Notably, the risk perception is found to be fundamental for controlling the magnitude of the infection peak, while the final infection size is mainly dictated by the infection rates. Low awareness leads to a single and strong infection peak, while a greater disease risk leads to shorter, although more frequent, peaks. The proposed model spontaneously captures relevant aspects of a pandemic event, highlighting the fundamental role of social strategies.        △ Less","13 August, 2020","physics.soc-ph,physics.bio-ph,q-bio.PE",
              A Study on The Effectiveness of Lock-down Measures to Control The Spread of COVID-19          ,2008.05876,https://arxiv.org/abs/2008.05876,https://arxiv.org/pdf/2008.05876,"Authors:SubhasKumarGhosh,SachchitGhosh,SaiShanmukhaNarumanchi","        The ongoing pandemic of coronavirus disease 2019-2020 (COVID-19) is caused by Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2). This pathogenic virus is able to spread asymptotically during its incubation stage through a vulnerable population. Given the state of healthcare, policymakers were urged to contain the spread of infection, minimize stress on the health systems and ensure public safety. Most effective tool that was at their disposal was to close non-essential business and issue a stay home order. In this paper we consider techniques to measure the effectiveness of stringency measures adopted by governments across the world. Analyzing effectiveness of control measures like lock-down allows us to understand whether the decisions made were optimal and resulted in a reduction of burden on the healthcare system. In specific we consider using a synthetic control to construct alternative scenarios and understand what would have been the effect on health if less stringent measures were adopted. We present analysis for The State of New York, United States, Italy and The Indian capital city Delhi and show how lock-down measures has helped and what the counterfactual scenarios would have been in comparison to the current state of affairs. We show that in The State of New York the number of deaths could have been 6 times higher, and in Italy, the number of deaths could have been 3 times higher by 26th of June, 2020.        △ Less","8 August, 2020",physics.soc-ph,
"              The Transformation of Patient-Clinician Relationships With AI-Based Medical Advice: A ""Bring Your Own Algorithm"" Era in Healthcare",2008.05855,https://arxiv.org/abs/2008.05855,https://arxiv.org/pdf/2008.05855,"Authors:OdedNov,YindalonAphinyanaphongs,YvonneW.Lui,DevinMann,MaurizioPorfiri,MarkRiedl,John-RossRizzo,BatiaWiesenfeld","        One of the dramatic trends at the intersection of computing and healthcare has been patients' increased access to medical information, ranging from self-tracked physiological data to genetic data, tests, and scans. Increasingly however, patients and clinicians have access to advanced machine learning-based tools for diagnosis, prediction, and recommendation based on large amounts of data, some of it patient-generated. Consequently, just as organizations have had to deal with a ""Bring Your Own Device"" (BYOD) reality in which employees use their personal devices (phones and tablets) for some aspects of their work, a similar reality of ""Bring Your Own Algorithm"" (BYOA) is emerging in healthcare with its own challenges and support demands. BYOA is changing patient-clinician interactions and the technologies, skills and workflows related to them. In this paper we argue that: (1) BYOA is changing the patient-clinician relationship and the nature of expert work in healthcare, and (2) better patient-clinician-information-interpretation relationships can be facilitated with solutions that integrate technological and organizational perspectives.        △ Less","13 August, 2020",cs.HC,
              Blockchain applications in Healthcare: A model for research          ,2008.05683,https://arxiv.org/abs/2008.05683,https://arxiv.org/pdf/2008.05683,"Authors:AmirHussainZolfaghari,HerbertDaly,MahdiNasiri,RoxanaSharifian","        Blockchain technology has rapidly evolved from an enabling technology for cryptocurrencies to a potential solution to a wider range of problems found in data-centric and distributed systems. Interest in this area has encouraged many recent innovations to address challenges that traditional approaches of design have been unable to meet. Healthcare Information Systems with issues around privacy, interoperability, data integrity, and access control is potentially an area where blockchain technology may have a significant impact. Blockchain, however, is a meta-technology, combining multiple techniques, as it is often important to determine how best to separate concerns in the design and implementation of such systems. This paper proposes a layered approach for the organization of blockchain in healthcare applications. Key issues driving the adoption of this technology are explored. A model presenting the points in each layer is explored. Finally, we present an example of how the perspective we describe can improve the development of Health Information Systems.        △ Less","13 August, 2020","cs.DC,cs.CY",
"              The Past, Present, and Future of COVID-19: A Data-Driven Perspective          ",2008.05531,https://arxiv.org/abs/2008.05531,https://arxiv.org/pdf/2008.05531,"Authors:AjwadAkil,IshratJahanEliza,Md.HasibulHussainHisham,FahimMorshed,NazmusSakib,NuwaisirRabi,AbirMohammadTurza,SriramChellappan,A.B.M.AlimAlIslam","        Epidemics and pandemics have ravaged human life since time. To combat these, novel ideas have always been created and deployed by humanity, with varying degrees of success. At this very moment, the COVID-19 pandemic is the singular global health crisis. Now, perhaps for the first time in human history, almost the whole of humanity is experiencing some form of hardship as a result of one invisible pathogen. This once again entails novel ideas for quick eradication, healing and recovery, whether it is healthcare, banking, travel, education or any other. For efficient policy-making, clear trends of past, present and future are vital for policy-makers. With the global impacts of COVID-19 so severe, equally important is the analysis of correlations between disease spread and various socio-economic and environmental factors. Furthermore, all of these need to be presented in an integrated manner in real-time to facilitate efficient policy making. To address these issues, in this study, we report results on our development and deployment of a web-based integrated real-time operational dashboard as an important decision support system for COVID-19. In our study, we conducted data-driven analysis based on available data from diverse authenticated sources to predict upcoming consequences of the pandemic through rigorous modeling and statistical analyses. We also explored correlations between pandemic spread and important socio-economic and environmental factors. Furthermore, we also present how outcomes of our work can facilitate efficient policy making in this critical hour.        △ Less","12 August, 2020",cs.CY,
              Multi-level Stress Assessment Using Multi-domain Fusion of ECG Signal          ,2008.05503,https://arxiv.org/abs/2008.05503,https://arxiv.org/pdf/2008.05503,"Authors:ZeeshanAhmad,NaimulKhan","        Stress analysis and assessment of affective states of mind using ECG as a physiological signal is a burning research topic in biomedical signal processing. However, existing literature provides only binary assessment of stress, while multiple levels of assessment may be more beneficial for healthcare applications. Furthermore, in present research, ECG signal for stress analysis is examined independently in spatial domain or in transform domains but the advantage of fusing these domains has not been fully utilized. To get the maximum advantage of fusing diferent domains, we introduce a dataset with multiple stress levels and then classify these levels using a novel deep learning approach by converting ECG signal into signal images based on R-R peaks without any feature extraction. Moreover, We made signal images multimodal and multidomain by converting them into time-frequency and frequency domain using Gabor wavelet transform (GWT) and Discrete Fourier Transform (DFT) respectively. Convolutional Neural networks (CNNs) are used to extract features from different modalities and then decision level fusion is performed for improving the classification accuracy. The experimental results on an in-house dataset collected with 15 users show that with proposed fusion framework and using ECG signal to image conversion, we reach an average accuracy of 85.45%.        △ Less","12 August, 2020","cs.CV,cs.LG",
              Deep learning for photoacoustic imaging: a survey          ,2008.04221,https://arxiv.org/abs/2008.04221,https://arxiv.org/pdf/2008.04221,"Authors:ChangchunYang,HengrongLan,FengGao,FeiGao","        Machine learning has been developed dramatically and witnessed a lot of applications in various fields over the past few years. This boom originated in 2009, when a new model emerged, that is, the deep artificial neural network, which began to surpass other established mature models on some important benchmarks. Later, it was widely used in academia and industry. Ranging from image analysis to natural language processing, it fully exerted its magic and now become the state-of-the-art machine learning models. Deep neural networks have great potential in medical imaging technology, medical data analysis, medical diagnosis and other healthcare issues, and is promoted in both pre-clinical and even clinical stages. In this review, we performed an overview of some new developments and challenges in the application of machine learning to medical image analysis, with a special focus on deep learning in photoacoustic imaging. The aim of this review is threefold: (i) introducing deep learning with some important basics, (ii) reviewing recent works that apply deep learning in the entire ecological chain of photoacoustic imaging, from image reconstruction to disease diagnosis, (iii) providing some open source materials and other resources for researchers interested in applying deep learning to photoacoustic imaging.        △ Less","11 October, 2020","cs.CV,eess.IV",
              HOLMES: Health OnLine Model Ensemble Serving for Deep Learning Models in Intensive Care Units          ,2008.04063,https://arxiv.org/abs/2008.04063,https://arxiv.org/pdf/2008.04063,"Authors:ShendaHong,YanboXu,AlindKhare,SatriaPriambada,KevinMaher,AlaaAljiffry,JimengSun,AlexeyTumanov","        Deep learning models have achieved expert-level performance in healthcare with an exclusive focus on training accurate models. However, in many clinical environments such as intensive care unit (ICU), real-time model serving is equally if not more important than accuracy, because in ICU patient care is simultaneously more urgent and more expensive. Clinical decisions and their timeliness, therefore, directly affect both the patient outcome and the cost of care. To make timely decisions, we argue the underlying serving system must be latency-aware. To compound the challenge, health analytic applications often require a combination of models instead of a single model, to better specialize individual models for different targets, multi-modal data, different prediction windows, and potentially personalized predictions. To address these challenges, we propose HOLMES-an online model ensemble serving framework for healthcare applications. HOLMES dynamically identifies the best performing set of models to ensemble for highest accuracy, while also satisfying sub-second latency constraints on end-to-end prediction. We demonstrate that HOLMES is able to navigate the accuracy/latency tradeoff efficiently, compose the ensemble, and serve the model ensemble pipeline, scaling to simultaneously streaming data from 100 patients, each producing waveform data at 250~Hz. HOLMES outperforms the conventional offline batch-processed inference for the same clinical task in terms of accuracy and latency (by order of magnitude). HOLMES is tested on risk prediction task on pediatric cardio ICU data with above 95% prediction accuracy and sub-second latency on 64-bed simulation.        △ Less","10 August, 2020","cs.LG,stat.ML",
              Individualized Prediction of COVID-19 Adverse outcomes with MLHO          ,2008.03869,https://arxiv.org/abs/2008.03869,https://arxiv.org/pdf/2008.03869,"Authors:HosseinEstiri,ZacharyH.Strasser,ShawnN.Murphy","        The COVID-19 pandemic has devastated the world with health and economic wreckage. Precise estimates of the COVID-19 adverse outcomes on individual patients could have led to better allocation of healthcare resources and more efficient targeted preventive measures. We developed MLHO (pronounced as melo) for predicting patient-level risk of hospitalization, ICU admission, need for mechanical ventilation, and death from patients' past (before COVID-19 infection) medical records. MLHO is an end-to-end Machine Learning pipeline that implements iterative sequential representation mining and feature and model selection to predict health outcomes. MLHO's architecture enables a parallel and outcome-oriented calibration, in which different statistical learning algorithms and vectors of features are simultaneously tested and leveraged to improve prediction of health outcomes. Using clinical data from a large cohort of over 14,000 patients, we modeled the four adverse outcomes utilizing about 600 features representing patients' before-COVID health records. Overall, the best predictions were obtained from extreme and gradient boosting models. The median AUC ROC for mortality prediction was 0.91, while the prediction performance ranged between 0.79 and 0.83 for ICU, hospitalization, and ventilation. We broadly describe the clusters of features that were utilized in modeling and their relative influence on predicting each outcome. As COVID-19 cases are re-surging in the U.S. and around the world, a Machine Learning pipeline like MLHO is crucial to improve our readiness for confronting the potential future waves of COVID-19, as well as other novel infectious diseases that may emerge in the near future.        △ Less","9 August, 2020","stat.ML,cs.LG,q-bio.QM",
"              Approximation Algorithms for Radius-Based, Two-Stage Stochastic Clustering Problems with Budget Constraints          ",2008.03325,https://arxiv.org/abs/2008.03325,https://arxiv.org/pdf/2008.03325,"Authors:BrianBrubach,NathanielGrammel,DavidG.Harris,AravindSrinivasan,LeonidasTsepenekas,AnilVullikanti","        The main focus of this paper is radius-based clustering problems in the two-stage stochastic setting with recourse, where the inherent stochasticity of the model comes in the form of a budget constraint. We also explore a number of variants where additional constraints are imposed on the first-stage decisions, specifically matroid and multi-knapsack constraints. Further, we show that our problems have natural applications to allocating healthcare testing centers.  The eventual goal is to provide results for supplier-like problems in the most general distributional setting, where there is only black-box access to the underlying distribution. Our framework unfolds in two steps. First, we develop algorithms for a restricted version of each problem, in which all possible scenarios are explicitly provided; second, we exploit structural properties of these algorithms and generalize them to the black-box setting. These key properties are: \textbf{(1)} the algorithms produce ``simple'' exponential families of black-box strategies, and \textbf{(2)} there exist \emph{efficient} ways to extend their output to the black-box case, which also preserve the approximation ratio exactly. We note that prior generalization approaches, i.e., variants of the \emph{Sample Average Approximation} method, can be used for the problems we consider, however they would yield worse approximation guarantees.        △ Less","7 August, 2020",cs.DS,
              COVID-19 in differential diagnosis of online symptom assessments          ,2008.03323,https://arxiv.org/abs/2008.03323,https://arxiv.org/pdf/2008.03323,"Authors:AnithaKannan,RichardChen,VigneshVenkataraman,GeoffreyJ.Tso,XavierAmatriain","        The COVID-19 pandemic has magnified an already existing trend of people looking for healthcare solutions online. One class of solutions are symptom checkers, which have become very popular in the context of COVID-19. Traditional symptom checkers, however, are based on manually curated expert systems that are inflexible and hard to modify, especially in a quickly changing situation like the one we are facing today. That is why all COVID-19 existing solutions are manual symptom checkers that can only estimate the probability of this disease and cannot contemplate alternative hypothesis or come up with a differential diagnosis. While machine learning offers an alternative, the lack of reliable data does not make it easy to apply to COVID-19 either. In this paper we present an approach that combines the strengths of traditional AI expert systems and novel deep learning models. In doing so we can leverage prior knowledge as well as any amount of existing data to quickly derive models that best adapt to the current state of the world and latest scientific knowledge. We use the approach to train a COVID-19 aware differential diagnosis model that can be used for medical decision support both for doctors or patients. We show that our approach is able to accurately model new incoming data about COVID-19 while still preserving accuracy on conditions that had been modeled in the past. While our approach shows evident and clear advantages for an extreme situation like the one we are currently facing, we also show that its flexibility generalizes beyond this concrete, but very important, example.        △ Less","7 August, 2020","cs.AI,cs.LG",
              Label-free Plasmonic Detection of Untethered Nanometer-sized Brownian Particles          ,2008.03086,https://arxiv.org/abs/2008.03086,https://arxiv.org/pdf/2008.03086,"Authors:MartinDieterBaaske,PeterSebastianNeu,MichelOrrit","        Optical detection of individual nanometer-sized analytes, virus particles, and protein molecules holds great promise for understanding and control of biological samples and healthcare applications. As fluorescent labels impose restrictions on detection bandwidth and require lengthy and invasive processes, label-free optical techniques are highly desirable. Powerful label-free optical methods have recently emerged, such as interferometric scattering microscopy, plasmonic nanoparticle-based assays and microcavity-based assays. Although highly sensitive, these methods are so far restricted to integration times in excess of microseconds. This often imposes a requirement to impede analyte motion during these periods via specific molecular tethers, unspecific adsorption or confining arrangements. Here we introduce an optical technique capable of transforming gold nanorods commonly used as photostable labels into highly localized high-speed probes. Our method provides a time resolution well below microseconds. This mitigates the requirement for molecular tethers and allows us to detect single untethered nanoparticles in Brownian motion traversing sub atto-liter sensing volumes. Our method opens a novel gateway for the investigation of highly localized and highly dynamic nanoscale systems and constitutes a first step towards the label-free recognition of single untethered proteins.        △ Less","7 August, 2020","physics.optics,physics.app-ph",10.1021/acsnano.0c07335 
              An Empirical Study on Developing Secure Mobile Health Apps: The Developers Perspective          ,2008.03034,https://arxiv.org/abs/2008.03034,https://arxiv.org/pdf/2008.03034,"Authors:BakheetAljedaani,AakashAhmad,MansoorehZahedi,M.AliBabar","        Mobile apps exploit embedded sensors and wireless connectivity of a device to empower users with portable computations, context-aware communication, and enhanced interaction. Specifically, mobile health apps (mHealth apps for short) are becoming integral part of mobile and pervasive computing to improve the availability and quality of healthcare services. Despite the offered benefits, mHealth apps face a critical challenge, i.e., security of health critical data that is produced and consumed by the app. Several studies have revealed that security specific issues of mHealth apps have not been adequately addressed. The objectives of this study are to empirically (a) investigate the challenges that hinder development of secure mHealth apps, (b) identify practices to develop secure apps, and (c) explore motivating factors that influence secure development. We conducted this study by collecting responses of 97 developers from 25 countries, across 06 continents, working in diverse teams and roles to develop mHealth apps for Android, iOS, and Windows platform. Qualitative analysis of the survey data is based on (i) 8 critical challenges, (ii) taxonomy of best practices to ensure security, and (iii) 6 motivating factors that impact secure mHealth apps. This research provides empirical evidence as practitioners view and guidelines to develop emerging and next generation of secure mHealth apps.        △ Less","7 August, 2020","cs.SE,cs.CY",
              On singular control for Lévy processes          ,2008.03021,https://arxiv.org/abs/2008.03021,https://arxiv.org/pdf/2008.03021,"Authors:KeiNoba,KazutoshiYamazaki","        We revisit the classical singular control problem of minimizing running and controlling costs. The problem arises in inventory control, as well as in healthcare management and mathematical finance. Existing studies have shown the optimality of a barrier strategy when driven by the Brownian motion or Lévy processes with one-side jumps. Under the assumption that the running cost function is convex, we show the optimality of a barrier strategy for a general class of Lévy processes. Numerical results are also given.        △ Less","7 August, 2020","math.PR,math.OC",
              A Multilingual Neural Machine Translation Model for Biomedical Data          ,2008.02878,https://arxiv.org/abs/2008.02878,https://arxiv.org/pdf/2008.02878,"Authors:AlexandreBérard,ZaeMyungKim,VassilinaNikoulina,EunjeongL.Park,MatthiasGallé","        We release a multilingual neural machine translation model, which can be used to translate text in the biomedical domain. The model can translate from 5 languages (French, German, Italian, Korean and Spanish) into English. It is trained with large amounts of generic and biomedical data, using domain tags. Our benchmarks show that it performs near state-of-the-art both on news (generic domain) and biomedical test sets, and that it outperforms the existing publicly released models. We believe that this release will help the large-scale multilingual analysis of the digital content of the COVID-19 crisis and of its effects on society, economy, and healthcare policies.  We also release a test set of biomedical text for Korean-English. It consists of 758 sentences from official guidelines and recent papers, all about COVID-19.        △ Less","6 August, 2020","cs.CL,cs.LG",
              Learning Insulin-Glucose Dynamics in the Wild          ,2008.02852,https://arxiv.org/abs/2008.02852,https://arxiv.org/pdf/2008.02852,"Authors:AndrewC.Miller,NicholasJ.Foti,EmilyFox","        We develop a new model of insulin-glucose dynamics for forecasting blood glucose in type 1 diabetics. We augment an existing biomedical model by introducing time-varying dynamics driven by a machine learning sequence model. Our model maintains a physiologically plausible inductive bias and clinically interpretable parameters -- e.g., insulin sensitivity -- while inheriting the flexibility of modern pattern recognition algorithms. Critical to modeling success are the flexible, but structured representations of subject variability with a sequence model. In contrast, less constrained models like the LSTM fail to provide reliable or physiologically plausible forecasts. We conduct an extensive empirical study. We show that allowing biomedical model dynamics to vary in time improves forecasting at long time horizons, up to six hours, and produces forecasts consistent with the physiological effects of insulin and carbohydrates.        △ Less","6 August, 2020","stat.ML,cs.LG,stat.AP",
              An Intelligent Non-Invasive Real Time Human Activity Recognition System for Next-Generation Healthcare,2008.02567,https://arxiv.org/abs/2008.02567,https://arxiv.org/pdf/2008.02567,"Authors:WilliamTaylor,SyedAzizShah,KiaDashtipour,AdnanZahid,QammerH.Abbasi,MuhammadAliImran","        Human motion detection is getting considerable attention in the field of Artificial Intelligence (AI) driven healthcare systems. Human motion can be used to provide remote healthcare solutions for vulnerable people by identifying particular movements such as falls, gait and breathing disorders. This can allow people to live more independent lifestyles and still have the safety of being monitored if more direct care is needed. At present wearable devices can provide real time monitoring by deploying equipment on a person's body. However, putting devices on a person's body all the time make it uncomfortable and the elderly tends to forget it to wear as well in addition to the insecurity of being tracked all the time. This paper demonstrates how human motions can be detected in quasi-real-time scenario using a non-invasive method. Patterns in the wireless signals presents particular human body motions as each movement induces a unique change in the wireless medium. These changes can be used to identify particular body motions. This work produces a dataset that contains patterns of radio wave signals obtained using software defined radios (SDRs) to establish if a subject is standing up or sitting down as a test case. The dataset was used to create a machine learning model, which was used in a developed application to provide a quasi-real-time classification of standing or sitting state. The machine learning model was able to achieve 96.70 % accuracy using the Random Forest algorithm using 10 fold cross validation. A benchmark dataset of wearable devices was compared to the proposed dataset and results showed the proposed dataset to have similar accuracy of nearly 90 %. The machine learning models developed in this paper are tested for two activities but the developed system is designed and applicable for detecting and differentiating x number of activities.        △ Less","6 August, 2020","eess.SP,cs.CY",10.3390/s20092653 
              Interpretable Multi-Step Reasoning with Knowledge Extraction on Complex Healthcare Question Answering          ,2008.02434,https://arxiv.org/abs/2008.02434,https://arxiv.org/pdf/2008.02434,"Authors:YeLiu,ShaikaChowdhury,ChenweiZhang,CorneliaCaragea,PhilipS.Yu","Healthcare question answering assistance aims to provide customer healthcare information, which widely appears in both Web and mobile Internet. The questions usually require the assistance to have proficient healthcare background knowledge as well as the reasoning ability on the knowledge. Recently a challenge involving complex healthcare reasoning, HeadQA dataset, has been proposed, which contains multiple-choice questions authorized for the public healthcare specialization exam. Unlike most other QA tasks that focus on linguistic understanding, HeadQA requires deeper reasoning involving not only knowledge extraction, but also complex reasoning with healthcare knowledge. These questions are the most challenging for current QA systems, and the current performance of the state-of-the-art method is slightly better than a random guess. In order to solve this challenging task, we present a Multi-step reasoning with Knowledge extraction framework (MurKe). The proposed framework first extracts the healthcare knowledge as supporting documents from the large corpus. In order to find the reasoning chain and choose the correct answer, MurKe iterates between selecting the supporting documents, reformulating the query representation using the supporting documents and getting entailment score for each choice using the entailment model. The reformulation module leverages selected documents for missing evidence, which maintains interpretability. Moreover, we are striving to make full use of off-the-shelf pre-trained models. With less trainable weight, the pre-trained model can easily adapt to healthcare tasks with limited training samples. From the experimental results and ablation study, our system is able to outperform several strong baselines on the HeadQA dataset.        △ Less","5 August, 2020","cs.AI,cs.IR",
              An Interpretable Deep Learning System for Automatically Scoring Request for Proposals          ,2008.02347,https://arxiv.org/abs/2008.02347,https://arxiv.org/pdf/2008.02347,"Authors:SubhadipMaji,AnudeepSrivatsavAppe,RaghavBali,VeeraRaghavendraChikka,ArijitGhoshChowdhury,VamsiMBhandaru","        The Managed Care system within Medicaid (US Healthcare) uses Request For Proposals (RFP) to award contracts for various healthcare and related services. RFP responses are very detailed documents (hundreds of pages) submitted by competing organisations to win contracts. Subject matter expertise and domain knowledge play an important role in preparing RFP responses along with analysis of historical submissions. Automated analysis of these responses through Natural Language Processing (NLP) systems can reduce time and effort needed to explore historical responses, and assisting in writing better responses. Our work draws parallels between scoring RFPs and essay scoring models, while highlighting new challenges and the need for interpretability. Typical scoring models focus on word level impacts to grade essays and other short write-ups. We propose a novel Bi-LSTM based regression model, and provide deeper insight into phrases which latently impact scoring of responses. We contend the merits of our proposed methodology using extensive quantitative experiments. We also qualitatively asses the impact of important phrases using human evaluators. Finally, we introduce a novel problem statement that can be used to further improve the state of the art in NLP based automatic scoring systems.        △ Less","5 August, 2020","cs.CL,cs.AI,cs.LG",
              Robust Tensor Principal Component Analysis: Exact Recovery via Deterministic Model          ,2008.02211,https://arxiv.org/abs/2008.02211,https://arxiv.org/pdf/2008.02211,"Authors:BoShen,Zhenyu,Kong","        Tensor, also known as multi-dimensional array, arises from many applications in signal processing, manufacturing processes, healthcare, among others. As one of the most popular methods in tensor literature, Robust tensor principal component analysis (RTPCA) is a very effective tool to extract the low rank and sparse components in tensors. In this paper, a new method to analyze RTPCA is proposed based on the recently developed tensor-tensor product and tensor singular value decomposition (t-SVD). Specifically, it aims to solve a convex optimization problem whose objective function is a weighted combination of the tensor nuclear norm and the l1-norm. In most of literature of RTPCA, the exact recovery is built on the tensor incoherence conditions and the assumption of a uniform model on the sparse support. Unlike this conventional way, in this paper, without any assumption of randomness, the exact recovery can be achieved in a completely deterministic fashion by characterizing the tensor rank-sparsity incoherence, which is an uncertainty principle between the low-rank tensor spaces and the pattern of sparse tensor.        △ Less","5 August, 2020","stat.ML,cs.LG,math.ST",
              IntelligentPooling: Practical Thompson Sampling for mHealth          ,2008.01571,https://arxiv.org/abs/2008.01571,https://arxiv.org/pdf/2008.01571,"Authors:SabinaTomkins,PengLiao,PredragKlasnja,SusanMurphy","        In mobile health (mHealth) smart devices deliver behavioral treatments repeatedly over time to a user with the goal of helping the user adopt and maintain healthy behaviors. Reinforcement learning appears ideal for learning how to optimally make these sequential treatment decisions. However, significant challenges must be overcome before reinforcement learning can be effectively deployed in a mobile healthcare setting. In this work we are concerned with the following challenges: 1) individuals who are in the same context can exhibit differential response to treatments 2) only a limited amount of data is available for learning on any one individual, and 3) non-stationary responses to treatment. To address these challenges we generalize Thompson-Sampling bandit algorithms to develop IntelligentPooling. IntelligentPooling learns personalized treatment policies thus addressing challenge one. To address the second challenge, IntelligentPooling updates each user's degree of personalization while making use of available data on other users to speed up learning. Lastly, IntelligentPooling allows responsivity to vary as a function of a user's time since beginning treatment, thus addressing challenge three. We show that IntelligentPooling achieves an average of 26% lower regret than state-of-the-art. We demonstrate the promise of this approach and its ability to learn from even a small group of users in a live clinical trial.        △ Less","31 July, 2020","cs.LG,cs.CY,stat.ML",
              Dynamically Extracting Outcome-Specific Problem Lists from Clinical Notes with Guided Multi-Headed Attention          ,2008.01197,https://arxiv.org/abs/2008.01197,https://arxiv.org/pdf/2008.01197,"Authors:JustinLovelace,NathanC.Hurley,AdrianD.Haimovich,BobakJ.Mortazavi","        Problem lists are intended to provide clinicians with a relevant summary of patient medical issues and are embedded in many electronic health record systems. Despite their importance, problem lists are often cluttered with resolved or currently irrelevant conditions. In this work, we develop a novel end-to-end framework that first extracts diagnosis and procedure information from clinical notes and subsequently uses the extracted medical problems to predict patient outcomes. This framework is both more performant and more interpretable than existing models used within the domain, achieving an AU-ROC of 0.710 for bounceback readmission and 0.869 for in-hospital mortality occurring after ICU discharge. We identify risk factors for both readmission and mortality outcomes and demonstrate that our framework can be used to develop dynamic problem lists that present clinical problems along with their quantitative importance. We conduct a qualitative user study with medical experts and demonstrate that they view the lists produced by our framework favorably and find them to be a more effective clinical decision support tool than a strong baseline.        △ Less","25 July, 2020","cs.IR,cs.LG",
              An Exact Method for Bisubmodular Function Maximization          ,2008.00988,https://arxiv.org/abs/2008.00988,https://arxiv.org/pdf/2008.00988,"Authors:QimengYu,SimgeKüçükyavuz","        Bisubmodularity--a natural generalization of submodularity--applies to set functions with two arguments and appears in a broad range of applications, including coupled sensor placement in infrastructure, coupled feature selection in machine learning, and drug-drug interaction detection in healthcare. In this paper, we study maximization problems with bisubmodular objective functions. We propose valid linear inequalities, namely the bisubmodular inequalities, for the hypograph of any bisubmodular function. We show that maximizing a bisubmodular function is equivalent to solving a mixed-integer linear program with exponentially many bisubmodular inequalities. Using this representation in a delayed constraint generation framework, we design the first exact algorithm to solve general bisubmodular maximization problems. Our computational experiments on the coupled sensor placement problem demonstrate the efficacy of our algorithm in constrained nonlinear bisubmodular maximization problems for which no existing exact methods are available.        △ Less","3 August, 2020",math.OC,
              Interpretable Sequence Learning for COVID-19 Forecasting          ,2008.00646,https://arxiv.org/abs/2008.00646,https://arxiv.org/pdf/2008.00646,"Authors:SercanO.Arik,Chun-LiangLi,JinsungYoon,RajarishiSinha,ArkadyEpshteyn,LongT.Le,VikasMenon,ShashankSingh,LeyouZhang,NateYoder,MartinNikoltchev,YashSonthalia,HootanNakhost,ElliKanal,TomasPfister","        We propose a novel approach that integrates machine learning into compartmental disease modeling to predict the progression of COVID-19. Our model is explainable by design as it explicitly shows how different compartments evolve and it uses interpretable encoders to incorporate covariates and improve performance. Explainability is valuable to ensure that the model's forecasts are credible to epidemiologists and to instill confidence in end-users such as policy makers and healthcare institutions. Our model can be applied at different geographic resolutions, and here we demonstrate it for states and counties in the United States. We show that our model provides more accurate forecasts, in metrics averaged across the entire US, than state-of-the-art alternatives, and that it provides qualitatively meaningful explanatory insights. Lastly, we analyze the performance of our model for different subgroups based on the subgroup distributions within the counties.        △ Less","3 August, 2020","cs.LG,stat.ML",
              Dynamic Discrete Choice Estimation with Partially Observable States and Hidden Dynamics          ,2008.00500,https://arxiv.org/abs/2008.00500,https://arxiv.org/pdf/2008.00500,"Authors:YanlingChang,AlfredoGarcia,ZhideWang","        Dynamic discrete choice models are used to estimate the intertemporal preferences of an agent as described by a reward function based upon observable histories of states and implemented actions. However, in many applications, such as reliability and healthcare, the system state is partially observable or hidden (e.g., the level of deterioration of an engine, the condition of a disease), and the decision maker only has access to information imperfectly correlated with the true value of the hidden state. In this paper, we consider the estimation of a dynamic discrete choice model with state variables and system dynamics that are hidden (or partially observed) to both the agent and the modeler, thus generalizing Rust's model to partially observable cases. We analyze the structural properties of the model and prove that this model is still identifiable if the cardinality of the state space, the discount factor, the distribution of random shocks, and the rewards for a given (reference) action are given. We analyze both theoretically and numerically the potential mis-specification errors that may be incurred when Rust's model is improperly used in partially observable settings. We further apply the developed model to a subset of Rust's dataset for bus engine mileage and replacement decisions. The results show that our model can improve model fit as measured by the log\log-likelihood function by 17.7%17.7\% and the log\log-likelihood ratio test shows that our model statistically outperforms Rust's model. Interestingly, our hidden state model also reveals an economically meaningful route assignment behavior in the dataset which was hitherto ignored, i.e. routes with lower mileage are assigned to buses believed to be in worse condition.        △ Less","2 August, 2020","cs.LG,stat.ML",
              An Explainable Machine Learning Model for Early Detection of Parkinson's Disease using LIME on DaTscan Imagery          ,2008.00238,https://arxiv.org/abs/2008.00238,https://arxiv.org/pdf/2008.00238,"Authors:PavanRajkumarMagesh,RichardDelwinMyloth,RijoJacksonTom","        Parkinson's disease (PD) is a degenerative and progressive neurological condition. Early diagnosis can improve treatment for patients and is performed through dopaminergic imaging techniques like the SPECT DaTscan. In this study, we propose a machine learning model that accurately classifies any given DaTscan as having Parkinson's disease or not, in addition to providing a plausible reason for the prediction. This is kind of reasoning is done through the use of visual indicators generated using Local Interpretable Model-Agnostic Explainer (LIME) methods. DaTscans were drawn from the Parkinson's Progression Markers Initiative database and trained on a CNN (VGG16) using transfer learning, yielding an accuracy of 95.2%, a sensitivity of 97.5%, and a specificity of 90.9%. Keeping model interpretability of paramount importance, especially in the healthcare field, this study utilises LIME explanations to distinguish PD from non-PD, using visual superpixels on the DaTscans. It could be concluded that the proposed system, in union with its measured interpretability and accuracy may effectively aid medical workers in the early diagnosis of Parkinson's Disease.        △ Less","1 August, 2020","cs.CV,cs.LG,eess.IV",
              Analysis of Prescription Drug Utilization with Beta Regression Models          ,2008.00048,https://arxiv.org/abs/2008.00048,https://arxiv.org/pdf/2008.00048,"Authors:GuojunGan,EmilianoA.Valdez","        The healthcare sector in the U.S. is complex and is also a large sector that generates about 20% of the country's gross domestic product. Healthcare analytics has been used by researchers and practitioners to better understand the industry. In this paper, we examine and demonstrate the use of Beta regression models to study the utilization of brand name drugs in the U.S. to understand the variability of brand name drug utilization across different areas. The models are fitted to public datasets obtained from the Medicare & Medicaid Services and the Internal Revenue Service. Integrated Nested Laplace Approximation (INLA) is used to perform the inference. The numerical results show that Beta regression models can fit the brand name drug claim rates well and including spatial dependence improves the performance of the Beta regression models. Such models can be used to reflect the effect of prescription drug utilization when updating an insured's health risk in a risk scoring model.        △ Less","31 July, 2020",stat.AP,
              A Review on the State of the Art in Non Contact Sensing for COVID-19          ,2007.16063,https://arxiv.org/abs/2007.16063,https://arxiv.org/pdf/2007.16063,"Authors:WilliamTaylor,QammerH.Abbasi,KiaDashtipour,ShujaAnsari,AzizShah,ArslanKhan,MuhammadAliImran","        COVID-19 disease, caused by SARS-CoV-2, has resulted in a global pandemic recently. With no approved vaccination or treatment, governments around the world have issued guidance to their citizens to remain at home in efforts to control the spread of the disease. The goal of controlling the spread of the virus is to prevent strain on hospital. In this paper, we have focus on how non-invasive methods are being used to detect the COVID-19 and assist healthcare workers in caring for COVID-19 patients. Early detection of the COVID-19 virus can allow for early isolation to prevent further spread. This study outlines the advantages and disadvantages and a breakdown of the methods applied in the current state-of-the-art approaches. In addition, the paper highlights some future research directions, which are required to be explored further to come up with innovative technologies to control this pandemic.        △ Less","28 July, 2020","cs.CY,eess.IV,physics.med-ph",
              IIT Kanpur Consulting Group: Using Machine Learning and Management Consulting for Social Good          ,2007.15628,https://arxiv.org/abs/2007.15628,https://arxiv.org/pdf/2007.15628,"Authors:TusharGoswamy,VatsalyaTandon,NaishadhParmar,RaunakShah,AyushGupta","        The IIT Kanpur Consulting Group is one of the pioneering research groups in India which focuses on the applications of Machine Learning and Strategy Consulting for social good. The group has been working since 2018 to help social organizations, nonprofits, and government entities in India leverage better insights from their data, with a special emphasis on the healthcare, environmental, and agriculture sectors. The group has worked on critical social problems which India is facing including Polio recurrence, COVID-19, air pollution and agricultural crop damage. This position paper summarises the focus areas and relevant projects which the group has worked on since its establishment, and also highlights the group's plans for using machine learning to address social problems during the COVID-19 crisis.        △ Less","30 July, 2020",cs.CY,
              AI-based Monitoring and Response System for Hospital Preparedness towards COVID-19 in Southeast Asia          ,2007.15619,https://arxiv.org/abs/2007.15619,https://arxiv.org/pdf/2007.15619,"Authors:TusharGoswamy,NaishadhParmar,AyushGupta,VatsalyaTandon,RaunakShah,VarunGoyal,SanyogGupta,KarishmaLaud,ShivamGupta,SudhanshuMishra,AshutoshModi","        This research paper proposes a COVID-19 monitoring and response system to identify the surge in the volume of patients at hospitals and shortage of critical equipment like ventilators in South-east Asian countries, to understand the burden on health facilities. This can help authorities in these regions with resource planning measures to redirect resources to the regions identified by the model. Due to the lack of publicly available data on the influx of patients in hospitals, or the shortage of equipment, ICU units or hospital beds that regions in these countries might be facing, we leverage Twitter data for gleaning this information. The approach has yielded accurate results for states in India, and we are working on validating the model for the remaining countries so that it can serve as a reliable tool for authorities to monitor the burden on hospitals.        △ Less","30 July, 2020","cs.CY,cs.CL,cs.LG",
              An early warning tool for predicting mortality risk of COVID-19 patients using machine learning          ,2007.15559,https://arxiv.org/abs/2007.15559,https://arxiv.org/pdf/2007.15559,"Authors:MuhammadE.H.Chowdhury,TawsifurRahman,AmithKhandakar,SomayaAl-Madeed,SusuM.Zughaier,SuhailA.R.Doi,HanadiHassen,MohammadT.Islam","        COVID-19 pandemic has created an extreme pressure on the global healthcare services. Fast, reliable and early clinical assessment of the severity of the disease can help in allocating and prioritizing resources to reduce mortality. In order to study the important blood biomarkers for predicting disease mortality, a retrospective study was conducted on 375 COVID-19 positive patients admitted to Tongji Hospital (China) from January 10 to February 18, 2020. Demographic and clinical characteristics, and patient outcomes were investigated using machine learning tools to identify key biomarkers to predict the mortality of individual patient. A nomogram was developed for predicting the mortality risk among COVID-19 patients. Lactate dehydrogenase, neutrophils (%), lymphocyte (%), high sensitive C-reactive protein, and age - acquired at hospital admission were identified as key predictors of death by multi-tree XGBoost model. The area under curve (AUC) of the nomogram for the derivation and validation cohort were 0.961 and 0.991, respectively. An integrated score (LNLCA) was calculated with the corresponding death probability. COVID-19 patients were divided into three subgroups: low-, moderate- and high-risk groups using LNLCA cut-off values of 10.4 and 12.65 with the death probability less than 5%, 5% to 50%, and above 50%, respectively. The prognostic model, nomogram and LNLCA score can help in early detection of high mortality risk of COVID-19 patients, which will help doctors to improve the management of patient stratification.        △ Less","29 July, 2020","q-bio.QM,cs.LG",
              Learner's Dilemma: IoT Devices Training Strategies in Collaborative Deep Learning          ,2007.15215,https://arxiv.org/abs/2007.15215,https://arxiv.org/pdf/2007.15215,"Authors:DeeptiGupta,OlumideKayode,SmritiBhatt,MaanakGupta,AliSamanTosun","        With the growth of Internet of Things (IoT) and mo-bile edge computing, billions of smart devices are interconnected to develop applications used in various domains including smart homes, healthcare and smart manufacturing. Deep learning has been extensively utilized in various IoT applications which require huge amount of data for model training. Due to privacy requirements, smart IoT devices do not release data to a remote third party for their use. To overcome this problem, collaborative approach to deep learning, also known as Collaborative DeepLearning (CDL) has been largely employed in data-driven applications. This approach enables multiple edge IoT devices to train their models locally on mobile edge devices. In this paper,we address IoT device training problem in CDL by analyzing the behavior of mobile edge devices using a game-theoretic model,where each mobile edge device aims at maximizing the accuracy of its local model at the same time limiting the overhead of participating in CDL. We analyze the Nash Equilibrium in anN-player static game model. We further present a novel cluster-based fair strategy to approximately solve the CDL game to enforce mobile edge devices for cooperation. Our experimental results and evaluation analysis in a real-world smart home deployment show that 80% mobile edge devices are ready to cooperate in CDL, while 20% of them do not train their local models collaboratively.        △ Less","29 July, 2020","cs.CR,cs.GT",
"              Fast, Structured Clinical Documentation via Contextual Autocomplete          ",2007.15153,https://arxiv.org/abs/2007.15153,https://arxiv.org/pdf/2007.15153,"Authors:DivyaGopinath,MonicaAgrawal,LukeMurray,StevenHorng,DavidKarger,DavidSontag","        We present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. We dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. By constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. Furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. To our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.        △ Less","29 July, 2020","cs.LG,cs.CL,cs.IR,stat.ML",
Healthcare Utilization and Perceived Health Status from Falun Gong Practitioners in Taiwan: A Pilot SF-36 Survey          ,2007.14926,https://arxiv.org/abs/2007.14926,https://arxiv.org/pdf/2007.14926,"Authors:Yu-WhueiHu,Li-ShanHuang,EricJ.Yeh,MaiHe","        Objective: Falun Gong (FLG) is a practice of mind and body focusing on moral character improvement along with meditative exercises. This 2002 pilot study explored perceived health status, medical resource utilization and related factors among Taiwanese FLG practitioners, compared to the general Taiwanese norm estimated by the 2001 National Health Interview Survey (NHIS). Methods: This cross-sectional, observational study was based on a voluntary, paper-based survey conducted from October 2002 to February 2003 using the same Taiwanese SF-36 instrument employed by the NHIS. Primary outcomes included eight SF-36 domain scores and the number of medical visits. One-sample t-tests, one-way ANOVA and multivariate linear regression analyses were performed. Results: The response rate was 75.6% (1,210/1,600). Compared to the norm, the study cohort had significantly higher scores in six of eight SF-36 domains across gender and age (p<0.05). Among those with chronic diseases, 70% to 89% reported their conditions either improved or cured. 74.2%, 79.2%, 83.3%, and 85.6% quitted alcohol drinking, smoking, chewing betel nuts, and gambling. 62.7% reported a reduced number of medical visits (mean=13.53 before; mean=5.87 after). Conclusions: In this subject cohort, practicing FLG led to higher perceived health scores and reduced health resource utilization compared to the norm.        △ Less","29 July, 2020",q-bio.OT,
              SAFER: Sparse Secure Aggregation for Federated Learning          ,2007.14861,https://arxiv.org/abs/2007.14861,https://arxiv.org/pdf/2007.14861,"Authors:ConstanceBeguier,EricW.Tramel","        Federated learning enables one to train a common machine learning model across separate, privately-held datasets via distributed model training. During federated training, only intermediate model parameters are transmitted to a central server which aggregates these parameters to create a new common model, thus exposing only intermediate parameters rather than the training data itself. However, some attacks (e.g. membership inference) are able to infer properties of local data from these intermediate model parameters. Hence, performing the aggregation of these client-specific model parameters in a secure way is required. Additionally, the communication cost is often the bottleneck of the federated systems, especially for large neural networks. So, limiting the number and the size of communications is necessary to efficiently train large neural architectures. In this article, we present an efficient and secure protocol for performing secure aggregation over compressed model updates in the context of collaborative, few-party federated learning, a context common in the medical, healthcare, and biotechnical use-cases of federated systems. By making compression-based federated techniques amenable to secure computation, we develop a secure aggregation protocol between multiple servers with very low communication and computation costs and without preprocessing overhead. Our experiments demonstrate the efficiency of this new approach for secure federated training of deep convolutional neural networks.        △ Less","7 September, 2020","stat.ML,cs.CR,cs.LG",
              Are Less Developed Countries More Likely to Manipulate Data During Pandemics? Evidence from Newcomb-Benford Law          ,2007.14841,https://arxiv.org/abs/2007.14841,https://arxiv.org/pdf/2007.14841,"Authors:VadimS.Balashov,YuxingYan,XiaodiZhu","        We use the Newcomb-Benford law to test if countries manipulate reported data during the COVID-19 pandemic. We find that democratic countries, countries with the higher Gross Domestic Product (GDP) per capita, higher healthcare expenditures, and better universal healthcare coverage are less likely to deviate from the Newcomb-Benford law. The relationship holds for the cumulative number of deaths and for the cumulative number of total cases but is more pronounced for the death toll. The findings are robust for the second digit tests, for a sub-sample of countries with regional data, and during the previous swine flu (H1N1) 2009-2010 pandemic.        △ Less","29 July, 2020","econ.GN,stat.AP",
"              An India-specific Compartmental Model for Covid-19: Projections and Intervention Strategies by Incorporating Geographical, Infrastructural and Response Heterogeneity          ",2007.14392,https://arxiv.org/abs/2007.14392,https://arxiv.org/pdf/2007.14392,"Authors:SanitGupta,SahilShah,SumitChaturvedi,PranavThakkar,ParvinderSolanki,SohamDibyachintan,SandeepanRoy,M.B.Sushma,AdwaitGodbole,NoufalJaseem,PradumnKumar,SuchetaRavikanti,AritraDas,GiridharaR.Babu,TarunBhatnagar,AvijitMaji,MithunK.Mitra,SaiVinjanampathy","        We present a compartmental meta-population model for the spread of Covid-19 in India. Our model simulates populations at a district or state level using an epidemiological model that is appropriate to Covid-19. Different districts are connected by a transportation matrix developed using available census data. We introduce uncertainties in the testing rates into the model that takes into account the disparate responses of the different states to the epidemic and also factors in the state of the public healthcare system. Our model allows us to generate qualitative projections of Covid-19 spread in India, and further allows us to investigate the effects of different proposed interventions. By building in heterogeneity at geographical and infrastructural levels and in local responses, our model aims to capture some of the complexity of epidemiological modeling appropriate to a diverse country such as India.        △ Less","28 July, 2020","q-bio.PE,physics.soc-ph",
"              Variants of BERT, Random Forests and SVM approach for Multimodal Emotion-Target Sub-challenge          ",2007.13928,https://arxiv.org/abs/2007.13928,https://arxiv.org/pdf/2007.13928,"Authors:HoangManhHung,Hyung-JeongYang,Soo-HyungKim,Guee-SangLee","        Emotion recognition has become a major problem in computer vision in recent years that made a lot of effort by researchers to overcome the difficulties in this task. In the field of affective computing, emotion recognition has a wide range of applications, such as healthcare, robotics, human-computer interaction. Due to its practical importance for other tasks, many techniques and approaches have been investigated for different problems and various data sources. Nevertheless, comprehensive fusion of the audio-visual and language modalities to get the benefits from them is still a problem to solve. In this paper, we present and discuss our classification methodology for MuSe-Topic Sub-challenge, as well as the data and results. For the topic classification, we ensemble two language models which are ALBERT and RoBERTa to predict 10 classes of topics. Moreover, for the classification of valence and arousal, SVM and Random forests are employed in conjunction with feature selection to enhance the performance.        △ Less","27 July, 2020","cs.CV,cs.CL,cs.LG",
              Off-policy Evaluation in Infinite-Horizon Reinforcement Learning with Latent Confounders          ,2007.13893,https://arxiv.org/abs/2007.13893,https://arxiv.org/pdf/2007.13893,"Authors:AndrewBennett,NathanKallus,LihongLi,AliMousavi","        Off-policy evaluation (OPE) in reinforcement learning is an important problem in settings where experimentation is limited, such as education and healthcare. But, in these very same settings, observed actions are often confounded by unobserved variables making OPE even more difficult. We study an OPE problem in an infinite-horizon, ergodic Markov decision process with unobserved confounders, where states and actions can act as proxies for the unobserved confounders. We show how, given only a latent variable model for states and actions, policy value can be identified from off-policy data. Our method involves two stages. In the first, we show how to use proxies to estimate stationary distribution ratios, extending recent work on breaking the curse of horizon to the confounded setting. In the second, we show optimal balancing can be combined with such learned ratios to obtain policy value while avoiding direct modeling of reward functions. We establish theoretical guarantees of consistency, and benchmark our method empirically.        △ Less","27 July, 2020","cs.LG,cs.AI,stat.ML",
              A Proposed Access Control-Based Privacy Preservation Model to Share Healthcare Data in Cloud          ,2007.13850,https://arxiv.org/abs/2007.13850,https://arxiv.org/pdf/2007.13850,"Authors:PankajKhatiwada,HariBhusal,AyanChatterjee,MartinW.Gerdess","Healthcare data in cloud computing facilitates the treatment of patients efficiently by sharing information about personal health data between the healthcare providers for medical consultation. Furthermore, retaining the confidentiality of data and patients' identity is a another challenging task. This paper presents the concept of an access control-based (AC) privacy preservation model for the mutual authentication of users and data owners in the proposed digital system. The proposed model offers a high-security guarantee and high efficiency. The proposed digital system consists of four different entities, user, data owner, cloud server, and key generation center (KGC). This approach makes the system more robust and highly secure, which has been verified with multiple scenarios. Besides, the proposed model consisted of the setup phase, key generation phase, encryption phase, validation phase, access control phase, and data sharing phase. The setup phases are run by the data owner, which takes input as a security parameter and generates the system master key and security parameter. Then, in the key generation phase, the private key is generated by KGC and is stored in the cloud server. After that, the generated private key is encrypted. Then, the session key is generated by KGC and granted to the user and cloud server for storing, and then, the results are verified in the validation phase using validation messages. Finally, the data is shared with the user and decrypted at the user-end. The proposed model outperforms other methods with a maximal genuine data rate of 0.91.        △ Less","27 July, 2020","cs.CR,cs.CY",
              CPAS: the UK's National Machine Learning-based Hospital Capacity Planning System for COVID-19          ,2007.13825,https://arxiv.org/abs/2007.13825,https://arxiv.org/pdf/2007.13825,"Authors:ZhaozhiQian,AhmedM.Alaa,MihaelavanderSchaar","        The coronavirus disease 2019 (COVID-19) global pandemic poses the threat of overwhelming healthcare systems with unprecedented demands for intensive care resources. Managing these demands cannot be effectively conducted without a nationwide collective effort that relies on data to forecast hospital demands on the national, regional, hospital and individual levels. To this end, we developed the COVID-19 Capacity Planning and Analysis System (CPAS) - a machine learning-based system for hospital resource planning that we have successfully deployed at individual hospitals and across regions in the UK in coordination with NHS Digital. In this paper, we discuss the main challenges of deploying a machine learning-based decision support system at national scale, and explain how CPAS addresses these challenges by (1) defining the appropriate learning problem, (2) combining bottom-up and top-down analytical approaches, (3) using state-of-the-art machine learning algorithms, (4) integrating heterogeneous data sources, and (5) presenting the result with an interactive and transparent interface. CPAS is one of the first machine learning-based systems to be deployed in hospitals on a national scale to address the COVID-19 pandemic - we conclude the paper with a summary of the lessons learned from this experience.        △ Less","27 July, 2020","cs.LG,stat.ML",
              Energy Efficient Fog based Healthcare Monitoring Infrastructure          ,2007.13801,https://arxiv.org/abs/2007.13801,https://arxiv.org/pdf/2007.13801,"Authors:IdaSyafizaM.Isa,TaisirE.H.El-Gorashi,MohamedO.I.Musa,JaafarM.H.Elmirghani","        Recent advances in mobile technologies and cloud computing services have inspired the development of cloud-based real-time health monitoring systems. However, the transfer of health-related data to the cloud contributes to the burden on the networking infrastructures, leading to high latency and increased power consumption. Fog computing is introduced to relieve this burden by bringing services to the users proximity. This study proposes a new fog computing architecture for health monitoring applications based on a Gigabit Passive Optical Network (GPON) access network. An Energy-Efficient Fog Computing (EEFC) model is developed using Mixed Integer Linear Programming (MILP) to optimize the number and location of fog devices at the network edge to process and analyze the health data for energy-efficient fog computing. The performance of the EEFC model at low data rates and high data rates health applications is studied. The outcome of the study reveals that a total energy saving of 36% and 52% are attained via processing and analysis the health data at the fog in comparison to conventional processing and analysis at the central cloud for low data rate application and high data rate application, respectively. We also developed a real-time heuristic; Energy Optimized Fog Computing (EOFC) heuristic, with energy consumption performance approaching the EEFC model. Furthermore, we examined the energy efficiency improvements under different scenarios of devices idle power consumption and traffic volume.        △ Less","27 July, 2020","cs.NI,eess.SP",
              Neural Temporal Point Processes For Modelling Electronic Health Records          ,2007.13794,https://arxiv.org/abs/2007.13794,https://arxiv.org/pdf/2007.13794,"Authors:JosephEnguehard,DanBusbridge,AdamBozson,ClaireWoodcock,NilsY.Hammerla","        The modelling of Electronic Health Records (EHRs) has the potential to drive more efficient allocation of healthcare resources, enabling early intervention strategies and advancing personalised healthcare. However, EHRs are challenging to model due to their realisation as noisy, multi-modal data occurring at irregular time intervals. To address their temporal nature, we treat EHRs as samples generated by a Temporal Point Process (TPP), enabling us to model what happened in an event with when it happened in a principled way. We gather and propose neural network parameterisations of TPPs, collectively referred to as Neural TPPs. We perform evaluations on synthetic EHRs as well as on a set of established benchmarks. We show that TPPs significantly outperform their non-TPP counterparts on EHRs. We also show that an assumption of many Neural TPPs, that the class distribution is conditionally independent of time, reduces performance on EHRs. Finally, our proposed attention-based Neural TPP performs favourably compared to existing models, and provides insight into how it models the EHR, an important step towards a component of clinical decision support systems.        △ Less","27 July, 2020","cs.LG,stat.ML",
              Overview of digital health surveillance system during COVID-19 pandemic: public health issues and misapprehensions          ,2007.13633,https://arxiv.org/abs/2007.13633,https://arxiv.org/pdf/2007.13633,"Authors:MollaRashiedHussein,EhsanulHoqueApu,ShahriarShahabuddin,AbdullahBinShams,RussellKabir","        Without proper medication and vaccination for the COVID-19, many governments are using automated digital healthcare surveillance system to prevent and control the spread. There is not enough literature explaining the concerns and privacy issues; hence, we have briefly explained the topics in this paper. We focused on digital healthcare surveillance system's privacy concerns and different segments. Further research studies should be conducted in different sectors. This paper provides an overview based on the published articles, which are not focusing on the privacy issues that much. Artificial intelligence and 5G networks combine the advanced digital healthcare surveillance system; whereas Bluetooth-based contact tracing systems have fewer privacy concerns. More studies are required to find the appropriate digital healthcare surveillance system, which would be ideal for monitoring, controlling, and predicting the COVID-19 trajectory.        △ Less","27 July, 2020","cs.CY,cs.CR",
              Batch Inverse Reinforcement Learning Using Counterfactuals for Understanding Decision Making          ,2007.13531,https://arxiv.org/abs/2007.13531,https://arxiv.org/pdf/2007.13531,"Authors:IoanaBica,DanielJarrett,AlihanHüyük,MihaelavanderSchaar","        A key challenge in modeling real-world decision-making is the fact that active experimentation is often impossible (e.g. in healthcare). The goal of batch inverse reinforcement learning is to recover and understand policies on the basis of demonstrated behaviour--i.e. trajectories of observations and actions made by an expert maximizing some unknown reward function. We propose incorporating counterfactual reasoning into modeling decision behaviours in this setting. At each decision point, counterfactuals answer the question: Given the current history of observations, what would happen if we took a particular action? First, this offers a principled approach to learning inherently interpretable reward functions, which enables understanding the cost-benefit tradeoffs associated with an expert's actions. Second, by estimating the effects of different actions, counterfactuals readily tackle the off-policy nature of policy evaluation in the batch setting. Not only does this alleviate the cold-start problem typical of conventional solutions, but also accommodates settings where the expert policies are depending on histories of observations rather than just current states. Through experiments in both real and simulated medical environments, we illustrate the effectiveness of our batch, counterfactual inverse reinforcement learning approach in recovering accurate and interpretable descriptions of expert behaviour.        △ Less","2 July, 2020","cs.LG,cs.AI,stat.ML",
              A Simple and Interpretable Predictive Model for Healthcare,2007.13351,https://arxiv.org/abs/2007.13351,https://arxiv.org/pdf/2007.13351,"Authors:SubhadipMaji,RaghavBali,SreeHarshaAnkem,KishoreVAyyadevara","        Deep Learning based models are currently dominating most state-of-the-art solutions for disease prediction. Existing works employ RNNs along with multiple levels of attention mechanisms to provide interpretability. These deep learning models, with trainable parameters running into millions, require huge amounts of compute and data to train and deploy. These requirements are sometimes so huge that they render usage of such models as unfeasible. We address these challenges by developing a simpler yet interpretable non-deep learning based model for application to EHR data. We model and showcase our work's results on the task of predicting first occurrence of a diagnosis, often overlooked in existing works. We push the capabilities of a tree based model and come up with a strong baseline for more sophisticated models. Its performance shows an improvement over deep learning based solutions (both, with and without the first-occurrence constraint) all the while maintaining interpretability.        △ Less","27 July, 2020","stat.ML,cs.LG,stat.AP",
              The Causality Inference of Public Interest in Restaurants and Bars on COVID-19 Daily Cases in the US: A Google Trends Analysis          ,2007.13255,https://arxiv.org/abs/2007.13255,https://arxiv.org/pdf/2007.13255,"Authors:MiladAsgariMehrabadi,NikilDutt,AmirM.Rahmani","        The COVID-19 coronavirus pandemic has affected virtually every region of the globe. At the time of conducting this study, the number of daily cases in the United States is more than any other country, and the trend is increasing in most of its states. Google trends provide public interest in various topics during different periods. Analyzing these trends using data mining methods might provide useful insights and observations regarding the COVID-19 outbreak. The objective of this study was to consider the predictive ability of different search terms (i.e., bars and restaurants) with regards to the increase of daily cases in the US. We considered the causation of two different search query trends, namely restaurant and bars, on daily positive cases in top-10 states/territories of the United States with the highest and lowest daily new positive cases. In addition, to measure the linear relation of different trends, we used Pearson correlation. Our results showed for states/territories with higher numbers of daily cases, the historical trends in search queries related to bars and restaurants, which mainly happened after re-opening, significantly affect the daily new cases, on average. California, for example, had most searches for restaurants on June 7th, 2020, which affected the number of new cases within two weeks after the peak with the P-value of .004 for Granger's causality test. Although a limited number of search queries were considered, Google search trends for restaurants and bars showed a significant effect on daily new cases for regions with higher numbers of daily new cases in the United States. We showed that such influential search trends could be used as additional information for prediction tasks in new cases of each region. This prediction can help healthcare leaders manage and control the impact of COVID-19 outbreaks on society and be prepared for the outcomes.        △ Less","26 July, 2020","stat.AP,cs.IR,cs.LG",
              Deep Kernel Survival Analysis and Subject-Specific Survival Time Prediction Intervals          ,2007.12975,https://arxiv.org/abs/2007.12975,https://arxiv.org/pdf/2007.12975,Authors:GeorgeH.Chen,"        Kernel survival analysis methods predict subject-specific survival curves and times using information about which training subjects are most similar to a test subject. These most similar training subjects could serve as forecast evidence. How similar any two subjects are is given by the kernel function. In this paper, we present the first neural network framework that learns which kernel functions to use in kernel survival analysis. We also show how to use kernel functions to construct prediction intervals of survival time estimates that are statistically valid for individuals similar to a test subject. These prediction intervals can use any kernel function, such as ones learned using our neural kernel learning framework or using random survival forests. Our experiments show that our neural kernel survival estimators are competitive with a variety of existing survival analysis methods, and that our prediction intervals can help compare different methods' uncertainties, even for estimators that do not use kernels. In particular, these prediction interval widths can be used as a new performance metric for survival analysis methods.        △ Less","25 July, 2020","stat.ML,cs.LG",
              A Canonical Architecture For Predictive Analytics on Longitudinal Patient Records          ,2007.12780,https://arxiv.org/abs/2007.12780,https://arxiv.org/pdf/2007.12780,"Authors:ParthasarathySuryanarayanan,BhavaniIyer,PrithwishChakraborty,BiboHao,ItaloBuleje,PiyushMadan,JamesCodella,AntonioFoncubierta,DivyaPathak,SarahMiller,AmolRajmane,ShannonHarrer,GigiYuan-Reed,DabySow","        Many institutions within the healthcare ecosystem are making significant investments in AI technologies to optimize their business operations at lower cost with improved patient outcomes. Despite the hype with AI, the full realization of this potential is seriously hindered by several systemic problems, including data privacy, security, bias, fairness, and explainability. In this paper, we propose a novel canonical architecture for the development of AI models in healthcare that addresses these challenges. This system enables the creation and management of AI predictive models throughout all the phases of their life cycle, including data ingestion, model building, and model promotion in production environments. This paper describes this architecture in detail, along with a qualitative evaluation of our experience of using it on real world problems.        △ Less","24 July, 2020","cs.LG,cs.AI,cs.CY",
              A unified survey on treatment effect heterogeneity modeling and uplift modeling          ,2007.12769,https://arxiv.org/abs/2007.12769,https://arxiv.org/pdf/2007.12769,"Authors:WeijiaZhang,JiuyongLi,LinLiu","        A central question in many fields of scientific research is to determine how an outcome would be affected by an action, or to measure the effect of an action (a.k.a treatment effect). In recent years, a need for estimating the heterogeneous treatment effects conditioning on the different characteristics of individuals has emerged from research fields such as personalized healthcare, social science, and online marketing. To meet the need, researchers and practitioners from different communities have developed algorithms by taking the treatment effect heterogeneity modeling approach and the uplift modeling approach, respectively. In this paper, we provide a unified survey of these two seemingly disconnected yet closely related approaches under the potential outcome framework. We then provide a structured survey of existing methods by emphasizing on their inherent connections with a set of unified notations to make comparisons of the different methods easy. We then review the main applications of the surveyed methods in personalized marketing, personalized medicine, and social studies. Finally, we summarize the existing software packages and present discussions based on the use of methods on synthetic, semi-synthetic and real world data sets and provide some general guidelines for choosing methods.        △ Less","13 July, 2020","stat.ME,cs.LG,stat.AP,stat.ML",
              Clinician-in-the-Loop Decision Making: Reinforcement Learning with Near-Optimal Set-Valued Policies          ,2007.12678,https://arxiv.org/abs/2007.12678,https://arxiv.org/pdf/2007.12678,"Authors:ShengpuTang,AdityaModi,MichaelW.Sjoding,JennaWiens","        Standard reinforcement learning (RL) aims to find an optimal policy that identifies the best action for each state. However, in healthcare settings, many actions may be near-equivalent with respect to the reward (e.g., survival). We consider an alternative objective -- learning set-valued policies to capture near-equivalent actions that lead to similar cumulative rewards. We propose a model-free algorithm based on temporal difference learning and a near-greedy heuristic for action selection. We analyze the theoretical properties of the proposed algorithm, providing optimality guarantees and demonstrate our approach on simulated environments and a real clinical task. Empirically, the proposed algorithm exhibits good convergence properties and discovers meaningful near-equivalent actions. Our work provides theoretical, as well as practical, foundations for clinician/human-in-the-loop decision making, in which humans (e.g., clinicians, patients) can incorporate additional knowledge (e.g., side effects, patient preference) when selecting among near-equivalent actions.        △ Less","24 July, 2020","cs.LG,cs.AI,stat.ML",
              Real-World Multi-Domain Data Applications for Generalizations to Clinical Settings          ,2007.12672,https://arxiv.org/abs/2007.12672,https://arxiv.org/pdf/2007.12672,"Authors:NooshinMojab,VahidNoroozi,DarvinYi,ManojPrabhakarNallabothula,AbdullahAleem,PhillipS.Yu,JoelleA.Hallak","        With promising results of machine learning based models in computer vision, applications on medical imaging data have been increasing exponentially. However, generalizations to complex real-world clinical data is a persistent problem. Deep learning models perform well when trained on standardized datasets from artificial settings, such as clinical trials. However, real-world data is different and translations are yielding varying results. The complexity of real-world applications in healthcare could emanate from a mixture of different data distributions across multiple device domains alongside the inevitable noise sourced from varying image resolutions, human errors, and the lack of manual gradings. In addition, healthcare applications not only suffer from the scarcity of labeled data, but also face limited access to unlabeled data due to HIPAA regulations, patient privacy, ambiguity in data ownership, and challenges in collecting data from different sources. These limitations pose additional challenges to applying deep learning algorithms in healthcare and clinical translations. In this paper, we utilize self-supervised representation learning methods, formulated effectively in transfer learning settings, to address limited data availability. Our experiments verify the importance of diverse real-world data for generalization to clinical settings. We show that by employing a self-supervised approach with transfer learning on a multi-domain real-world dataset, we can achieve 16% relative improvement on a standardized dataset over supervised baselines.        △ Less","24 July, 2020",cs.CV,
              Improved Slice-wise Tumour Detection in Brain MRIs by Computing Dissimilarities between Latent Representations          ,2007.12528,https://arxiv.org/abs/2007.12528,https://arxiv.org/pdf/2007.12528,"Authors:Alexandra-IoanaAlbu,AlinaEnescu,LuigiMalagò","        Anomaly detection for Magnetic Resonance Images (MRIs) can be solved with unsupervised methods by learning the distribution of healthy images and identifying anomalies as outliers. In presence of an additional dataset of unlabelled data containing also anomalies, the task can be framed as a semi-supervised task with negative and unlabelled sample points. Recently, in Albu et al., 2020, we have proposed a slice-wise semi-supervised method for tumour detection based on the computation of a dissimilarity function in the latent space of a Variational AutoEncoder, trained on unlabelled data. The dissimilarity is computed between the encoding of the image and the encoding of its reconstruction obtained through a different autoencoder trained only on healthy images. In this paper we present novel and improved results for our method, obtained by training the Variational AutoEncoders on a subset of the HCP and BRATS-2018 datasets and testing on the remaining individuals. We show that by training the models on higher resolution images and by improving the quality of the reconstructions, we obtain results which are comparable with different baselines, which employ a single VAE trained on healthy individuals. As expected, the performance of our method increases with the size of the threshold used to determine the presence of an anomaly.        △ Less","24 July, 2020","cs.LG,eess.IV,stat.ML",
              Impact of Medical Data Imprecision on Learning Results          ,2007.12375,https://arxiv.org/abs/2007.12375,https://arxiv.org/pdf/2007.12375,"Authors:MeiWang,JianwenSu,HaiqinLu","        Test data measured by medical instruments often carry imprecise ranges that include the true values. The latter are not obtainable in virtually all cases. Most learning algorithms, however, carry out arithmetical calculations that are subject to uncertain influence in both the learning process to obtain models and applications of the learned models in, e.g. prediction. In this paper, we initiate a study on the impact of imprecision on prediction results in a healthcare application where a pre-trained model is used to predict future state of hyperthyroidism for patients. We formulate a model for data imprecisions. Using parameters to control the degree of imprecision, imprecise samples for comparison experiments can be generated using this model. Further, a group of measures are defined to evaluate the different impacts quantitatively. More specifically, the statistics to measure the inconsistent prediction for individual patients are defined. We perform experimental evaluations to compare prediction results based on the data from the original dataset and the corresponding ones generated from the proposed precision model using the long-short-term memories (LSTM) network. The results against a real world hyperthyroidism dataset provide insights into how small imprecisions can cause large ranges of predicted results, which could cause mis-labeling and inappropriate actions (treatments or no treatments) for individual patients.        △ Less","24 July, 2020","cs.LG,cs.AI,stat.ML",
              Dynamic Knowledge Distillation for Black-box Hypothesis Transfer Learning          ,2007.12355,https://arxiv.org/abs/2007.12355,https://arxiv.org/pdf/2007.12355,"Authors:YiqinYu,XuMin,ShiwanZhao,JingMei,FeiWang,DongshengLi,KenneyNg,ShaochunLi","        In real world applications like healthcare, it is usually difficult to build a machine learning prediction model that works universally well across different institutions. At the same time, the available model is often proprietary, i.e., neither the model parameter nor the data set used for model training is accessible. In consequence, leveraging the knowledge hidden in the available model (aka. the hypothesis) and adapting it to a local data set becomes extremely challenging. Motivated by this situation, in this paper we aim to address such a specific case within the hypothesis transfer learning framework, in which 1) the source hypothesis is a black-box model and 2) the source domain data is unavailable. In particular, we introduce a novel algorithm called dynamic knowledge distillation for hypothesis transfer learning (dkdHTL). In this method, we use knowledge distillation with instance-wise weighting mechanism to adaptively transfer the ""dark"" knowledge from the source hypothesis to the target domain.The weighting coefficients of the distillation loss and the standard loss are determined by the consistency between the predicted probability of the source hypothesis and the target ground-truth label.Empirical results on both transfer learning benchmark datasets and a healthcare dataset demonstrate the effectiveness of our method.        △ Less","6 August, 2020","cs.LG,stat.ML",
              User-driven Analysis of Longitudinal Health Data with Hidden Markov Models for Clinical Insights          ,2007.12346,https://arxiv.org/abs/2007.12346,https://arxiv.org/pdf/2007.12346,Authors:BumChulKwon,"        A goal of clinical researchers is to understand the progression of a disease through a set of biomarkers. Researchers often conduct observational studies, where they collect numerous samples from selected subjects throughout multiple years. Hidden Markov Models (HMMs) can be applied to discover latent states and their transition probabilities over time. However, it is challenging for clinical researchers to interpret the outcomes and to gain insights about the disease. Thus, this demo introduces an interactive visualization system called DPVis, which was designed to help researchers to interactively explore HMM outcomes. The demo provides guidelines of how to implement the clinician-in-the-loop approach for analyzing longitudinal, observational health data with visual analytics.        △ Less","24 July, 2020",cs.HC,
              Clinical Recommender System: Predicting Medical Specialty Diagnostic Choices with Neural Network Ensembles          ,2007.12161,https://arxiv.org/abs/2007.12161,https://arxiv.org/pdf/2007.12161,"Authors:MortezaNoshad,IvanaJankovic,JonathanH.Chen","        The growing demand for key healthcare resources such as clinical expertise and facilities has motivated the emergence of artificial intelligence (AI) based decision support systems. We address the problem of predicting clinical workups for specialty referrals. As an alternative for manually-created clinical checklists, we propose a data-driven model that recommends the necessary set of diagnostic procedures based on the patients' most recent clinical record extracted from the Electronic Health Record (EHR). This has the potential to enable health systems expand timely access to initial medical specialty diagnostic workups for patients. The proposed approach is based on an ensemble of feed-forward neural networks and achieves significantly higher accuracy compared to the conventional clinical checklists.        △ Less","23 July, 2020","cs.LG,cs.IR,stat.ML",
              Privacy-preserving Artificial Intelligence Techniques in Biomedicine          ,2007.11621,https://arxiv.org/abs/2007.11621,https://arxiv.org/pdf/2007.11621,"Authors:ReihanehTorkzadehmahani,RezaNasirigerdeh,DavidB.Blumenthal,TimKacprowski,MarkusList,JulianMatschinske,JulianSpäth,NinaKerstinWenke,BélaBihari,TobiasFrisch,AnneHartebrodt,Anne-ChristinHausschild,DominikHeider,AndreasHolzinger,WalterHötzendorfer,MarkusKastelitz,RudolfMayer,CristianNogales,AnastasiaPustozerova,RichardRöttger,HaraldH.H.W.Schmidt,AmeliSchwalber,ChristofTschohl,AndreaWohner,JanBaumbach","        Artificial intelligence (AI) has been successfully applied in numerous scientific domains including biomedicine and healthcare. Here, it has led to several breakthroughs ranging from clinical decision support systems, image analysis to whole genome sequencing. However, training an AI model on sensitive data raises also concerns about the privacy of individual participants. Adversary AIs, for example, can abuse even summary statistics of a study to determine the presence or absence of an individual in a given dataset. This has resulted in increasing restrictions to access biomedical data, which in turn is detrimental for collaborative research and impedes scientific progress. Hence there has been an explosive growth in efforts to harness the power of AI for learning from sensitive data while protecting patients' privacy. This paper provides a structured overview of recent advances in privacy-preserving AI techniques in biomedicine. It places the most important state-of-the-art approaches within a unified taxonomy, and discusses their strengths, limitations, and open problems.        △ Less","22 July, 2020","cs.CR,cs.AI",
              Understanding the temporal evolution of COVID-19 research through machine learning and natural language processing          ,2007.11604,https://arxiv.org/abs/2007.11604,https://arxiv.org/pdf/2007.11604,"Authors:AshkanEbadi,PengchengXi,StéphaneTremblay,BruceSpencer,RamanPall,AlexanderWong","        The outbreak of the novel coronavirus disease 2019 (COVID-19), caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has been continuously affecting human lives and communities around the world in many ways, from cities under lockdown to new social experiences. Although in most cases COVID-19 results in mild illness, it has drawn global attention due to the extremely contagious nature of SARS-CoV-2. Governments and healthcare professionals, along with people and society as a whole, have taken any measures to break the chain of transition and flatten the epidemic curve. In this study, we used multiple data sources, i.e., PubMed and ArXiv, and built several machine learning models to characterize the landscape of current COVID-19 research by identifying the latent topics and analyzing the temporal evolution of the extracted research themes, publications similarity, and sentiments, within the time-frame of January- May 2020. Our findings confirm the types of research available in PubMed and ArXiv differ significantly, with the former exhibiting greater diversity in terms of COVID-19 related issues and the latter focusing more on intelligent systems/tools to predict/diagnose COVID-19. The special attention of the research community to the high-risk groups and people with complications was also confirmed.        △ Less","22 July, 2020","cs.LG,cs.DL,cs.IR",
              Internet of Things for Current COVID-19 and Future Pandemics: An Exploratory Study          ,2007.11147,https://arxiv.org/abs/2007.11147,https://arxiv.org/pdf/2007.11147,"Authors:MohammadNasajpour,SeyedaminPouriyeh,RezaM.Parizi,MohsenDorodchi,MariaValero,HamidR.Arabnia","        In recent years, the Internet of Things (IoT) has drawn convincing research ground as a new research topic in a wide variety of academic and industrial disciplines, especially in healthcare. The IoT revolution is reshaping modern healthcare systems by incorporating technological, economic, and social prospects. It is evolving healthcare systems from conventional to more personalized healthcare systems through which patients can be diagnosed, treated, and monitored more easily. The current global challenge of the pandemic caused by the novel severe contagious respiratory syndrome coronavirus 2 presents the greatest global public health crisis since the pandemic influenza outbreak of 1918. At the time this paper was written, the number of diagnosed COVID-19 cases around the world had reached more than 31 million. Since the pandemic started, there has been a rapid effort in different research communities to exploit a wide variety of technologies to combat this worldwide threat, and IoT technology is one of the pioneers in this area. In the context of COVID-19, IoT enabled /linked devices/applications are utilized to lower the possible spread of COVID-19 to others by early diagnosis, monitoring patients, and practicing defined protocols after patient recovery. This paper surveys the role of IoT-based technologies in COVID-19 and reviews the state-of-the-art architectures, platforms, applications, and industrial IoT-based solutions combating COVID-19 in three main phases, including early diagnosis, quarantine time, and after recovery.        △ Less","25 September, 2020",cs.CY,
"              Challenges in Developing Secure Mobile Health Applications, A Systematic Review          ",2007.10876,https://arxiv.org/abs/2007.10876,https://arxiv.org/pdf/2007.10876,"Authors:BakheetAljedaani,MAliBabar","        Mobile health (mHealth) applications (apps) have gained significant popularity over the last few years due to its tremendous benefits. However, the sensitivity of healthcare data makes the security of mHealth apps a serious concern. The use of poor security practices and lack of Security Knowledge (SK) on developers side can embed several vulnerabilities in mHealth apps. In this review paper, we aim at identifying and analysing the challenges that the developers of mHealth apps face with respect to security. The knowledge of such challenges can help to reduce the risk of developing insecure mHealth apps. We followed Systematic Literature Review (SLR) method for this review. We selected 26 studies using predefined criteria and used thematic analysis method for analysing the extracted data. We identified seven challenges that can affect the development of secure mHealth apps. We have presented a conceptual framework which highlights the correlation between the identified challenges. Whilst mHealth apps development organizations might overlook the security, we conclude that our findings can be beneficial to assist them to identify the weaknesses and improve their security practices. Similarly, the developers of mHealth apps can identify the challenges they are facing to enable them to develop mHealth apps that do not pose security risk for users. Our review suggests further support for mHealth apps developers by continuously providing the needed SK, seeking to hire a security expert in the domain of mHealth app and providing sufficient time to deliver an app.        △ Less","21 July, 2020",cs.SE,
              Nine Recommendations for Decision Aid Implementation from the Clinician Perspective          ,2007.10797,https://arxiv.org/abs/2007.10797,https://arxiv.org/pdf/2007.10797,"Authors:AnshuAnkolekar,BenG.L.Vanneste,EstherBloemen-vanGurp,JoepvanRoermund,AdrianaBerlanga,CherylRoumen,EvertvanLimbergen,LudyLutgens,TomMarcelissen,PhilippeLambin,AndreDekker,RianneFijten","        Background: Shared decision-making (SDM) aims to empower patients to take an active role in their treatment choices, supported by clinicians and patient decision aids (PDAs). The purpose of this study is to explore barriers and possible facilitators to SDM and a PDA in the prostate cancer trajectory. In the process we identify possible actions that organizations and individuals can take to support implementation in practice.  Methods: We use the Ottawa Model of Research Use as a framework to determine the barriers and facilitators to SDM and PDAs from the perspective of clinicians. Semi-structured interviews were conducted with urologists (n=4), radiation oncologists (n=3), and oncology nurses (n=2), focusing on the current decision-making process experienced by these stakeholders. Questions included their attitudes towards SDM and PDAs, barriers to implementation and possible strategies to overcome them.  Results: Time pressure and patient characteristics were cited as major barriers by 55% of the clinicians we interviewed. Structural factors such as external quotas for certain treatment procedures were also considered as barriers by 44% of the clinicians. Facilitating factors involved organizational changes to em-bed PDAs in the treatment trajectory, training in using PDAs as a tool for SDM, and clinician motivation by disseminating positive clinical outcomes. Our findings also suggest a role for external stakeholders such as healthcare insurers in creating economic incentives to facilitate implementation.  Conclusion: Our findings highlight the importance of a multi-faceted implementation strategy to support SDM. While clinician motivation and patient activation are essential, structural/economic barriers may hamper implementation. Action must also be taken at the administrative and policy levels to foster a collaborative environment for SDM and, in the process, for PDAs.        △ Less","21 July, 2020",cs.CY,
              Generalization and Invariances in the Presence of Unobserved Confounding          ,2007.10653,https://arxiv.org/abs/2007.10653,https://arxiv.org/pdf/2007.10653,"Authors:AlexisBellot,MihaelavanderSchaar","        The ability to extrapolate, or generalize, from observed to new related environments is central to any form of reliable machine learning, yet most methods fail when moving beyond i.i.di.i.d data. In some cases, the reason lies in a misappreciation of the causal structure that governs the data, and in particular as a consequence of the influence of unobserved confounders that drive changes in observed distributions and distort correlations. In this paper, we argue for defining generalization with respect to a broader class of distribution shifts (defined as arising from interventions in the underlying causal model), including changes in observed, unobserved and target variable distributions. We propose a new robust learning principle that may be paired with any gradient-based learning algorithm. This learning principle has explicit generalization guarantees, and relates robustness with certain invariances in the causal model, clarifying why, in some cases, test performance lags training performance. We demonstrate the empirical performance of our approach on healthcare data from different modalities, including image and speech data.        △ Less","6 October, 2020","stat.ML,cs.LG",
              The Future AI in Healthcare: A Tsunami of False Alarms or a Product of Experts?          ,2007.10502,https://arxiv.org/abs/2007.10502,https://arxiv.org/pdf/2007.10502,Authors:GariD.Clifford,"        Recent significant increases in affordable and accessible computational power and data storage have enabled machine learning to provide almost unbelievable classification and prediction performances compared to well-trained humans. There have been some promising (but limited) results in the complex healthcare landscape, particularly in imaging. This promise has led some individuals to leap to the conclusion that we will solve an ever-increasing number of problems in human health and medicine by applying `artificial intelligence' to `big (medical) data'. The scientific literature has been inundated with algorithms, outstripping our ability to review them effectively. Unfortunately, I argue that most, if not all of these publications or commercial algorithms make several fundamental errors. I argue that because everyone (and therefore every algorithm) has blind spots, there are multiple `best' algorithms, each of which excels on different types of patients or in different contexts. Consequently, we should vote many algorithms together, weighted by their overall performance, their independence from each other, and a set of features that define the context (i.e., the features that maximally discriminate between the situations when one algorithm outperforms another). This approach not only provides a better performing classifier or predictor but provides confidence intervals so that a clinician can judge how to respond to an alert. Moreover, I argue that a sufficient number of (mostly) independent algorithms that address the same problem can be generated through a large international competition/challenge, lasting many months and define the conditions for a successful event. Finally, I propose introducing the requirement for major grantees to run challenges in the final year of funding to maximize the value of research and select a new generation of grantees.        △ Less","26 July, 2020",cs.CY,
              Visualizing Deep Graph Generative Models for Drug Discovery          ,2007.10333,https://arxiv.org/abs/2007.10333,https://arxiv.org/pdf/2007.10333,"Authors:KaranYang,ChengxiZang,FeiWang","        Drug discovery aims at designing novel molecules with specific desired properties for clinical trials. Over past decades, drug discovery and development have been a costly and time consuming process. Driven by big chemical data and AI, deep generative models show great potential to accelerate the drug discovery process. Existing works investigate different deep generative frameworks for molecular generation, however, less attention has been paid to the visualization tools to quickly demo and evaluate model's results. Here, we propose a visualization framework which provides interactive visualization tools to visualize molecules generated during the encoding and decoding process of deep graph generative models, and provide real time molecular optimization functionalities. Our work tries to empower black box AI driven drug discovery models with some visual interpretabilities.        △ Less","20 July, 2020","cs.LG,cs.HC,stat.ML",
              An Empirical Characterization of Fair Machine Learning For Clinical Risk Prediction          ,2007.10306,https://arxiv.org/abs/2007.10306,https://arxiv.org/pdf/2007.10306,"Authors:StephenR.Pfohl,AgataForyciarz,NigamH.Shah","        The use of machine learning to guide clinical decision making has the potential to worsen existing health disparities. Several recent works frame the problem as that of algorithmic fairness, a framework that has attracted considerable attention and criticism. However, the appropriateness of this framework is unclear due to both ethical as well as technical considerations, the latter of which include trade-offs between measures of fairness and model performance that are not well-understood for predictive models of clinical outcomes. To inform the ongoing debate, we conduct an empirical study to characterize the impact of penalizing group fairness violations on an array of measures of model performance and group fairness. We repeat the analyses across multiple observational healthcare databases, clinical outcomes, and sensitive attributes. We find that procedures that penalize differences between the distributions of predictions across groups induce nearly-universal degradation of multiple performance metrics within groups. On examining the secondary impact of these procedures, we observe heterogeneity of the effect of these procedures on measures of fairness in calibration and ranking across experimental conditions. Beyond the reported trade-offs, we emphasize that analyses of algorithmic fairness in healthcare lack the contextual grounding and causal awareness necessary to reason about the mechanisms that lead to health disparities, as well as about the potential of algorithmic fairness methods to counteract those mechanisms. In light of these limitations, we encourage researchers building predictive models for clinical use to step outside the algorithmic fairness frame and engage critically with the broader sociotechnical context surrounding the use of machine learning in healthcare.        △ Less","20 July, 2020","stat.ML,cs.CY,cs.LG,stat.AP",
              Coronavirus Knowledge Graph: A Case Study          ,2007.10287,https://arxiv.org/abs/2007.10287,https://arxiv.org/pdf/2007.10287,"Authors:ChongyanChen,IslamAkefEbeid,YiBu,YingDing","        The emergence of the novel COVID-19 pandemic has had a significant impact on global healthcare and the economy over the past few months. The virus's rapid widespread has led to a proliferation in biomedical research addressing the pandemic and its related topics. One of the essential Knowledge Discovery tools that could help the biomedical research community understand and eventually find a cure for COVID-19 are Knowledge Graphs. The CORD-19 dataset is a collection of publicly available full-text research articles that have been recently published on COVID-19 and coronavirus topics. Here, we use several Machine Learning, Deep Learning, and Knowledge Graph construction and mining techniques to formalize and extract insights from the PubMed dataset and the CORD-19 dataset to identify COVID-19 related experts and bio-entities. Besides, we suggest possible techniques to predict related diseases, drug candidates, gene, gene mutations, and related compounds as part of a systematic effort to apply Knowledge Discovery methods to help biomedical researchers tackle the pandemic.        △ Less","3 July, 2020","cs.AI,cs.CL",
              A Comprehensive Evaluation of Multi-task Learning and Multi-task Pre-training on EHR Time-series Data          ,2007.10185,https://arxiv.org/abs/2007.10185,https://arxiv.org/pdf/2007.10185,"Authors:MatthewB.A.McDermott,BretNestor,EvanKim,WancongZhang,AnnaGoldenberg,PeterSzolovits,MarzyehGhassemi","        Multi-task learning (MTL) is a machine learning technique aiming to improve model performance by leveraging information across many tasks. It has been used extensively on various data modalities, including electronic health record (EHR) data. However, despite significant use on EHR data, there has been little systematic investigation of the utility of MTL across the diverse set of possible tasks and training schemes of interest in healthcare. In this work, we examine MTL across a battery of tasks on EHR time-series data. We find that while MTL does suffer from common negative transfer, we can realize significant gains via MTL pre-training combined with single-task fine-tuning. We demonstrate that these gains can be achieved in a task-independent manner and offer not only minor improvements under traditional learning, but also notable gains in a few-shot learning context, thereby suggesting this could be a scalable vehicle to offer improved performance in important healthcare contexts.        △ Less","20 July, 2020","cs.LG,stat.ML",
              Unsupervised anomaly detection for discrete sequence healthcare data          ,2007.10098,https://arxiv.org/abs/2007.10098,https://arxiv.org/pdf/2007.10098,"Authors:VictoriaSnorovikhina,AlexeyZaytsev","        Fraud in healthcare is widespread, as doctors could prescribe unnecessary treatments to increase bills. Insurance companies want to detect these anomalous fraudulent bills and reduce their losses. Traditional fraud detection methods use expert rules and manual data processing.  Recently, machine learning techniques automate this process, but hand-labeled data is extremely costly and usually out of date. We propose a machine learning model that automates fraud detection in an unsupervised way. Two deep learning approaches include LSTM neural network for prediction next patient visit and a seq2seq model. For normalization of produced anomaly scores, we propose Empirical Distribution Function (EDF) approach. So, the algorithm works with high class imbalance problems.  We use real data on sequences of patients' visits data from Allianz company for the validation. The models provide state-of-the-art results for unsupervised anomaly detection for fraud detection in healthcare. Our EDF approach further improves the quality of LSTM model.        △ Less","12 October, 2020","cs.LG,stat.ML",
              How are you? Introducing stress-based text tailoring          ,2007.09970,https://arxiv.org/abs/2007.09970,https://arxiv.org/pdf/2007.09970,"Authors:SimoneBalloccu,EhudReiter,AlexandraJohnstone,ClaireFyfe","        Can stress affect not only your life but also how you read and interpret a text? Healthcare has shown evidence of such dynamics and in this short paper we discuss customising texts based on user stress level, as it could represent a critical factor when it comes to user engagement and behavioural change. We first show a real-world example in which user behaviour is influenced by stress, then, after discussing which tools can be employed to assess and measure it, we propose an initial method for tailoring the document by exploiting complexity reduction and affect enforcement. The result is a short and encouraging text which requires less commitment to be read and understood. We believe this work in progress can raise some interesting questions on a topic that is often overlooked in NLG.        △ Less","20 July, 2020","cs.CL,cs.HC",
              Trends in Cuban research output: publications and patents          ,2007.09638,https://arxiv.org/abs/2007.09638,https://arxiv.org/pdf/2007.09638,Authors:YassetPerez-Riverol,"        Cuban science and technology are known for important achievements, particularly in human healthcare and biotechnology. During the second half of XX century, the country developed a system of scientific institutions to address and solve major economical, cultural, social and health problems. However, the economic crisis faced by the island during the last three decades has had a major impact in Cuban scientific research. In addition to decreased investment, the emigration of thousands of young as well as senior scientists to other countries have had a major impact in Cuban research output. To date, no systematic analysis regarding scientific publications, citations, or patents granted to Cuban authors during this period, are available. Here, an analysis of Cuban scientific production since 1970, with an especial focus on the last three decades (1990 - 2019), is provided. All national metrics are compared with other countries, emphasizing those from Latin America. Preliminary results show that Cuban scientific publications are increased at a lower rate (two-fold) compare with several Latin American countries (five-fold average). In addition, since 2014 the annual number of Cuban scientific publications is decreasing. Finally, the analysis shows that most young Cuban authors with the higher index of citations (1990-2019) are working abroad. All the data and the code related to this study are open and can be found in GitHub (https://github.com/ypriverol/cubascience).        △ Less","19 July, 2020",cs.DL,
              Automated Phenotyping via Cell Auto Training (CAT) on the Cell DIVE Platform          ,2007.09471,https://arxiv.org/abs/2007.09471,https://arxiv.org/pdf/2007.09471,"Authors:AlbertoSantamaria-Pang,AnupSood,DanMeyer,AritraChowdhury,FionaGinty","        We present a method for automatic cell classification in tissue samples using an automated training set from multiplexed immunofluorescence images. The method utilizes multiple markers stained in situ on a single tissue section on a robust hyperplex immunofluorescence platform (Cell DIVE, GE Healthcare) that provides multi-channel images allowing analysis at single cell/sub-cellular levels. The cell classification method consists of two steps: first, an automated training set from every image is generated using marker-to-cell staining information. This mimics how a pathologist would select samples from a very large cohort at the image level. In the second step, a probability model is inferred from the automated training set. The probabilistic model captures staining patterns in mutually exclusive cell types and builds a single probability model for the data cohort. We have evaluated the proposed approach to classify: i) immune cells in cancer and ii) brain cells in neurological degenerative diseased tissue with average accuracies above 95%.        △ Less","18 July, 2020","eess.IV,cs.CV,q-bio.CB,q-bio.QM",10.1109/BIBM47256.2019.8983271 
              Toward a Deep Learning-Driven Intrusion Detection Approach for Internet of Things          ,2007.09342,https://arxiv.org/abs/2007.09342,https://arxiv.org/pdf/2007.09342,"Authors:MengmengGe,NaeemFirdousSyed,XipingFu,ZubairBaig,AntonioRobles-Kelly","        Internet of Things (IoT) has brought along immense benefits to our daily lives encompassing a diverse range of application domains that we regularly interact with, ranging from healthcare automation to transport and smart environments. However, due to the limitation of constrained resources and computational capabilities, IoT networks are prone to various cyber attacks. Thus, defending the IoT network against adversarial attacks is of vital importance. In this paper, we present a novel intrusion detection approach for IoT networks through the application of a deep learning technique. We adopt a cutting-edge IoT dataset comprising IoT traces and realistic attack traffic, including denial of service, distributed denial of service, reconnaissance and information theft attacks. We utilise the header field information in individual packets as generic features to capture general network behaviours, and develop a feed-forward neural networks model with embedding layers (to encode high-dimensional categorical features) for multi-class classification. The concept of transfer learning is subsequently adopted to encode high-dimensional categorical features to build a binary classifier. Results obtained through the evaluation of the proposed approach demonstrate a high classification accuracy for both binary and multi-class classifiers.        △ Less","18 July, 2020",cs.CR,
              Efficient Facemask Sterilization via Forced Ozone Convection          ,2007.09280,https://arxiv.org/abs/2007.09280,https://arxiv.org/pdf/2007.09280,"Authors:JosephSchwan,TroyR.Alva,GiorgioNava,CarlaBerrospeRodriguez,JustinW.Chartron,JoshuaMorgan,LorenzoMangolini","        During the beginning of 2020, the Covid-19 pandemic took the world by surprise, rapidly spreading undetected between and within many countries and wreaking havoc on the global economy both through death tolls and lockdowns. Healthcare professionals treating the coronavirus patients grapple with a massive and unprecedented shortage of Facepiece Respirators (FPRs) and other personal protective equipment (PPE), which act as fundamental tools to protect the health of the medical staff treating the patients affected by the coronavirus. While many FPRs are designed to be disposable single-use devices, the development of sterilization strategies is necessary to circumvent future shortages. Here, we describe the development of a plasma-based method to sterilize PPE such as FPRs with ozone. The novel design uses a flow-through configuration where ozone directly flows through the fibers of the PPE through the maintenance of a pressure gradient. Canonical ozone-based methods place the mask into a sealed ozone-containing enclosure but lack pressurization to permeate the mask fibers. In this device, ozone is created through an atmospheric pressure Dielectric Barrier Discharge (DBD) fed with compressed air. Due to limited supply and clinical need of FPRs, we demonstrated sterilization with surgical masks. We demonstrate rapid sterilization using E. coli as a model pathogen. A flow-through configuration enables a >400% improvement of the sterilization efficiency with respect to the canonical approach. This method has potential for a broad and cost-effective utilization. Using the power supply from a readily available plasma ball toy, a plastic box, a glass tube, steel mesh, and 3D printed components, we designed and tested an extremely affordable portable prototype system for rapid single mask sterilization which produced comparable results to its large high-cost equivalent.        △ Less","17 July, 2020","physics.med-ph,physics.plasm-ph",
              Visual Explanation for Identification of the Brain Bases for Dyslexia on fMRI Data          ,2007.09260,https://arxiv.org/abs/2007.09260,https://arxiv.org/pdf/2007.09260,"Authors:LauraTomazDaSilva,NathaliaBianchiniEsper,DuncanD.Ruiz,FelipeMeneguzzi,AugustoBuchweitz","        Brain imaging of mental health, neurodevelopmental and learning disorders has coupled with machine learning to identify patients based only on their brain activation, and ultimately identify features that generalize from smaller samples of data to larger ones. However, the success of machine learning classification algorithms on neurofunctional data has been limited to more homogeneous data sets of dozens of participants. More recently, larger brain imaging data sets have allowed for the application of deep learning techniques to classify brain states and clinical groups solely from neurofunctional features. Deep learning techniques provide helpful tools for classification in healthcare applications, including classification of structural 3D brain images. Recent approaches improved classification performance of larger functional brain imaging data sets, but they fail to provide diagnostic insights about the underlying conditions or provide an explanation from the neural features that informed the classification. We address this challenge by leveraging a number of network visualization techniques to show that, using such techniques in convolutional neural network layers responsible for learning high-level features, we are able to provide meaningful images for expert-backed insights into the condition being classified. Our results show not only accurate classification of developmental dyslexia from the brain imaging alone, but also provide automatic visualizations of the features involved that match contemporary neuroscientific knowledge, indicating that the visual explanations do help in unveiling the neurological bases of the disorder being classified.        △ Less","17 July, 2020","eess.IV,cs.LG,q-bio.NC",
              A Systematic Review of Natural Language Processing for Knowledge Management in Healthcare,2007.09134,https://arxiv.org/abs/2007.09134,https://arxiv.org/pdf/2007.09134,"Authors:GangaPrasadBasyal,BhaskarP.Rimal,DavidZeng","        Driven by the visions of Data Science, recent years have seen a paradigm shift in Natural Language Processing (NLP). NLP has set the milestone in text processing and proved to be the preferred choice for researchers in the healthcare domain. The objective of this paper is to identify the potential of NLP, especially, how NLP is used to support the knowledge management process in the healthcare domain, making data a critical and trusted component in improving the health outcomes. This paper provides a comprehensive survey of the state-of-the-art NLP research with a particular focus on how knowledge is created, captured, shared, and applied in the healthcare domain. Our findings suggest, first, the techniques of NLP those supporting knowledge management extraction and knowledge capture processes in healthcare. Second, we propose a conceptual model for the knowledge extraction process through NLP. Finally, we discuss a set of issues, challenges, and proposed future research areas.        △ Less","17 July, 2020",cs.CY,10.5121/csit.2020.100921 
              Dementia Prediction Applying Variational Quantum Classifier          ,2007.08653,https://arxiv.org/abs/2007.08653,https://arxiv.org/pdf/2007.08653,"Authors:DanielSierra-Sosa,JuanArcila-Moreno,BegonyaGarcia-Zapirain,CristianCastillo-Olea,AdelElmaghraby","        Dementia is the fifth cause of death worldwide with 10 million new cases every year. Healthcare applications using machine learning techniques have almost reached the physical limits while more data is becoming available resulting from the increasing rate of diagnosis. Recent research in Quantum Machine Learning (QML) techniques have found different approaches that may be useful to accelerate the training process of existing machine learning models and provide an alternative to learn more complex patterns. This work aims to report a real-world application of a Quantum Machine Learning Algorithm, in particular, we found that using the implemented version for Variational Quantum Classiffication (VQC) in IBM's framework Qiskit allows predicting dementia in elderly patients, this approach proves to provide more consistent results when compared with a classical Support Vector Machine (SVM) with a linear kernel using different number of features.        △ Less","14 July, 2020",quant-ph,
              Prediction of the onset of cardiovascular diseases from electronic health records using multi-task gated recurrent units          ,2007.08491,https://arxiv.org/abs/2007.08491,https://arxiv.org/pdf/2007.08491,"Authors:FernandoAndreotti,FrankS.Heldt,BaselAbu-Jamous,MingLi,AvelinoJaver,OliverCarr,StojanJovanovic,NadezdaLipunova,BenjaminIrving,RabiaT.Khan,RobertDürichen","        In this work, we propose a multi-task recurrent neural network with attention mechanism for predicting cardiovascular events from electronic health records (EHRs) at different time horizons. The proposed approach is compared to a standard clinical risk predictor (QRISK) and machine learning alternatives using 5-year data from a NHS Foundation Trust. The proposed model outperforms standard clinical risk scores in predicting stroke (AUC=0.85) and myocardial infarction (AUC=0.89), considering the largest time horizon. Benefit of using an \gls{mt} setting becomes visible for very short time horizons, which results in an AUC increase between 2-6%. Further, we explored the importance of individual features and attention weights in predicting cardiovascular events. Our results indicate that the recurrent neural network approach benefits from the hospital longitudinal information and demonstrates how machine learning techniques can be applied to secondary care.        △ Less","16 July, 2020","cs.LG,stat.ML",
              Towards Debiasing Sentence Representations          ,2007.08100,https://arxiv.org/abs/2007.08100,https://arxiv.org/pdf/2007.08100,"Authors:PaulPuLiang,IreneMengzeLi,EmilyZheng,YaoChongLim,RuslanSalakhutdinov,Louis-PhilippeMorency","        As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.        △ Less","16 July, 2020","cs.CL,cs.LG",
              Model-Based Manipulation of Linear Flexible Objects with Visual Curvature Feedback          ,2007.08083,https://arxiv.org/abs/2007.08083,https://arxiv.org/pdf/2007.08083,"Authors:PengChang,TaskinPadir","        Manipulation of deformable objects is a desired skill in making robots ubiquitous in manufacturing, service, healthcare, and security. Deformable objects are common in our daily lives, e.g., wires, clothes, bed sheets, etc., and are significantly more difficult to model than rigid objects. In this study, we investigate vision-based manipulation of linear flexible objects such as cables. We propose a geometric modeling method that is based on visual feedback to develop a general representation of the linear flexible object that is subject to gravity. The model characterizes the shape of the object by combining the curvatures on two projection planes. In this approach, we achieve tracking of the position and orientation (pose) of a cable-like object, the pose of its tip, and the pose of the selected grasp point on the object, which enables closed-loop manipulation of the object. We demonstrate the feasibility of our approach by completing the Plug Task used in the 2015 DARPA Robotics Challenge Finals, which involves unplugging a power cable from one socket and plugging it into another. Experiments show that we can successfully complete the task autonomously within 30 seconds.        △ Less","15 July, 2020",cs.RO,
              Neural Topic Models with Survival Supervision: Jointly Predicting Time-to-Event Outcomes and Learning How Clinical Features Relate          ,2007.07796,https://arxiv.org/abs/2007.07796,https://arxiv.org/pdf/2007.07796,"Authors:LinhongLi,RenZuo,AmandaCoston,JeremyC.Weiss,GeorgeH.Chen","        In time-to-event prediction problems, a standard approach to estimating an interpretable model is to use Cox proportional hazards, where features are selected based on lasso regularization or stepwise regression. However, these Cox-based models do not learn how different features relate. As an alternative, we present an interpretable neural network approach to jointly learn a survival model to predict time-to-event outcomes while simultaneously learning how features relate in terms of a topic model. In particular, we model each subject as a distribution over ""topics"", which are learned from clinical features as to help predict a time-to-event outcome. From a technical standpoint, we extend existing neural topic modeling approaches to also minimize a survival analysis loss function. We study the effectiveness of this approach on seven healthcare datasets on predicting time until death as well as hospital ICU length of stay, where we find that neural survival-supervised topic models achieves competitive accuracy with existing approaches while yielding interpretable clinical ""topics"" that explain feature relationships.        △ Less","15 July, 2020","cs.LG,stat.ML",
"              SaYoPillow: A Blockchain-Enabled, Privacy-Assured Framework for Stress Detection, Prediction and Control Considering Sleeping Habits in the IoMT          ",2007.07377,https://arxiv.org/abs/2007.07377,https://arxiv.org/pdf/2007.07377,"Authors:LaavanyaRachakonda,AnandK.Bapatla,SarajuP.Mohanty,EliasKougianos","        Considering today's lifestyle, people just sleep forgetting the benefits it provides to the human body. The reasons for not having a productive sleep could be many. Smart-Yoga Pillow (SaYoPillow) is envisioned as a device that may help in recognizing the importance of a good quality sleep to alleviate stress while establishing a measurable relationship between stress and sleeping habits. A system that analyzes the sleeping habits by continuously monitoring the physiological changes that occur during rapid eye movement (REM) and non-rapid eye movement (NREM) stages of sleep is proposed in the current work. In addition to the physiological parameter changes, factors such as sleep duration, snoring range, eye movement, and limb movements are also monitored. The SaYoPillow system is processed at the edge level with the storage being at the cloud. Not having to compromise the user's privacy, SaYoPillow proposes secure data transmission for both uploading and retrieving, and secure storage and communications as an attempt to reduce malicious attacks on healthcare. A user interface is provided for the user to control data accessibility and visibility. SaYoPillow has 96% accuracy which is close to other existing research works. However, SaYoPillow is the only work with security features as well as only work that considers sleeping habits for stress.        △ Less","14 July, 2020",cs.CY,
              Generic Outlier Detection in Multi-Armed Bandit          ,2007.07293,https://arxiv.org/abs/2007.07293,https://arxiv.org/pdf/2007.07293,"Authors:YikunBan,JingruiHe","        In this paper, we study the problem of outlier arm detection in multi-armed bandit settings, which finds plenty of applications in many high-impact domains such as finance, healthcare, and online advertising. For this problem, a learner aims to identify the arms whose expected rewards deviate significantly from most of the other arms. Different from existing work, we target the generic outlier arms or outlier arm groups whose expected rewards can be larger, smaller, or even in between those of normal arms. To this end, we start by providing a comprehensive definition of such generic outlier arms and outlier arm groups. Then we propose a novel pulling algorithm named GOLD to identify such generic outlier arms. It builds a real-time neighborhood graph based on upper confidence bounds and catches the behavior pattern of outliers from normal arms. We also analyze its performance from various aspects. In the experiments conducted on both synthetic and real-world data sets, the proposed algorithm achieves 98 % accuracy while saving 83 % exploration cost on average compared with state-of-the-art techniques.        △ Less","14 July, 2020","cs.LG,cs.AI,stat.ML",
              Attend And Discriminate: Beyond the State-of-the-Art for Human Activity Recognition using Wearable Sensors          ,2007.07172,https://arxiv.org/abs/2007.07172,https://arxiv.org/pdf/2007.07172,"Authors:AlirezaAbedin,MahsaEhsanpour,QinfengShi,HamidRezatofighi,DamithC.Ranasinghe","        Wearables are fundamental to improving our understanding of human activities, especially for an increasing number of healthcare applications from rehabilitation to fine-grained gait analysis. Although our collective know-how to solve Human Activity Recognition (HAR) problems with wearables has progressed immensely with end-to-end deep learning paradigms, several fundamental opportunities remain overlooked. We rigorously explore these new opportunities to learn enriched and highly discriminating activity representations. We propose: i) learning to exploit the latent relationships between multi-channel sensor modalities and specific activities; ii) investigating the effectiveness of data-agnostic augmentation for multi-modal sensor data streams to regularize deep HAR models; and iii) incorporating a classification loss criterion to encourage minimal intra-class representation differences whilst maximising inter-class differences to achieve more discriminative features. Our contributions achieves new state-of-the-art performance on four diverse activity recognition problem benchmarks with large margins -- with up to 6% relative margin improvement. We extensively validate the contributions from our design concepts through extensive experiments, including activity misalignment measures, ablation studies and insights shared through both quantitative and qualitative studies.        △ Less","14 July, 2020","cs.LG,cs.HC,stat.ML",
              A Systematic Identification of Formal and Semi-formalLanguages and Techniques for Software-intensiveSystems-of-Systems Requirements Modeling          ,2007.07031,https://arxiv.org/abs/2007.07031,https://arxiv.org/pdf/2007.07031,"Authors:CristianeAparecidaLana,MilenaGuessi,PabloOliveiraAntonino,DieterRombach,ElisaYumiNakagawaA","        Software-intensive Systems-of-Systems (SoS) refer to an arrangement of managerially and operationally independent systems(i.e., constituent systems), which work collaboratively towards the achievement of global missions. Because some SoS are developed for critical domains, such as healthcare and transportation, there is an increasing need to attain higher quality levels, which often justifies additional costs that can be incurred by adopting formal and semi-formal approaches (i.e., languages and techniques) for modeling requirements. Various approaches have been employed, but a detailed landscape is still missing, and it is not well known whether they are appropriate for addressing the inherent characteristics of SoS. The main contribution of this article is to present this landscape by reporting on the state of the art in SoS requirements modeling. This landscape was built by means of a systematic mapping and shows formal and semi-formal approaches grouped from model-based to property-oriented ones. Most of them have been tested in safety-critical domains, where formal approaches such as finite state machines are aimed at critical system parts, while semi-formal approaches (e.g., UML and i*) address non-critical parts. Although formal and semi-formal modeling is an essential activity, the quality of SoS requirements does not rely solely on which formalism is used, but also on the availability of supporting tools/mechanisms that enable, for instance, requirements verification along the SoS lifecycle        △ Less","14 July, 2020","cs.SE,eess.SY",10.1109/JSYST.2018.2874061 
              Securing the Insecure: A First-Line-of-Defense for Nanoscale Communication Systems Operating in THz Band          ,2007.06818,https://arxiv.org/abs/2007.06818,https://arxiv.org/pdf/2007.06818,"Authors:WaqasAman,M.MahboobUrRahman,HassanT.Abbas,MuhammadArslanKhalid,MuhammadA.Imran,AkramAlomainy,QammerH.Abbasi","        Nanoscale communication systems operating in Ter-ahertz (THz) band are anticipated to revolutionise the healthcaresystems of the future. Global wireless data traffic is undergoinga rapid growth. However, wireless systems, due to their broad-casting nature, are vulnerable to malicious security breaches. Inaddition, advances in quantum computing poses a risk to existingcrypto-based information security. It is of the utmost importanceto make the THz systems resilient to potential active and passiveattacks which may lead to devastating consequences, especiallywhen handling sensitive patient data in healthcare systems. Newstrategies are needed to analyse these malicious attacks and topropose viable countermeasures. In this manuscript, we presenta new authentication mechanism for nanoscale communicationsystems operating in THz band at the physical layer. We assessedan impersonation attack on a THz system. We propose usingpath loss as a fingerprint to conduct authentication via two-stephypothesis testing for a transmission device. We used hiddenMarkov Model (HMM) viterbi algorithm to enhance the outputof hypothesis testing. We also conducted transmitter identificationusing maximum likelihood and Gaussian mixture model (GMM)expectation maximization algorithms. Our simulations showedthat the error probabilities are a decreasing functions of SNR. At 10 dB with 0.2 false alarm, the detection probability was almostone. We further observed that HMM out-performs hypothesistesting at low SNR regime (10% increase in accuracy is recordedat SNR =5 dB) whereas the GMM is useful when groundtruths are noisy. Our work addresses major security gaps facedby communication system either through malicious breachesor quantum computing, enabling new applications of nanoscalesystems for Industry 4.0.        △ Less","14 July, 2020","eess.SP,cs.CR,cs.ET",
              Enterprise Architecture in Healthcare Systems: A systematic literature review          ,2007.06767,https://arxiv.org/abs/2007.06767,https://arxiv.org/pdf/2007.06767,"Authors:SilvanoHerculanodaLuzJúnior,FranciscoÍcaroCiprianoSilva,GustavoSousaGalisaAlbuquerque,FranciscoPetrônioAlencardeMedeiros,HeremitaBrasileiroLira","        Enterprise architecture (EA) has been present in scientific literature since the 1980s and has branched out into several research fields. EA delivers value by presenting business and ICT leaders with recommendations for adjusting policies and projects to achieve business goals. Although there are many works on the EA application in healthcare systems, the literature lacks studies that provide a systematic approach to this topic specifically. This work presents a deep and broad Systematic Literature Review (SLR) to select studies demonstrating current EA practices in healthcare systems. The researchers established an SLR protocol returning 280 primary studies after the first step of the Data Selection and a consolidated inclusion of 46 articles after the second step. They assessed the level of disagreement during the team's evaluations using Cohen's Kappa. This SLR revealed essential aspects of state-of-the-art EA application in healthcare systems, such as the most used methodologies and tools, best practices, and criteria considered for their choice. It also analyzed the main positive impacts, challenges, and critical success factors described by the studies' authors based on empirical approaches. Besides, this work brings the main publication channels and the most influential authors on the topic of EA in Healthcare systems.        △ Less","17 July, 2020",cs.CY,10.17632/44bygxg8w3.1 
              An Enhanced Text Classification to Explore Health based Indian Government Policy Tweets          ,2007.06511,https://arxiv.org/abs/2007.06511,https://arxiv.org/pdf/2007.06511,"Authors:AarzooDhiman,DurgaToshniwal","        Government-sponsored policy-making and scheme generations is one of the means of protecting and promoting the social, economic, and personal development of the citizens. The evaluation of effectiveness of these schemes done by government only provide the statistical information in terms of facts and figures which do not include the in-depth knowledge of public perceptions, experiences and views on the topic. In this research work, we propose an improved text classification framework that classifies the Twitter data of different health-based government schemes. The proposed framework leverages the language representation models (LR models) BERT, ELMO, and USE. However, these LR models have less real-time applicability due to the scarcity of the ample annotated data. To handle this, we propose a novel GloVe word embeddings and class-specific sentiments based text augmentation approach (named Mod-EDA) which boosts the performance of text classification task by increasing the size of labeled data. Furthermore, the trained model is leveraged to identify the level of engagement of citizens towards these policies in different communities such as middle-income and low-income groups.        △ Less","18 August, 2020","cs.CL,cs.LG",
              Deep Claim: Payer Response Prediction from Claims Data with Deep Learning          ,2007.06229,https://arxiv.org/abs/2007.06229,https://arxiv.org/pdf/2007.06229,"Authors:Byung-HakKim,SeshadriSridharan,AndyAtwal,VarunGanapathi","        Each year, almost 10% of claims are denied by payers (i.e., health insurance plans). With the cost to recover these denials and underpayments, predicting payer response (likelihood of payment) from claims data with a high degree of accuracy and precision is anticipated to improve healthcare staffs' performance productivity and drive better patient financial experience and satisfaction in the revenue cycle (Barkholz, 2017). However, constructing advanced predictive analytics models has been considered challenging in the last twenty years. That said, we propose a (low-level) context-dependent compact representation of patients' historical claim records by effectively learning complicated dependencies in the (high-level) claim inputs. Built on this new latent representation, we demonstrate that a deep learning-based framework, Deep Claim, can accurately predict various responses from multiple payers using 2,905,026 de-identified claims data from two US health systems. Deep Claim's improvements over carefully chosen baselines in predicting claim denials are most pronounced as 22.21% relative recall gain (at 95% precision) on Health System A, which implies Deep Claim can find 22.21% more denials than the best baseline system.        △ Less","13 July, 2020","cs.LG,cs.CY,stat.ML",
              VAFL: a Method of Vertical Asynchronous Federated Learning          ,2007.06081,https://arxiv.org/abs/2007.06081,https://arxiv.org/pdf/2007.06081,"Authors:TianyiChen,XiaoJin,YuejiaoSun,WotaoYin","        Horizontal Federated learning (FL) handles multi-client data that share the same set of features, and vertical FL trains a better predictor that combine all the features from different clients. This paper targets solving vertical FL in an asynchronous fashion, and develops a simple FL method. The new method allows each client to run stochastic gradient algorithms without coordination with other clients, so it is suitable for intermittent connectivity of clients. This method further uses a new technique of perturbed local embedding to ensure data privacy and improve communication efficiency. Theoretically, we present the convergence rate and privacy level of our method for strongly convex, nonconvex and even nonsmooth objectives separately. Empirically, we apply our method to FL on various image and healthcare datasets. The results compare favorably to centralized and synchronous FL methods.        △ Less","12 July, 2020","cs.LG,cs.DC,math.OC,stat.ML",
              Exploiting Uncertainties from Ensemble Learners to Improve Decision-Making in Healthcare AI          ,2007.06063,https://arxiv.org/abs/2007.06063,https://arxiv.org/pdf/2007.06063,"Authors:YingshuiTan,BaihongJin,XiangyuYue,YuxinChen,AlbertoSangiovanniVincentelli","        Ensemble learning is widely applied in Machine Learning (ML) to improve model performance and to mitigate decision risks. In this approach, predictions from a diverse set of learners are combined to obtain a joint decision. Recently, various methods have been explored in literature for estimating decision uncertainties using ensemble learning; however, determining which metrics are a better fit for certain decision-making applications remains a challenging task. In this paper, we study the following key research question in the selection of uncertainty metrics: when does an uncertainty metric outperforms another? We answer this question via a rigorous analysis of two commonly used uncertainty metrics in ensemble learning, namely ensemble mean and ensemble variance. We show that, under mild assumptions on the ensemble learners, ensemble mean is preferable with respect to ensemble variance as an uncertainty metric for decision making. We empirically validate our assumptions and theoretical results via an extensive case study: the diagnosis of referable diabetic retinopathy.        △ Less","12 July, 2020","cs.LG,cs.CV,stat.ML",
              MeDaS: An open-source platform as service to help break the walls between medicine and informatics          ,2007.06013,https://arxiv.org/abs/2007.06013,https://arxiv.org/pdf/2007.06013,"Authors:LiangZhang,JohannLi,PingLi,XiaoyuanLu,PeiyiShen,GuangmingZhu,SyedAfaqShah,MohammedBennarmoun,KunQian,BjörnW.Schuller","        In the past decade, deep learning (DL) has achieved unprecedented success in numerous fields including computer vision, natural language processing, and healthcare. In particular, DL is experiencing an increasing development in applications for advanced medical image analysis in terms of analysis, segmentation, classification, and furthermore. On the one hand, tremendous needs that leverage the power of DL for medical image analysis are arising from the research community of a medical, clinical, and informatics background to jointly share their expertise, knowledge, skills, and experience. On the other hand, barriers between disciplines are on the road for them often hampering a full and efficient collaboration. To this end, we propose our novel open-source platform, i.e., MeDaS -- the MeDical open-source platform as Service. To the best of our knowledge, MeDaS is the first open-source platform proving a collaborative and interactive service for researchers from a medical background easily using DL related toolkits, and at the same time for scientists or engineers from information sciences to understand the medical knowledge side. Based on a series of toolkits and utilities from the idea of RINV (Rapid Implementation aNd Verification), our proposed MeDaS platform can implement pre-processing, post-processing, augmentation, visualization, and other phases needed in medical image analysis. Five tasks including the subjects of lung, liver, brain, chest, and pathology, are validated and demonstrated to be efficiently realisable by using MeDaS.        △ Less","13 July, 2020","cs.CV,eess.IV",
              Hardware Implementation of Deep Network Accelerators Towards Healthcare and Biomedical Applications          ,2007.05657,https://arxiv.org/abs/2007.05657,https://arxiv.org/pdf/2007.05657,"Authors:MostafaRahimiAzghadi,CoreyLammie,JasonK.Eshraghian,MelikaPayvand,ElisaDonati,BernabeLinares-Barranco,GiacomoIndiveri","        With the advent of dedicated Deep Learning (DL) accelerators and neuromorphic processors, new opportunities are emerging for applying deep and Spiking Neural Network (SNN) algorithms to healthcare and biomedical applications at the edge. This can facilitate the advancement of the medical Internet of Things (IoT) systems and Point of Care (PoC) devices. In this paper, we provide a tutorial describing how various technologies ranging from emerging memristive devices, to established Field Programmable Gate Arrays (FPGAs), and mature Complementary Metal Oxide Semiconductor (CMOS) technology can be used to develop efficient DL accelerators to solve a wide variety of diagnostic, pattern recognition, and signal processing problems in healthcare. Furthermore, we explore how spiking neuromorphic processors can complement their DL counterparts for processing biomedical signals. After providing the required background, we unify the sparsely distributed research on neural network and neuromorphic hardware implementations as applied to the healthcare domain. In addition, we benchmark various hardware platforms by performing a biomedical electromyography (EMG) signal processing task and drawing comparisons among them in terms of inference delay and energy. Finally, we provide our analysis of the field and share a perspective on the advantages, disadvantages, challenges, and opportunities that different accelerators and neuromorphic processors introduce to healthcare and biomedical domains. This paper can serve a large audience, ranging from nanoelectronics researchers, to biomedical and healthcare practitioners in grasping the fundamental interplay between hardware, algorithms, and clinical adoption of these tools, as we shed light on the future of deep networks and spiking neuromorphic processing systems as proponents for driving biomedical circuits and systems forward.        △ Less","10 July, 2020","cs.AR,cs.LG,eess.SP",
              Deep Contextual Clinical Prediction with Reverse Distillation          ,2007.05611,https://arxiv.org/abs/2007.05611,https://arxiv.org/pdf/2007.05611,"Authors:RohanS.Kodialam,RebeccaBoiarsky,DavidSontag","Healthcare providers are increasingly using learned methods to predict and understand long-term patient outcomes in order to make meaningful interventions. However, despite innovations in this area, deep learning models often struggle to match performance of shallow linear models in predicting these outcomes, making it difficult to leverage such techniques in practice. In this work, motivated by the task of clinical prediction from insurance claims, we present a new technique called reverse distillation which pretrains deep models by using high-performing linear models for initialization. We make use of the longitudinal structure of insurance claims datasets to develop Self Attention with Reverse Distillation, or SARD, an architecture that utilizes a combination of contextual embedding, temporal embedding and self-attention mechanisms and most critically is trained via reverse distillation. SARD outperforms state-of-the-art methods on multiple clinical prediction outcomes, with ablation studies revealing that reverse distillation is a primary driver of these improvements.        △ Less","10 July, 2020","cs.LG,cs.AI,stat.ML",
              EMIXER: End-to-end Multimodal X-ray Generation via Self-supervision          ,2007.05597,https://arxiv.org/abs/2007.05597,https://arxiv.org/pdf/2007.05597,"Authors:SiddharthBiswal,PeiyeZhuang,AyisPyrros,NasirSiddiqui,SanmiKoyejo,JimengSun","        Deep generative models have enabled the automated synthesis of high-quality data for diverse applications. However, the most effective generative models are specialized to data from a single domain (e.g., images or text). Real-world applications such as healthcare require multi-modal data from multiple domains (e.g., both images and corresponding text), which are difficult to acquire due to limited availability and privacy concerns and are much harder to synthesize. To tackle this joint synthesis challenge, we propose an End-to-end MultImodal X-ray genERative model (EMIXER) for jointly synthesizing x-ray images and corresponding free-text reports, all conditional on diagnosis labels. EMIXER is an conditional generative adversarial model by 1) generating an image based on a label, 2) encoding the image to a hidden embedding, 3) producing the corresponding text via a hierarchical decoder from the image embedding, and 4) a joint discriminator for assessing both the image and the corresponding text. EMIXER also enables self-supervision to leverage vast amount of unlabeled data. Extensive experiments with real X-ray reports data illustrate how data augmentation using synthesized multimodal samples can improve the performance of a variety of supervised tasks including COVID-19 X-ray classification with very limited samples. The quality of generated images and reports are also confirmed by radiologists. We quantitatively show that EMIXER generated synthetic datasets can augment X-ray image classification, report generation models to achieve 5.94% and 6.9% improvement on models trained only on real data samples. Taken together, our results highlight the promise of state of generative models to advance clinical machine learning.        △ Less","10 July, 2020","eess.IV,cs.CV,cs.LG",
              Dynamic Tuberculosis Screening for Healthcare Employees          ,2007.05568,https://arxiv.org/abs/2007.05568,https://arxiv.org/pdf/2007.05568,"Authors:MahsaKiani,TugceIsik,BurakEksioglu","        Regular tuberculosis (TB) screening is required for healthcare employees since they can come into contact with infected patients. TB is a serious, contagious, and potentially deadly disease. Early detection of the disease, even when it is in latent form, prevents the spread of the disease and helps with treatment. Currently, there are two types of TB diagnostic tests on the market: skin test and blood test. The cost of the blood test is much higher than the skin test. However, the possibility of getting a false positive or false negative result in skin test is higher especially for persons with specific characteristics, which can increase costs. In this study, we categorize healthcare employees into multiple risk groups based on the department they work in, the specific job they do, and their birth country. We create a Markov decision process (MDP) model to decide which TB test should be taken by each employee group to minimize the total costs related to testing, undetected infections, employees' time lost. Due to the curse of dimensionality, we use approximate dynamic programming (ADP) to obtain a near-optimal solution. By analyzing this solution to the ADP we specify not only the type but the frequency with which each test should be taken. Based on this analysis, we propose a simple policy that can be used by healthcare facilities since such facilities may not have the expertise or the resources to develop and solve sophisticated optimization models.        △ Less","10 July, 2020",math.OC,
              A Consent Model for Blockchain-based Distributed Data Sharing Platforms          ,2007.04847,https://arxiv.org/abs/2007.04847,https://arxiv.org/pdf/2007.04847,"Authors:VikasJaiman,VisaraUrovi","        In modern healthcare systems, being able to share electronic health records is crucial for providing quality care and for enabling a larger spectrum of health services. Health data sharing is dependent on obtaining individual consent which, in turn, is hindered by a lack of resources. To this extend, blockchain-based platforms facilitate data sharing by inherently creating a trusted distributed network of users. These users are enabled to share their data without depending on the time and resources of specific players (such as the health services). In blockchain-based platforms, data governance mechanisms become very important due to the need to specify and monitor data sharing and data use conditions. In this paper, we present a blockchain-based data sharing consent model for access control over individual health data. We use smart contracts to dynamically represent the individual consent over health data and to enable data requesters to search and access them. The dynamic consent model extends upon two ontologies: the Data Use Ontology (DUO) which models the individual consent of users and the Automatable Discovery and Access Matrix (ADA-M) which describes queries from data requesters. We deploy the model on Ethereum blockchain and evaluate different data sharing scenarios. The contribution of this paper is to create an individual consent model for health data sharing platforms. Such a model guarantees that individual consent is respected and that there is accountability for all the participants in the data sharing platform. The evaluation of our solution indicates that such a data sharing model provides a flexible approach to decide how the data is used by data requesters. Our experimental evaluation shows that the proposed model is efficient and adapts to personalized access control policies in data sharing.        △ Less","9 July, 2020",cs.DC,10.1109/ACCESS.2020.3014565 
              Magnetic Immunoassays: A Review of Virus and Pathogen Detection Before and Amidst the Coronavirus Disease-19 (COVID-19)          ,2007.04809,https://arxiv.org/abs/2007.04809,https://arxiv.org/pdf/2007.04809,"Authors:KaiWu,RenataSaha,DiqingSu,VenkatramanaD.Krishna,JinmingLiu,MaximC-JCheeran,Jian-PingWang","        The novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), which causes coronavirus disease 2019 (COVID-19), is a threat to the global healthcare system and economic security. As of July 2020, no specific drugs or vaccines are yet available for COVID-19, fast and accurate diagnosis for SARS-CoV-2 is essential in slowing down the spread of COVID-19 and for efficient implementation of control and containment strategies. Magnetic immunoassay is a novel and emerging topic representing the frontiers of current biosensing and magnetics areas. The past decade has seen rapid growth in applying magnetic tools for biological and biomedical applications. Recent advances in magnetic materials and nanotechnologies have transformed current diagnostic methods to nanoscale and pushed the detection limit to early stage disease diagnosis. Herein, this review covers the literatures of magnetic immunoassay platforms for virus and pathogen detections, before COVID-19. We reviewed the popular magnetic immunoassay platforms including magnetoresistance (MR) sensors, magnetic particle spectroscopy (MPS), and nuclear magnetic resonance (NMR). Magnetic Point-of-Care (POC) diagnostic kits are also reviewed aiming at developing plug-and-play diagnostics to manage the SARS-CoV-2 outbreak as well as preventing future epidemics. In addition, other platforms that use magnetic materials as auxiliary tools for enhanced pathogen and virus detections are also covered. The goal of this review is to inform the researchers of diagnostic and surveillance platforms for SARS-CoV-2 and their performances.        △ Less","9 July, 2020","physics.med-ph,physics.bio-ph",
              Automated Chest CT Image Segmentation of COVID-19 Lung Infection based on 3D U-Net          ,2007.04774,https://arxiv.org/abs/2007.04774,https://arxiv.org/pdf/2007.04774,"Authors:DominikMüller,IñakiSotoRey,FrankKramer","        The coronavirus disease 2019 (COVID-19) affects billions of lives around the world and has a significant impact on public healthcare. Due to rising skepticism towards the sensitivity of RT-PCR as screening method, medical imaging like computed tomography offers great potential as alternative. For this reason, automated image segmentation is highly desired as clinical decision support for quantitative assessment and disease monitoring. However, publicly available COVID-19 imaging data is limited which leads to overfitting of traditional approaches. To address this problem, we propose an innovative automated segmentation pipeline for COVID-19 infected regions, which is able to handle small datasets by utilization as variant databases. Our method focuses on on-the-fly generation of unique and random image patches for training by performing several preprocessing methods and exploiting extensive data augmentation. For further reduction of the overfitting risk, we implemented a standard 3D U-Net architecture instead of new or computational complex neural network architectures. Through a 5-fold cross-validation on 20 CT scans of COVID-19 patients, we were able to develop a highly accurate as well as robust segmentation model for lungs and COVID-19 infected regions without overfitting on the limited data. Our method achieved Dice similarity coefficients of 0.956 for lungs and 0.761 for infection. We demonstrated that the proposed method outperforms related approaches, advances the state-of-the-art for COVID-19 segmentation and improves medical image analysis with limited data. The code and model are available under the following link: https://github.com/frankkramer-lab/covid19.MIScnn        △ Less","24 June, 2020","eess.IV,cs.CV,cs.LG",
              Collapsing Bandits and Their Application to Public Health Interventions          ,2007.04432,https://arxiv.org/abs/2007.04432,https://arxiv.org/pdf/2007.04432,"Authors:AdityaMate,JacksonA.Killian,HaifengXu,AndrewPerrault,MilindTambe","        We propose and study Collpasing Bandits, a new restless multi-armed bandit (RMAB) setting in which each arm follows a binary-state Markovian process with a special structure: when an arm is played, the state is fully observed, thus ""collapsing"" any uncertainty, but when an arm is passive, no observation is made, thus allowing uncertainty to evolve. The goal is to keep as many arms in the ""good"" state as possible by planning a limited budget of actions per round. Such Collapsing Bandits are natural models for many healthcare domains in which workers must simultaneously monitor patients and deliver interventions in a way that maximizes the health of their patient cohort. Our main contributions are as follows: (i) Building on the Whittle index technique for RMABs, we derive conditions under which the Collapsing Bandits problem is indexable. Our derivation hinges on novel conditions that characterize when the optimal policies may take the form of either ""forward"" or ""reverse"" threshold policies. (ii) We exploit the optimality of threshold policies to build fast algorithms for computing the Whittle index, including a closed-form. (iii) We evaluate our algorithm on several data distributions including data from a real-world healthcare task in which a worker must monitor and deliver interventions to maximize their patients' adherence to tuberculosis medication. Our algorithm achieves a 3-order-of-magnitude speedup compared to state-of-the-art RMAB techniques while achieving similar performance.        △ Less","4 July, 2020","cs.LG,cs.AI,stat.ML",
              Disproportionate incidence of COVID-19 in African Americans correlates with dynamic segregation          ,2007.04130,https://arxiv.org/abs/2007.04130,https://arxiv.org/pdf/2007.04130,"Authors:AleixBassolas,SandroSousa,VincenzoNicosia","        Socio-economic disparities quite often have a central role in the unfolding of large-scale catastrophic events. One of the most concerning aspects of the ongoing COVID-19 pandemics is that it disproportionately affects people from Black and African American backgrounds creating an unexpected infection gap. Interestingly, the abnormal impact on these ethnic groups seem to be almost uncorrelated with other risk factors, including co-morbidity, poverty, level of education, access to healthcare, residential segregation, and response to cures. A proposed explanation for the observed incidence gap is that people from African American backgrounds are more often employed in low-income service jobs, and are thus more exposed to infection through face-to-face contacts, but the lack of direct data has not allowed to draw strong conclusions in this sense so far. Here we introduce the concept of dynamic segregation, that is the extent to which a given group of people is internally clustered or exposed to other groups, as a result of mobility and commuting habits. By analysing census and mobility data on more than 120 major US cities, we found that the dynamic segregation of African American communities is significantly associated with the weekly excess COVID-19 incidence and mortality in those communities. The results confirm that knowing where people commute to, rather than where they live, is much more relevant for disease modelling.        △ Less","8 July, 2020","physics.soc-ph,q-bio.PE",
              MCU-Net: A framework towards uncertainty representations for decision support system patient referrals in healthcare contexts          ,2007.03995,https://arxiv.org/abs/2007.03995,https://arxiv.org/pdf/2007.03995,Authors:NabeelSeedat,"        Incorporating a human-in-the-loop system when deploying automated decision support is critical in healthcare contexts to create trust, as well as provide reliable performance on a patient-to-patient basis. Deep learning methods while having high performance, do not allow for this patient-centered approach due to the lack of uncertainty representation. Thus, we present a framework of uncertainty representation evaluated for medical image segmentation, using MCU-Net which combines a U-Net with Monte Carlo Dropout, evaluated with four different uncertainty metrics. The framework augments this by adding a human-in-the-loop aspect based on an uncertainty threshold for automated referral of uncertain cases to a medical professional. We demonstrate that MCU-Net combined with epistemic uncertainty and an uncertainty threshold tuned for this application maximizes automated performance on an individual patient level, yet refers truly uncertain cases. This is a step towards uncertainty representations when deploying machine learning based decision support in healthcare settings.        △ Less","25 August, 2020","cs.LG,cs.CV,stat.ML",
              Segmentation of Pulmonary Opacification in Chest CT Scans of COVID-19 Patients          ,2007.03643,https://arxiv.org/abs/2007.03643,https://arxiv.org/pdf/2007.03643,"Authors:KeeganLensink,IssamLaradji,MarcoLaw,PaoloEmilioBarbano,SavvasNicolaou,WilliamParker,EldadHaber","        The Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) has rapidly spread into a global pandemic. A form of pneumonia, presenting as opacities with in a patient's lungs, is the most common presentation associated with this virus, and great attention has gone into how these changes relate to patient morbidity and mortality. In this work we provide open source models for the segmentation of patterns of pulmonary opacification on chest Computed Tomography (CT) scans which have been correlated with various stages and severities of infection. We have collected 663 chest CT scans of COVID-19 patients from healthcare centers around the world, and created pixel wise segmentation labels for nearly 25,000 slices that segment 6 different patterns of pulmonary opacification. We provide open source implementations and pre-trained weights for multiple segmentation models trained on our dataset. Our best model achieves an opacity Intersection-Over-Union score of 0.76 on our test set, demonstrates successful domain adaptation, and predicts the volume of opacification within 1.7\% of expert radiologists. Additionally, we present an analysis of the inter-observer variability inherent to this task, and propose methods for appropriate probabilistic approaches.        △ Less","8 July, 2020","eess.IV,cs.CV",
              Optimal Witnessing of Healthcare IoT Data Using Blockchain Logging Contract          ,2007.03330,https://arxiv.org/abs/2007.03330,https://arxiv.org/pdf/2007.03330,"Authors:MohammadHosseinChinaei,HassanHabibiGharakheili,VijaySivaraman","        Verification of data generated by wearable sensors is increasingly becoming of concern to health service providers and insurance companies. There is a need for a verification framework that various authorities can request a verification service for the local network data of a target IoT device. In this paper, we leverage blockchain as a distributed platform to realize an on-demand verification scheme. This allows authorities to automatically transact with connected devices for witnessing services. A public request is made for witness statements on the data of a target IoT that is transmitted on its local network, and subsequently, devices (in close vicinity of the target IoT) offer witnessing service.  Our contributions are threefold: (1) We develop a system architecture based on blockchain and smart contract that enables authorities to dynamically avail a verification service for data of a subject device from a distributed set of witnesses which are willing to provide (in a privacy-preserving manner) their local wireless measurement in exchange of monetary return; (2) We then develop a method to optimally select witnesses in such a way that the verification error is minimized subject to monetary cost constraints; (3) Lastly, we evaluate the efficacy of our scheme using real Wi-Fi session traces collected from a five-storeyed building with more than thirty access points, representative of a hospital. According to the current pricing schedule of the Ethereum public blockchain, our scheme enables healthcare authorities to verify data transmitted from a typical wearable device with the verification error of the order 0.01% at cost of less than two dollars for one-hour witnessing service.        △ Less","7 July, 2020",cs.CR,
              A Boolean Control Network Approach to the Formal Verification of Feedback Context-Aware Pervasive Systems          ,2007.03065,https://arxiv.org/abs/2007.03065,https://arxiv.org/pdf/2007.03065,"Authors:FabioA.Schreiber,MariaElenaValcher","        The emergence of Context-aware systems in the domains of autonomic, monitoring, and safety-critical applications asks for the definition of methods to formally assess their correctness and dependability properties. Many of these properties are common to Automatic Control systems, a field that developed well established analysis and design techniques to formalize and investigate them. In this paper, we use Boolean Control Networks, to discuss some properties of a feedback Context-aware system in a case study based on a healthcare management example.        △ Less","6 July, 2020",eess.SY,
              Estimation of Ground Contacts from Human Gait by a Wearable Inertial Measurement Unit using machine learning          ,2007.02433,https://arxiv.org/abs/2007.02433,/search/?searchtype=author&query=Umer%2C+M+J,"Authors:MuhammadJunaidUmer,QaiserRiaz","        Robotics system for rehabilitation of movement disorders and motion assistance are gaining increased intention. In this scenario estimation of ground contact is an active area of research in robotics and healthcare. This article addresses the estimation and classification of right and left foot during the healthy human gait based on the IMU sensor data of chest and lower back. For this purpose we have collected an IMU data of 48 subjects by using two smartphones at chest and lower back of the human body and one smart watch at right ankle of the body. To show the robustness of our approach data was collected at six different surfaces (road tiles carpet grass concrete and soil). The recorded data of lower back and chest sensor was segmented into single steps on the basis of right ankle sensor data, then we computed a total of 408 features from time frequency and wavelet domain of each segmented step. For classification task we have trained two machine learning classifiers SVM and RF with 10 fold cross validation method. We performed classification experiments at individual surfaces, hard surfaces, soft surfaces and all surfaces, highest accuracy was achieved at individual surfaces with accuracy index of 98.88%. Furthermore, classification rate at hard soft and all surface are 95.60%, 94.38% and 95.05% respectively. The results shows that estimation of ground contact form normal human walk at different surfaces can be performed with high accuracy using 6D data of angular velocities and accelerations from chest and lower back location of the body.        △ Less","8 July, 2020","cs.CV,cs.LG",
              A Modern Non-SQL Approach to Radiology-Centric Search Engine Design with Clinical Validation          ,2007.02124,https://arxiv.org/abs/2007.02124,https://arxiv.org/pdf/2007.02124,"Authors:NingchengLi,GuyMaresh,MaxwellCretcher,KhashayarFarsad,RamseyAl-Hakim,JohnKaufman,JudyGichoya","Healthcare data is increasing in size at an unprecedented speed with much attention on big data analysis and Artificial Intelligence application for quality assurance, clinical training, severity triaging, and decision support. Radiology is well-suited for innovation given its intrinsically paired linguistic and visual data. Previous attempts to unlock this information goldmine were encumbered by heterogeneity of human language, proprietary search algorithms, and lack of medicine-specific search performance matrices. We present a de novo process of developing a document-based, secure, efficient, and accurate search engine in the context of Radiology. We assess our implementation of the search engine with comparison to pre-existing manually collected clinical databases used previously for clinical research projects in addition to computational performance benchmarks and survey feedback. By leveraging efficient database architecture, search capability, and clinical thinking, radiologists are at the forefront of harnessing the power of healthcare data.        △ Less","4 July, 2020","cs.IR,cs.CY",
              MQT-TZ: Secure MQTT Broker for Biomedical Signal Processing on the Edge          ,2007.01555,https://arxiv.org/abs/2007.01555,https://arxiv.org/pdf/2007.01555,"Authors:CarlosSegarra,RicardDelgado-Gonzalo,ValerioSchiavoni","        Physical health records belong to healthcare providers, but the information contained within belongs to each patient. In an increasing manner, more health-related data is being acquired by wearables and other IoT devices following the ever-increasing trend of the ""Quantified Self"". Even though data protection regulations (e.g., GDPR) encourage the usage of privacy-preserving processing techniques, most of the current IoT infrastructure was not originally conceived for such purposes. One of the most used communication protocols, MQTT, is a lightweight publish-subscribe protocol commonly used in the Edge and IoT applications. In MQTT, the broker must process data on clear text, hence exposing a large attack surface for a malicious agent to steal/tamper with this health-related data. In this paper, we introduce MQT-TZ, a secure MQTT broker leveraging Arm TrustZone, a popular Trusted Execution Environment (TEE). We define a mutual TLS-based handshake and a two-layer encryption for end-to-end security using the TEE as a trusted proxy. We provide quantitative evaluation of our open-source PoC on streaming ECGs in real time and highlight the trade-offs.        △ Less","3 July, 2020",cs.CR,10.3233/SHTI200177 
              Deep Learning Defenses Against Adversarial Examples for Dynamic Risk Assessment          ,2007.01017,https://arxiv.org/abs/2007.01017,https://arxiv.org/pdf/2007.01017,"Authors:XabierEcheberria-Barrio,AmaiaGil-Lerchundi,InesGoicoechea-Telleria,RaulOrduna-Urrutia","        Deep Neural Networks were first developed decades ago, but it was not until recently that they started being extensively used, due to their computing power requirements. Since then, they are increasingly being applied to many fields and have undergone far-reaching advancements. More importantly, they have been utilized for critical matters, such as making decisions in healthcare procedures or autonomous driving, where risk management is crucial. Any mistakes in the diagnostics or decision-making in these fields could entail grave accidents, and even death. This is preoccupying, because it has been repeatedly reported that it is straightforward to attack this type of models. Thus, these attacks must be studied to be able to assess their risk, and defenses need to be developed to make models more robust. For this work, the most widely known attack was selected (adversarial attack) and several defenses were implemented against it (i.e. adversarial training, dimensionality reduc tion and prediction similarity). The obtained outcomes make the model more robust while keeping a similar accuracy. The idea was developed using a breast cancer dataset and a VGG16 and dense neural network model, but the solutions could be applied to datasets from other areas and different convolutional and dense deep neural network models.        △ Less","2 July, 2020","cs.LG,cs.CR,stat.ML",
              Learning to search efficiently for causally near-optimal treatments          ,2007.00973,https://arxiv.org/abs/2007.00973,https://arxiv.org/pdf/2007.00973,"Authors:SamuelHåkansson,ViktorLindblom,OmerGottesman,FredrikD.Johansson","        Finding an effective medical treatment often requires a search by trial and error. Making this search more efficient by minimizing the number of unnecessary trials could lower both costs and patient suffering. We formalize this problem as learning a policy for finding a near-optimal treatment in a minimum number of trials using a causal inference framework. We give a model-based dynamic programming algorithm which learns from observational data while being robust to unmeasured confounding. To reduce time complexity, we suggest a greedy algorithm which bounds the near-optimality constraint. The methods are evaluated on synthetic and real-world healthcare data and compared to model-free reinforcement learning. We find that our methods compare favorably to the model-free baseline while offering a more transparent trade-off between search time and treatment efficacy.        △ Less","2 July, 2020","cs.LG,stat.ML",
              Automated Empathy Detection for Oncology Encounters          ,2007.00809,https://arxiv.org/abs/2007.00809,https://arxiv.org/pdf/2007.00809,"Authors:ZhuohaoChen,JamesGibson,Ming-ChangChiu,QiaohongHu,TaraKKnight,DaniellaMeeker,JamesATulsky,KathrynIPollak,ShrikanthNarayanan","        Empathy involves understanding other people's situation, perspective, and feelings. In clinical interactions, it helps clinicians establish rapport with a patient and support patient-centered care and decision making. Understanding physician communication through observation of audio-recorded encounters is largely carried out with manual annotation and analysis. However, manual annotation has a prohibitively high cost. In this paper, a multimodal system is proposed for the first time to automatically detect empathic interactions in recordings of real-world face-to-face oncology encounters that might accelerate manual processes. An automatic speech and language processing pipeline is employed to segment and diarize the audio as well as for transcription of speech into text. Lexical and acoustic features are derived to help detect both empathic opportunities offered by the patient, and the expressed empathy by the oncologist. We make the empathy predictions using Support Vector Machines (SVMs) and evaluate the performance on different combinations of features in terms of average precision (AP).        △ Less","1 July, 2020","eess.AS,cs.SD",
              TeleVital: Enhancing the quality of contactless health assessment          ,2007.00762,https://arxiv.org/abs/2007.00762,https://arxiv.org/pdf/2007.00762,"Authors:JithinSunny,JoelJogy,RohanRout,RakshitNaidu","        In the midst of rising positive cases of COVID-19, the hospitals face a newfound difficulty to prioritize on their patients and accommodate them. Moreover, crowding of patients at hospitals pose a threat to the healthcare workers and other patients at the hospital. With that in mind, a non-contact method of measuring the necessary vitals such as heart rate, respiratory rate and SPO2_2 will prove highly beneficial for the hospitals to tackle this issue. This paper discusses our approach in achieving the non-contact measurement of vitals with the sole help of a webcam and further our design of an e-hospital platform for doctors and patients to attend appointments virtually. The platform also provides the doctor with an option to provide with voice-based prescriptions or digital prescriptions, to simplify the daily, exhausting routine of a doctor.        △ Less","26 June, 2020",cs.HC,
              Towards User Friendly Medication Mapping Using Entity-Boosted Two-Tower Neural Network          ,2007.00492,https://arxiv.org/abs/2007.00492,https://arxiv.org/pdf/2007.00492,"Authors:ShaoqingYuan,ParminderBhatia,BusraCelikkaya,HaiyangLiu,KyunghwanChoi","        Recent advancements in medical entity linking have been applied in the area of scientific literature and social media data. However, with the adoption of telemedicine and conversational agents such as Alexa in healthcare settings, medical name inference has become an important task. Medication name inference is the task of mapping user friendly medication names from a free-form text to a concept in a normalized medication list. This is challenging due to the differences in the use of medical terminology from health care professionals and user conversations coming from the lay public. We begin with mapping descriptive medication phrases (DMP) to standard medication names (SMN). Given the prescriptions of each patient, we want to provide them with the flexibility of referring to the medication in their preferred ways. We approach this as a ranking problem which maps SMN to DMP by ordering the list of medications in the patient's prescription list obtained from pharmacies. Furthermore, we leveraged the output of intermediate layers and performed medication clustering. We present the Medication Inference Model (MIM) achieving state-of-the-art results. By incorporating medical entities based attention, we have obtained further improvement for ranking models.        △ Less","9 October, 2020","cs.CL,cs.CY,cs.LG,stat.ML",
              Deep Neural Networks for Computational Optical Form Measurements          ,2007.00319,https://arxiv.org/abs/2007.00319,https://arxiv.org/pdf/2007.00319,"Authors:LaraHoffmann,ClemensElster","        Deep neural networks have been successfully applied in many different fields like computational imaging, medical healthcare, signal processing, or autonomous driving. In a proof-of-principle study, we demonstrate that computational optical form measurement can also benefit from deep learning. A data-driven machine learning approach is explored to solve an inverse problem in the accurate measurement of optical surfaces. The approach is developed and tested using virtual measurements with known ground truth.        △ Less","1 July, 2020","eess.IV,cs.LG,physics.ins-det",
              Identifying Causal Effect Inference Failure with Uncertainty-Aware Models          ,2007.00163,https://arxiv.org/abs/2007.00163,https://arxiv.org/pdf/2007.00163,"Authors:AndrewJesson,SörenMindermann,UriShalit,YarinGal","        Recommending the best course of action for an individual is a major application of individual-level causal effect estimation. This application is often needed in safety-critical domains such as healthcare, where estimating and communicating uncertainty to decision-makers is crucial. We introduce a practical approach for integrating uncertainty estimation into a class of state-of-the-art neural network methods used for individual-level causal estimates. We show that our methods enable us to deal gracefully with situations of ""no-overlap"", common in high-dimensional data, where standard applications of causal effect approaches fail. Further, our methods allow us to handle covariate shift, where test distribution differs to train distribution, common when systems are deployed in practice. We show that when such a covariate shift occurs, correctly modeling uncertainty can keep us from giving overconfident and potentially harmful recommendations. We demonstrate our methodology with a range of state-of-the-art models. Under both covariate shift and lack of overlap, our uncertainty-equipped methods can alert decisions makers when predictions are not to be trusted while outperforming their uncertainty-oblivious counterparts.        △ Less","30 June, 2020","cs.LG,stat.ML",
              Conditional Gradient Methods for Convex Optimization with Function Constraints          ,2007.00153,https://arxiv.org/abs/2007.00153,https://arxiv.org/pdf/2007.00153,"Authors:GuanghuiLan,EdwinRomeijn,ZhiqiangZhou","        Conditional gradient methods have attracted much attention in both machine learning and optimization communities recently. These simple methods can guarantee the generation of sparse solutions. In addition, without the computation of full gradients, they can handle huge-scale problems sometimes even with an exponentially increasing number of decision variables. This paper aims to significantly expand the application areas of these methods by presenting new conditional gradient methods for solving convex optimization problems with general affine and nonlinear constraints. More specifically, we first present a new constraint extrapolated condition gradient (CoexCG) method that can achieve an O(1/ε2){\cal O}(1/ε^2) iteration complexity for both smooth and structured nonsmooth function constrained convex optimization. We further develop novel variants of CoexCG, namely constraint extrapolated and dual regularized conditional gradient (CoexDurCG) methods, that can achieve similar iteration complexity to CoexCG but allow adaptive selection for algorithmic parameters. We illustrate the effectiveness of these methods for solving an important class of radiation therapy treatment planning problems arising from healthcare industry. To the best of our knowledge, all the algorithmic schemes and their complexity results are new in the area of projection-free methods.        △ Less","20 July, 2020","math.OC,cs.LG",
              A Deep Learning Pipeline for Patient Diagnosis Prediction Using Electronic Health Records          ,2006.16926,https://arxiv.org/abs/2006.16926,https://arxiv.org/pdf/2006.16926,"Authors:LeopoldFranz,YashRajShrestha,BibekPaudel","        Augmentation of disease diagnosis and decision-making in healthcare with machine learning algorithms is gaining much impetus in recent years. In particular, in the current epidemiological situation caused by COVID-19 pandemic, swift and accurate prediction of disease diagnosis with machine learning algorithms could facilitate identification and care of vulnerable clusters of population, such as those having multi-morbidity conditions. In order to build a useful disease diagnosis prediction system, advancement in both data representation and development of machine learning architectures are imperative. First, with respect to data collection and representation, we face severe problems due to multitude of formats and lack of coherency prevalent in Electronic Health Records (EHRs). This causes hindrance in extraction of valuable information contained in EHRs. Currently, no universal global data standard has been established. As a useful solution, we develop and publish a Python package to transform public health dataset into an easy to access universal format. This data transformation to an international health data format facilitates researchers to easily combine EHR datasets with clinical datasets of diverse formats. Second, machine learning algorithms that predict multiple disease diagnosis categories simultaneously remain underdeveloped. We propose two novel model architectures in this regard. First, DeepObserver, which uses structured numerical data to predict the diagnosis categories and second, ClinicalBERT_Multi, that incorporates rich information available in clinical notes via natural language processing methods and also provides interpretable visualizations to medical practitioners. We show that both models can predict multiple diagnoses simultaneously with high accuracy.        △ Less","23 June, 2020","cs.CY,cs.CL,cs.LG",
              From predictions to prescriptions: A data-driven response to COVID-19          ,2006.16509,https://arxiv.org/abs/2006.16509,https://arxiv.org/pdf/2006.16509,"Authors:DimitrisBertsimas,LéonardBoussioux,RyanCoryWright,ArthurDelarue,VassilisDigalakisJr.,AlexandreJacquillat,DrissLahlouKitane,GalitLukin,MichaelLingzhiLi,LucaMingardi,OmidNohadani,AgniOrfanoudaki,TheodorePapalexopoulos,IvanPaskov,JeanPauphilet,OmarSkaliLami,BartolomeoStellato,HamzaTaziBouardi,KimberlyVillalobosCarballo,HollyWiberg,CynthiaZeng","        The COVID-19 pandemic has created unprecedented challenges worldwide. Strained healthcare providers make difficult decisions on patient triage, treatment and care management on a daily basis. Policy makers have imposed social distancing measures to slow the disease, at a steep economic price. We design analytical tools to support these decisions and combat the pandemic. Specifically, we propose a comprehensive data-driven approach to understand the clinical characteristics of COVID-19, predict its mortality, forecast its evolution, and ultimately alleviate its impact. By leveraging cohort-level clinical data, patient-level hospital data, and census-level epidemiological data, we develop an integrated four-step approach, combining descriptive, predictive and prescriptive analytics. First, we aggregate hundreds of clinical studies into the most comprehensive database on COVID-19 to paint a new macroscopic picture of the disease. Second, we build personalized calculators to predict the risk of infection and mortality as a function of demographics, symptoms, comorbidities, and lab values. Third, we develop a novel epidemiological model to project the pandemic's spread and inform social distancing policies. Fourth, we propose an optimization model to re-allocate ventilators and alleviate shortages. Our results have been used at the clinical level by several hospitals to triage patients, guide care management, plan ICU capacity, and re-distribute ventilators. At the policy level, they are currently supporting safe back-to-work policies at a major institution and equitable vaccine distribution planning at a major pharmaceutical company, and have been integrated into the US Center for Disease Control's pandemic forecast.        △ Less","29 June, 2020","stat.AP,math.OC,q-bio.PE,stat.ML",
              Optimized lockdown strategies for curbing the spread of COVID-19: A South African case study          ,2006.16379,https://arxiv.org/abs/2006.16379,https://arxiv.org/pdf/2006.16379,"Authors:LaurentzE.Olivier,StefanBotha,IanK.Craig","        To curb the spread of COVID-19, many governments around the world have implemented tiered lockdowns with varying degrees of stringency. Lockdown levels are typically increased when the disease spreads and reduced when the disease abates. A predictive control approach is used to develop optimized lockdown strategies for curbing the spread of COVID-19. These strategies are then applied to South African data. The South African case is of immediate interest as the number of confirmed infectious cases does not appear to have peaked yet (at the time of writing), while at the same time the South African government is busy reducing the degree of lockdown. An epidemiological model for the spread of COVID-19 in South Africa was previously developed, and is used in conjunction with a hybrid model predictive controller to optimize lockdown management under different policy scenarios. Scenarios considered include how flatten the curve to a level that the healthcare system can cope with, how to balance lives and livelihoods, and what impact the compliance of the population to the lockdown measures has on the spread of COVID-19.        △ Less","29 June, 2020","q-bio.PE,physics.soc-ph",
              Adversarial Multi-Source Transfer Learning in Healthcare: Application to Glucose Prediction for Diabetic People          ,2006.15940,https://arxiv.org/abs/2006.15940,https://arxiv.org/pdf/2006.15940,"Authors:MaximeDeBois,MounîmA.ElYacoubi,MehdiAmmi","        Deep learning has yet to revolutionize general practices in healthcare, despite promising results for some specific tasks. This is partly due to data being in insufficient quantities hurting the training of the models. To address this issue, data from multiple health actors or patients could be combined by capitalizing on their heterogeneity through the use of transfer learning.  To improve the quality of the transfer between multiple sources of data, we propose a multi-source adversarial transfer learning framework that enables the learning of a feature representation that is similar across the sources, and thus more general and more easily transferable. We apply this idea to glucose forecasting for diabetic people using a fully convolutional neural network. The evaluation is done by exploring various transfer scenarios with three datasets characterized by their high inter and intra variability.  While transferring knowledge is beneficial in general, we show that the statistical and clinical accuracies can be further improved by using of the adversarial training methodology, surpassing the current state-of-the-art results. In particular, it shines when using data from different datasets, or when there is too little data in an intra-dataset situation. To understand the behavior of the models, we analyze the learnt feature representations and propose a new metric in this regard. Contrary to a standard transfer, the adversarial transfer does not discriminate the patients and datasets, helping the learning of a more general feature representation.  The adversarial training framework improves the learning of a general feature representation in a multi-source environment, enhancing the knowledge transfer to an unseen target.  The proposed method can help improve the efficiency of data shared by different health actors in the training of deep models.        △ Less","29 June, 2020","cs.CV,cs.LG,eess.IV",
              Exploring Optimal Control With Observations at a Cost          ,2006.15757,https://arxiv.org/abs/2006.15757,https://arxiv.org/pdf/2006.15757,"Authors:RuiAguiar,NikkaMofid,HyunjiAlexNam","        There has been a current trend in reinforcement learning for healthcare literature, where in order to prepare clinical datasets, researchers will carry forward the last results of the non-administered test known as the last-observation-carried-forward (LOCF) value to fill in gaps, assuming that it is still an accurate indicator of the patient's current state. These values are carried forward without maintaining information about exactly how these values were imputed, leading to ambiguity. Our approach models this problem using OpenAI Gym's Mountain Car and aims to address when to observe the patient's physiological state and partly how to intervene, as we have assumed we can only act after following an observation. So far, we have found that for a last-observation-carried-forward implementation of the state space, augmenting the state with counters for each state variable tracking the time since last observation was made, improves the predictive performance of an agent, supporting the notion of ""informative missingness"", and using a neural network based Dynamics Model to predict the most probable next state value of non-observed state variables instead of carrying forward the last observed value through LOCF further improves the agent's performance, leading to faster convergence and reduced variance.        △ Less","28 June, 2020","cs.LG,cs.AI",
              Compress or Interfere?          ,2006.15342,https://arxiv.org/abs/2006.15342,https://arxiv.org/pdf/2006.15342,"Authors:AlaaAwadAbdellatif,LutfiSamara,AmrMohamed,MohsenGuizani,AimanErbad,AbdullaAl-Ali","        Rapid evolution of wireless medical devices and network technologies has fostered the growth of remote monitoring systems. Such new technologies enable monitoring patients' medical records anytime and anywhere without limiting patients' activities. However, critical challenges have emerged with remote monitoring systems due to the enormous amount of generated data that need to be efficiently processed and wirelessly transmitted to the service providers in time. Thus, in this paper, we leverage full-duplex capabilities for fast transmission, while tackling the trade-off between Quality of Service (QoS) requirements and consequent self-interference (SI) for efficient remote monitoring healthcare systems. The proposed framework jointly considers the residual SI resulting from simultaneous transmission and reception along with the compressibility feature of medical data in order to optimize the data transmission over wireless channels, while maintaining the application's QoS constraint. Our simulation results demonstrate the efficiency of the proposed solution in terms of minimizing the transmission power, residual self-interference, and encoding distortion.        △ Less","27 June, 2020","eess.SP,cs.NI",10.1109/SAHCN.2019.8824824 
              Software Enabled Security Architecture and Mechanisms for Securing 5G Network Services          ,2006.15270,https://arxiv.org/abs/2006.15270,https://arxiv.org/pdf/2006.15270,"Authors:VijayVaradharajan,UdayTupakula,KallolKarmakar","        The 5G network systems are evolving and have complex network infrastructures. There is a great deal of work in this area focused on meeting the stringent service requirements for the 5G networks. Within this context, security requirements play a critical role as 5G networks can support a range of services such as healthcare services, financial and critical infrastructures. 3GPP and ETSI have been developing security frameworks for 5G networks. Our work in 5G security has been focusing on the design of security architecture and mechanisms enabling dynamic establishment of secure and trusted end to end services as well as development of mechanisms to proactively detect and mitigate security attacks in virtualised network infrastructures. The focus of this paper is on the latter, namely the facilities and mechanisms, and the design of a security architecture providing facilities and mechanisms to detect and mitigate specific security attacks. We have developed and implemented a simplified version of the security architecture using Software Defined Networks (SDN) and Network Function Virtualisation (NFV) technologies. The specific security functions developed in this architecture can be directly integrated into the 5G core network facilities enhancing its security. We describe the design and implementation of the security architecture and demonstrate how it can efficiently mitigate specific types of attacks.        △ Less","26 June, 2020","cs.CR,cs.NI",
              Mapping the South African health landscape in response to COVID-19          ,2006.15216,https://arxiv.org/abs/2006.15216,https://arxiv.org/pdf/2006.15216,"Authors:NompumeleloMtsweni,HerkulaasMvECombrink,VukosiMarivate","        When the COVID-19 disease pandemic infiltrated the world, there was an immediate need for accurate information. As with any outbreak, the outbreak follows a clear trajectory, and subsequently, the supporting information for that outbreak needs to address the needs associated with that stage of the outbreak. At first, there was a need to inform the public of the information related to the initial situation related to the ""who"" of the COVID-19 disease. However, as time continued, the ""where"", ""when"" and ""how to"" related questions started to emerge in relation to the public healthcare system themselves. Questions surrounding the health facilities including COVID-19 hospital bed capacity, locations of designated COVID-19 facilities, and general information related to these facilities were not easily accessible to the general public. Furthermore, the available information was found to be outdated, fragmented across several platforms, and still had gaps in the data related to these facilities. To rectify this problem, a group of volunteers working on the covid19za project stepped in to assist. Each member leading a part of the project chose to focus on one of four problems related to the challenges associated with the Hospital information including: data quality, data completeness, data source validation and data visualisation capacity. As the project developed, so did the sophistication of the data, visualisation and core function of the project. The future prospects of this project relate to a Progressive Web Application that will avail this information for the public as well as healthcare workers through comprehensive mapping and data quality.        △ Less","26 June, 2020",cs.CY,
              ELMV: a Ensemble-Learning Approach for Analyzing Electrical Health Records with Significant Missing Values          ,2006.14942,https://arxiv.org/abs/2006.14942,https://arxiv.org/pdf/2006.14942,"Authors:LucasJ.Liu,HongweiZhang,JianzhongDi,JinChen","        Many real-world Electronic Health Record (EHR) data contains a large proportion of missing values. Leaving substantial portion of missing information unaddressed usually causes significant bias, which leads to invalid conclusion to be drawn. On the other hand, training a machine learning model with a much smaller nearly-complete subset can drastically impact the reliability and accuracy of model inference. Data imputation algorithms that attempt to replace missing data with meaningful values inevitably increase the variability of effect estimates with increased missingness, making it unreliable for hypothesis validation. We propose a novel Ensemble-Learning for Missing Value (ELMV) framework, which introduces an effective approach to construct multiple subsets of the original EHR data with a much lower missing rate, as well as mobilizing a dedicated support set for the ensemble learning in the purpose of reducing the bias caused by substantial missing values. ELMV has been evaluated on a real-world healthcare data for critical feature identification as well as a batch of simulation data with different missing rates for outcome prediction. On both experiments, ELMV clearly outperforms conventional missing value imputation methods and ensemble learning models.        △ Less","25 June, 2020","cs.LG,stat.ML",
              Trust-by-Design: Evaluating Issues and Perceptions within Clinical Passporting          ,2006.14864,https://arxiv.org/abs/2006.14864,https://arxiv.org/pdf/2006.14864,"Authors:WillAbramson,NicoleE.vanDeursen,WilliamJBuchanan","        A substantial administrative burden is placed on healthcare professionals as they manage and progress through their careers. Identity verification, pre-employment screening and appraisals: the bureaucracy associated with each of these processes takes precious time out of a healthcare professional's day. Time that could have been spent focused on patient care. In the midst of the COVID-19 crisis, it is more important than ever to optimize these professionals' time. This paper presents the synthesis of a design workshop held at the Royal College of Physicians of Edinburgh (RCPE) and subsequent interviews with healthcare professionals. The main research question posed is whether these processes can be re-imagined using digital technologies, specifically Self-Sovereign Identity? A key contribution in the paper is the development of a set of user-led requirements and design principles for identity systems used within healthcare. These are then contrasted with the design principles found in the literature. The results of this study confirm the need and potential of professionalising identity and credential management throughout a healthcare professional's career.        △ Less","26 June, 2020","cs.CR,cs.CY",10.30953/bhty.v3.140 
              Monitoring of process and risk-adjusted medical outcomes using a multi-stage MEWMA chart          ,2006.14737,https://arxiv.org/abs/2006.14737,https://arxiv.org/pdf/2006.14737,"Authors:DoaaAyad,NokuthabaSibanda","        Most statistical process control programmes in healthcare focus on surveillance of outcomes at the final stage of a procedure, such as mortality or failure rates. Such an approach ignores the multi-stage nature of these procedures, in which a patient progresses through several stages prior to the final stage. In this paper, we develop a multi-stage control chart based on a multivariate exponentially weighted moving average (EWMA) test statistic derived from score equations. This allows simultaneous monitoring of all intermediate and final stage outcomes of a healthcare process, with adjustment for underlying patient risk factors and dependence between outcome variables. Use of the EWMA test statistics allows quick detection of small gradual changes in any part of the process. Three advantages of the approach are: better understanding of how outcomes at different stages relate to each other, explicit monitoring of upstream stage outcomes may help curtail trends that lead to poorer end-stage outcomes and understanding the impact of each stage can help determine the most effective allocation of quality improvement resources. Simulations are performed to test the control charts under various types of hypothesised shifts, and the results are summarised using out-of-control average run lengths.        △ Less","25 June, 2020","stat.ME,stat.OT",
              Neural Decomposition: Functional ANOVA with Variational Autoencoders          ,2006.14293,https://arxiv.org/abs/2006.14293,https://arxiv.org/pdf/2006.14293,"Authors:KasparMärtens,ChristopherYau","        Variational Autoencoders (VAEs) have become a popular approach for dimensionality reduction. However, despite their ability to identify latent low-dimensional structures embedded within high-dimensional data, these latent representations are typically hard to interpret on their own. Due to the black-box nature of VAEs, their utility for healthcare and genomics applications has been limited. In this paper, we focus on characterising the sources of variation in Conditional VAEs. Our goal is to provide a feature-level variance decomposition, i.e. to decompose variation in the data by separating out the marginal additive effects of latent variables z and fixed inputs c from their non-linear interactions. We propose to achieve this through what we call Neural Decomposition - an adaptation of the well-known concept of functional ANOVA variance decomposition from classical statistics to deep learning models. We show how identifiability can be achieved by training models subject to constraints on the marginal properties of the decoder networks. We demonstrate the utility of our Neural Decomposition on a series of synthetic examples as well as high-dimensional genomics data.        △ Less","26 August, 2020","stat.ML,cs.LG",
              Strictly Batch Imitation Learning by Energy-based Distribution Matching          ,2006.14154,https://arxiv.org/abs/2006.14154,https://arxiv.org/pdf/2006.14154,"Authors:DanielJarrett,IoanaBica,MihaelavanderSchaar","        Consider learning a policy purely on the basis of demonstrated behavior---that is, with no access to reinforcement signals, no knowledge of transition dynamics, and no further interaction with the environment. This *strictly batch imitation learning* problem arises wherever live experimentation is costly, such as in healthcare. One solution is simply to retrofit existing algorithms for apprenticeship learning to work in the offline setting. But such an approach bargains heavily on model estimation or off-policy evaluation, and can be indirect and inefficient. We argue that a good solution should be able to explicitly parameterize a policy (i.e. respecting action conditionals), implicitly account for rollout dynamics (i.e. respecting state marginals), and---crucially---operate in an entirely offline fashion. To meet this challenge, we propose a novel technique by *energy-based distribution matching* (EDM): By identifying parameterizations of the (discriminative) model of a policy with the (generative) energy function for state distributions, EDM provides a simple and effective solution that equivalently minimizes a divergence between the occupancy measures of the demonstrator and the imitator. Through experiments with application to control tasks and healthcare settings, we illustrate consistent performance gains over existing algorithms for strictly batch imitation learning.        △ Less","24 June, 2020","stat.ML,cs.LG",
              Time Series Analysis and Forecasting of COVID-19 Cases Using LSTM and ARIMA Models          ,2006.13852,https://arxiv.org/abs/2006.13852,https://arxiv.org/pdf/2006.13852,Authors:ArkoBarman,"        Coronavirus disease 2019 (COVID-19) is a global public health crisis that has been declared a pandemic by World Health Organization. Forecasting country-wise COVID-19 cases is necessary to help policymakers and healthcare providers prepare for the future. This study explores the performance of several Long Short-Term Memory (LSTM) models and Auto-Regressive Integrated Moving Average (ARIMA) model in forecasting the number of confirmed COVID-19 cases. Time series of daily cumulative COVID-19 cases were used for generating 1-day, 3-day, and 5-day forecasts using several LSTM models and ARIMA. Two novel k-period performance metrics - k-day Mean Absolute Percentage Error (kMAPE) and k-day Median Symmetric Accuracy (kMdSA) - were developed for evaluating the performance of the models in forecasting time series values for multiple days. Errors in prediction using kMAPE and kMdSA for LSTM models were both as low as 0.05%, while those for ARIMA were 0.07% and 0.06% respectively. LSTM models slightly underestimated while ARIMA slightly overestimated the numbers in the forecasts. The performance of LSTM models is comparable to ARIMA in forecasting COVID-19 cases. While ARIMA requires longer sequences, LSTMs can perform reasonably well with sequence sizes as small as 3. However, LSTMs require a large number of training samples. Further, the development of k-period performance metrics proposed is likely to be useful for performance evaluation of time series models in predicting multiple periods. Based on the k-period performance metrics proposed, both LSTMs and ARIMA are useful for time series analysis and forecasting for COVID-19.        △ Less","5 June, 2020","cs.LG,stat.AP",
              Local Interpretability of Calibrated Prediction Models: A Case of Type 2 Diabetes Mellitus Screening Test          ,2006.13815,https://arxiv.org/abs/2006.13815,https://arxiv.org/pdf/2006.13815,"Authors:SimonKocbek,PrimozKocbek,LeonaCilar,GregorStiglic",        Machine Learning (ML) models are often complex and difficult to interpret due to their 'black-box' characteristics. Interpretability of a ML model is usually defined as the degree to which a human can understand the cause of decisions reached by a ML model. Interpretability is of extremely high importance in many fields of healthcare due to high levels of risk related to decisions based on ML models. Calibration of the ML model outputs is another issue often overlooked in the application of ML models in practice. This paper represents an early work in examination of prediction model calibration impact on the interpretability of the results. We present a use case of a patient in diabetes screening prediction scenario and visualize results using three different techniques to demonstrate the differences between calibrated and uncalibrated regularized regression model.        △ Less,"2 June, 2020","stat.ME,cs.LG,stat.AP,stat.ML",
              Benchmark and Best Practices for Biomedical Knowledge Graph Embeddings          ,2006.13774,https://arxiv.org/abs/2006.13774,https://arxiv.org/pdf/2006.13774,"Authors:DavidChang,IvanaBalazevic,CarlAllen,DanielChawla,CynthiaBrandt,RichardAndrewTaylor","        Much of biomedical and healthcare data is encoded in discrete, symbolic form such as text and medical codes. There is a wealth of expert-curated biomedical domain knowledge stored in knowledge bases and ontologies, but the lack of reliable methods for learning knowledge representation has limited their usefulness in machine learning applications. While text-based representation learning has significantly improved in recent years through advances in natural language processing, attempts to learn biomedical concept embeddings so far have been lacking. A recent family of models called knowledge graph embeddings have shown promising results on general domain knowledge graphs, and we explore their capabilities in the biomedical domain. We train several state-of-the-art knowledge graph embedding models on the SNOMED-CT knowledge graph, provide a benchmark with comparison to existing methods and in-depth discussion on best practices, and make a case for the importance of leveraging the multi-relational nature of knowledge graphs for learning biomedical knowledge representation. The embeddings, code, and materials will be made available to the communitY.        △ Less","24 June, 2020","cs.AI,cs.CL",
              Using Deep Learning and Explainable Artificial Intelligence in Patients' Choices of Hospital Levels          ,2006.13427,https://arxiv.org/abs/2006.13427,https://arxiv.org/pdf/2006.13427,"Authors:LichinChen,YuTsao,Ji-TianSheu","        In countries that enabled patients to choose their own providers, a common problem is that the patients did not make rational decisions, and hence, fail to use healthcare resources efficiently. This might cause problems such as overwhelming tertiary facilities with mild condition patients, thus limiting their capacity of treating acute and critical patients. To address such maldistributed patient volume, it is essential to oversee patients choices before further evaluation of a policy or resource allocation. This study used nationwide insurance data, accumulated possible features discussed in existing literature, and used a deep neural network to predict the patients choices of hospital levels. This study also used explainable artificial intelligence methods to interpret the contribution of features for the general public and individuals. In addition, we explored the effectiveness of changing data representations. The results showed that the model was able to predict with high area under the receiver operating characteristics curve (AUC) (0.90), accuracy (0.90), sensitivity (0.94), and specificity (0.97) with highly imbalanced label. Generally, social approval of the provider by the general public (positive or negative) and the number of practicing physicians serving per ten thousand people of the located area are listed as the top effecting features. The changing data representation had a positive effect on the prediction improvement. Deep learning methods can process highly imbalanced data and achieve high accuracy. The effecting features affect the general public and individuals differently. Addressing the sparsity and discrete nature of insurance data leads to better prediction. Applications using deep learning technology are promising in health policy making. More work is required to interpret models and practice implementation.        △ Less","23 June, 2020","cs.CY,cs.AI,cs.LG",
              Momentum Contrastive Learning for Few-Shot COVID-19 Diagnosis from Chest CT Images          ,2006.13276,https://arxiv.org/abs/2006.13276,https://arxiv.org/pdf/2006.13276,"Authors:XiaocongChen,LinaYao,TaoZhou,JinmingDong,YuZhang","        The current pandemic, caused by the outbreak of a novel coronavirus (COVID-19) in December 2019, has led to a global emergency that has significantly impacted economies, healthcare systems and personal wellbeing all around the world. Controlling the rapidly evolving disease requires highly sensitive and specific diagnostics. While real-time RT-PCR is the most commonly used, these can take up to 8 hours, and require significant effort from healthcare professionals. As such, there is a critical need for a quick and automatic diagnostic system. Diagnosis from chest CT images is a promising direction. However, current studies are limited by the lack of sufficient training samples, as acquiring annotated CT images is time-consuming. To this end, we propose a new deep learning algorithm for the automated diagnosis of COVID-19, which only requires a few samples for training. Specifically, we use contrastive learning to train an encoder which can capture expressive feature representations on large and publicly available lung datasets and adopt the prototypical network for classification. We validate the efficacy of the proposed model in comparison with other competing methods on two publicly available and annotated COVID-19 CT datasets. Our results demonstrate the superior performance of our model for the accurate diagnosis of COVID-19 based on chest CT images.        △ Less","16 June, 2020","eess.IV,cs.CV,cs.LG",
              Successful implementation of discrete event simulation: the case of an Italian emergency department          ,2006.13062,https://arxiv.org/abs/2006.13062,https://arxiv.org/pdf/2006.13062,"Authors:ArthurKramer,ClioDosi,ManuelIori,MatteoVignoli","        This paper focuses on the study of a practical management problem faced by a healthcare {\it emergency department} (ED) located in the north of Italy. The objective of our study was to propose organisational changes in the selected ED, which admits approximately 7000 patients per month, aiming at improving key performance indicators related to patient satisfaction, such as the waiting time. Our study is based on a design thinking process that adopts a {\it discrete event simulation} (DES) model as the main tool for proposing changes. We used the DES model to propose and evaluate the impact of different improving scenarios. The model is based on historical data, on the observation of the current ED situation, and information obtained from the ED staff. The results obtained by the DES model have been compared with those related to the existing ED setting, and then validated by the ED managers. Based on the results we obtained, one of the tested scenarios was selected by the ED for implementation.        △ Less","23 June, 2020","cs.CY,math.OC",
              Unsupervised ensembling of multiple software sensors: a new approach for electrocardiogram-derived respiration using one or two channels          ,2006.13054,https://arxiv.org/abs/2006.13054,https://arxiv.org/pdf/2006.13054,"Authors:JohnMalik,Yu-TingLin,RonenTalmon,Hau-TiengWu","        While several electrocardiogram-derived respiratory (EDR) algorithms have been proposed to extract breathing activity from a single-channel ECG signal, conclusively identifying a superior technique is challenging. We propose viewing each EDR algorithm as a {\em software sensor} that records the breathing activity from the ECG signal, and ensembling those software sensors to achieve a higher quality EDR signal. We refer to the output of the proposed ensembling algorithm as the {\em ensembled EDR}. We test the algorithm on a large scale database of 116 whole-night polysomnograms and compare the ensembled EDR signal with four respiratory signals recorded from four different hardware sensors. The proposed algorithm consistently improves upon other algorithms, and we envision its clinical value and its application in future healthcare.        △ Less","23 June, 2020","eess.SP,physics.data-an,stat.AP",
              A Causally Formulated Hazard Ratio Estimation through Backdoor Adjustment on Structural Causal Model          ,2006.12573,https://arxiv.org/abs/2006.12573,https://arxiv.org/pdf/2006.12573,"Authors:RiddhimanAdib,PaulGriffin,SheikhIqbalAhamed,MohammadAdibuzzaman","        Identifying causal relationships for a treatment intervention is a fundamental problem in health sciences. Randomized controlled trials (RCTs) are considered the gold standard for identifying causal relationships. However, recent advancements in the theory of causal inference based on the foundations of structural causal models (SCMs) have allowed the identification of causal relationships from observational data, under certain assumptions. Survival analysis provides standard measures, such as the hazard ratio, to quantify the effects of an intervention. While hazard ratios are widely used in clinical and epidemiological studies for RCTs, a principled approach does not exist to compute hazard ratios for observational studies with SCMs. In this work, we review existing approaches to compute hazard ratios as well as their causal interpretation, if it exists. We also propose a novel approach to compute hazard ratios from observational studies using backdoor adjustment through SCMs and do-calculus. Finally, we evaluate the approach using experimental data for Ewing's sarcoma.        △ Less","22 June, 2020","stat.ME,cs.LG,stat.ML",
              Automated machine vision enabled detection of movement disorders from hand drawn spirals          ,2006.12121,https://arxiv.org/abs/2006.12121,https://arxiv.org/pdf/2006.12121,"Authors:NabeelSeedat,VeredAharonson,IlanaSchlesinger","        A widely used test for the diagnosis of Parkinson's disease (PD) and Essential tremor (ET) is hand-drawn shapes,where the analysis is observationally performed by the examining neurologist. This method is subjective and is prone to bias amongst different physicians. Due to the similarities in the symptoms of the two diseases, they are often misdiagnosed.Studies which attempt to automate the process typically use digitized input, where the tablet or specialized equipment are not affordable in many clinical settings. This study uses a dataset of scanned pen and paper drawings and a convolutional neural network (CNN) to perform classification between PD, ET and control subjects. The discrimination accuracy of PD from controls was 98.2%. The discrimination accuracy of PD from ET and from controls was 92%. An ablation study was conducted and indicated that correct hyper parameter optimization can increases the accuracy up to 4.33%. Finally, the study indicates the viability of using a CNN-enabled machine vision system to provide robust and accurate detection of movement disorders from hand drawn spirals.        △ Less","22 June, 2020","cs.CV,cs.LG",
              Machine learning discrimination of Parkinson's Disease stages from walker-mounted sensors data          ,2006.12094,https://arxiv.org/abs/2006.12094,https://arxiv.org/pdf/2006.12094,"Authors:NabeelSeedat,VeredAharonson","        Clinical methods that assess gait in Parkinson's Disease (PD) are mostly qualitative. Quantitative methods necessitate costly instrumentation or cumbersome wearable devices, which limits their usability. Only few of these methods can discriminate different stages in PD progression. This study applies machine learning methods to discriminate six stages of PD. The data was acquired by low cost walker-mounted sensors in an experiment at a movement disorders clinic and the PD stages were clinically labeled. A large set of features, some unique to this study are extracted and three feature selection methods are compared using a multi-class Random Forest (RF) classifier. The feature subset selected by the Analysis of Variance (ANOVA) method provided performance similar to the full feature set: 93% accuracy and had significantly shorter computation time. Compared to PCA, this method also enabled clinical interpretability of the selected features, an essential attribute to healthcare applications. All selected-feature sets are dominated by information theoretic features and statistical features and offer insights into the characteristics of gait deterioration in PD. The results indicate a feasibility of machine learning to accurately classify PD severity stages from kinematic signals acquired by low-cost, walker-mounted sensors and implies a potential to aid medical practitioners in the quantitative assessment of PD progression. The study presents a solution to the small and noisy data problem, which is common in most sensor-based healthcare assessments.        △ Less","22 June, 2020","cs.LG,eess.SP,stat.ML",
              Security and Privacy for mHealth and uHealth Systems: a Systematic Mapping Study          ,2006.12069,https://arxiv.org/abs/2006.12069,https://arxiv.org/pdf/2006.12069,"Authors:LeonardoHornIwaya,AakashAhmad,M.AliBabar","        An increased adoption of mobile health (mHealth) and ubiquitous health (uHealth) systems empower users with handheld devices and embedded sensors for a broad range of healthcare services. However, m/uHealth systems face significant challenges related to data security and privacy that must be addressed to increase the pervasiveness of such systems. This study aims to systematically identify, classify, compare, and evaluate state-of-the-art on security and privacy of m/uHealth systems. We conducted a systematic mapping study (SMS) based on 365 qualitatively selected studies to (i) classify the types, frequency, and demography of published research and (ii) synthesize and categorize research themes, (iii) recurring challenges, (iv) prominent solutions (i.e., research outcomes) and their (v) reported evaluations (i.e., practical validations). Results suggest that the existing research on security and privacy of m/uHealth systems primarily focuses on select group of control families (compliant with NIST800-53), protection of systems and information, access control, authentication, individual participation, and privacy authorisation. In contrast, areas of data governance, security and privacy policies, and program management are under-represented, although these are critical to most of the organizations that employ m/uHealth systems. Most research proposes new solutions with limited validation, reflecting a lack of evaluation of security and privacy of m/uHealth in the real world. Empirical research, development, and validation of m/uHealth security and privacy is still incipient, which may discourage practitioners from readily adopting solutions from the literature. This SMS facilitates knowledge transfer, enabling researchers and practitioners to engineer security and privacy for emerging and next generation of m/uHealth systems.        △ Less","22 June, 2020","cs.CR,cs.CY",10.1109/ACCESS.2020.3015962 
              Students Need More Attention: BERT-based AttentionModel for Small Data with Application to AutomaticPatient Message Triage          ,2006.11991,https://arxiv.org/abs/2006.11991,https://arxiv.org/pdf/2006.11991,"Authors:ShijingSi,RuiWang,JedrekWosik,HaoZhang,DavidDov,GuoyinWang,RicardoHenao,LawrenceCarin","        Small and imbalanced datasets commonly seen in healthcare represent a challenge when training classifiers based on deep learning models. So motivated, we propose a novel framework based on BioBERT (Bidirectional Encoder Representations from Transformers forBiomedical TextMining). Specifically, (i) we introduce Label Embeddings for Self-Attention in each layer of BERT, which we call LESA-BERT, and (ii) by distilling LESA-BERT to smaller variants, we aim to reduce overfitting and model size when working on small datasets. As an application, our framework is utilized to build a model for patient portal message triage that classifies the urgency of a message into three categories: non-urgent, medium and urgent. Experiments demonstrate that our approach can outperform several strong baseline classifiers by a significant margin of 4.3% in terms of macro F1 score. The code for this project is publicly available at \url{https://github.com/shijing001/text_classifiers}.        △ Less","21 June, 2020","cs.CL,cs.LG",
              Using Fault Injection to Assess Blockchain Systems in Presence of Faulty Smart Contracts          ,2006.11597,https://arxiv.org/abs/2006.11597,https://arxiv.org/pdf/2006.11597,"Authors:ÁkosHajdu,NaghmehIvaki,ImreKocsis,AttilaKlenik,LászlóGönczy,NunoLaranjeiro,HenriqueMadeira,AndrásPataricza","        Blockchain has become particularly popular due to its promise to support business-critical services in very different domains (e.g., retail, supply chains, healthcare). Blockchain systems rely on complex middleware, like Ethereum or Hyperledger Fabric, that allow running smart contracts, which specify business logic in cooperative applications. The presence of software defects or faults in these contracts has notably been the cause of failures, including severe security problems. In this paper, we use a software implemented fault injection (SWIFI) technique to assess the behavior of permissioned blockchain systems in the presence of faulty smart contracts. We emulate the occurrence of general software faults (e.g., missing variable initialization) and also blockchain-specific software faults (e.g., missing require statement on transaction sender) in smart contracts code to observe the impact on the overall system dependability (i.e., reliability and integrity). We also study the effectiveness of formal verification (i.e., done by solc-verify) and runtime protections (e.g., using the assert statement) mechanisms in detection of injected faults. Results indicate that formal verification as well as additional runtime protections have to complement built-in platform checks to guarantee the proper dependability of blockchain systems and applications. The work presented in this paper allows smart contract developers to become aware of possible faults in smart contracts and to understand the impact of their presence. It also provides valuable information for middleware developers to improve the behavior (e.g., overall fault tolerance) of their systems.        △ Less","20 June, 2020",cs.SE,
              In-Memory Resistive RAM Implementation of Binarized Neural Networks for Medical Applications          ,2006.11595,https://arxiv.org/abs/2006.11595,https://arxiv.org/pdf/2006.11595,"Authors:BogdanPenkovsky,MarcBocquet,TifennHirtzlin,Jacques-OlivierKlein,EtienneNowak,ElisaVianello,Jean-MichelPortal,DamienQuerlioz","        The advent of deep learning has considerably accelerated machine learning development. The deployment of deep neural networks at the edge is however limited by their high memory and energy consumption requirements. With new memory technology available, emerging Binarized Neural Networks (BNNs) are promising to reduce the energy impact of the forthcoming machine learning hardware generation, enabling machine learning on the edge devices and avoiding data transfer over the network. In this work, after presenting our implementation employing a hybrid CMOS - hafnium oxide resistive memory technology, we suggest strategies to apply BNNs to biomedical signals such as electrocardiography and electroencephalography, keeping accuracy level and reducing memory requirements. We investigate the memory-accuracy trade-off when binarizing whole network and binarizing solely the classifier part. We also discuss how these results translate to the edge-oriented Mobilenet~V1 neural network on the Imagenet task. The final goal of this research is to enable smart autonomous healthcare devices.        △ Less","20 June, 2020","eess.SP,cs.ET",
              Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey          ,2006.11371,https://arxiv.org/abs/2006.11371,https://arxiv.org/pdf/2006.11371,"Authors:ArunDas,PaulRad","        Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.        △ Less","22 June, 2020","cs.CV,cs.AI,cs.LG",
              A Qualitative Evaluation of Language Models on Automatic Question-Answering for COVID-19          ,2006.10964,https://arxiv.org/abs/2006.10964,https://arxiv.org/pdf/2006.10964,"Authors:DavidOniani,YanshanWang","        COVID-19 has resulted in an ongoing pandemic and as of 12 June 2020, has caused more than 7.4 million cases and over 418,000 deaths. The highly dynamic and rapidly evolving situation with COVID-19 has made it difficult to access accurate, on-demand information regarding the disease. Online communities, forums, and social media provide potential venues to search for relevant questions and answers, or post questions and seek answers from other members. However, due to the nature of such sites, there are always a limited number of relevant questions and responses to search from, and posted questions are rarely answered immediately. With the advancements in the field of natural language processing, particularly in the domain of language models, it has become possible to design chatbots that can automatically answer consumer questions. However, such models are rarely applied and evaluated in the healthcare domain, to meet the information needs with accurate and up-to-date healthcare data. In this paper, we propose to apply a language model for automatically answering questions related to COVID-19 and qualitatively evaluate the generated responses. We utilized the GPT-2 language model and applied transfer learning to retrain it on the COVID-19 Open Research Dataset (CORD-19) corpus. In order to improve the quality of the generated responses, we applied 4 different approaches, namely tf-idf, BERT, BioBERT, and USE to filter and retain relevant sentences in the responses. In the performance evaluation step, we asked two medical experts to rate the responses. We found that BERT and BioBERT, on average, outperform both tf-idf and USE in relevance-based sentence filtering tasks. Additionally, based on the chatbot, we created a user-friendly interactive web application to be hosted online.        △ Less","23 June, 2020","cs.IR,cs.AI,cs.CL",
"              SSHealth: Toward Secure, Blockchain-Enabled Healthcare Systems          ",2006.10843,https://arxiv.org/abs/2006.10843,https://arxiv.org/pdf/2006.10843,"Authors:AlaaAwadAbdellatif,AbeerZ.Al-Marridi,AmrMohamed,AimanErbad,CarlaFabianaChiasserini,AhmedRefaey","        The future of healthcare systems is being shaped by incorporating emerged technological innovations to drive new models for patient care. By acquiring, integrating, analyzing, and exchanging medical data at different system levels, new practices can be introduced, offering a radical improvement to healthcare services. This paper presents a novel smart and secure Healthcare system (ssHealth), which, leveraging advances in edge computing and blockchain technologies, permits epidemics discovering, remote monitoring, and fast emergency response. The proposed system also allows for secure medical data exchange among local healthcare entities, thus realizing the integration of multiple national and international entities and enabling the correlation of critical medical events for, e.g., emerging epidemics management and control. In particular, we develop a blockchain-based architecture and enable a flexible configuration thereof, which optimize medical data sharing between different health entities and fulfil the diverse levels of Quality of Service (QoS) that ssHealth may require. Finally, we highlight the benefits of the proposed ssHealth system and possible directions for future research.        △ Less","18 June, 2020","cs.CY,cs.CR,cs.NI",10.1109/MNET.011.1900553 
              Pervasive Communications Technologies For Managing Pandemics          ,2006.10805,https://arxiv.org/abs/2006.10805,https://arxiv.org/pdf/2006.10805,"Authors:MuhammadIlyas,BasitQureshi","        Pandemics always have had serious consequences unless they were effectively contained. Recent experiences with COVID-19 show that by using a smart and swift approach to deal with pandemics, avoids overwhelming of healthcare systems, and reduces the loss of precious life. This paper is about using smart technologies such as Mobile Edge Clouds (MEC), Internet of Things (IoT), and Artificial Intelligence (AI), as an approach to effectively manage pandemics. IoT provides pervasive connectivity among various devices and can be used for collecting information such as location and symptoms of potentially infected individuals. MECs provide cloud services on the edge, integrating IoT infrastructure and execution of sophisticated AI algorithms in the Cloud. In this paper, we develop a prototype to demonstrate the convergence of pervasive technologies to support research in managing pandemics. Low-cost Single Board Computers (SBC) based clusters are integrated within MEC to support remote medical teams in the field. The prototype implements a lightweight Docker container orchestrated by Kubernetes eco-system which is deployed on the clusters. The prototype successfully demonstrates that mobile medical facilities can utilize the proposed solution to collect information and execute AI algorithms while on the go. Finally, we present a discussion on the role of converging pervasive technologies on managing pandemics.        △ Less","18 June, 2020",cs.CY,
              The United Nations Sustainable Development Goals in Systems Engineering: Eliciting sustainability requirements          ,2006.10528,https://arxiv.org/abs/2006.10528,https://arxiv.org/pdf/2006.10528,Authors:IanBrooks,"        This paper discusses a PhD research project testing the hypothesis that using the United Nations Sustainable Development Goals(SDG) as explicit inputs to drive the Software Requirements Engineering process will result in requirements with improved sustainability benefits. The research has adopted the Design Science Research Method (DSRM) [21] to test a process named SDG Assessment for Requirements Elicitation (SDGARE). Three DSRM cycles are being used to test the hypothesis in safety-critical, highprecision, software-intensive systems in aerospace and healthcare. Initial results from the first two DSRM cycles support the hypothesis. However, these cycles are in a plan-driven (waterfall) development context and future research agenda would be a similar application in an Agile development context.        △ Less","12 June, 2020",cs.CY,10.1145/3401335.3401359 
              Privacy-Preserving Technology to Help Millions of People: Federated Prediction Model for Stroke Prevention          ,2006.10517,https://arxiv.org/abs/2006.10517,https://arxiv.org/pdf/2006.10517,"Authors:CeJu,RuihuiZhao,JichaoSun,XiguangWei,BoZhao,YangLiu,HongshanLi,TianjianChen,XinweiZhang,DashanGao,BenTan,HanYu,YuanJin","        prevention of stroke with its associated risk factors has been one of the public health priorities worldwide. Emerging artificial intelligence technology is being increasingly adopted to predict stroke. Because of privacy concerns, patient data are stored in distributed electronic health record (EHR) databases, voluminous clinical datasets, which prevent patient data from being aggregated and restrains AI technology to boost the accuracy of stroke prediction with centralized training data. In this work, our scientists and engineers propose a privacy-preserving scheme to predict the risk of stroke and deploy our federated prediction model on cloud servers. Our system of federated prediction model asynchronously supports any number of client connections and arbitrary local gradient iterations in each communication round. It adopts federated averaging during the model training process, without patient data being taken out of the hospitals during the whole process of model training and forecasting. With the privacy-preserving mechanism, our federated prediction model trains over all the healthcare data from hospitals in a certain city without actual data sharing among them. Therefore, it is not only secure but also more accurate than any single prediction model that trains over the data only from one single hospital. Especially for small hospitals with few confirmed stroke cases, our federated model boosts model performance by 10%~20% in several machine learning metrics. To help stroke experts comprehend the advantage of our prediction system more intuitively, we developed a mobile app that collects the key information of patients' statistics and demonstrates performance comparisons between the federated prediction model and the single prediction model during the federated training process.        △ Less","15 June, 2020","cs.LG,cs.CR",
              Self-Attention Enhanced Patient Journey Understanding in Healthcare System          ,2006.10516,https://arxiv.org/abs/2006.10516,https://arxiv.org/pdf/2006.10516,"Authors:XuepingPeng,GuodongLong,TaoShen,SenWang,JingJiang","        Understanding patients' journeys in healthcare system is a fundamental prepositive task for a broad range of AI-based healthcare applications. This task aims to learn an informative representation that can comprehensively encode hidden dependencies among medical events and its inner entities, and then the use of encoding outputs can greatly benefit the downstream application-driven tasks. A patient journey is a sequence of electronic health records (EHRs) over time that is organized at multiple levels: patient, visits and medical codes. The key challenge of patient journey understanding is to design an effective encoding mechanism which can properly tackle the aforementioned multi-level structured patient journey data with temporal sequential visits and a set of medical codes. This paper proposes a novel self-attention mechanism that can simultaneously capture the contextual and temporal relationships hidden in patient journeys. A multi-level self-attention network (MusaNet) is specifically designed to learn the representations of patient journeys that is used to be a long sequence of activities. The MusaNet is trained in end-to-end manner using the training data derived from EHRs. We evaluated the efficacy of our method on two medical application tasks with real-world benchmark datasets. The results have demonstrated the proposed MusaNet produces higher-quality representations than state-of-the-art baseline methods. The source code is available in https://github.com/xueping/MusaNet.        △ Less","18 June, 2020","cs.LG,stat.ML",
              Exergames for telerehabilitation          ,2006.10110,https://arxiv.org/abs/2006.10110,https://arxiv.org/pdf/2006.10110,Authors:SatishReddyBethi,"        Recent advancements in technology have improved the connectivity between humans enhancing the transfer of information. Leveraging these technological marvels in the healthcare industry has led to the development of telehealth allowing patients and clinicians to receive and administer treatment remotely. Telerehabilitation is a subset of telehealth that facilitates remote rehabilitation treatment for patients. Providing rehabilitative services to the aging baby boomer population requires tech-savvy solutions to augment the therapists and clinicians for effective remote monitoring and tele-medicine. Hence, this thesis develops easy-to-use exergames for low-cost mechatronic devices targeting rehabilitation of post-stroke patients. Specifically, it demonstrates wearable inertial sensors for exergames consisting of an animated virtual coach for providing patients with instructions for performing range of motion exercises. Next, a gaming environment is developed for task-specific rehabilitation such as eating. Finally, exergames are developed for rehabilitation of pincer grasping. In addition to gamified interfaces providing an engaging rehabilitation experience to the user, the data acquired from the mechatronic devices facilitate data-driven telerehabilitation.        △ Less","8 June, 2020",cs.HC,
              Simulation-Optimization of Automated Material Handling Systems in a Healthcare Facility          ,2006.10031,https://arxiv.org/abs/2006.10031,https://arxiv.org/pdf/2006.10031,"Authors:AmoghBhosekar,TugceIsik,SandraEksioglu,KadeGilstrap,RobertAllen","        Automated material handling systems are used in healthcare facilities to optimize material flow, minimize workforce requirements, reduce the risk of contamination, and reduce injuries. This study proposes a framework that integrates data analysis with system simulation and optimization to address the following research questions: (i) What are the implications of redesigning a hospital's material handling system? (ii) What are the implications of improving a hospital's material handling process? This paper develops a case study using data from the Greenville Memorial Hospital (GMH) in South Carolina, USA. The case study is focused on the delivery of surgical cases to operating rooms at GMH via Automated Guided Vehicles (AGVs). The data analysis provides distributions of travel times, AGV utilization, and AGV movement patterns in the current system. The results of data analysis are integrated in a simulation-optimization model that incorporates the size of AGV fleet and the corresponding routes to improve system efficiency, increase AGV utilization, and reduce congestion. To address research question (i), a redesign of AGV pathways is evaluated to determine whether congestion is reduced. For research question (ii), the implementation of a Kanban system is proposed to improve AGV utilization by controlling the number of AGVs used daily, based on the volume of surgical cases. An extensive sensitivity analysis, simulation-optimization experiments, and a pilot study are conducted and indicate that the proposed Kanban system leads to significant reductions in congestion and travel times and increased utilization of AGVs.        △ Less","17 June, 2020",math.OC,
              Is Working From Home The New Norm? An Observational Study Based on a Large Geo-tagged COVID-19 Twitter Dataset          ,2006.08581,https://arxiv.org/abs/2006.08581,https://arxiv.org/pdf/2006.08581,"Authors:YunheFeng,WenjunZhou","        As the COVID-19 pandemic swept over the world, people discussed facts, expressed opinions, and shared sentiments on social media. Since the reaction to COVID-19 in different locations may be tied to local cases, government regulations, healthcare resources and socioeconomic factors, we curated a large geo-tagged Twitter dataset and performed exploratory analysis by location. Specifically, we collected 650,563 unique geo-tagged tweets across the United States (50 states and Washington, D.C.) covering the date range from January 25 to May 10, 2020. Tweet locations enabled us to conduct region-specific studies such as tweeting volumes and sentiment, sometimes in response to local regulations and reported COVID-19 cases. During this period, many people started working from home. The gap between workdays and weekends in hourly tweet volumes inspired us to propose algorithms to estimate work engagement during the COVID-19 crisis. This paper also summarizes themes and topics of tweets in our dataset using both social media exclusive tools (i.e., #hashtags, @mentions) and the latent Dirichlet allocation model. We welcome requests for data sharing and conversations for more insights.  Dataset link: http://covid19research.site/geo-tagged_twitter_datasets/        △ Less","15 June, 2020",cs.SI,
              Femtomolar-level detection of SARS-CoV-2 spike proteins using toroidal plasmonic metasensors          ,2006.08536,https://arxiv.org/abs/2006.08536,https://arxiv.org/pdf/2006.08536,"Authors:ArashAhmadivand,BurakGerislioglu,ZeinabRamezani,AjeetKaushik,PandiarajManickam,S.AmirGhoreishi","        Effective and efficient management of human betacoronavirus severe acute respiratory syndrome (SARS)-CoV-2 infection i.e., COVID-19 pandemic, required sensitive sensors with short sample-to-result durations for performing diagnostics. In this direction, one of appropriate alternative approach to detect SARS-CoV-2 at low level (fmol) is exploring plasmonic metasensor technology for COVID-19 diagnostics, which offers exquisite opportunities in advanced healthcare programs, and modern clinical diagnostics. The intrinsic merits of plasmonic metasensors stem from their capability to squeeze electromagnetic fields, simultaneously in frequency, time, and space. However, the detection of low-molecular weight biomolecules at low densities is a typical drawback of conventional metasensors that has recently been addressed using toroidal metasurface technology. This research reports fabrication of a miniaturized plasmonic immunosensor based on toroidal electrodynamics concept that can sustain robustly confined plasmonic modes with ultranarrow lineshapes in the terahertz (THz) frequencies. By exciting toroidal dipole mode using our quasi-infinite metasurface and a judiciously optimized protocol based on functionalized gold nanoparticles (NPs) conjugated with the specific monoclonal antibody of SARS-CoV-2 onto the metasurface, the resonance shifts for diverse concentrations of the spike protein is monitored. Possessing molecular weight around ~76 kDa allowed us to detect the presence of spike protein with significantly low LoD ~4.2 fmol.        △ Less","15 June, 2020","physics.optics,physics.med-ph",
              An Unsupervised Machine Learning Approach to Assess the ZIP Code Level Impact of COVID-19 in NYC          ,2006.08361,https://arxiv.org/abs/2006.08361,https://arxiv.org/pdf/2006.08361,"Authors:FadouaKhmaissia,PegahSaghebHaghighi,AartheJayaprakash,ZhenweiWu,SokratisPapadopoulos,YuanLai,FreddyT.Nguyen","        New York City has been recognized as the world's epicenter of the novel Coronavirus pandemic. To identify the key inherent factors that are highly correlated to the Increase Rate of COVID-19 new cases in NYC, we propose an unsupervised machine learning framework. Based on the assumption that ZIP code areas with similar demographic, socioeconomic, and mobility patterns are likely to experience similar outbreaks, we select the most relevant features to perform a clustering that can best reflect the spread, and map them down to 9 interpretable categories. We believe that our findings can guide policy makers to promptly anticipate and prevent the spread of the virus by taking the right measures.        △ Less","18 September, 2020","cs.CY,stat.ML",
              The hidden side of COVID-19 spread in Italy          ,2006.08356,https://arxiv.org/abs/2006.08356,https://arxiv.org/pdf/2006.08356,"Authors:LuigiBrugnano,FeliceIavernaro,PaoloZanzottera","        Background. The paper concerns the SARS-CoV2 (COVID-19) pandemic that, starting from the end of February 2020, began spreading along the Italian peninsula, by first attacking small communities in north regions, and then extending to the center and south of Italy, including the two main islands.  Objective. The creation of a forecast model that manages to alert the decision-making bodies and, in particular, the healthcare system, to hinder the emergence of any other pandemic outbreaks, or the arrival of subsequent pandemic waves.  Methods. A new mathematical model to describe the pandemic is given. The model includes the class of undiagnosed infected people, and has a multi-region extension, to cope with the in-time and in-space heterogeneity of the epidemic.  Results. We obtain a robust and reliable tool for the forecast of the total and active cases, which can be also used to simulate different scenarios.  Conclusions. We are able to address a number of issues, such as assessing the adoption of the lockdown in Italy, started from 11 March 2020, and how to employ a rapid screening test campaign for containing the epidemic.        △ Less","11 June, 2020","q-bio.PE,physics.soc-ph",
              Using Reinforcement Learning to Allocate and Manage Service Function Chains in Cellular Networks          ,2006.07349,https://arxiv.org/abs/2006.07349,https://arxiv.org/pdf/2006.07349,"Authors:GutoLeoniSantos,PatriciaTakakoEndo","        It is expected that the next generation cellular networks provide a connected society with fully mobility to empower the socio-economic transformation. Several other technologies will benefits of this evolution, such as Internet of Things, smart cities, smart agriculture, vehicular networks, healthcare applications, and so on. Each of these scenarios presents specific requirements and demands different network configurations. To deal with this heterogeneity, virtualization technology is key technology. Indeed, the network function virtualization (NFV) paradigm provides flexibility for the network manager, allocating resources according to the demand, and reduces acquisition and operational costs. In addition, it is possible to specify an ordered set of network virtual functions (VNFs) for a given service, which is called as service function chain (SFC). However, besides the advantages from service virtualization, it is expected that network performance and availability do not be affected by its usage. In this paper, we propose the use of reinforcement learning to deploy a SFC of cellular network service and manage the VNFs operation. We consider that the SFC is deployed by the reinforcement learning agent considering a scenarios with distributed data centers, where the VNFs are deployed in virtual machines in commodity servers. The NFV management is related to create, delete, and restart the VNFs. The main purpose is to reduce the number of lost packets taking into account the energy consumption of the servers. We use the Proximal Policy Optimization (PPO2) algorithm to implement the agent and preliminary results show that the agent is able to allocate the SFC and manage the VNFs, reducing the number of lost packets.        △ Less","6 July, 2020","cs.NI,cs.LG,stat.ML",
              A Drone-based Networked System and Methods for Combating Coronavirus Disease (COVID-19) Pandemic          ,2006.06943,https://arxiv.org/abs/2006.06943,https://arxiv.org/pdf/2006.06943,"Authors:AdarshKumar,KritiSharma,HarvinderSingh,SagarGuptaNaugriya,SukhpalSinghGill,RajkumarBuyya","        Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus. It is similar to influenza viruses and raises concerns through alarming levels of spread and severity resulting in an ongoing pandemic worldwide. Within eight months (by August 2020), it infected 24.0 million persons worldwide and over 824 thousand have died. Drones or Unmanned Aerial Vehicles (UAVs) are very helpful in handling the COVID-19 pandemic. This work investigates the drone-based systems, COVID-19 pandemic situations, and proposes an architecture for handling pandemic situations in different scenarios using real-time and simulation-based scenarios. The proposed architecture uses wearable sensors to record the observations in Body Area Networks (BANs) in a push-pull data fetching mechanism. The proposed architecture is found to be useful in remote and highly congested pandemic areas where either the wireless or Internet connectivity is a major issue or chances of COVID-19 spreading are high. It collects and stores the substantial amount of data in a stipulated period and helps to take appropriate action as and when required. In real-time drone-based healthcare system implementation for COVID-19 operations, it is observed that a large area can be covered for sanitization, thermal image collection, and patient identification within a short period (2 KMs within 10 minutes approx.) through aerial route. In the simulation, the same statistics are observed with an addition of collision-resistant strategies working successfully for indoor and outdoor healthcare operations. Further, open challenges are identified and promising research directions are highlighted.        △ Less","31 August, 2020","cs.DC,eess.SP",
              Towards Early Diagnosis of Epilepsy from EEG Data          ,2006.06675,https://arxiv.org/abs/2006.06675,https://arxiv.org/pdf/2006.06675,"Authors:DiyuanLu,SebastianBauer,ValentinNeubert,LaraSophieCostard,FelixRosenow,JochenTriesch","        Epilepsy is one of the most common neurological disorders, affecting about 1% of the population at all ages. Detecting the development of epilepsy, i.e., epileptogenesis (EPG), before any seizures occur could allow for early interventions and potentially more effective treatments. Here, we investigate if modern machine learning (ML) techniques can detect EPG from intra-cranial electroencephalography (EEG) recordings prior to the occurrence of any seizures. For this we use a rodent model of epilepsy where EPG is triggered by electrical stimulation of the brain. We propose a ML framework for EPG identification, which combines a deep convolutional neural network (CNN) with a prediction aggregation method to obtain the final classification decision. Specifically, the neural network is trained to distinguish five second segments of EEG recordings taken from either the pre-stimulation period or the post-stimulation period. Due to the gradual development of epilepsy, there is enormous overlap of the EEG patterns before and after the stimulation. Hence, a prediction aggregation process is introduced, which pools predictions over a longer period. By aggregating predictions over one hour, our approach achieves an area under the curve (AUC) of 0.99 on the EPG detection task. This demonstrates the feasibility of EPG prediction from EEG recordings.        △ Less","17 June, 2020","cs.LG,eess.SP,stat.ML",
              Revolutionizing Future Healthcare using Wireless on the Walls (WoW)          ,2006.06479,https://arxiv.org/abs/2006.06479,https://arxiv.org/pdf/2006.06479,"Authors:JalilurRehmanKazim,TieJunCui,AhmedZoha,LianlinLi,SyedAzizShah,AkramAlomainy,MuhammadAliImran,QammerH.Abbasi","        Following the standardization and deployment of fifth generation (5G) network, researchers have shifted their focus to beyond 5G communication. Existing technologies have brought forth a plethora of applications that could not have been imagined in the past years. Beyond 5G will enable us to rethink the capability, it will offer in various sectors including agriculture, search and rescue and more specifically in the delivery of health care services. Unobtrusive and non-invasive measurements using radio frequency (RF) sensing, monitoring and control of wearable medical devices are the areas that would potentially benefit from beyond 5G. Applications such as RF sensing, device charging and remote patient monitoring will be a key challenge using millimetre (mmWave) communication. The mmWaves experience multi-path induced fading, where the rate of attenuation is larger as compared to the microwaves. Eventually, mmWave communication systems would require range extenders and guided surfaces. A proposed solution is the use of intelligent reflective surfaces, which will have the ability to manipulate electromagnetic (EM) signals. These intelligent surfaces mounted and/or coated on walls aka - Intelligent Walls are planar and active surfaces, which will be a key element in beyond 5G and 6G communication. These intelligent walls equipped with machine learning algorithm and computation power would have the ability to manipulate EM waves and act as gateways in the heterogeneous network environment. The article presents the application and vision of intelligent walls for next-generation healthcare in the era of beyond 5G.        △ Less","11 June, 2020",eess.SP,
              The Computational Patient has Diabetes and a COVID          ,2006.06435,https://arxiv.org/abs/2006.06435,https://arxiv.org/pdf/2006.06435,"Authors:PietroBarbiero,PietroLió","        Medicine is moving from a curative discipline to a preventative discipline relying on personalised and precise treatment plans. The complex and multi level pathophysiological patterns of most diseases require a systemic medicine approach and are challenging current medical therapies. On the other hand, computational medicine is a vibrant interdisciplinary field that could help move from an organ-centered approach to a process-oriented approach. The ideal computational patient would require an international interdisciplinary effort, of larger scientific and technological interdisciplinarity than the Human Genome Project. When deployed, such a patient would have a profound impact on how healthcare is delivered to patients. Here we present a computational patient model that integrates, refines and extends recent mechanistic or phenomenological models of cardiovascular, RAS and diabetic processes. Our aim is twofold: analyse the modularity and composability of the model-building blocks of the computational patient and to study the dynamical properties of well-being and disease states in a broader functional context. We present results from a number of experiments among which we characterise the dynamic impact of COVID-19 and type-2 diabetes (T2D) on cardiovascular and inflammation conditions. We tested these experiments under different exercise, meal and drug regimens. We report results showing the striking importance of transient dynamical responses to acute state conditions and we provide guidelines for system design principles for the inter-relationship between modules and components in systemic medicine. Finally this initial computational Patient can be used as a toolbox for further modifications and extensions.        △ Less","18 July, 2020","cs.CE,q-bio.QM",
              Privacy-Aware Activity Classification from First Person Office Videos          ,2006.06246,https://arxiv.org/abs/2006.06246,https://arxiv.org/pdf/2006.06246,"Authors:ParthoGhosh,Md.AbrarIstiak,NayeebRashid,AhsanHabibAkash,RidwanAbrar,AnkanGhoshDastider,AsifShahriyarSushmit,TaufiqHasan","        In the advent of wearable body-cameras, human activity classification from First-Person Videos (FPV) has become a topic of increasing importance for various applications, including in life-logging, law-enforcement, sports, workplace, and healthcare. One of the challenging aspects of FPV is its exposure to potentially sensitive objects within the user's field of view. In this work, we developed a privacy-aware activity classification system focusing on office videos. We utilized a Mask-RCNN with an Inception-ResNet hybrid as a feature extractor for detecting, and then blurring out sensitive objects (e.g., digital screens, human face, paper) from the videos. For activity classification, we incorporate an ensemble of Recurrent Neural Networks (RNNs) with ResNet, ResNext, and DenseNet based feature extractors. The proposed system was trained and evaluated on the FPV office video dataset that includes 18-classes made available through the IEEE Video and Image Processing (VIP) Cup 2019 competition. On the original unprotected FPVs, the proposed activity classifier ensemble reached an accuracy of 85.078% with precision, recall, and F1 scores of 0.88, 0.85 & 0.86, respectively. On privacy protected videos, the performances were slightly degraded, with accuracy, precision, recall, and F1 scores at 73.68%, 0.79, 0.75, and 0.74, respectively. The presented system won the 3rd prize in the IEEE VIP Cup 2019 competition.        △ Less","11 June, 2020",cs.CV,
              Balancing Fairness and Efficiency in an Optimization Model          ,2006.05963,https://arxiv.org/abs/2006.05963,https://arxiv.org/pdf/2006.05963,"Authors:VioletXinyingChen,J.N.Hooker","        Optimization models generally aim for efficiency by maximizing total benefit or minimizing cost. Yet a trade-off between fairness and efficiency is an important element of many practical decisions. We propose a principled and practical method for balancing these two criteria in an optimization model. Following a critical assessment of existing schemes, we define a set of social welfare functions (SWFs) that combine Rawlsian leximax fairness and utilitarianism and overcome some of the weaknesses of previous approaches. In particular, we regulate the equity/efficiency trade-off with a single parameter that has a meaningful interpretation in practical contexts. We formulate the SWFs using mixed integer constraints and sequentially maximize them subject to constraints that define the problem at hand. After providing practical step-by-step instructions for implementation, we demonstrate the method on problems of realistic size involving healthcare resource allocation and disaster preparation. The solution times are modest, ranging from a fraction of a second to 18 seconds for a given value of the trade-off parameter.        △ Less","10 June, 2020","math.OC,cs.AI",
              Tracking the Twitter attention around the research efforts on the COVID-19 pandemic          ,2006.05783,https://arxiv.org/abs/2006.05783,https://arxiv.org/pdf/2006.05783,"Authors:ZhichaoFang,RodrigoCostas","        The outbreak of the COVID-19 pandemic has been accompanied by a bulk of scientific research and related Twitter discussions. To unravel the public concerns about the COVID-19 crisis reflected in the science-based Twitter conversations, this study tracked the Twitter attention around the COVID-19 research efforts during the first three months of 2020. On the basis of nearly 1.4 million Twitter mentions of 6,162 COVID-19-related scientific publications, we investigated the temporal tweeting dynamic and the Twitter users involved in the online discussions around COVID-19-related research. The results show that the quantity of Twitter mentions of COVID-19-related publications was on rising. Scholarly-oriented Twitter users played an influential role in disseminating research outputs on COVID-19, with their tweets being frequently retweeted. Over time, a change in the focus of the Twitter discussions can be observed, from the initial attention to virological and clinical research to more practical topics, such as the potential treatments, the countermeasures by the governments, the healthcare measures, and the influences on the economy and society, in more recent times.        △ Less","10 June, 2020","cs.DL,cs.SI",
"              Applying Deep-Learning-Based Computer Vision to Wireless Communications: Methodologies, Opportunities, and Challenges          ",2006.05782,https://arxiv.org/abs/2006.05782,https://arxiv.org/pdf/2006.05782,"Authors:YuTian,GaofengPan,Mohamed-SlimAlouini","        Deep learning (DL) has seen great success in the computer vision (CV) field, and related techniques have been used in security, healthcare, remote sensing, and many other fields. As a parallel development, visual data has become universal in daily life, easily generated by ubiquitous low-cost cameras. Therefore, exploring DL-based CV may yield useful information about objects, such as their number, locations, distribution, motion, etc. Intuitively, DL-based CV can also facilitate and improve the designs of wireless communications, especially in dynamic network scenarios. However, so far, such work is rare in the literature. The primary purpose of this article, then, is to introduce ideas about applying DL-based CV in wireless communications to bring some novel degrees of freedom to both theoretical research and engineering applications. To illustrate how DL-based CV can be applied in wireless communications, an example of using a DL-based CV with a millimeter-wave (mmWave) system is given to realize optimal mmWave multiple-input and multiple-output (MIMO) beamforming in mobile scenarios. In this example, we propose a framework to predict future beam indices from previously observed beam indices and images of street views using ResNet, 3-dimensional ResNext, and a long short-term memory network. The experimental results show that our frameworks achieve much higher accuracy than the baseline method, and that visual data can significantly improve the performance of the MIMO beamforming system. Finally, we discuss the opportunities and challenges of applying DL-based CV in wireless communications.        △ Less","16 July, 2020","eess.SP,cs.CV,cs.LG",
              A Machine Learning Early Warning System: Multicenter Validation in Brazilian Hospitals          ,2006.05514,https://arxiv.org/abs/2006.05514,https://arxiv.org/pdf/2006.05514,"Authors:JhonatanKobylarz,HenriqueD.P.dosSantos,FelipeBarletta,MateusCichelerodaSilva,RenataVieira,HugoM.P.Morales,CristiandaCostaRocha","        Early recognition of clinical deterioration is one of the main steps for reducing inpatient morbidity and mortality. The challenging task of clinical deterioration identification in hospitals lies in the intense daily routines of healthcare practitioners, in the unconnected patient data stored in the Electronic Health Records (EHRs) and in the usage of low accuracy scores. Since hospital wards are given less attention compared to the Intensive Care Unit, ICU, we hypothesized that when a platform is connected to a stream of EHR, there would be a drastic improvement in dangerous situations awareness and could thus assist the healthcare team. With the application of machine learning, the system is capable to consider all patient's history and through the use of high-performing predictive models, an intelligent early warning system is enabled. In this work we used 121,089 medical encounters from six different hospitals and 7,540,389 data points, and we compared popular ward protocols with six different scalable machine learning methods (three are classic machine learning models, logistic and probabilistic-based models, and three gradient boosted models). The results showed an advantage in AUC (Area Under the Receiver Operating Characteristic Curve) of 25 percentage points in the best Machine Learning model result compared to the current state-of-the-art protocols. This is shown by the generalization of the algorithm with leave-one-group-out (AUC of 0.949) and the robustness through cross-validation (AUC of 0.961). We also perform experiments to compare several window sizes to justify the use of five patient timestamps. A sample dataset, experiments, and code are available for replicability purposes.        △ Less","9 June, 2020","cs.LG,stat.ML",
              Can artificial intelligence (AI) be used to accurately detect tuberculosis (TB) from chest x-ray? A multiplatform evaluation of five AI products used for TB screening in a high TB-burden setting          ,2006.05509,https://arxiv.org/abs/2006.05509,https://arxiv.org/pdf/2006.05509,"Authors:ZhiZhenQin,ShahriarAhmed,MohammadShahnewazSarker,KishorPaul,AhammadShafiqSikderAdel,TasneemNaheyan,SayeraBanu,JacobCreswell","        Powered by artificial intelligence (AI), particularly deep neural networks, computer aided detection (CAD) tools can be trained to recognize TB-related abnormalities on chest radiographs, thereby screening large numbers of people and reducing the pressure on healthcare professionals. Addressing the lack of studies comparing the performance of different products, we evaluated five AI software platforms specific to TB: CAD4TB (v6), InferReadDR (v2), Lunit INSIGHT for Chest Radiography (v4.9.0) , JF CXR-1 (v2) by and qXR (v3) by on an unseen dataset of chest X-rays collected in three TB screening center in Dhaka, Bangladesh. The 23,566 individuals included in the study all received a CXR read by a group of three Bangladeshi board-certified radiologists. A sample of CXRs were re-read by US board-certified radiologists. Xpert was used as the reference standard. All five AI platforms significantly outperformed the human readers. The areas under the receiver operating characteristic curves are qXR: 0.91 (95% CI:0.90-0.91), Lunit INSIGHT CXR: 0.89 (95% CI:0.88-0.89), InferReadDR: 0.85 (95% CI:0.84-0.86), JF CXR-1: 0.85 (95% CI:0.84-0.85), CAD4TB: 0.82 (95% CI:0.81-0.83). We also proposed a new analytical framework that evaluates a screening and triage test and informs threshold selection through tradeoff between cost efficiency and ability to triage. Further, we assessed the performance of the five AI algorithms across the subgroups of age, use cases, and prior TB history, and found that the threshold scores performed differently across different subgroups. The positive results of our evaluation indicate that these AI products can be useful screening and triage tools for active case finding in high TB-burden regions.        △ Less","9 June, 2020","eess.IV,cs.CV,cs.LG,q-bio.QM",
"              Blockchain in the management of science: conceptual models, promises and challenges          ",2006.05483,https://arxiv.org/abs/2006.05483,https://arxiv.org/pdf/2006.05483,Authors:ArtyomKosmarski,"        Blockchain has received much attention recently, due to its promises of verifiable, permanent, decentralized, and efficient data handling. In 2017-2019 blockchain and associated technologies such as smart contracts has progressed beyond cryptocurrencies, and has been adopted in banking, retail, healthcare, and other fields. This study critically examines recent applications of blockchain in science, touching upon different stages of research cycle, from data management to publishing, peer review, research evaluation and funding. The paper is based upon a review of blockchain projects, relevant literature, a set of interviews and focus groups with startup founders, scholars, librarians, IT experts from the EU, USA, Russia, and Belarus. Proponents of blockchain for science present this technology as a tool to make science free from bias, red tape, data fraud, as well as provide innovative means to secure financial backing for new ideas. However, these projects face a set of challenges. One issue concerns introducing crypto economy, with its financial incentives, into science, a field that emphasizes disinterested and non-pecuniary pursuit of truth. Another source of concern relates to the ongoing conflict between the principle of decentralization inherent to blockchain and the practice of forcing it from above, by the state and other centralized entities.        △ Less","9 June, 2020",cs.CY,
              Cost-effective Interactive Attention Learning with Neural Attention Processes          ,2006.05419,https://arxiv.org/abs/2006.05419,https://arxiv.org/pdf/2006.05419,"Authors:JayHeo,JunhyeonPark,HyewonJeong,KwangJoonKim,JuhoLee,EunhoYang,SungJuHwang","        We propose a novel interactive learning framework which we refer to as Interactive Attention Learning (IAL), in which the human supervisors interactively manipulate the allocated attentions, to correct the model's behavior by updating the attention-generating network. However, such a model is prone to overfitting due to scarcity of human annotations, and requires costly retraining. Moreover, it is almost infeasible for the human annotators to examine attentions on tons of instances and features. We tackle these challenges by proposing a sample-efficient attention mechanism and a cost-effective reranking algorithm for instances and features. First, we propose Neural Attention Process (NAP), which is an attention generator that can update its behavior by incorporating new attention-level supervisions without any retraining. Secondly, we propose an algorithm which prioritizes the instances and the features by their negative impacts, such that the model can yield large improvements with minimal human feedback. We validate IAL on various time-series datasets from multiple domains (healthcare, real-estate, and computer vision) on which it significantly outperforms baselines with conventional attention mechanisms, or without cost-effective reranking, with substantially less retraining and human-model interaction cost.        △ Less","9 June, 2020","cs.LG,cs.HC,stat.ML",
              Serverless on FHIR: Deploying machine learning models for healthcare on the cloud          ,2006.04748,https://arxiv.org/abs/2006.04748,https://arxiv.org/pdf/2006.04748,"Authors:BellRajEapen,KamranSartipi,NormArcher","        Machine Learning (ML) plays a vital role in implementing digital health. The advances in hardware and the democratization of software tools have revolutionized machine learning. However, the deployment of ML models -- the mathematical representation of the task to be performed -- for effective and efficient clinical decision support at the point of care is still a challenge. ML models undergo constant improvement of their accuracy and predictive power with a high turnover rate. Updating models consumed by downstream health information systems is essential for patient safety. We introduce a functional taxonomy and a four-tier architecture for cloud-based model deployment for digital health. The four tiers are containerized microservices for maintainability, serverless architecture for scalability, function as a service for portability and FHIR schema for discoverability. We call this architecture Serverless on FHIR and propose this as a standard to deploy digital health applications that can be consumed by downstream systems such as EMRs and visualization tools.        △ Less","8 June, 2020","cs.CY,cs.LG,q-bio.QM",
              Machine Learning Interpretability and Its Impact on Smart Campus Projects          ,2006.04300,https://arxiv.org/abs/2006.04300,https://arxiv.org/pdf/2006.04300,"Authors:RaghadZenki,MuMu","        Machine learning (ML) has shown increasing abilities for predictive analytics over the last decades. It is becoming ubiquitous in different fields, such as healthcare, criminal justice, finance and smart city. For instance, the University of Northampton is building a smart system with multiple layers of IoT and software-defined networks (SDN) on its new Waterside Campus. The system can be used to optimize smart buildings energy efficiency, improve the health and safety of its tenants and visitors, assist crowd management and way-finding, and improve the Internet connectivity.        △ Less","7 June, 2020","cs.CY,cs.LG",
              Multi-Task Temporal Shift Attention Networks for On-Device Contactless Vitals Measurement          ,2006.03790,https://arxiv.org/abs/2006.03790,https://arxiv.org/pdf/2006.03790,"Authors:XinLiu,JoshFromm,ShwetakPatel,DanielMcDuff","        Telehealth and remote health monitoring have become increasingly important during the SARS-CoV-2 pandemic and it is widely expected that this will have a lasting impact on healthcare practices. These tools can help reduce the risk of exposing patients and medical staff to infection, make healthcare services more accessible, and allow providers to see more patients. However, objective measurement of vital signs is challenging without direct contact with a patient. We present a video-based and on-device optical cardiopulmonary vital sign measurement approach. It leverages a novel multi-task temporal shift convolutional attention network (MTTS-CAN) and enables real-time cardiovascular and respiratory measurements on mobile platforms. We evaluate our system on an ARM CPU and achieve state-of-the-art accuracy while running at over 150 frames per second which enables real-time applications. Systematic experimentation on large benchmark datasets reveals that our approach leads to substantial (20%-50%) reductions in error and generalizes well across datasets.        △ Less","6 June, 2020","eess.SP,cs.CV,eess.IV",
              Artificial Intelligence-based Clinical Decision Support for COVID-19 -- Where Art Thou?          ,2006.03434,https://arxiv.org/abs/2006.03434,https://arxiv.org/pdf/2006.03434,"Authors:MathiasUnberath,KimiaGhobadi,ScottLevin,JeremiahHinson,GregoryDHager","        The COVID-19 crisis has brought about new clinical questions, new workflows, and accelerated distributed healthcare needs. While artificial intelligence (AI)-based clinical decision support seemed to have matured, the application of AI-based tools for COVID-19 has been limited to date. In this perspective piece, we identify opportunities and requirements for AI-based clinical decision support systems and highlight challenges that impact ""AI readiness"" for rapidly emergent healthcare challenges.        △ Less","5 June, 2020","cs.CY,cs.AI,cs.CV",
              Short-Run Health Consequences of Retirement and Pension Benefits: Evidence from China          ,2006.02900,https://arxiv.org/abs/2006.02900,https://arxiv.org/pdf/2006.02900,"Authors:PlamenNikolov,AlanAdelman","        This paper examines the impact of the New Rural Pension Scheme (NRPS) in China. Exploiting the staggered implementation of an NRPS policy expansion that began in 2009, we use a difference-in-difference approach to study the effects of the introduction of pension benefits on the health status, health behaviors, and healthcare utilization of rural Chinese adults age 60 and above. The results point to three main conclusions. First, in addition to improvements in self-reported health, older adults with access to the pension program experienced significant improvements in several important measures of health, including mobility, self-care, usual activities, and vision. Second, regarding the functional domains of mobility and self-care, we found that the females in the study group led in improvements over their male counterparts. Third, in our search for the mechanisms that drive positive retirement program results, we find evidence that changes in individual health behaviors, such as a reduction in drinking and smoking, and improved sleep habits, play an important role. Our findings point to the potential benefits of retirement programs resulting from social spillover effects. In addition, these programs may lessen the morbidity burden among the retired population.        △ Less","2 June, 2020",econ.GN,10.1515/fhep-2017-0031 
              Efficient refinements on YOLOv3 for real-time detection and assessment of diabetic foot Wagner grades          ,2006.02322,https://arxiv.org/abs/2006.02322,https://arxiv.org/pdf/2006.02322,"Authors:AifuHan,YongzeZhang,AjuanLi,ChangjinLi,FengyingZhao,QiujieDong,QinLiu,YantingLiu,XimeiShen,SunjieYan,ShengzongZhou","        Currently, the screening of Wagner grades of diabetic feet (DF) still relies on professional podiatrists. However, in less-developed countries, podiatrists are scarce, which led to the majority of undiagnosed patients. In this study, we proposed the real-time detection and location method for Wagner grades of DF based on refinements on YOLOv3. We collected 2,688 data samples and implemented several methods, such as a visual coherent image mixup, label smoothing, and training scheduler revamping, based on the ablation study. The experimental results suggested that the refinements on YOLOv3 achieved an accuracy of 91.95% and the inference speed of a single picture reaches 31ms with the NVIDIA Tesla V100. To test the performance of the model on a smartphone, we deployed the refinements on YOLOv3 models on an Android 9 system smartphone. This work has the potential to lead to a paradigm shift for clinical treatment of the DF in the future, to provide an effective healthcare solution for DF tissue analysis and healing status.        △ Less","3 June, 2020",cs.CV,
"              Influence of Absolute Humidity, Temperature and Population Density on COVID-19 Spread and Decay Durations: Multi-prefecture Study in Japan          ",2006.02197,https://arxiv.org/abs/2006.02197,https://arxiv.org/pdf/2006.02197,"Authors:EssamA.Rashed,SachikoKodera,JoseGomez-Tames,AkimasaHirata","        This study analyzed the spread and decay durations of the COVID-19 pandemic in different prefectures of Japan. During the pandemic, affordable healthcare was widely available in Japan and the medical system did not suffer a collapse, making accurate comparisons between prefectures possible. For the 16 prefectures included in this study that had daily maximum confirmed cases exceeding ten, the number of daily confirmed cases follow bell-shape or log-normal distribution in most prefectures. A good correlation was observed between the spread and decay durations. However, some exceptions were observed in areas where travelers returned from foreign countries, which were defined as the origins of infection clusters. Excluding these prefectures, the population density was shown to be a major factor affecting the spread and decay patterns, with R2=0.39 (p<0.05) and 0.42 (p<0.05), respectively, approximately corresponding to social distancing. The maximum absolute humidity was found to affect the decay duration normalized by the population density (R2>0.36, p <0.05). Our findings indicate that the estimated pandemic spread duration, based on the multivariate analysis of maximum absolute humidity, ambient temperature, and population density (adjusted R2=0.53, p-value<0.05), could prove useful for intervention planning during potential future pandemics, including a second COVID-19 outbreak.        △ Less","21 July, 2020","q-bio.PE,cs.CY",10.3390/ijerph17155354 
              Estimating horizontal movement performance of patient beds and the impact on emergency evacuation time          ,2006.02169,https://arxiv.org/abs/2006.02169,https://arxiv.org/pdf/2006.02169,"Authors:JaeyoungKwak,MichaelH.Lees,WentongCai,AhmadRezaPourghaderi,MarcusE.H.Ong","        Emergency evacuation of patients from a hospital can be challenging in the event of a fire. Most emergency evacuation studies are based on the assumption that pedestrians are ambulant and can egress by themselves. However, this is often not the case during emergency evacuations in healthcare facilities such as hospitals and nursing homes. To investigate emergency evacuations in such healthcare facilities, we performed a series of controlled experiments to study the dynamics of patient beds in horizontal movement. We considered a patient bed because it is one of the commonly used devices to transport patients within healthcare facilities. Through a series of controlled experiments, we examined the change of velocity in corner turning movements and speed reductions in multiple trips between both ends of a straight corridor. Based on the experimental results, we then developed a mathematical model of total evacuation time prediction for a patient bed horizontally moving in a healthcare facility. Factoring uncertainty in the horizontal movement, we produced the probability distribution of movement duration and estimated the probability that an evacuation can be safely performed within certain amount of time. In addition, we predicted that the evacuation time would be longer than the prediction results from an existing model which assumes constant movement speed. Our results from the model demonstrated good agreement with our experimental results.        △ Less","28 July, 2020",physics.soc-ph,
              Modeling COVID-19 dynamics in Illinois under non-pharmaceutical interventions          ,2006.02036,https://arxiv.org/abs/2006.02036,https://arxiv.org/pdf/2006.02036,"Authors:GeorgeN.Wong,ZacharyJ.Weiner,AlexeiV.Tkachenko,AhmedElbanna,SergeiMaslov,NigelGoldenfeld","        We present modeling of the COVID-19 epidemic in Illinois, USA, capturing the implementation of a Stay-at-Home order and scenarios for its eventual release. We use a non-Markovian age-of-infection model that is capable of handling long and variable time delays without changing its model topology. Bayesian estimation of model parameters is carried out using Markov Chain Monte Carlo (MCMC) methods. This framework allows us to treat all available input information, including both the previously published parameters of the epidemic and available local data, in a uniform manner. To accurately model deaths as well as demand on the healthcare system, we calibrate our predictions to total and in-hospital deaths as well as hospital and ICU bed occupancy by COVID-19 patients. We apply this model not only to the state as a whole but also its sub-regions in order to account for the wide disparities in population size and density. Without prior information on non-pharmaceutical interventions (NPIs), the model independently reproduces a mitigation trend closely matching mobility data reported by Google and Unacast. Forward predictions of the model provide robust estimates of the peak position and severity and also enable forecasting the regional-dependent results of releasing Stay-at-Home orders. The resulting highly constrained narrative of the epidemic is able to provide estimates of its unseen progression and inform scenarios for sustainable monitoring and control of the epidemic.        △ Less","15 June, 2020","q-bio.PE,physics.med-ph,physics.soc-ph",
              Securing Your Collaborative Jupyter Notebooks in the Cloud using Container and Load Balancing Services          ,2006.01818,https://arxiv.org/abs/2006.01818,https://arxiv.org/pdf/2006.01818,"Authors:Haw-minnLu,AdrianKwong,JoseUnpingco","        Jupyter has become the go-to platform for developing data applications but data and security concerns, especially when dealing with healthcare, have become paramount for many institutions and applications dealing with sensitive information. How then can we continue to enjoy the data analysis and machine learning opportunities provided by Jupyter and the Python ecosystem while guaranteeing auditable compliance with security and privacy concerns? We will describe the architecture and implementation of a cloud based platform based on Jupyter that integrates with Amazon Web Services (AWS) and uses containerized services without exposing the platform to the vulnerabilities present in Kubernetes and JupyterHub. This architecture addresses the HIPAA requirements to ensure both security and privacy of data. The architecture uses an AWS service to provide JSON Web Tokens (JWT) for authentication as well as network control. Furthermore, our architecture enables secure collaboration and sharing of Jupyter notebooks. Even though our platform is focused on Jupyter notebooks and JupyterLab, it also supports R-Studio and bespoke applications that share the same authentication mechanisms. Further, the platform can be extended to other cloud services other than AWS.        △ Less","2 June, 2020","cs.CR,cs.NI",
"              Psychiatric Home Treatment for Inpatient Care -- Design, Implementation and Participation          ",2006.01523,https://arxiv.org/abs/2006.01523,https://arxiv.org/pdf/2006.01523,"Authors:StefanHochwarter,PierreTangermann,MartinHeinze,JulianSchwarz","        The use of information and communication technologies (ICT) to support long-term care is gaining attention, also in the light of population ageing. Known in Scandinavian countries under the term of welfare technology, it aims to increase the quality of life and independence of people with physical, psychological or social impairments. In Germany, a new form of psychiatric home treatment, inpatient equivalent treatment (IET), is offered since 2018. It should allow service users with severe mental health issues to stay in their familiar environment during crisis, while being treated in the same complexity and flexibility like in an inpatient unit. However, this change in delivering healthcare services leads to sociotechnical challenges, such as coordination of work, integration into existing healthcare workflows and ensuring continuity of care. Hence, the objective of this exploratory study is to examine how information and communication technologies (ICT) interact in the new setting and how this process can be improved. Further, we also ask how service users can participate in designing home treatment services. Methodologically, this study follows a qualitative research approach. Different methods including participant observation, interviews and focus groups were conducted to answer the research questions. Data was collected during a field visit at the psychiatric department of a German clinic in summer 2019. Field notes and interviews were analyzed using the R package for qualitative data analysis RQDA. A list of socio-technical challenges and opportunities related to IET were identified. New forms of communication, gaps in documentation practices and continuity of care are seen to be highly relevant for designing and implementing home treatment services in psychiatric care. We also discuss how service users and health professionals can take pro-active part in designing these services.        △ Less","2 June, 2020",cs.CY,
              CT-based COVID-19 Triage: Deep Multitask Learning Improves Joint Identification and Severity Quantification          ,2006.01441,https://arxiv.org/abs/2006.01441,https://arxiv.org/pdf/2006.01441,"Authors:MikhailGoncharov,MaximPisov,AlexeyShevtsov,BorisShirokikh,AnvarKurmukov,IvanBlokhin,ValeriaChernina,AlexanderSolovev,VictorGombolevskiy,SergeyMorozov,MikhailBelyaev","        The current COVID-19 pandemic overloads healthcare systems, including radiology departments. Though several deep learning approaches were developed to assist in CT analysis, nobody considered study triage directly as a computer science problem. We describe two basic setups: Identification of COVID-19 to prioritize studies of potentially infected patients to isolate them as early as possible; Severity quantification to highlight studies of severe patients and direct them to a hospital or provide emergency medical care. We formalize these tasks as binary classification and estimation of affected lung percentage. Though similar problems were well-studied separately, we show that existing methods provide reasonable quality only for one of these setups. To consolidate both triage approaches, we employ a multitask learning and propose a convolutional neural network to combine all available labels within a single model. We train our model on approximately 2000 publicly available CT studies and test it with a carefully designed set consisting of 33 COVID patients, 32 healthy patients, and 36 patients with other lung pathologies to emulate a typical patient flow in an out-patient hospital. The developed model achieved 0.951 ROC AUC for Identification of COVID-19 and 0.98 Spearman Correlation for Severity quantification. We release all the code and create a public leaderboard, where other community members can test their models on our dataset.        △ Less","2 June, 2020","eess.IV,cs.CV",
              DeepCoDA: personalized interpretability for compositional health data          ,2006.01392,https://arxiv.org/abs/2006.01392,https://arxiv.org/pdf/2006.01392,"Authors:ThomasP.Quinn,DangNguyen,SantuRana,SunilGupta,SvethaVenkatesh","        Interpretability allows the domain-expert to directly evaluate the model's relevance and reliability, a practice that offers assurance and builds trust. In the healthcare setting, interpretable models should implicate relevant biological mechanisms independent of technical factors like data pre-processing. We define personalized interpretability as a measure of sample-specific feature attribution, and view it as a minimum requirement for a precision health model to justify its conclusions. Some health data, especially those generated by high-throughput sequencing experiments, have nuances that compromise precision health models and their interpretation. These data are compositional, meaning that each feature is conditionally dependent on all other features. We propose the Deep Compositional Data Analysis (DeepCoDA) framework to extend precision health modelling to high-dimensional compositional data, and to provide personalized interpretability through patient-specific weights. Our architecture maintains state-of-the-art performance across 25 real-world data sets, all while producing interpretations that are both personalized and fully coherent for compositional data.        △ Less","16 June, 2020","cs.LG,cs.AI,stat.ML",
              CoAID: COVID-19 Healthcare Misinformation Dataset          ,2006.00885,https://arxiv.org/abs/2006.00885,https://arxiv.org/pdf/2006.00885,"Authors:LimengCui,DongwonLee","        As the COVID-19 virus quickly spreads around the world, unfortunately, misinformation related to COVID-19 also gets created and spreads like wild fire. Such misinformation has caused confusion among people, disruptions in society, and even deadly consequences in health problems. To be able to understand, detect, and mitigate such COVID-19 misinformation, therefore, has not only deep intellectual values but also huge societal impacts. To help researchers combat COVID-19 health misinformation, therefore, we present CoAID (Covid-19 heAlthcare mIsinformation Dataset), with diverse COVID-19 healthcare misinformation, including fake news on websites and social platforms, along with users' social engagement about such news. CoAID includes 3,235 news, 294,692 related user engagements, 851 social platform posts about COVID-19, and ground truth labels. The dataset is available at: https://github.com/cuilimeng/CoAID.        △ Less","26 August, 2020","cs.SI,cs.CL",
              Automated Delineation of Hospital Service Areas and Hospital Referral Regions by Modularity Optimization          ,2006.00275,https://arxiv.org/abs/2006.00275,https://arxiv.org/pdf/2006.00275,"Authors:YujieHu,FahuiWang,ImamXierali","        Objective. To develop an automated, data-driven, and scale-flexible method to delineate HSAs and HRRs that are up-to-date, representative of all patients, and have the optimal localization of hospital visits. Data Sources. The 2011 State Inpatient Database (SID) in Florida from the Healthcare Cost and Utilization Project (HCUP). Study Design. A network optimization method was used to redefine HSAs and HRRs by maximizing patient-to-hospital flows within each HSA/HRR while minimizing flows between them. We first constructed as many HSAs/HRRs as existing Dartmouth units in Florida, and then compared the two by various metrics. Next, we sought to derive the optimal numbers and configurations of HSAs/HRRs that best reflect the modularity of hospitalization patterns in Florida. Principal Findings. The HSAs/HRRs by our method are favored over the Dartmouth units in balance of region size and market structure, shape, and most importantly, local hospitalization. Conclusions. The new method is automated, scale-flexible, and effective in capturing the natural structure of healthcare system. It has great potential for applications in delineating other healthcare service areas or in larger geographic regions.        △ Less","30 May, 2020","stat.AP,cs.CY,physics.soc-ph",10.1111/1475-6773.12616 
              Global Short-Term Forecasting of Covid-19 Cases          ,2006.00111,https://arxiv.org/abs/2006.00111,https://arxiv.org/pdf/2006.00111,"Authors:ThiagodePaulaOliveira,RafaeldeAndradeMoral","        The continuously growing number of COVID-19 cases pressures healthcare services worldwide. Accurate short-term forecasting is thus vital to support country-level policy making. The strategies adopted by countries to combat the pandemic vary, generating different uncertainty levels about the actual number of cases. Accounting for the hierarchical structure of the data and accommodating extra-variability is therefore fundamental. We introduce a new modelling framework to describe the course of the pandemic with great accuracy, and provide short-term daily forecasts for every country in the world. We show that our model generates highly accurate forecasts up to six days ahead, and use estimated model components to cluster countries based on recent events. We introduce statistical novelty in terms of modelling the autoregressive parameter as a function of time, increasing predictive power and flexibility to adapt to each country. Our model can also be used to forecast the number of deaths, study the effects of covariates (such as lockdown policies), and generate forecasts for smaller regions within countries. Consequently, it has strong implications for global planning and decision making. We constantly update forecasts and make all results freely available to any country in the world through an online Shiny dashboard.        △ Less","29 May, 2020","stat.ME,stat.AP",
              Synthesizing lesions using contextual GANs improves breast cancer classification on mammograms          ,2006.00086,https://arxiv.org/abs/2006.00086,https://arxiv.org/pdf/2006.00086,"Authors:EricWu,KevinWu,WilliamLotter","        Data scarcity and class imbalance are two fundamental challenges in many machine learning applications to healthcare. Breast cancer classification in mammography exemplifies these challenges, with a malignancy rate of around 0.5% in a screening population, which is compounded by the relatively small size of lesions (~1% of the image) in malignant cases. Simultaneously, the prevalence of screening mammography creates a potential abundance of non-cancer exams to use for training. Altogether, these characteristics lead to overfitting on cancer cases, while under-utilizing non-cancer data. Here, we present a novel generative adversarial network (GAN) model for data augmentation that can realistically synthesize and remove lesions on mammograms. With self-attention and semi-supervised learning components, the U-net-based architecture can generate high resolution (256x256px) outputs, as necessary for mammography. When augmenting the original training set with the GAN-generated samples, we find a significant improvement in malignancy classification performance on a test set of real mammogram patches. Overall, the empirical results of our algorithm and the relevance to other medical imaging paradigms point to potentially fruitful further applications.        △ Less","29 May, 2020","eess.IV,cs.CV",
              Quasi-orthonormal Encoding for Machine Learning Applications          ,2006.00038,https://arxiv.org/abs/2006.00038,https://arxiv.org/pdf/2006.00038,Authors:Haw-minnLu,"        Most machine learning models, especially artificial neural networks, require numerical, not categorical data. We briefly describe the advantages and disadvantages of common encoding schemes. For example, one-hot encoding is commonly used for attributes with a few unrelated categories and word embeddings for attributes with many related categories (e.g., words). Neither is suitable for encoding attributes with many unrelated categories, such as diagnosis codes in healthcare applications. Application of one-hot encoding for diagnosis codes, for example, can result in extremely high dimensionality with low sample size problems or artificially induce machine learning artifacts, not to mention the explosion of computing resources needed. Quasi-orthonormal encoding (QOE) fills the gap. We briefly show how QOE compares to one-hot encoding. We provide example code of how to implement QOE using popular ML libraries such as Tensorflow and PyTorch and a demonstration of QOE to MNIST handwriting samples.        △ Less","29 May, 2020","cs.LG,stat.ML",
              Efficient Discrete Feature Encoding for Variational Quantum Classifier          ,2005.14382,https://arxiv.org/abs/2005.14382,https://arxiv.org/pdf/2005.14382,"Authors:HiroshiYano,YudaiSuzuki,RudyRaymond,NaokiYamamoto","        Recent days have witnessed significant interests in applying quantum-enhanced techniques for solving machine learning tasks in, e.g., classification, regression, and recommender systems. Variational methods that use quantum resources of imperfect quantum devices with the help of classical computing techniques are popular for supervised learning. Variational Quantum Classification (VQC) is one of such variational methods with possible quantum advantage in using quantum-enhanced features that are hard to compute by classical methods. Its performance depends on the mapping of classical features into quantum-enhanced feature space. Although there have been many quantum-mapping functions proposed so far, there is little discussion on efficient mapping of discrete features, such as, race, gender, marriage status and others that are often significant for classifying datasets of interest. We first introduce the use of Quantum Random Access Coding (QRAC) to map such discrete features efficiently into limited number of qubits for VQC. We numerically show that QRAC can help speeding up the training of VQC by reducing its parameters via reduction on the number of qubits for the mapping. We confirm the effectiveness of the QRAC in VQC by experimenting on classification of healthcare datasets with both simulators and real quantum devices.        △ Less","29 May, 2020",quant-ph,
              Scaling Participation -- What Does the Concept of Managed Communities Offer for Participatory Design?          ,2005.14045,https://arxiv.org/abs/2005.14045,https://arxiv.org/pdf/2005.14045,"Authors:StefanHochwarter,BabakA.Farshchian","        This paper investigates mechanisms for scaling participation in participatory design (PD). Specifically, the paper focuses on managed communities, one strategy of generification work. We first give a brief introduction on the issue of scaling in PD, followed by exploring the strategy of managed communities in PD. This exploration is underlined by an ongoing case study in the healthcare sector, and we propose solutions to observed challenges. The paper ends with a critical reflection on the possibilities managed communities offer for PD. Managed communities have much to offer beyond mere generification work for large-scale information systems, but we need to pay attention to core PD values that are in danger of being sidelined in the process.        △ Less","28 May, 2020",cs.CY,10.1145/3384772.3385143 
              CLOCS: Contrastive Learning of Cardiac Signals          ,2005.13249,https://arxiv.org/abs/2005.13249,https://arxiv.org/pdf/2005.13249,"Authors:DaniKiyasseh,TingtingZhu,DavidA.Clifton","        The healthcare industry generates troves of unlabelled physiological data. This data can be exploited via contrastive learning, a self-supervised pre-training mechanism that encourages representations of instances to be similar to one another. We propose a family of contrastive learning methods, CLOCS, that encourages representations across time, leads, and patients to be similar to one another. We show that CLOCS consistently outperforms the state-of-the-art approach, SimCLR, on both linear evaluation and fine-tuning downstream tasks. We also show that CLOCS achieves strong generalization performance with only 25% of labelled training data. Furthermore, our training procedure naturally generates patient-specific representations that can be used to quantify patient-similarity.        △ Less","27 May, 2020","cs.LG,eess.SP,stat.ML",
              Med-BERT: pre-trained contextualized embeddings on large-scale structured electronic health records for disease prediction          ,2005.12833,https://arxiv.org/abs/2005.12833,https://arxiv.org/pdf/2005.12833,"Authors:LailaRasmy,YangXiang,ZiqianXie,CuiTao,DeguiZhi","        Deep learning (DL) based predictive models from electronic health records (EHR) deliver impressive performance in many clinical tasks. Large training cohorts, however, are often required to achieve high accuracy, hindering the adoption of DL-based models in scenarios with limited training data size. Recently, bidirectional encoder representations from transformers (BERT) and related models have achieved tremendous successes in the natural language processing domain. The pre-training of BERT on a very large training corpus generates contextualized embeddings that can boost the performance of models trained on smaller datasets. We propose Med-BERT, which adapts the BERT framework for pre-training contextualized embedding models on structured diagnosis data from 28,490,650 patients EHR dataset. Fine-tuning experiments are conducted on two disease-prediction tasks: (1) prediction of heart failure in patients with diabetes and (2) prediction of pancreatic cancer from two clinical databases. Med-BERT substantially improves prediction accuracy, boosting the area under receiver operating characteristics curve (AUC) by 2.02-7.12%. In particular, pre-trained Med-BERT substantially improves the performance of tasks with very small fine-tuning training sets (300-500 samples) boosting the AUC by more than 20% or equivalent to the AUC of 10 times larger training set. We believe that Med-BERT will benefit disease-prediction studies with small local training datasets, reduce data collection expenses, and accelerate the pace of artificial intelligence aided healthcare.        △ Less","22 May, 2020","cs.CL,cs.LG,cs.NE",
              SNARKs to the rescue: proof-of-contact in zero knowledge          ,2005.12676,https://arxiv.org/abs/2005.12676,https://arxiv.org/pdf/2005.12676,"Authors:ZacharyRatliff,JoudKhoury","        This paper describes techniques to help with COVID-19 automated contact tracing, and with the restoration efforts. We describe a decentralized protocol for ``proof-of-contact'' in zero knowledge where a person can publish a short cryptographic proof attesting to the fact that they have been infected and that they have come in contact with a set of people without revealing any information about any of the people involved. More importantly, we describe how to compose these proofs to support broader functionality such as proofs of nnth-order exposure which can further speed up automated contact tracing. The cryptographic proofs are publicly verifiable, and places the burden on the person proving contact and not on third parties or healthcare providers rendering the system more decentralized, and accordingly more scalable.        △ Less","20 July, 2020","cs.CR,cs.CY",
              Personalized Early Stage Alzheimer's Disease Detection: A Case Study of President Reagan's Speeches          ,2005.12385,https://arxiv.org/abs/2005.12385,https://arxiv.org/pdf/2005.12385,"Authors:NingWang,FanLuo,VishalPeddagangireddy,K.P.Subbalakshmi,R.Chandramouli","        Alzheimer`s disease (AD)-related global healthcare cost is estimated to be $1 trillion by 2050. Currently, there is no cure for this disease; however, clinical studies show that early diagnosis and intervention helps to extend the quality of life and inform technologies for personalized mental healthcare. Clinical research indicates that the onset and progression of Alzheimer`s disease lead to dementia and other mental health issues. As a result, the language capabilities of patient start to decline. In this paper, we show that machine learning-based unsupervised clustering of and anomaly detection with linguistic biomarkers are promising approaches for intuitive visualization and personalized early stage detection of Alzheimer`s disease. We demonstrate this approach on 10 year`s (1980 to 1989) of President Ronald Reagan`s speech data set. Key linguistic biomarkers that indicate early-stage AD are identified. Experimental results show that Reagan had early onset of Alzheimer`s sometime between 1983 and 1987. This finding is corroborated by prior work that analyzed his interviews using a statistical technique. The proposed technique also identifies the exact speeches that reflect linguistic biomarkers for early stage AD.        △ Less","8 May, 2020","cs.CY,cs.CL",
              The challenges of deploying artificial intelligence models in a rapidly evolving pandemic          ,2005.12137,https://arxiv.org/abs/2005.12137,https://arxiv.org/pdf/2005.12137,"Authors:YipengHu,JosephJacob,GeoffreyJMParker,DavidJHawkes,JohnRHurst,DanailStoyanov","        The COVID-19 pandemic, caused by the severe acute respiratory syndrome coronavirus 2, emerged into a world being rapidly transformed by artificial intelligence (AI) based on big data, computational power and neural networks. The gaze of these networks has in recent years turned increasingly towards applications in healthcare. It was perhaps inevitable that COVID-19, a global disease propagating health and economic devastation, should capture the attention and resources of the world's computer scientists in academia and industry. The potential for AI to support the response to the pandemic has been proposed across a wide range of clinical and societal challenges, including disease forecasting, surveillance and antiviral drug discovery. This is likely to continue as the impact of the pandemic unfolds on the world's people, industries and economy but a surprising observation on the current pandemic has been the limited impact AI has had to date in the management of COVID-19. This correspondence focuses on exploring potential reasons behind the lack of successful adoption of AI models developed for COVID-19 diagnosis and prognosis, in front-line healthcare services. We highlight the moving clinical needs that models have had to address at different stages of the epidemic, and explain the importance of translating models to reflect local healthcare environments. We argue that both basic and applied research are essential to accelerate the potential of AI models, and this is particularly so during a rapidly evolving pandemic. This perspective on the response to COVID-19, may provide a glimpse into how the global scientific community should react to combat future disease outbreaks more effectively.        △ Less","19 May, 2020","cs.CY,cs.LG",10.1038/s42256-020-0185-2 
              SecureABC: Secure AntiBody Certificates for COVID-19          ,2005.11833,https://arxiv.org/abs/2005.11833,https://arxiv.org/pdf/2005.11833,"Authors:ChrisHicks,DavidButler,CarstenMaple,JonCrowcroft","        COVID-19 has resulted in unprecedented social distancing policies being enforced worldwide. As governments seek to restore their economies, open workplaces and permit travel there is a demand for technologies that may alleviate the requirement for social distancing whilst also protecting healthcare services. In this work we explore the controversial technique of so-called immunity passports and present SecureABC: a decentralised, privacy-preserving protocol for issuing and verifying antibody certificates. We consider the implications of antibody certificate systems, develop a set of risk-minimising principles and a security framework for their evaluation, and show that these may be satisfied in practice. Finally, we also develop two additional protocols that minimise individual discrimination but which still allow for collective transmission risk to be estimated. We use these two protocols to illustrate the utility-privacy trade-offs of antibody certificates and their alternatives.        △ Less","12 October, 2020",cs.CR,
"              Virtual Screening of Plant Metabolites against Main protease, RNA-dependent RNA polymerase and Spike protein of SARS-CoV-2: Therapeutics option of COVID-19          ",2005.11254,https://arxiv.org/abs/2005.11254,https://arxiv.org/pdf/2005.11254,"Authors:MdSorwerAlamParvez,KaziFaizulAzim,AbdusShukurImran,TopuRaihan,AklimaBegum,TasfiaSaiyaraShammi,SabbirHowlader,FarhanaRumzumBhuiyan,MahmudulHasan","        Covid-19, a serious respiratory complications caused by SARS-CoV-2 has become one of the global threat to human healthcare system. The present study evaluated the possibility of plant originated approved 117 therapeutics against the main protease protein (MPP), RNA-dependent RNA polymerase (RdRp) and spike protein (S) of SARS-CoV-2 including drug surface analysis by using molecular docking through drug repurposing approaches. The molecular interaction study revealed that Rifampin (-16.3 kcal/mol) were topmost inhibitor of MPP where Azobechalcone were found most potent plant therapeutics for blocking the RdRp (-15.9 kcal /mol) and S (-14.4 kcal/mol) protein of SARS-CoV-2. After the comparative analysis of all docking results, Azobechalcone, Rifampin, Isolophirachalcone, Tetrandrine and Fangchinoline were exhibited as the most potential inhibitory plant compounds for targeting the key proteins of SARS-CoV-2. However, amino acid positions; H41, C145, and M165 of MPP played crucial roles for the drug surface interaction where F368, L371, L372, A375, W509, L514, Y515 were pivotal for RdRP. In addition, the drug interaction surface of S proteins also showed similar patterns with all of its maximum inhibitors. ADME analysis also strengthened the possibility of screened plant therapeutics as the potent drug candidates against SARS-C with the highest drug friendliness.        △ Less","22 May, 2020","q-bio.GN,q-bio.BM",
              Feature selection for gesture recognition in Internet-of-Things for healthcare,2005.11031,https://arxiv.org/abs/2005.11031,https://arxiv.org/pdf/2005.11031,"Authors:GiuliaCisotto,MartinaCapuzzo,AnnaV.Guglielmi,AndreaZanella","        Internet of Things is rapidly spreading across several fields, including healthcare, posing relevant questions related to communication capabilities, energy efficiency and sensors unobtrusiveness. Particularly, in the context of recognition of gestures, e.g., grasping of different objects, brain and muscular activity could be simultaneously recorded via EEG and EMG, respectively, and analyzed to identify the gesture that is being accomplished, and the quality of its performance. This paper proposes a new algorithm that aims (i) to robustly extract the most relevant features to classify different grasping tasks, and (ii) to retain the natural meaning of the selected features. This, in turn, gives the opportunity to simplify the recording setup to minimize the data traffic over the communication network, including Internet, and provide physiologically significant features for medical interpretation. The algorithm robustness is ensured both by consensus clustering as a feature selection strategy, and by nested cross-validation scheme to evaluate its classification performance.        △ Less","22 May, 2020","cs.CV,cs.LG,eess.SP",10.1109/ICC40277.2020.9149381 
              Feeling Like It is Time to Reopen Now? COVID-19 New Normal Scenarios based on Reopening Sentiment Analytics          ,2005.10961,https://arxiv.org/abs/2005.10961,https://arxiv.org/pdf/2005.10961,"Authors:JimSamuel,Md.MokhlesurRahman,G.G.Md.NawazAli,YanaSamuel,AlexanderPelaez","        The Coronavirus pandemic has created complex challenges and adverse circumstances. This research discovers public sentiment amidst problematic socioeconomic consequences of the lockdown, and explores ensuing four potential sentiment associated scenarios. The severity and brutality of COVID-19 have led to the development of extreme feelings, and emotional and mental healthcare challenges. This research identifies emotional consequences - the presence of extreme fear, confusion and volatile sentiments, mixed along with trust and anticipation. It is necessary to gauge dominant public sentiment trends for effective decisions and policies. This study analyzes public sentiment using Twitter Data, time-aligned to COVID-19, to identify dominant sentiment trends associated with the push to 'reopen' the economy. Present research uses textual analytics methodologies to analyze public sentiment support for two potential divergent scenarios - an early opening and a delayed opening, and consequences of each. Present research concludes on the basis of exploratory textual analytics and textual data visualization, that Tweets data from American Twitter users shows more trust sentiment support, than fear, for reopening the US economy. With additional validation, this could present a valuable time sensitive opportunity for state governments, the federal government, corporations and societal leaders to guide the nation into a successful new normal future.        △ Less","21 May, 2020","cs.IR,cs.SI",10.20944/preprints202005.0318.v1 
              CHEER: Rich Model Helps Poor Model via Knowledge Infusion          ,2005.10918,https://arxiv.org/abs/2005.10918,https://arxiv.org/pdf/2005.10918,"Authors:CaoXiao,TrongNghiaHoang,ShendaHong,TengfeiMa,JimengSun","        There is a growing interest in applying deep learning (DL) to healthcare, driven by the availability of data with multiple feature channels in rich-data environments (e.g., intensive care units). However, in many other practical situations, we can only access data with much fewer feature channels in a poor-data environments (e.g., at home), which often results in predictive models with poor performance. How can we boost the performance of models learned from such poor-data environment by leveraging knowledge extracted from existing models trained using rich data in a related environment? To address this question, we develop a knowledge infusion framework named CHEER that can succinctly summarize such rich model into transferable representations, which can be incorporated into the poor model to improve its performance. The infused model is analyzed theoretically and evaluated empirically on several datasets. Our empirical results showed that CHEER outperformed baselines by 5.60% to 46.80% in terms of the macro-F1 score on multiple physiological datasets.        △ Less","21 May, 2020","cs.LG,stat.ML",
              Neural ODEs for Informative Missingness in Multivariate Time Series          ,2005.10693,https://arxiv.org/abs/2005.10693,https://arxiv.org/pdf/2005.10693,"Authors:MansuraHabiba,BarakA.Pearlmutter","        Informative missingness is unavoidable in the digital processing of continuous time series, where the value for one or more observations at different time points are missing. Such missing observations are one of the major limitations of time series processing using deep learning. Practical applications, e.g., sensor data, healthcare, weather, generates data that is in truth continuous in time, and informative missingness is a common phenomenon in these datasets. These datasets often consist of multiple variables, and often there are missing values for one or many of these variables. This characteristic makes time series prediction more challenging, and the impact of missing input observations on the accuracy of the final output can be significant. A recent novel deep learning model called GRU-D is one early attempt to address informative missingness in time series data. On the other hand, a new family of neural networks called Neural ODEs (Ordinary Differential Equations) are natural and efficient for processing time series data which is continuous in time. In this paper, a deep learning model is proposed that leverages the effective imputation of GRU-D, and the temporal continuity of Neural ODEs. A time series classification task performed on the PhysioNet dataset demonstrates the performance of this architecture.        △ Less","19 May, 2020","cs.LG,stat.ML",
"              The Optimal Design of Clinical Trials with Potential Biomarker Effects, A Novel Computational Approach          ",2005.10494,https://arxiv.org/abs/2005.10494,https://arxiv.org/pdf/2005.10494,"Authors:YitaoLu,JulieZhou,LiXing,XuekuiZhang","        As a future trend of healthcare, personalized medicine tailors medical treatments to individual patients. It requires to identify a subset of patients with the best response to treatment. The subset can be defined by a biomarker (e.g. expression of a gene) and its cutoff value. Topics on subset identification have received massive attention. There are over 2 million hits by keyword searches on Google Scholar. However, how to properly incorporate the identified subsets/biomarkers to design clinical trials is not trivial and rarely discussed in the literature, which leads to a gap between research results and real-world drug development.  To fill in this gap, we formulate the problem of clinical trial design into an optimization problem involving high-dimensional integration, and propose a novel computational solution based on Monte-Carlo and smoothing methods. Our method utilizes the modern techniques of General-Purpose computing on Graphics Processing Units for large-scale parallel computing. Compared to the standard method in three-dimensional problems, our approach is more accurate and 133 times faster. This advantage increases when dimensionality increases. Our method is scalable to higher-dimensional problems since the precision bound is a finite number not affected by dimensionality.  Our software will be available on GitHub and CRAN, which can be applied to guide the design of clinical trials to incorporate the biomarker better. Although our research is motivated by the design of clinical trials, the method can be used widely to solve other optimization problems involving high-dimensional integration.        △ Less","21 May, 2020","stat.ME,stat.CO",
              Privacy Preserving Face Recognition Utilizing Differential Privacy          ,2005.10486,https://arxiv.org/abs/2005.10486,https://arxiv.org/pdf/2005.10486,"Authors:M.A.P.Chamikara,P.Bertok,I.Khalil,D.Liu,S.Camtepe","        Facial recognition technologies are implemented in many areas, including but not limited to, citizen surveillance, crime control, activity monitoring, and facial expression evaluation. However, processing biometric information is a resource-intensive task that often involves third-party servers, which can be accessed by adversaries with malicious intent. Biometric information delivered to untrusted third-party servers in an uncontrolled manner can be considered a significant privacy leak (i.e. uncontrolled information release) as biometrics can be correlated with sensitive data such as healthcare or financial records. In this paper, we propose a privacy-preserving technique for ""controlled information release"", where we disguise an original face image and prevent leakage of the biometric features while identifying a person. We introduce a new privacy-preserving face recognition protocol named PEEP (Privacy using EigEnface Perturbation) that utilizes local differential privacy. PEEP applies perturbation to Eigenfaces utilizing differential privacy and stores only the perturbed data in the third-party servers to run a standard Eigenface recognition algorithm. As a result, the trained model will not be vulnerable to privacy attacks such as membership inference and model memorization attacks. Our experiments show that PEEP exhibits a classification accuracy of around 70% - 90% under standard privacy settings.        △ Less","4 July, 2020","cs.CR,cs.DB",10.1016/j.cose.2020.101951 
"              Application Management in Fog Computing Environments: A Taxonomy, Review and Future Directions          ",2005.10460,https://arxiv.org/abs/2005.10460,https://arxiv.org/pdf/2005.10460,"Authors:RedowanMahmud,KotagiriRamamohanarao,RajkumarBuyya","        The Internet of Things (IoT) paradigm is being rapidly adopted for the creation of smart environments in various domains. The IoT-enabled Cyber-Physical Systems (CPSs) associated with smart city, healthcare, Industry 4.0 and Agtech handle a huge volume of data and require data processing services from different types of applications in real-time. The Cloud-centric execution of IoT applications barely meets such requirements as the Cloud datacentres reside at a multi-hop distance from the IoT devices. \textit{Fog computing}, an extension of Cloud at the edge network, can execute these applications closer to data sources. Thus, Fog computing can improve application service delivery time and resist network congestion. However, the Fog nodes are highly distributed, heterogeneous and most of them are constrained in resources and spatial sharing. Therefore, efficient management of applications is necessary to fully exploit the capabilities of Fog nodes. In this work, we investigate the existing application management strategies in Fog computing and review them in terms of architecture, placement and maintenance. Additionally, we propose a comprehensive taxonomy and highlight the research gaps in Fog-based application management. We also discuss a perspective model and provide future research directions for further improvement of application management in Fog computing.        △ Less","21 May, 2020","cs.DC,eess.SP",10.1145/3403955 
              A refinement checking based strategy for component-based systems evolution          ,2005.10295,https://arxiv.org/abs/2005.10295,https://arxiv.org/pdf/2005.10295,"Authors:JoséDihego,AugustoSampaio,MarcelOliveira","        We propose inheritance and refinement relations for a CSP-based component model (BRIC), which supports a constructive design based on composition rules that preserve classical concurrency properties such as deadlock freedom. The proposed relations allow extension of functionality, whilst preserving behavioural properties. A notion of extensibility is defined on top of a behavioural relation called convergence, which distinguishes inputs from outputs and the context where they are communicated, allowing extensions to reuse existing events with different purposes. We mechanise the strategy for extensibility verification using the FDR4 tool, and illustrate our results with an autonomous healthcare robot case study.        △ Less","20 May, 2020",cs.SE,10.1016/j.jss.2020.110598 
              Uncertainty representation for early phase clinical test evaluations: a case study          ,2005.10011,https://arxiv.org/abs/2005.10011,https://arxiv.org/pdf/2005.10011,"Authors:SaraGraziadio,KevinJ.Wilson","        In early clinical test evaluations the potential benefits of the introduction of a new technology into the healthcare system are assessed in the challenging situation of limited available empirical data. The aim of these evaluations is to provide additional evidence for the decision maker, who is typically a funder or the company developing the test, to evaluate which technologies should progress to the next stage of evaluation. In this paper we consider the evaluation of a diagnostic test for patients suffering from Chronic Obstructive Pulmonary Disease (COPD). We describe the use of graphical models, prior elicitation and uncertainty analysis to provide the required evidence to allow the test to progress to the next stage of evaluation. We specifically discuss inferring an influence diagram from a care pathway and conducting an elicitation exercise to allow specification of prior distributions over all model parameters. We describe the uncertainty analysis, via Monte Carlo simulation, which allowed us to demonstrate that the potential value of the test was robust to uncertainties. This paper provides a case study illustrating how a careful Bayesian analysis can be used to enhance early clinical test evaluations.        △ Less","20 May, 2020",stat.AP,
              Spatial Heterogeneity Can Lead to Substantial Local Variations in COVID-19 Timing and Severity          ,2005.09850,https://arxiv.org/abs/2005.09850,https://arxiv.org/pdf/2005.09850,"Authors:LoringJ.Thomas,PengHuang,FanYin,XiaoshuangIrisLuo,ZackW.Almquist,JohnR.Hipp,CarterT.Butts","        Standard epidemiological models for COVID-19 employ variants of compartment (SIR) models at local scales, implicitly assuming spatially uniform local mixing. Here, we examine the effect of employing more geographically detailed diffusion models based on known spatial features of interpersonal networks, most particularly the presence of a long-tailed but monotone decline in the probability of interaction with distance, on disease diffusion. Based on simulations of unrestricted COVID-19 diffusion in 19 U.S cities, we conclude that heterogeneity in population distribution can have large impacts on local pandemic timing and severity, even when aggregate behavior at larger scales mirrors a classic SIR-like pattern. Impacts observed include severe local outbreaks with long lag time relative to the aggregate infection curve, and the presence of numerous areas whose disease trajectories correlate poorly with those of neighboring areas. A simple catchment model for hospital demand illustrates potential implications for health care utilization, with substantial disparities in the timing and extremity of impacts even without distancing interventions. Likewise, analysis of social exposure to others who are morbid or deceased shows considerable variation in how the epidemic can appear to individuals on the ground, potentially affecting risk assessment and compliance with mitigation measures. These results demonstrate the potential for spatial network structure to generate highly non-uniform diffusion behavior even at the scale of cities, and suggest the importance of incorporating such structure when designing models to inform healthcare planning, predict community outcomes, or identify potential disparities.        △ Less","20 May, 2020","physics.soc-ph,cs.SI,q-bio.PE",
              A comparative study of machine learning techniques used in non-clinical systems for continuous healthcare of independent livings          ,2005.09502,https://arxiv.org/abs/2005.09502,https://arxiv.org/pdf/2005.09502,"Authors:ZahidIqbal,RafiaIlyas,WaseemShahzad,IrumInayat","        New technologies are adapted to made progress in healthcare especially for independent livings. Medication at distance is leading to integrate technologies with medical. Machine learning methods in collaboration with wearable sensor network technology are used to find hidden patterns in data, detect patient movements, observe habits of patient, analyze clinical data of patient, find intention of patients and make decision on the bases of gathered data. This research performs comparative study on non-clinical systems in healthcare for independent livings. In this study, these systems are sub-divided w.r.t their working into two types: single purpose systems and multi-purpose systems. Systems that are built for single specific purpose (e.g. detect fall, detect emergent state of chronic disease patient) and cannot support healthcare generically are known as single purpose systems, where multi-purpose systems are built to serve for multiple problems (e.g. heart attack etc.) by using single system. This study analyzes usages of machine learning techniques in healthcare systems for independent livings. Answer Set Programming (ASP), Artificial Neural Networks, Classification, Sampling and Rule Based Reasoning etc. are some state of art techniques used to determine emergent situations and observe changes in patient data. Among all methods, ASP logic is used most widely, it is due to its feature to deal with incomplete data. It is also observed that system using ANN shows better accuracy than other systems. It is observed that most of the systems created are for single purpose. In this work, 10 single purpose systems and 5 multi-purpose systems are studied. There is need to create more generic systems that can be used for patients with multiple diseases. Also most of the systems created are prototypical. There is need to create systems that can serve healthcare services in real world.        △ Less","19 May, 2020",cs.CY,
"              The Skincare project, an interactive deep learning system for differential diagnosis of malignant skin lesions. Technical Report          ",2005.09448,https://arxiv.org/abs/2005.09448,https://arxiv.org/pdf/2005.09448,"Authors:DanielSonntag,FabrizioNunnari,Hans-JürgenProfitlich","        A shortage of dermatologists causes long wait times for patients who seek dermatologic care. In addition, the diagnostic accuracy of general practitioners has been reported to be lower than the accuracy of artificial intelligence software. This article describes the Skincare project (H2020, EIT Digital). Contributions include enabling technology for clinical decision support based on interactive machine learning (IML), a reference architecture towards a Digital European Healthcare Infrastructure (also cf. EIT MCPS), technical components for aggregating digitised patient information, and the integration of decision support technology into clinical test-bed environments. However, the main contribution is a diagnostic and decision support system in dermatology for patients and doctors, an interactive deep learning system for differential diagnosis of malignant skin lesions. In this article, we describe its functionalities and the user interfaces to facilitate machine learning from human input. The baseline deep learning system, which delivers state-of-the-art results and the potential to augment general practitioners and even dermatologists, was developed and validated using de-identified cases from a dermatology image data base (ISIC), which has about 20000 cases for development and validation, provided by board-certified dermatologists defining the reference standard for every case. ISIC allows for differential diagnosis, a ranked list of eight diagnoses, that is used to plan treatments in the common setting of diagnostic ambiguity. We give an overall description of the outcome of the Skincare project, and we focus on the steps to support communication and coordination between humans and machine in IML. This is an integral part of the development of future cognitive assistants in the medical domain, and we describe the necessary intelligent user interfaces.        △ Less","19 May, 2020","eess.IV,cs.CV,cs.LG",
              Measles Rash Identification Using Residual Deep Convolutional Neural Network          ,2005.09112,https://arxiv.org/abs/2005.09112,https://arxiv.org/pdf/2005.09112,"Authors:KimberlyGlock,CharlieNapier,AndreLouie,ToddGary,JosephGigante,WilliamSchaffner,QingguoWang","        Measles is extremely contagious and is one of the leading causes of vaccine-preventable illness and death in developing countries, claiming more than 100,000 lives each year. Measles was declared eliminated in the US in 2000 due to decades of successful vaccination for the measles. As a result, an increasing number of US healthcare professionals and the public have never seen the disease. Unfortunately, the Measles resurged in the US in 2019 with 1,282 confirmed cases. To assist in diagnosing measles, we collected more than 1300 images of a variety of skin conditions, with which we employed residual deep convolutional neural network to distinguish measles rash from other skin conditions, in an aim to create a phone application in the future. On our image dataset, our model reaches a classification accuracy of 95.2%, sensitivity of 81.7%, and specificity of 97.1%, indicating the model is effective in facilitating an accurate detection of measles to help contain measles outbreaks.        △ Less","11 August, 2020","eess.IV,cs.LG,q-bio.QM",
              BLAZE: Blazing Fast Privacy-Preserving Machine Learning          ,2005.09042,https://arxiv.org/abs/2005.09042,https://arxiv.org/pdf/2005.09042,"Authors:ArpitaPatra,AjithSuresh","        Machine learning tools have illustrated their potential in many significant sectors such as healthcare and finance, to aide in deriving useful inferences. The sensitive and confidential nature of the data, in such sectors, raise natural concerns for the privacy of data. This motivated the area of Privacy-preserving Machine Learning (PPML) where privacy of the data is guaranteed. Typically, ML techniques require large computing power, which leads clients with limited infrastructure to rely on the method of Secure Outsourced Computation (SOC). In SOC setting, the computation is outsourced to a set of specialized and powerful cloud servers and the service is availed on a pay-per-use basis. In this work, we explore PPML techniques in the SOC setting for widely used ML algorithms-- Linear Regression, Logistic Regression, and Neural Networks.  We propose BLAZE, a blazing fast PPML framework in the three server setting tolerating one malicious corruption over a ring (\Z{\ell}). BLAZE achieves the stronger security guarantee of fairness (all honest servers get the output whenever the corrupt server obtains the same). Leveraging an input-independent preprocessing phase, BLAZE has a fast input-dependent online phase relying on efficient PPML primitives such as: (i) A dot product protocol for which the communication in the online phase is independent of the vector size, the first of its kind in the three server setting; (ii) A method for truncation that shuns evaluating expensive circuit for Ripple Carry Adders (RCA) and achieves a constant round complexity. This improves over the truncation method of ABY3 (Mohassel et al., CCS 2018) that uses RCA and consumes a round complexity that is of the order of the depth of RCA.  An extensive benchmarking of BLAZE for the aforementioned ML algorithms over a 64-bit ring in both WAN and LAN settings shows massive improvements over ABY3.        △ Less","18 May, 2020","cs.CR,cs.LG",10.14722/ndss.2020.24202 
              Local and Global Explanations of Agent Behavior: Integrating Strategy Summaries with Saliency Maps          ,2005.08874,https://arxiv.org/abs/2005.08874,https://arxiv.org/pdf/2005.08874,"Authors:TobiasHuber,KatharinaWeitz,ElisabethAndré,OfraAmir","        With advances in reinforcement learning (RL), agents are now being developed in high-stakes application domains such as healthcare and transportation. Explaining the behavior of these agents is challenging, as the environments in which they act have large state spaces, and their decision-making can be affected by delayed rewards, making it difficult to analyze their behavior. To address this problem, several approaches have been developed. Some approaches attempt to convey the global\textit{global} behavior of the agent, describing the actions it takes in different states. Other approaches devised local\textit{local} explanations which provide information regarding the agent's decision-making in a particular state. In this paper, we combine global and local explanation methods, and evaluate their joint and separate contributions, providing (to the best of our knowledge) the first user study of combined local and global explanations for RL agents. Specifically, we augment strategy summaries that extract important trajectories of states from simulations of the agent with saliency maps which show what information the agent attends to. Our results show that the choice of what states to include in the summary (global information) strongly affects people's understanding of agents: participants shown summaries that included important states significantly outperformed participants who were presented with agent behavior in a randomly set of chosen world-states. We find mixed results with respect to augmenting demonstrations with saliency maps (local information), as the addition of saliency maps did not significantly improve performance in most cases. However, we do find some evidence that saliency maps can help users better understand what information the agent relies on in its decision making, suggesting avenues for future work that can further improve explanations of RL agents.        △ Less","29 May, 2020","cs.LG,cs.AI,cs.HC,cs.NE,stat.ML",
              VerifyMed -- A blockchain platform for transparent trust in virtualized healthcare: Proof-of-concept          ,2005.08804,https://arxiv.org/abs/2005.08804,https://arxiv.org/pdf/2005.08804,"Authors:Jens-AndreasHanssenRensaa,DaniloGligoroski,KatinaKralevska,AntonHasselgren,ArildFaxvaag","        Patients living in a digitized world can now interact with medical professionals through online services such as chat applications, video conferencing or indirectly through consulting services. These applications need to tackle several fundamental trust issues: 1. Checking and confirming that the person they are interacting with is a real person; 2. Validating that the healthcare professional has competence within the field in question; and 3. Confirming that the healthcare professional has a valid license to practice. In this paper, we present VerifyMed -- the first proof-of-concept platform, built on Ethereum, for transparently validating the authorization and competence of medical professionals using blockchain technology. Our platform models trust relationships within the healthcare industry to validate professional clinical authorization. Furthermore, it enables a healthcare professional to build a portfolio of real-life work experience and further validates the competence by storing outcome metrics reported by the patients. The extensive realistic simulations show that with our platform, an average cost for creating a smart contract for a treatment and getting it approved is around 1 USD, and the cost for evaluating a treatment is around 50 cents.        △ Less","18 May, 2020","cs.CR,cs.NI",10.1145/1122445.1122456 
              Spotr: GPS Spoofing Detection via Device Fingerprinting          ,2005.08787,https://arxiv.org/abs/2005.08787,https://arxiv.org/pdf/2005.08787,"Authors:MahsaForuhandeh,AbdullahZ.Mohammed,GregorKildow,PaulBerges,RyanGerdes","        As the worlds predominant navigation system GPS is critical to modern life, finding applications in diverse areas like information security, healthcare, marketing, and power and water grid management. Unfortunately this diversification has only served to underscore the insecurity of GPS and the critical need to harden this system against manipulation and exploitation. A wide variety of attacks against GPS have already been documented, both in academia and industry. Several defenses have been proposed to combat these attacks, but they are ultimately insufficient due to scope, expense, complexity, or robustness. With this in mind, we present our own solution: fingerprinting of GPS satellites. We assert that it is possible to create signatures, or fingerprints, of the satellites (more specifically their transmissions) that allow one to determine nearly instantly whether a received GPS transmission is authentic or not. Furthermore, in this paper we demonstrate that this solution detects all known spoofing attacks, that it does so while being fast, cheap, and simpler than previous solutions, and that it is highly robust with respect to environmental factors.        △ Less","18 May, 2020","eess.SP,eess.SY",
              A note on 'Collider bias undermines our understanding of COVID-19 disease risk and severity' and how causal Bayesian networks both expose and resolve the problem          ,2005.08608,https://arxiv.org/abs/2005.08608,https://arxiv.org/pdf/2005.08608,Authors:NormanFenton,"        An important recent preprint by Griffith et al highlights how 'collider bias' in studies of COVID19 undermines our understanding of the disease risk and severity. This is typically caused by the data being restricted to people who have undergone COVID19 testing, among whom healthcare workers are overrepresented. For example, collider bias caused by smokers being underrepresented in the dataset may (at least partly) explain empirical results that suggest smoking reduces the risk of COVID19. We extend the work of Griffith et al making more explicit use of graphical causal models to interpret observed data. We show that their smoking example can be clarified and improved using Bayesian network models with realistic data and assumptions. We show that there is an even more fundamental problem for risk factors like 'stress' which, unlike smoking, is more rather than less prevalent among healthcare workers; in this case, because of a combination of collider bias from the biased dataset and the fact that 'healthcare worker' is a confounding variable, it is likely that studies will wrongly conclude that stress reduces rather than increases the risk of COVID19. Indeed, ""being in close contact with COVID19 people"" reduces the risk of COVID19. To avoid such potentially erroneous conclusions, any analysis of observational data must take account of the underlying causal structure including colliders and confounders. If analysts fail to do this explicitly then any conclusions they make about the effect of specific risk factors on COVID19 are likely to be flawed.        △ Less","19 May, 2020",stat.ME,
              A Bayesian Multi-Layered Record Linkage Procedure to Analyze Functional Status of Medicare Patients with Traumatic Brain Injury          ,2005.08549,https://arxiv.org/abs/2005.08549,https://arxiv.org/pdf/2005.08549,"Authors:MingyangShan,KaliThomas,RoeeGutman","        Understanding the association between injury severity and patients' potential for recovery is crucial to providing better care for patients with traumatic brain injury (TBI). Estimation of this relationship requires clinical information on injury severity, patient demographics, and healthcare utilization, which are often obtained from separate data sources. Because of privacy and confidentiality regulations, these data sources do not include unique identifiers to link records across data sources. Record linkage is a process to identify records that represent the same entity across data sources in the absence of unique identifiers. These processes commonly rely on agreement between variables that appear in both data sources to link records. However, when the number of records in each file is large, this task is computationally intensive and may result in false links. Blocking is a data partitioning technique that reduces the number of possible links that should be considered. Healthcare providers can be used as blocks in applications of record linkage with healthcare datasets. However, providers may not be uniquely identified across files. We propose a Bayesian record linkage procedure that simultaneously performs block-level and record-level linkage. This iterative approach incorporates the record-level linkage within block pairs to improve the accuracy of the block-level linkage. Subsequently, the algorithm improves record-level linkage using the accurate partitioning of the linkage space through blocking. We demonstrate that our proposed method provides improved performance compared to existing Bayesian record linkage methods that do not incorporate blocking. The proposed procedure is then used to merge registry data from the National Trauma Data Bank with Medicare claims data to estimate the relationship between injury severity and TBI patients' recovery.        △ Less","18 May, 2020",stat.ME,
              Remote health monitoring and diagnosis in the time of COVID-19          ,2005.08537,https://arxiv.org/abs/2005.08537,https://arxiv.org/pdf/2005.08537,"Authors:JoachimA.Behar,ChengyuLiu,KevinKotzen,KentaTsutsui,ValentinaD.A.Corino,JanmajaySingh,MarcoA.F.Pimentel,PhilipWarrick,SebastianZaunseder,FernandoAndreotti,DavidSebag,GeorgyPopanitsa,PatrickE.McSharry,WalterKarlen,ChandanKarmakar,GariD.Clifford","        Coronavirus disease (COVID-19) is caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) that is rapidly spreading across the globe. The clinical spectrum of SARS-CoV-2 pneumonia ranges from mild to critically ill cases and requires early detection and monitoring, within a clinical environment for critical cases and remotely for mild cases. The fear of contamination in clinical environments has led to a dramatic reduction in on-site referrals for routine care. There has also been a perceived need to continuously monitor non-severe COVID- 19 patients, either from their quarantine site at home, or dedicated quarantine locations (e.g., hotels). Thus, the pandemic has driven incentives to innovate and enhance or create new routes for providing healthcare services at distance. In particular, this has created a dramatic impetus to find innovative ways to remotely and effectively monitor patient health status. In this paper we present a short review of remote health monitoring initiatives taken in 19 states during the time of the pandemic. We emphasize in the discussion particular aspects that are common ground for the reviewed states, in particular the future impact of the pandemic on remote health monitoring and consideration on data privacy.        △ Less","15 October, 2020",cs.CY,10.1088/1361-6579/abba0a 
              Lissajous scanning magnetic particle imaging as a multifunctional platform for magnetic hyperthermia therapy          ,2005.08503,https://arxiv.org/abs/2005.08503,https://arxiv.org/pdf/2005.08503,"Authors:JamesWells,ShaileyTwamley,AparnaSekar,AntjeLudwig,HendrikPaysen,OlafKosch,FrankWiekhorst","        The use of engineered nanoscale magnetic materials in healthcare and biomedical technologies is rapidly growing. Two examples which have recently attracted significant attention are magnetic particle imaging (MPI) for biological monitoring, and magnetic field hyperthermia (MFH) for cancer therapy. Here for the first time, the capability of a Lissajous scanning MPI device to act as a standalone platform to support the application of MFH cancer treatment is presented. The platform is shown to offer functionalities for nanoparticle localization, focused hyperthermia therapy application, and non-invasive tissue thermometry in one device. Combined, these capabilities have the potential to significantly enhance the accuracy, effectiveness and safety of MFH therapy. Measurements of nanoparticle hyperthermia during protracted exposure to the MPI scanner's 3D imaging field sequence revealed spatially focused heating, with a maximum that is significantly enhanced compared with a simple 1-dimensional sinusoidal excitation. The observed spatial heating behavior is qualitatively described based on a phenomenological model considering torques exerted in the Brownian regime. In-vitro cell studies using a human acute monocytic leukemia cell line (THP-1) demonstrated strong suppression of both structural integrity and metabolic activity within 24 h following a 40 min MFH treatment actuated within the Lissajous MPI scanner. Furthermore, reconstructed MPI images of the nanoparticles distributed among the cells, and the temperature-sensitivity of the MPI imaging signal obtained during treatment are demonstrated. In summary, combined Lissajous MPI and MFH technologies are presented; demonstrating for the first time their potential for cancer treatment with maximum effectiveness, and minimal collateral damage to surrounding tissues.        △ Less","18 May, 2020",physics.med-ph,
              predCOVID-19: A Systematic Study of Clinical Predictive Models for Coronavirus Disease 2019          ,2005.08302,https://arxiv.org/abs/2005.08302,https://arxiv.org/pdf/2005.08302,"Authors:PatrickSchwab,AugustDuMontSchütte,BenediktDietz,StefanBauer","        Coronavirus Disease 2019 (COVID-19) is a rapidly emerging respiratory disease caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Due to the rapid human-to-human transmission of SARS-CoV-2, many healthcare systems are at risk of exceeding their healthcare capacities, in particular in terms of SARS-CoV-2 tests, hospital and intensive care unit (ICU) beds and mechanical ventilators. Predictive algorithms could potentially ease the strain on healthcare systems by identifying those who are most likely to receive a positive SARS-CoV-2 test, be hospitalised or admitted to the ICU. Here, we study clinical predictive models that estimate, using machine learning and based on routinely collected clinical data, which patients are likely to receive a positive SARS-CoV-2 test, require hospitalisation or intensive care. To evaluate the predictive performance of our models, we perform a retrospective evaluation on clinical and blood analysis data from a cohort of 5644 patients. Our experimental results indicate that our predictive models identify (i) patients that test positive for SARS-CoV-2 a priori at a sensitivity of 75% (95% CI: 67%, 81%) and a specificity of 49% (95% CI: 46%, 51%), (ii) SARS-CoV-2 positive patients that require hospitalisation with 0.92 AUC (95% CI: 0.81, 0.98), and (iii) SARS-CoV-2 positive patients that require critical care with 0.98 AUC (95% CI: 0.95, 1.00). In addition, we determine which clinical features are predictive to what degree for each of the aforementioned clinical tasks. Our results indicate that predictive models trained on routinely collected clinical data could be used to predict clinical pathways for COVID-19, and therefore help inform care and prioritise resources.        △ Less","17 May, 2020","cs.LG,stat.ML",
              Sustaining the economy under partial lockdown: A pandemic centric approach          ,2005.08273,https://arxiv.org/abs/2005.08273,https://arxiv.org/pdf/2005.08273,"Authors:SaketSaurabh,AyushTrivedi,NithilakshP.Lokesh,BhagyashreeGaikwad","        As the world fights to contain and control the spread of the Novel Coronavirus, countries are imposing severe measures from restrictions on travel and social gatherings to complete lockdowns. Lockdowns, though effective in controlling the virus spread, leaves a massive economic impact. In a country like India with 21.9 % of its population below the poverty line, lockdowns have a direct impact on the livelihood of a large part of the population. Our approach conforms to healthcare and state practices of reducing human to human contact, by optimizing the lockdown strategy. We propose resuming economic activities while keeping healthcare facilities from being overwhelmed. We model the coronavirus pandemic as SEIR dynamic model for a set of states as nodes with certain population and analyze the model output before and after complete lockdown. Social distancing that people would willingly follow, in the no lockdown situation is modeled as being influenced with the knowledge of the current number of infection by imitating Granovetter threshold model. We then provide optimal lockdown policy solutions for the duration of ten weeks using NSGA-II optimization algorithm. While there are many studies that focus on modelling the transmission of COVID-19, ours is one of the few attempts to strike a balance between number of infections and economic operations.        △ Less","17 May, 2020","physics.soc-ph,q-fin.GN",
              Social Interaction Layers in Complex Networks for the Dynamical Epidemic Modeling of COVID-19 in Brazil          ,2005.08125,https://arxiv.org/abs/2005.08125,https://arxiv.org/pdf/2005.08125,"Authors:LeonardoF.S.Scabini,LucasC.Ribas,MarianeB.Neiva,AltamirG.B.Junior,AlexJ.F.Farfán,OdemirM.Bruno","        We are currently living in a state of uncertainty due to the pandemic caused by the Sars-CoV-2 virus. There are several factors involved in the epidemic spreading such as the individual characteristics of each city/country. The true shape of the epidemic dynamics is a large, complex system such as most of the social systems. In this context, Complex networks are a great candidate to analyze these systems due to their ability to tackle structural and dynamical properties. Therefore this study presents a new approach to model the COVID-19 epidemic using a multi-layer complex network, where nodes represent people, edges are social contacts, and layers represent different social activities. The model improves the traditional SIR and it is applied to study the Brazilian epidemic by analyzing possible future actions and their consequences. The network is characterized using statistics of infection, death, and hospitalization time. To simulate isolation, social distancing, or precautionary measures we remove layers and/or reduce the intensity of social contacts. Results show that even taking various optimistic assumptions, the current isolation levels in Brazil still may lead to a critical scenario for the healthcare system and a considerable death toll (average of 149,000). If all activities return to normal, the epidemic growth may suffer a steep increase, and the demand for ICU beds may surpass 3 times the country's capacity. This would surely lead to a catastrophic scenario, as our estimation reaches an average of 212,000 deaths even considering that all cases are effectively treated. The increase of isolation (up to a lockdown) shows to be the best option to keep the situation under the healthcare system capacity, aside from ensuring a faster decrease of new case occurrences (months of difference), and a significantly smaller death toll (average of 87,000).        △ Less","20 May, 2020","physics.soc-ph,cs.SI,q-bio.PE",
              kD-STR: A Method for Spatio-Temporal Data Reduction and Modelling          ,2005.08111,https://arxiv.org/abs/2005.08111,https://arxiv.org/pdf/2005.08111,"Authors:LiamSteadman,NathanGriffiths,StephenJarvis,MarkBell,ShaunHelman,CarolineWallbank","        Analysing and learning from spatio-temporal datasets is an important process in many domains, including transportation, healthcare and meteorology. In particular, data collected by sensors in the environment allows us to understand and model the processes acting within the environment. Recently, the volume of spatio-temporal data collected has increased significantly, presenting several challenges for data scientists. Methods are therefore needed to reduce the quantity of data that needs to be processed in order to analyse and learn from spatio-temporal datasets. In this paper, we present the k-Dimensional Spatio-Temporal Reduction method (kD-STR) for reducing the quantity of data used to store a dataset whilst enabling multiple types of analysis on the reduced dataset. kD-STR uses hierarchical partitioning to find spatio-temporal regions of similar instances and models the instances within each region to summarise the dataset. We demonstrate the generality of kD-STR with 3 datasets exhibiting different spatio-temporal characteristics and present results for a range of data modelling techniques. Finally, we compare kD-STR with other techniques for reducing the volume of spatio-temporal data. Our results demonstrate that kD-STR is effective in reducing spatio-temporal data and generalises to datasets that exhibit different properties.        △ Less","16 May, 2020",cs.DB,
              A Deep Learning based Wearable Healthcare IoT Device for AI-enabled Hearing Assistance Automation          ,2005.08076,https://arxiv.org/abs/2005.08076,https://arxiv.org/pdf/2005.08076,"Authors:FraserYoung,LZhang,RichardJiang,HanLiu,ConorWall","        With the recent booming of artificial intelligence (AI), particularly deep learning techniques, digital healthcare is one of the prevalent areas that could gain benefits from AI-enabled functionality. This research presents a novel AI-enabled Internet of Things (IoT) device operating from the ESP-8266 platform capable of assisting those who suffer from impairment of hearing or deafness to communicate with others in conversations. In the proposed solution, a server application is created that leverages Google's online speech recognition service to convert the received conversations into texts, then deployed to a micro-display attached to the glasses to display the conversation contents to deaf people, to enable and assist conversation as normal with the general population. Furthermore, in order to raise alert of traffic or dangerous scenarios, an 'urban-emergency' classifier is developed using a deep learning model, Inception-v4, with transfer learning to detect/recognize alerting/alarming sounds, such as a horn sound or a fire alarm, with texts generated to alert the prospective user. The training of Inception-v4 was carried out on a consumer desktop PC and then implemented into the AI based IoT application. The empirical results indicate that the developed prototype system achieves an accuracy rate of 92% for sound recognition and classification with real-time performance.        △ Less","16 May, 2020","cs.HC,cs.CV,cs.CY",
"              Health Access Broker: Secure, Patient-Controlled Management of Personal Health Records in the Cloud          ",2005.07987,https://arxiv.org/abs/2005.07987,https://arxiv.org/pdf/2005.07987,"Authors:ZainabAbaid,ArashShaghaghi,RavinGunawardena,SurangaSeneviratne,ArunaSeneviratne,SanjayJha","        Secure and privacy-preserving management of Personal Health Records (PHRs) has proved to be a major challenge in modern healthcare. Current solutions generally do not offer patients a choice in where the data is actually stored and also rely on at least one fully trusted element that patients must also trust with their data. In this work, we present the Health Access Broker (HAB), a patient-controlled service for secure PHR sharing that (a) does not impose a specific storage location (uniquely for a PHR system), and (b) does not assume any of its components to be fully secure against adversarial threats. Instead, HAB introduces a novel auditing and intrusion-detection mechanism where its workflow is securely logged and continuously inspected to provide auditability of data access and quickly detect any intrusions.        △ Less","16 May, 2020",cs.CR,
              Social Media Usage in Kuwait: A Comparison of Perspectives Between Healthcare Practitioners and Patients          ,2005.07898,https://arxiv.org/abs/2005.07898,https://arxiv.org/pdf/2005.07898,"Authors:FatemahHusain,VivianMotti","        Social Media has been transforming numerous activities of everyday life, impacting also healthcare. However, few studies investigate the medical use of social media by patients and medical practitioners, especially in the Arabian Gulf region and Kuwait. To understand the behavior of patients and medical practitioners in social media toward healthcare and medical purposes, we conducted user studies. Through an online survey, we identified a decrease in patients and medical practitioners use of social media for medical purposes. Patients reported to be more aware than practitioners concerning: health education, health-related network support, and communication activities. While practitioners use social media mostly as a source of medical information, for clinician marketing and for professional development. The findings highlighted the need to design a social media platform that support healthcare online campaign, professional career identity, medical repository, and social privacy setting to increase users engagements toward medical purposes.        △ Less","16 May, 2020","cs.CY,cs.HC",
              6G Communication Technology: A Vision on Intelligent Healthcare,2005.07532,https://arxiv.org/abs/2005.07532,https://arxiv.org/pdf/2005.07532,"Authors:SabuzimaNayak,RiponPatgiri","        6G is a promising communication technology that will dominate the entire health market from 2030 onward. It will dominate not only health sector but also diverse sectors. It is expected that 6G will revolutionize many sectors including healthcare. Healthcare will be fully AI-driven and dependent on 6G communication technology, which will change our perception of lifestyle. Currently, time and space are the key barriers to health care and 6G will be able to overcome these barriers. Also, 6G will be proven as a game changing technology for healthcare. Therefore, in this perspective, we envision healthcare system for the era of 6G communication technology. Also, various new methodologies have to be introduced to enhance our lifestyle, which is addressed in this perspective, including Quality of Life (QoL), Intelligent Wearable Devices (IWD), Intelligent Internet of Medical Things (IIoMT), Hospital-to-Home (H2H) services, and new business model. In addition, we expose the role of 6G communication technology in telesurgery, Epidemic and Pandemic.        △ Less","16 April, 2020","cs.NI,cs.CY",
              A Survey on Security and Privacy Issues in Modern Healthcare Systems: Attacks and Defenses          ,2005.07359,https://arxiv.org/abs/2005.07359,https://arxiv.org/pdf/2005.07359,"Authors:AKMIqridarNewaz,AmitKumarSikder,MohammadAshiqurRahman,A.SelcukUluagac","        The recent advancements in computing systems and wireless communications have made healthcare systems more efficient than before. Modern healthcare devices can monitor and manage different health conditions of the patients automatically without any manual intervention from medical professionals. Additionally, the use of implantable medical devices (IMDs), body area networks (BANs), and Internet of Things (IoT) technologies in healthcare systems improve the overall patient monitoring and treatment process. However, these systems are complex in software and hardware, and optimizing between security, privacy, and treatment is crucial for healthcare systems as any security or privacy violation can lead to severe effects on patients' treatments and overall health conditions. Indeed, the healthcare domain is increasingly facing security challenges and threats due to numerous design flaws and the lack of proper security measures in healthcare devices and applications. In this paper, we explore various security and privacy threats to healthcare systems and discuss the consequences of these threats. We present a detailed survey of different potential attacks and discuss their impacts. Furthermore, we review the existing security measures proposed for healthcare systems and discuss their limitations. Finally, we conclude the paper with future research directions toward securing healthcare systems against common vulnerabilities.        △ Less","15 May, 2020",cs.CR,
              Hierarchical causal variance decomposition for institution and provider comparisons in healthcare,2005.07314,https://arxiv.org/abs/2005.07314,https://arxiv.org/pdf/2005.07314,"Authors:BoChen,OlliSaarela","        Disease-specific quality indicators (QIs) are used to compare institutions and health care providers in terms processes or outcomes relevant to treatment of a particular condition. In the context of surgical cancer treatments, the performance variations can be due to hospital and/or surgeon level differences, creating a hierarchical clustering. We consider how the observed variation in care received at patient level can be decomposed into that causally explained by the hospital performance, surgeon performance within hospital, patient case-mix, and unexplained (residual) variation. For this purpose, we derive a four-way variance decomposition, with particular attention to the causal interpretation of the components. For estimation, we use inputs from a mixed-effect model with nested random hospital/surgeon-specific effects, and a multinomial logistic model for the hospital/surgeon-specific patient populations. We investigate the performance of our methods in a simulation study.        △ Less","21 May, 2020",stat.ME,
              E-Health Sensitive Data Dissemination Exploiting Trust and Mobility of Users          ,2005.07296,https://arxiv.org/abs/2005.07296,https://arxiv.org/pdf/2005.07296,"Authors:AgnaldoBatista,MicheleNogueira,AldriSantos","        E-health services handle a massive amount of sensitive data, requiring reliability and privacy. The advent of new technologies drives e-health services into their continuous provision outside traditional care institutions. This creates uncertain and unreliable conditions, resulting in the challenge of controlling sensitive user data dissemination. Then, there is a gap in sensitive data dissemination under situations requiring fast response (e.g., cardiac arrest). This obligates networks to provide reliable sensitive data dissemination under user mobility, dynamic network topology, and occasional interactions between the devices. In this article, we propose STEALTH, a system that employs social trust and communities of interest to address these challenges. STEALTH follows two steps: clustering and dissemination. In the first, STEALTH groups devices based on the interests of their users, forming communities of interest. A healthcare urgency launches the second, in which STEALTH disseminates user sensitive data to devices belonging to specific communities, subjected to the level of trust between devices. Simulation results demonstrate that STEALTH ensures data dissemination to people who can contribute toward an efficient service. STEALTH has achieved up to 97.14% of reliability in accessing sensitive data with a maximum latency of 170 ms, and up to 100% of availability during emergencies.        △ Less","14 May, 2020","cs.CY,cs.NI,cs.SI",
              Wearable Internet of Things for Personalized Healthcare Study of Trends and Latent Research          ,2005.06958,https://arxiv.org/abs/2005.06958,https://arxiv.org/pdf/2005.06958,"Authors:SamiyaKhan,MansafAlam","        In this age of heterogeneous systems, diverse technologies are integrated to create application-specific solutions. The recent upsurge in acceptance of technologies such as cloud computing and ubiquitous Internet has cleared the path for Internet of Things (IoT). Moreover, the increasing Internet penetration with the rising use of mobile devices has inspired an era of technology that allows interfacing of physical objects and connecting them to Internet for developing applications serving a wide range of purposes. Recent developments in the area of wearable devices has led to the creation of another segment in IoT, which can be conveniently referred to as Wearable Internet of Things (WIoT). Research in this area promises to personalize healthcare in previously unimaginable ways by allowing individual tracking of wellness and health information. This chapter shall cover the different facets of Wearable Internet of Things (WIoT) and ways in which it is a key driving technology behind the concept of personalized healthcare. It shall discuss the theoretical aspects of WIoT, focusing on functionality, design and applicability. Moreover, it shall also elaborate on the role of wearable sensors, big data and cloud computing as enabling technologies for WIoT.        △ Less","20 April, 2020",cs.DC,
              When Wireless Communication Faces COVID-19: Combating the Pandemic and Saving the Economy          ,2005.06637,https://arxiv.org/abs/2005.06637,https://arxiv.org/pdf/2005.06637,"Authors:NasirSaeed,AhmedBader,TareqY.Al-Naffouri,Mohamed-SlimAlouini","        The year 2020 is experiencing a global health and economic crisis due to the COVID-19 pandemic. Countries across the world are using digital technologies to fight this global crisis. These digital technologies, in one way or another, strongly rely on the availability of wireless communication technologies. In this paper, we present the role of wireless communications in the COVID-19 pandemic from different perspectives. First, we show how these technologies are helping to combat this pandemic, including monitoring of the virus spread, enabling healthcare automation, and allowing virtual education and conferencing. Also, we show the importance of digital inclusiveness in the pandemic and possible solutions to connect the unconnected. Next, we discuss the challenges faced by wireless technologies, including privacy, security, and misinformation. Then, we present the importance of wireless communication technologies in the survival of the global economy, such as automation of industries and supply chain, e-commerce, and supporting occupations that are at risk. Finally, we reveal that how the technologies developed during the pandemic can be helpful in the post-pandemic era.        △ Less","6 June, 2020","cs.CY,cs.NI",
              Patient Similarity Analysis with Longitudinal Health Data          ,2005.06630,https://arxiv.org/abs/2005.06630,https://arxiv.org/pdf/2005.06630,"Authors:AhmedAllam,MatthiasDittberner,AnnaSintsova,DominiqueBrodbeck,MichaelKrauthammer","Healthcare professionals have long envisioned using the enormous processing powers of computers to discover new facts and medical knowledge locked inside electronic health records. These vast medical archives contain time-resolved information about medical visits, tests and procedures, as well as outcomes, which together form individual patient journeys. By assessing the similarities among these journeys, it is possible to uncover clusters of common disease trajectories with shared health outcomes. The assignment of patient journeys to specific clusters may in turn serve as the basis for personalized outcome prediction and treatment selection. This procedure is a non-trivial computational problem, as it requires the comparison of patient data with multi-dimensional and multi-modal features that are captured at different times and resolutions. In this review, we provide a comprehensive overview of the tools and methods that are used in patient similarity analysis with longitudinal data and discuss its potential for improving clinical decision making.        △ Less","14 May, 2020","cs.LG,cs.CY,q-bio.QM,stat.AP,stat.ML",
              Development of Computable Phenotype to Identify and Characterize Transitions in Acuity Status in Intensive Care Unit          ,2005.05163,https://arxiv.org/abs/2005.05163,https://arxiv.org/pdf/2005.05163,"Authors:YuanfengRen,TylerJ.Loftus,RahulSaiKasula,PrudhveeNarasimhaSadha,ParisaRashidi,AzraBihorac,TezcanOzrazgat-Baslanti","        Background: In the United States, 5.7 million patients are admitted annually to intensive care units (ICU), with costs exceeding $82 billion. Although close monitoring and dynamic assessment of patient acuity are key aspects of ICU care, both are limited by the time constraints imposed on healthcare providers. Methods: Using the University of Florida Health (UFH) Integrated Data Repository as Honest Broker, we created a database with electronic health records data from a retrospective study cohort of 38,749 adult patients admitted to ICU at UF Health between 06/01/2014 and 08/22/2019. This repository includes demographic information, comorbidities, vital signs, laboratory values, medications with date and timestamps, and diagnoses and procedure codes for all index admission encounters as well as encounters within 12 months prior to index admission and 12 months follow-up. We developed algorithms to identify acuity status of the patient every four hours during each ICU stay. Results: We had 383,193 encounters (121,800 unique patients) admitted to the hospital, and 51,073 encounters (38,749 unique patients) with at least one ICU stay that lasted more than four hours. These patients requiring ICU admission had longer median hospital stay (7 days vs. 1 day) and higher in-hospital mortality (9.6% vs. 0.4%) compared with those not admitted to the ICU. Among patients who were admitted to the ICU and expired during hospital admission, more deaths occurred in the ICU than on general hospital wards (7.4% vs. 0.8%, respectively). Conclusions: We developed phenotyping algorithms that determined patient acuity status every four hours while admitted to the ICU. This approach may be useful in developing prognostic and clinical decision-support tools to aid patients, caregivers, and providers in shared decision-making processes regarding resource use and escalation of care.        △ Less","27 April, 2020","q-bio.QM,cs.LG,stat.ML",
              A Federated Learning Framework for Healthcare IoT devices          ,2005.05083,https://arxiv.org/abs/2005.05083,https://arxiv.org/pdf/2005.05083,"Authors:BinhangYuan,SongGe,WenhuiXing","        The Internet of Things (IoT) revolution has shown potential to give rise to many medical applications with access to large volumes of healthcare data collected by IoT devices. However, the increasing demand for healthcare data privacy and security makes each IoT device an isolated island of data. Further, the limited computation and communication capacity of wearable healthcare devices restrict the application of vanilla federated learning. To this end, we propose an advanced federated learning framework to train deep neural networks, where the network is partitioned and allocated to IoT devices and a centralized server. Then most of the training computation is handled by the powerful server. The sparsification of activations and gradients significantly reduces the communication overhead. Empirical study have suggested that the proposed framework guarantees a low accuracy loss, while only requiring 0.2% of the synchronization traffic in vanilla federated learning.        △ Less","7 May, 2020","cs.LG,cs.DC,eess.SP",
              Continual Learning Using Multi-view Task Conditional Neural Networks          ,2005.05080,https://arxiv.org/abs/2005.05080,https://arxiv.org/pdf/2005.05080,"Authors:HonglinLi,PayamBarnaghi,ShirinEnshaeifar,FriederGanz","        Conventional deep learning models have limited capacity in learning multiple tasks sequentially. The issue of forgetting the previously learned tasks in continual learning is known as catastrophic forgetting or interference. When the input data or the goal of learning change, a continual model will learn and adapt to the new status. However, the model will not remember or recognise any revisits to the previous states. This causes performance reduction and re-training curves in dealing with periodic or irregularly reoccurring changes in the data or goals. The changes in goals or data are referred to as new tasks in a continual learning model. Most of the continual learning methods have a task-known setup in which the task identities are known in advance to the learning model. We propose Multi-view Task Conditional Neural Networks (Mv-TCNN) that does not require to known the reoccurring tasks in advance. We evaluate our model on standard datasets using MNIST, CIFAR10, CIFAR100, and also a real-world dataset that we have collected in a remote healthcare monitoring study (i.e. TIHM dataset). The proposed model outperforms the state-of-the-art solutions in continual learning and adapting to new tasks that are not defined in advance.        △ Less","13 July, 2020","cs.LG,stat.ML",
              MOMBAT: Heart Rate Monitoring from Face Video using Pulse Modeling and Bayesian Tracking          ,2005.04618,https://arxiv.org/abs/2005.04618,https://arxiv.org/pdf/2005.04618,"Authors:PuneetGupta,BrojeshwarBhowmick,ArpanPal","        A non-invasive yet inexpensive method for heart rate (HR) monitoring is of great importance in many real-world applications including healthcare, psychology understanding, affective computing and biometrics. Face videos are currently utilized for such HR monitoring, but unfortunately this can lead to errors due to the noise introduced by facial expressions, out-of-plane movements, camera parameters (like focus change) and environmental factors. We alleviate these issues by proposing a novel face video based HR monitoring method MOMBAT, that is, MOnitoring using Modeling and BAyesian Tracking. We utilize out-of-plane face movements to define a novel quality estimation mechanism. Subsequently, we introduce a Fourier basis based modeling to reconstruct the cardiovascular pulse signal at the locations containing the poor quality, that is, the locations affected by out-of-plane face movements. Furthermore, we design a Bayesian decision theory based HR tracking mechanism to rectify the spurious HR estimates. Experimental results reveal that our proposed method, MOMBAT outperforms state-of-the-art HR monitoring methods and performs HR monitoring with an average absolute error of 1.329 beats per minute and the Pearson correlation between estimated and actual heart rate is 0.9746. Moreover, it demonstrates that HR monitoring is significantly        △ Less","10 May, 2020",cs.CV,
              Is Deep Reinforcement Learning Ready for Practical Applications in Healthcare? A Sensitivity Analysis of Duel-DDQN for Hemodynamic Management in Sepsis Patients          ,2005.04301,https://arxiv.org/abs/2005.04301,https://arxiv.org/pdf/2005.04301,"Authors:MingYuLu,ZacharyShahn,DabySow,FinaleDoshi-Velez,Li-weiH.Lehman","        The potential of Reinforcement Learning (RL) has been demonstrated through successful applications to games such as Go and Atari. However, while it is straightforward to evaluate the performance of an RL algorithm in a game setting by simply using it to play the game, evaluation is a major challenge in clinical settings where it could be unsafe to follow RL policies in practice. Thus, understanding sensitivity of RL policies to the host of decisions made during implementation is an important step toward building the type of trust in RL required for eventual clinical uptake. In this work, we perform a sensitivity analysis on a state-of-the-art RL algorithm (Dueling Double Deep Q-Networks)applied to hemodynamic stabilization treatment strategies for septic patients in the ICU. We consider sensitivity of learned policies to input features, embedding model architecture, time discretization, reward function, and random seeds. We find that varying these settings can significantly impact learned policies, which suggests a need for caution when interpreting RL agent output.        △ Less","27 August, 2020","cs.LG,stat.ML",
              E-Quarantine: A Smart Health System for Monitoring Coronavirus Patients for Remotely Quarantine          ,2005.04187,https://arxiv.org/abs/2005.04187,https://arxiv.org/pdf/2005.04187,"Authors:DoaaMoheyEl-Din,AboulEllaHassanein,EhabE.Hassanien,WalaaM.E.Hussein","        Coronavirus becomes officially a global pandemic due to the speed spreading off in various countries. An increasing number of infected with this disease causes the Inability problem to fully care in hospitals and afflict many doctors and nurses inside the hospitals. This paper proposes a smart health system that monitors the patients holding the Coronavirus remotely. Due to protect the lives of the health services members (like physicians and nurses) from infection. This smart system observes the people with this disease based on putting many sensors to record many features of their patients in every second. These parameters include measuring the patient's temperature, respiratory rate, pulse rate, blood pressure, and time. The proposed system saves lives and improves making decisions in dangerous cases. It proposes using artificial intelligence and Internet-of-things to make remotely quarantine and develop decisions in various situations. It provides monitoring patients remotely and guarantees giving patients medicines and getting complete health care without anyone getting sick with this disease. It targets two people's slides the most serious medical conditions and infection and the lowest serious medical conditions in their houses. Observing in hospitals for the most serious medical cases that cause infection in thousands of healthcare members so there is a big need to uses it. Other less serious patients slide, this system enables physicians to monitor patients and get the healthcare from patient's houses to save places for the critical cases in hospitals.        △ Less","5 May, 2020",eess.SP,
"              Evidence Inference 2.0: More Data, Better Models          ",2005.04177,https://arxiv.org/abs/2005.04177,https://arxiv.org/pdf/2005.04177,"Authors:JayDeYoung,EricLehman,BenNye,IainJ.Marshall,ByronC.Wallace","        How do we most effectively treat a disease or condition? Ideally, we could consult a database of evidence gleaned from clinical trials to answer such questions. Unfortunately, no such database exists; clinical trial results are instead disseminated primarily via lengthy natural language articles. Perusing all such articles would be prohibitively time-consuming for healthcare practitioners; they instead tend to depend on manually compiled systematic reviews of medical literature to inform care.  NLP may speed this process up, and eventually facilitate immediate consult of published evidence. The Evidence Inference dataset was recently released to facilitate research toward this end. This task entails inferring the comparative performance of two treatments, with respect to a given outcome, from a particular article (describing a clinical trial) and identifying supporting evidence. For instance: Does this article report that chemotherapy performed better than surgery for five-year survival rates of operable cancers? In this paper, we collect additional annotations to expand the Evidence Inference dataset by 25\%, provide stronger baseline models, systematically inspect the errors that these make, and probe dataset quality. We also release an abstract only (as opposed to full-texts) version of the task for rapid model prototyping. The updated corpus, documentation, and code for new baselines and evaluations are available at http://evidence-inference.ebm-nlp.com/.        △ Less","14 May, 2020",cs.CL,
              Convolutional Sparse Support Estimator Based Covid-19 Recognition from X-ray Images          ,2005.04014,https://arxiv.org/abs/2005.04014,https://arxiv.org/pdf/2005.04014,"Authors:MehmetYamac,MeteAhishali,AysenDegerli,SerkanKiranyaz,MuhammadE.H.Chowdhury,MoncefGabbouj","        Coronavirus disease (Covid-19) has been the main agenda of the whole world since it came in sight in December 2019. It has already caused thousands of causalities and infected several millions worldwide. Any technological tool that can be provided to healthcare practitioners to save time, effort, and possibly lives has crucial importance. The main tools practitioners currently use to diagnose Covid-19 are Reverse Transcription-Polymerase Chain reaction (RT-PCR) and Computed Tomography (CT), which require significant time, resources and acknowledged experts. X-ray imaging is a common and easily accessible tool that has great potential for Covid-19 diagnosis. In this study, we propose a novel approach for Covid-19 recognition from chest X-ray images. Despite the importance of the problem, recent studies in this domain produced not so satisfactory results due to the limited datasets available for training. Recall that Deep Learning techniques can generally provide state-of-the-art performance in many classification tasks when trained properly over large datasets, such data scarcity can be a crucial obstacle when using them for Covid-19 detection. Alternative approaches such as representation-based classification (collaborative or sparse representation) might provide satisfactory performance with limited size datasets, but they generally fall short in performance or speed compared to Machine Learning methods. To address this deficiency, Convolution Support Estimation Network (CSEN) has recently been proposed as a bridge between model-based and Deep Learning approaches by providing a non-iterative real-time mapping from query sample to ideally sparse representation coefficient' support, which is critical information for class decision in representation based techniques.        △ Less","8 May, 2020","eess.IV,cs.CV,cs.LG",
              An Extensive Study on Cross-Dataset Bias and Evaluation Metrics Interpretation for Machine Learning applied to Gastrointestinal Tract Abnormality Classification          ,2005.03912,https://arxiv.org/abs/2005.03912,https://arxiv.org/pdf/2005.03912,"Authors:VajiraThambawita,DebeshJha,HugoLewiHammer,HåvardD.Johansen,DagJohansen,PålHalvorsen,MichaelA.Riegler","        Precise and efficient automated identification of Gastrointestinal (GI) tract diseases can help doctors treat more patients and improve the rate of disease detection and identification. Currently, automatic analysis of diseases in the GI tract is a hot topic in both computer science and medical-related journals. Nevertheless, the evaluation of such an automatic analysis is often incomplete or simply wrong. Algorithms are often only tested on small and biased datasets, and cross-dataset evaluations are rarely performed. A clear understanding of evaluation metrics and machine learning models with cross datasets is crucial to bring research in the field to a new quality level. Towards this goal, we present comprehensive evaluations of five distinct machine learning models using Global Features and Deep Neural Networks that can classify 16 different key types of GI tract conditions, including pathological findings, anatomical landmarks, polyp removal conditions, and normal findings from images captured by common GI tract examination instruments. In our evaluation, we introduce performance hexagons using six performance metrics such as recall, precision, specificity, accuracy, F1-score, and Matthews Correlation Coefficient to demonstrate how to determine the real capabilities of models rather than evaluating them shallowly. Furthermore, we perform cross-dataset evaluations using different datasets for training and testing. With these cross-dataset evaluations, we demonstrate the challenge of actually building a generalizable model that could be used across different hospitals. Our experiments clearly show that more sophisticated performance metrics and evaluation methods need to be applied to get reliable models rather than depending on evaluations of the splits of the same dataset, i.e., the performance metrics should always be interpreted together rather than relying on a single metric.        △ Less","8 May, 2020","cs.LG,cs.MM,stat.ML",
              Hierarchical Deep Convolutional Neural Networks for Multi-category Diagnosis of Gastrointestinal Disorders on Histopathological Images          ,2005.03868,https://arxiv.org/abs/2005.03868,https://arxiv.org/pdf/2005.03868,"Authors:RasoulSali,SodiqAdewole,LubainaEhsan,LeeA.Denson,PaulKelly,BeatriceC.Amadi,LoriHoltz,SyedAsadAli,SeanR.Moore,SanaSyed,DonaldE.Brown","        Deep convolutional neural networks(CNNs) have been successful for a wide range of computer vision tasks, including image classification. A specific area of the application lies in digital pathology for pattern recognition in the tissue-based diagnosis of gastrointestinal(GI) diseases. This domain can utilize CNNs to translate histopathological images into precise diagnostics. This is challenging since these complex biopsies are heterogeneous and require multiple levels of assessment. This is mainly due to structural similarities in different parts of the GI tract and shared features among different gut diseases. Addressing this problem with a flat model that assumes all classes (parts of the gut and their diseases) are equally difficult to distinguish leads to an inadequate assessment of each class. Since the hierarchical model restricts classification error to each sub-class, it leads to a more informative model than a flat model. In this paper, we propose to apply the hierarchical classification of biopsy images from different parts of the GI tract and the receptive diseases within each. We embedded a class hierarchy into the plain VGGNet to take advantage of its layers' hierarchical structure. The proposed model was evaluated using an independent set of image patches from 373 whole slide images. The results indicate that the hierarchical model can achieve better results than the flat model for multi-category diagnosis of GI disorders using histopathological images.        △ Less","6 August, 2020","eess.IV,cs.LG,stat.ML",
              Predictive Modeling of ICU Healthcare-Associated Infections from Imbalanced Data. Using Ensembles and a Clustering-Based Undersampling Approach          ,2005.03582,https://arxiv.org/abs/2005.03582,https://arxiv.org/pdf/2005.03582,"Authors:FernandoSánchez-Hernández,JuanCarlosBallesteros-Herráez,MohamedS.Kraiem,MercedesSánchez-Barba,MaríaN.Moreno-García","        Early detection of patients vulnerable to infections acquired in the hospital environment is a challenge in current health systems given the impact that such infections have on patient mortality and healthcare costs. This work is focused on both the identification of risk factors and the prediction of healthcare-associated infections in intensive-care units by means of machine-learning methods. The aim is to support decision making addressed at reducing the incidence rate of infections. In this field, it is necessary to deal with the problem of building reliable classifiers from imbalanced datasets. We propose a clustering-based undersampling strategy to be used in combination with ensemble classifiers. A comparative study with data from 4616 patients was conducted in order to validate our proposal. We applied several single and ensemble classifiers both to the original dataset and to data preprocessed by means of different resampling methods. The results were analyzed by means of classic and recent metrics specifically designed for imbalanced data classification. They revealed that the proposal is more efficient in comparison with other approaches.        △ Less","7 May, 2020","cs.LG,stat.ML",10.3390/app9245287 
"              Exploratory Analysis of Covid-19 Tweets using Topic Modeling, UMAP, and DiGraphs          ",2005.03082,https://arxiv.org/abs/2005.03082,https://arxiv.org/pdf/2005.03082,"Authors:CatherineOrdun,SanjayPurushotham,EdwardRaff","        This paper illustrates five different techniques to assess the distinctiveness of topics, key terms and features, speed of information dissemination, and network behaviors for Covid19 tweets. First, we use pattern matching and second, topic modeling through Latent Dirichlet Allocation (LDA) to generate twenty different topics that discuss case spread, healthcare workers, and personal protective equipment (PPE). One topic specific to U.S. cases would start to uptick immediately after live White House Coronavirus Task Force briefings, implying that many Twitter users are paying attention to government announcements. We contribute machine learning methods not previously reported in the Covid19 Twitter literature. This includes our third method, Uniform Manifold Approximation and Projection (UMAP), that identifies unique clustering-behavior of distinct topics to improve our understanding of important themes in the corpus and help assess the quality of generated topics. Fourth, we calculated retweeting times to understand how fast information about Covid19 propagates on Twitter. Our analysis indicates that the median retweeting time of Covid19 for a sample corpus in March 2020 was 2.87 hours, approximately 50 minutes faster than repostings from Chinese social media about H7N9 in March 2013. Lastly, we sought to understand retweet cascades, by visualizing the connections of users over time from fast to slow retweeting. As the time to retweet increases, the density of connections also increase where in our sample, we found distinct users dominating the attention of Covid19 retweeters. One of the simplest highlights of this analysis is that early-stage descriptive methods like regular expressions can successfully identify high-level themes which were consistently verified as important through every subsequent analysis.        △ Less","6 May, 2020","cs.SI,cs.LG",
              AVAC: A Machine Learning based Adaptive RRAM Variability-Aware Controller for Edge Devices          ,2005.03077,https://arxiv.org/abs/2005.03077,https://arxiv.org/pdf/2005.03077,"Authors:ShikharTuli,ShreshthTuli","        Recently, the Edge Computing paradigm has gained significant popularity both in industry and academia. Researchers now increasingly target to improve performance and reduce energy consumption of such devices. Some recent efforts focus on using emerging RRAM technologies for improving energy efficiency, thanks to their no leakage property and high integration density. As the complexity and dynamism of applications supported by such devices escalate, it has become difficult to maintain ideal performance by static RRAM controllers. Machine Learning provides a promising solution for this, and hence, this work focuses on extending such controllers to allow dynamic parameter updates. In this work we propose an Adaptive RRAM Variability-Aware Controller, AVAC, which periodically updates Wait Buffer and batch sizes using on-the-fly learning models and gradient ascent. AVAC allows Edge devices to adapt to different applications and their stages, to improve computation performance and reduce energy consumption. Simulations demonstrate that the proposed model can provide up to 29% increase in performance and 19% decrease in energy, compared to static controllers, using traces of real-life healthcare applications on a Raspberry-Pi based Edge deployment.        △ Less","6 May, 2020","eess.SY,cs.LG",
              Simultaneous estimation of the effective reproducing number and the detection rate of COVID-19          ,2005.02766,https://arxiv.org/abs/2005.02766,https://arxiv.org/pdf/2005.02766,Authors:YoriyukiYamagata,"        A major difficulty to estimate RR (the effective reproducing number) of COVID-19 is that most cases of COVID-19 infection are mild or asymptomatic, therefore true number of infection is difficult to determine. This paper estimates the daily change of RR and the detection rate simultaneously using a Bayesian model. The analysis using synthesized data shows that our model correctly estimates RR and detects a short-term shock of the detection rate. Then, we apply our model to data from several countries to evaluate the effectiveness of public healthcare measures. Our analysis focuses Japan, which employs a moderate measure to keep ""social distance"". The result indicates a downward trend and now RR becomes below 11. Although our analysis is preliminary, this may suggest that a moderate policy still can prevent epidemic of COVID-19.        △ Less","13 May, 2020",physics.soc-ph,
              Dual-Sampling Attention Network for Diagnosis of COVID-19 from Community Acquired Pneumonia          ,2005.02690,https://arxiv.org/abs/2005.02690,https://arxiv.org/pdf/2005.02690,"Authors:XiOuyang,JiayuHuo,LimingXia,FeiShan,JunLiu,ZhanhaoMo,FuhuaYan,ZhongxiangDing,QiYang,BinSong,FengShi,HuanYuan,YingWei,XiaohuanCao,YaozongGao,DijiaWu,QianWang,DinggangShen","        The coronavirus disease (COVID-19) is rapidly spreading all over the world, and has infected more than 1,436,000 people in more than 200 countries and territories as of April 9, 2020. Detecting COVID-19 at early stage is essential to deliver proper healthcare to the patients and also to protect the uninfected population. To this end, we develop a dual-sampling attention network to automatically diagnose COVID- 19 from the community acquired pneumonia (CAP) in chest computed tomography (CT). In particular, we propose a novel online attention module with a 3D convolutional network (CNN) to focus on the infection regions in lungs when making decisions of diagnoses. Note that there exists imbalanced distribution of the sizes of the infection regions between COVID-19 and CAP, partially due to fast progress of COVID-19 after symptom onset. Therefore, we develop a dual-sampling strategy to mitigate the imbalanced learning. Our method is evaluated (to our best knowledge) upon the largest multi-center CT data for COVID-19 from 8 hospitals. In the training-validation stage, we collect 2186 CT scans from 1588 patients for a 5-fold cross-validation. In the testing stage, we employ another independent large-scale testing dataset including 2796 CT scans from 2057 patients. Results show that our algorithm can identify the COVID-19 images with the area under the receiver operating characteristic curve (AUC) value of 0.944, accuracy of 87.5%, sensitivity of 86.9%, specificity of 90.1%, and F1-score of 82.0%. With this performance, the proposed algorithm could potentially aid radiologists with COVID-19 diagnosis from CAP, especially in the early stage of the COVID-19 outbreak.        △ Less","19 May, 2020","cs.CV,eess.IV",
              Vehicle Routing and Scheduling for Regular Mobile Healthcare Services          ,2005.02618,https://arxiv.org/abs/2005.02618,https://arxiv.org/pdf/2005.02618,"Authors:CosminPascaru,PaulDiac","        We propose our solution to a particular practical problem in the domain of vehicle routing and scheduling. The generic task is finding the best allocation of the minimum number of \emph{mobile resources} that can provide periodical services in remote locations. These \emph{mobile resources} are based at a single central location. Specifications have been defined initially for a real-life application that is the starting point of an ongoing project. Particularly, the goal is to mitigate health problems in rural areas around a city in Romania. Medically equipped vans are programmed to start daily routes from county capital, provide a given number of examinations in townships within the county and return to the capital city in the same day. From the health care perspective, each van is equipped with an ultrasound scanner, and they are scheduled to investigate pregnant woman each trimester aiming to diagnose potential problems. The project is motivated by reports currently ranking Romania as the country with the highest infant mortality rate in the European Union.  We developed our solution in two phases: modeling of the most relevant parameters and data available for our goal and then design and implement an algorithm that provides an optimized solution. The most important metric of an output scheduling is the number of vans that are necessary to provide a given amount of examination time per township, followed by total travel time or fuel consumption, number of different routes, and others. Our solution implements two probabilistic algorithms out of which we chose the one that performs the best.        △ Less","6 May, 2020","cs.NE,cs.AI",10.1109/ICTAI.2018.00080 
              Approaches and Applications of Early Classification of Time Series: A Review          ,2005.02595,https://arxiv.org/abs/2005.02595,https://arxiv.org/pdf/2005.02595,"Authors:AshishGupta,HariPrabhatGupta,BhaskarBiswas,TanimaDutta","        Early classification of time series has been extensively studied for minimizing class prediction delay in time-sensitive applications such as healthcare and finance. A primary task of an early classification approach is to classify an incomplete time series as soon as possible with some desired level of accuracy. Recent years have witnessed several approaches for early classification of time series. As most of the approaches have solved the early classification problem with different aspects, it becomes very important to make a thorough review of the existing solutions to know the current status of the area. These solutions have demonstrated reasonable performance in a wide range of applications including human activity recognition, gene expression based health diagnostic, industrial monitoring, and so on. In this paper, we present a systematic review of current literature on early classification approaches for both univariate and multivariate time series. We divide various existing approaches into four exclusive categories based on their proposed solution strategies. The four categories include prefix based, shapelet based, model based, and miscellaneous approaches. The authors also discuss the applications of early classification in many areas including industrial monitoring, intelligent transportation, and medical. Finally, we provide a quick summary of the current literature with future research directions.        △ Less","15 October, 2020","cs.LG,stat.ML",10.1109/TAI.2020.3027279 
              Unsupervised Pre-trained Models from Healthy ADLs Improve Parkinson's Disease Classification of Gait Patterns          ,2005.02589,https://arxiv.org/abs/2005.02589,https://arxiv.org/pdf/2005.02589,"Authors:AnirudhSom,NarayananKrishnamurthi,MatthewBuman,PavanTuraga","        Application and use of deep learning algorithms for different healthcare applications is gaining interest at a steady pace. However, use of such algorithms can prove to be challenging as they require large amounts of training data that capture different possible variations. This makes it difficult to use them in a clinical setting since in most health applications researchers often have to work with limited data. Less data can cause the deep learning model to over-fit. In this paper, we ask how can we use data from a different environment, different use-case, with widely differing data distributions. We exemplify this use case by using single-sensor accelerometer data from healthy subjects performing activities of daily living - ADLs (source dataset), to extract features relevant to multi-sensor accelerometer gait data (target dataset) for Parkinson's disease classification. We train the pre-trained model using the source dataset and use it as a feature extractor. We show that the features extracted for the target dataset can be used to train an effective classification model. Our pre-trained source model consists of a convolutional autoencoder, and the target classification model is a simple multi-layer perceptron model. We explore two different pre-trained source models, trained using different activity groups, and analyze the influence the choice of pre-trained model has over the task of Parkinson's disease classification.        △ Less","6 May, 2020","cs.LG,cs.CV,stat.ML",
"              The Pace and Pulse of the Fight against Coronavirus across the US, A Google Trends Approach          ",2005.02489,https://arxiv.org/abs/2005.02489,https://arxiv.org/pdf/2005.02489,"Authors:TichakundaMangono,PeterSmittenaar,YaelCaplan,VincentS.Huang,StaciSutermaster,HannahKemp,SemaK.Sgaier","        The coronavirus pandemic is impacting our lives at unprecedented speed and scale - including how we eat and work, what we worry about, how much we move, and our ability to earn. Google Trends can be used as a proxy for what people are thinking, needing, and planning. We use it to provide both insights into, and potential indicators of, important changes in information-seeking patterns during pandemics like COVID-19. Key questions we address are: (1) What is the relationship between the coronavirus outbreak and internet searches related to healthcare seeking, government support programs, media sources of different ideologies, planning around social activities, travel, and food, and new coronavirus-specific behaviors and concerns?; (2) How does the popularity of search terms differ across states and regions and can we explain these differences?; (3) Can we find distinct, tangible search patterns across states suggestive of policy gaps to inform pandemic response? (4) Does Google Trends data correlate with and potentially precede real-life events? We suggest strategic shifts for policy makers to improve the precision and effectiveness of non-pharmaceutical interventions (NPIs) and recommend the development of a real-time dashboard as a decision-making tool. Methods used include trend analysis of US search data; geographic analyses of the differences in search popularity across US states during March 1st to April 15th, 2020; and Principal Component Analyses (PCA) to extract search patterns across states.        △ Less","5 May, 2020",cs.CY,
              Performance of RPL in Healthcare Wireless Sensor Network          ,2005.02454,https://arxiv.org/abs/2005.02454,https://arxiv.org/pdf/2005.02454,"Authors:BassamAl-Shargabi,MohammedAleswid",        The new advances of the Internet of Things (IoT) technology can be utilized to promote service delivery in several real-life applications such as healthcare systems. The Routing Protocol for Low Power and Loss Network (RPL) is a routing protocol designed to serve as a proper routing protocol for packets in Wireless Sensor Networks (WSN). Among the most prominent issues exist in the RPL protocol are packet loss within the WSN and sensors power consumption especially in healthcare WSNs. Multiple Objective Functions (OF) in RPL intended to find the routes from source nodes to a destination node. This paper presents an evaluation to discover which OF is more efficient for a WSN in a healthcare scenario where the Packet Delivery Ratio (PDR) of WSN and the sensors' power consumption are prominent concerns. Expected transmission Count (ETX) and Objective Function Zero (OF0) of RPL were examined in various network densities and network topologies such as the grid and random topology. The simulation outcomes revealed that the OF0 is more efficient regarding the PDR and power consumption compared to the ETX in random        △ Less,"5 April, 2020","cs.NI,cs.PF",10.30534/ijeter/2020/31832020 
              Self-Training with Improved Regularization for Few-Shot Chest X-Ray Classification          ,2005.02231,https://arxiv.org/abs/2005.02231,https://arxiv.org/pdf/2005.02231,"Authors:DeeptaRajan,JayaramanJ.Thiagarajan,AlexandrosKarargyris,SatyanandaKashyap","        Automated diagnostic assistants in healthcare necessitate accurate AI models that can be trained with limited labeled data, can cope with severe class imbalances and can support simultaneous prediction of multiple disease conditions. To this end, we present a novel few-shot learning approach that utilizes a number of key components to enable robust modeling in such challenging scenarios. Using an important use-case in chest X-ray classification, we provide several key insights on the effective use of data augmentation, self-training via distillation and confidence tempering for few-shot learning in medical imaging. Our results show that using only ~10% of the labeled data, we can build predictive models that match the performance of classifiers trained in a large-scale data setting.        △ Less","2 May, 2020","cs.CV,cs.LG",
              AR-Therapist: Design and Simulation of an AR-Game Environment as a CBT for Patients with ADHD          ,2005.02189,https://arxiv.org/abs/2005.02189,https://arxiv.org/pdf/2005.02189,"Authors:SaadAlqithami,MusaadAlzahrani,AbdulkareemAlzahrani,AhmedMustafa","        Attention Deficit Hyperactivity Disorder is one of the most common neurodevelopmental disorders in which patients have difficulties related to inattention, hyperactivity, and impulsivity. Those patients are in need of a psychological therapy use Cognitive Behavioral Therapy (CBT) to enhance the way they think and behave. This type of therapy is mostly common in treating patients with anxiety and depression but also is useful in treating autism, obsessive compulsive disorder and post-traumatic stress disorder. A major limitation of traditional CBT is that therapists may face difficulty in optimizing patients' neuropsychological stimulus following a specified treatment plan. Other limitations include availability, accessibility and level-of-experience of the therapists. Hence, this paper aims to design and simulate a generic cognitive model that can be used as an appropriate alternative treatment to traditional CBT, we term as ""AR-Therapist."" This model takes advantage of the current developments of augmented reality to engage patients in both real and virtual game-based environments.        △ Less","1 May, 2020","cs.HC,cs.GR",10.3390/healthcare7040146 
              Stealing Links from Graph Neural Networks          ,2005.02131,https://arxiv.org/abs/2005.02131,https://arxiv.org/pdf/2005.02131,"Authors:XinleiHe,JinyuanJia,MichaelBackes,NeilZhenqiangGong,YangZhang","        Graph data, such as chemical networks and social networks, may be deemed confidential/private because the data owner often spends lots of resources collecting the data or the data contains sensitive information, e.g., social relationships. Recently, neural networks were extended to graph data, which are known as graph neural networks (GNNs). Due to their superior performance, GNNs have many applications, such as healthcare analytics, recommender systems, and fraud detection. In this work, we propose the first attacks to steal a graph from the outputs of a GNN model that is trained on the graph. Specifically, given a black-box access to a GNN model, our attacks can infer whether there exists a link between any pair of nodes in the graph used to train the model. We call our attacks link stealing attacks. We propose a threat model to systematically characterize an adversary's background knowledge along three dimensions which in total leads to a comprehensive taxonomy of 8 different link stealing attacks. We propose multiple novel methods to realize these 8 attacks. Extensive experiments on 8 real-world datasets show that our attacks are effective at stealing links, e.g., AUC (area under the ROC curve) is above 0.95 in multiple cases. Our results indicate that the outputs of a GNN model reveal rich information about the structure of the graph used to train the model.        △ Less","5 October, 2020","cs.CR,cs.LG",
              Dual Stage Classification of Hand Gestures using Surface Electromyogram          ,2005.01711,https://arxiv.org/abs/2005.01711,https://arxiv.org/pdf/2005.01711,"Authors:KarushSuri,RinkiGupta","        Surface electromyography (sEMG) is becoming exceeding useful in applications involving analysis of human motion such as in human-machine interface, assistive technology, healthcare and prosthetic development. The proposed work presents a novel dual stage classification approach for classification of grasping gestures from sEMG signals. A statistical assessment of these activities is presented to determine the similar characteristics between the considered activities. Similar activities are grouped together. In the first stage of classification, an activity is identified as belonging to a group, which is then further classified as one of the activities within the group in the second stage of classification. The performance of the proposed approach is compared to the conventional single stage classification approach in terms of classification accuracies. The classification accuracies obtained using the proposed dual stage classification are significantly higher as compared to that for single stage classification.        △ Less","26 April, 2020","eess.SP,cs.LG,cs.NE,stat.ML",10.1109/SPIN.2018.8474145 
"              Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems          ",2005.01643,https://arxiv.org/abs/2005.01643,https://arxiv.org/pdf/2005.01643,"Authors:SergeyLevine,AviralKumar,GeorgeTucker,JustinFu","        In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.        △ Less","4 May, 2020","cs.LG,cs.AI,stat.ML",
"              StackGenVis: Alignment of Data, Algorithms, and Models for Stacking Ensemble Learning Using Performance Metrics          ",2005.01575,https://arxiv.org/abs/2005.01575,https://arxiv.org/pdf/2005.01575,"Authors:AngelosChatzimparmpas,RafaelM.Martins,KostiantynKucher,AndreasKerren","        In machine learning (ML), ensemble methods such as bagging, boosting, and stacking are widely-established approaches that regularly achieve top-notch predictive performance. Stacking (also called ""stacked generalization"") is an ensemble method that combines heterogeneous base models, arranged in at least one layer, and then employs another metamodel to summarize the predictions of those models. Although it may be a highly-effective approach for increasing the predictive performance of ML, generating a stack of models from scratch can be a cumbersome trial-and-error process. This challenge stems from the enormous space of available solutions, with different sets of data instances and features that could be used for training, several algorithms to choose from, and instantiations of these algorithms using diverse parameters (i.e., models) that perform differently according to various metrics. In this work, we present a knowledge generation model, which supports ensemble learning with the use of visualization, and a visual analytics system for stacked generalization. Our system, StackGenVis, assists users in dynamically adapting performance metrics, managing data instances, selecting the most important features for a given data set, choosing a set of top-performant and diverse algorithms, and measuring the predictive performance. In consequence, our proposed tool helps users to decide between distinct models and to reduce the complexity of the resulting stack by removing overpromising and underperforming models. The applicability and effectiveness of StackGenVis are demonstrated with two use cases: a real-world healthcare data set and a collection of data related to sentiment/stance detection in texts. Finally, the tool has been evaluated through interviews with three ML experts.        △ Less","17 September, 2020","cs.LG,cs.HC,stat.ML",
              Interactive distributed cloud-based web-server systems for the smart healthcare industry          ,2005.01442,https://arxiv.org/abs/2005.01442,https://arxiv.org/pdf/2005.01442,Authors:AlmagulBaurzhanovnaKondybayeva,"        The work aims to investigate the possible contemporary interactive cloud based solutions in the fields of the applied medicine for the smart Healthcare as the data visualization open-source free system distributed under the MIT license. A comparative study of a number of the well-known implementations of the Ray Casting algorithms was studied. A new method of numerical calculus is proposed for calculating the volume -- the method of spheres, as well as a proposal for paralleling the algorithm on graphic accelerators in a linearly homogeneous computing environment using the block decomposition methods. For the artifacts control -- algorithm of the cubic interpolation was used. The cloud server architecture was proposed.        △ Less","14 April, 2020","cs.OH,cs.CY",
              Microfluidic QCSK Transmitter and Receiver Design for Molecular Communication          ,2005.01353,https://arxiv.org/abs/2005.01353,https://arxiv.org/pdf/2005.01353,"Authors:DadiBi,YanshaDeng","        The design of components with molecular communication (MC) functionalities can bring an opportunity to enable some emerging applications in fields from personal healthcare to modern industry. In this paper, we propose the designs of the microfluidic transmitter and receiver with quadruple concentration shift keying (QCSK) modulation and demodulation functionalities. To do so, we first present an AND gate design, and then apply it to the QCSK transmitter and receiver design. The QCSK transmitter is capable of modulating two input signals to four different concentration levels, and the QCSK receiver can demodulate a received signal to two outputs. More importantly, we also establish a mathematical framework to theoretically characterize our proposed microfluidic circuits. Based on this, we first derive the output concentration distribution of our proposed AND gate design, and provide the insight into the selection of design parameters to ensure an exhibition of desired behaviour. We further derive the output concentration distributions of the QCSK transmitter and receiver. Simulation results obtained in COMSOL Multiphysics not only show the desired behaviour of all the proposed microfluidic circuits, but also demonstrate the accuracy of the proposed mathematical framework.        △ Less","11 May, 2020","cs.ET,eess.SP",
              Deep Convolutional Neural Networks to Diagnose COVID-19 and other Pneumonia Diseases from Posteroanterior Chest X-Rays          ,2005.00845,https://arxiv.org/abs/2005.00845,https://arxiv.org/pdf/2005.00845,Authors:PierreG.B.Moutounet-Cartan,"        The article explores different deep convolutional neural network architectures trained and tested on posteroanterior chest X-rays of 327 patients who are healthy (152 patients), diagnosed with COVID-19 (125), and other types of pneumonia (48). In particular, this paper looks at the deep convolutional neural networks VGG16 and VGG19, InceptionResNetV2 and InceptionV3, as well as Xception, all followed by a flat multi-layer perceptron and a final 30% drop-out. The paper has found that the best performing network is VGG16 with a final 3030% drop-out trained over 3 classes (COVID-19, No Finding, Other Pneumonia). It has an internal cross-validated accuracy of 93.9(±3.4)93.9(\pm3.4)%, a COVID-19 sensitivity of 87.7(−1.9,+2)87.7(-1.9,+2)%, and a No Finding sensitivity of 96.8(±0.8)96.8(\pm0.8)%. The respective external cross-validated values are 84.1(±13.5)84.1(\pm13.5)%, 87.7(−1.9,2)87.7(-1.9,2)%, and 96.8(±0.8)96.8(\pm0.8)%. The model optimizer was Adam with a 1e-4 learning rate, and categorical cross-entropy loss. It is hoped that, once this research will be put to practice in hospitals, healthcare professionals will be able in the medium to long-term to diagnosing through machine learning tools possible pneumonia, and if detected, whether it is linked to a COVID-19 infection, allowing the detection of new possible COVID-19 foyers after the end of possible ""stop-and-go"" lockdowns as expected by until a vaccine is found and widespread. Furthermore, in the short-term, it is hoped practitioners can compare the diagnosis from the deep convolutional neural networks with possible RT-PCR testing results, and if clashing, a Computed Tomography could be performed as they are more accurate in showing COVID-19 pneumonia.        △ Less","2 May, 2020","eess.IV,cs.LG,stat.ML",
              Deep ConvLSTM with self-attention for human activity decoding using wearables          ,2005.00698,https://arxiv.org/abs/2005.00698,https://arxiv.org/pdf/2005.00698,"Authors:SatyaP.Singh,AiméLay-Ekuakille,DeepakGangwar,MadanKumarSharma,SukritGupta","        Decoding human activity accurately from wearable sensors can aid in applications related to healthcare and context awareness. The present approaches in this domain use recurrent and/or convolutional models to capture the spatio-temporal features from time series data from multiple sensors. We propose a deep neural network architecture that not only captures the spatio-temporal features of multiple sensor time series data, but also selects, learns important time points by utilizing a self-attention mechanism. We show the validity of the proposed approach across different data sampling strategies on six public datasets and demonstrate that the self-attention mechanism gave significant improvement in performance over deep networks using a combination of recurrent and convolution networks. We also show that the proposed approach gave a statistically significant performance enhancement over previous state-of-the-art methods for the tested datasets. The proposed methods open avenues for better decoding of human activity from multiple body sensors over extended periods of time. The code implementation for the proposed model is available at https://github.com/isukrit/encodingHumanActivity        △ Less","2 May, 2020","cs.HC,cs.LG,eess.SP",
              Attend to Medical Ontologies: Content Selection for Clinical Abstractive Summarization          ,2005.00163,https://arxiv.org/abs/2005.00163,https://arxiv.org/pdf/2005.00163,"Authors:SajadSotudeh,NazliGoharian,RossW.Filice","        Sequence-to-sequence (seq2seq) network is a well-established model for text summarization task. It can learn to produce readable content; however, it falls short in effectively identifying key regions of the source. In this paper, we approach the content selection problem for clinical abstractive summarization by augmenting salient ontological terms into the summarizer. Our experiments on two publicly available clinical data sets (107,372 reports of MIMIC-CXR, and 3,366 reports of OpenI) show that our model statistically significantly boosts state-of-the-art results in terms of Rouge metrics (with improvements: 2.9% RG-1, 2.5% RG-2, 1.9% RG-L), in the healthcare domain where any range of improvement impacts patients' welfare.        △ Less","30 April, 2020",cs.CL,
              Activity Detection from Wearable Electromyogram Sensors using Hidden Markov Model          ,2005.00107,https://arxiv.org/abs/2005.00107,https://arxiv.org/pdf/2005.00107,"Authors:RinkiGupta,KarushSuri","        Surface electromyography (sEMG) has gained significant importance during recent advancements in consumer electronics for healthcare systems, gesture analysis and recognition and sign language communication. For such a system, it is imperative to determine the regions of activity in a continuously recorded sEMG signal. The proposed work provides a novel activity detection approach based on Hidden Markov Models (HMM) using sEMG signals recorded when various hand gestures are performed. Detection procedure is designed based on a probabilistic outlook by making use of mathematical models. The requirement of a threshold for activity detection is obviated making it subject and activity independent. Correctness of the predicted outputs is asserted by classifying the signal segments around the detected transition regions as activity or rest. Classified outputs are compared with the transition regions in a stimulus given to the subject to perform the activity. The activity onsets are detected with an average of 96.25% accuracy whereas the activity termination regions with an average of 87.5% accuracy with the considered set of six activities and four subjects.        △ Less","26 April, 2020","eess.SP,cs.LG,stat.ML",10.1109/ICCMC.2018.8488070 
              Consumer Wearables and Affective Computing for Wellbeing Support          ,2005.00093,https://arxiv.org/abs/2005.00093,https://arxiv.org/pdf/2005.00093,"Authors:StanisławSaganowski,PrzemysławKazienko,MaciejDzieżyc,PatrycjaJakimów,JoannaKomoszyńska,WeronikaMichalska,AnnaDutkowiak,AdamPolak,AdamDziadek,MichałUjma","        Wearables equipped with pervasive sensors enable us to monitor physiological and behavioral signals in our everyday life. We propose the WellAff system able to recognize affective states for wellbeing support. It also includes health care scenarios, in particular patients with chronic kidney disease (CKD) suffering from bipolar disorders. For the need of a large-scale field study, we revised over 50 off-the-shelf devices in terms of usefulness for emotion, stress, meditation, sleep, and physical activity recognition and analysis. Their usability directly comes from the types of sensors they possess as well as the quality and availability of raw signals. We found there is no versatile device suitable for all purposes. Using Empatica E4 and Samsung Galaxy Watch, we have recorded physiological signals from 11 participants over many weeks. The gathered data enabled us to train a classifier that accurately recognizes strong affective states.        △ Less","1 September, 2020","cs.HC,cs.CY",
              Two Burning Questions on COVID-19: Did shutting down the economy help? Can we (partially) reopen the economy without risking the second wave?          ,2005.00072,https://arxiv.org/abs/2005.00072,https://arxiv.org/pdf/2005.00072,"Authors:AnishAgarwal,AbdullahAlomar,ArnabSarker,DevavratShah,DennisShen,CindyYang","        As we reach the apex of the COVID-19 pandemic, the most pressing question facing us is: can we even partially reopen the economy without risking a second wave? We first need to understand if shutting down the economy helped. And if it did, is it possible to achieve similar gains in the war against the pandemic while partially opening up the economy? To do so, it is critical to understand the effects of the various interventions that can be put into place and their corresponding health and economic implications. Since many interventions exist, the key challenge facing policy makers is understanding the potential trade-offs between them, and choosing the particular set of interventions that works best for their circumstance. In this memo, we provide an overview of Synthetic Interventions (a natural generalization of Synthetic Control), a data-driven and statistically principled method to perform what-if scenario planning, i.e., for policy makers to understand the trade-offs between different interventions before having to actually enact them. In essence, the method leverages information from different interventions that have already been enacted across the world and fits it to a policy maker's setting of interest, e.g., to estimate the effect of mobility-restricting interventions on the U.S., we use daily death data from countries that enforced severe mobility restrictions to create a ""synthetic low mobility U.S."" and predict the counterfactual trajectory of the U.S. if it had indeed applied a similar intervention. Using Synthetic Interventions, we find that lifting severe mobility restrictions and only retaining moderate mobility restrictions (at retail and transit locations), seems to effectively flatten the curve. We hope this provides guidance on weighing the trade-offs between the safety of the population, strain on the healthcare system, and impact on the economy.        △ Less","10 May, 2020","econ.EM,cs.LG,stat.AP",
              Indirect Identification of Psychosocial Risks from Natural Language          ,2004.14554,https://arxiv.org/abs/2004.14554,https://arxiv.org/pdf/2004.14554,"Authors:KristenC.Allen,AlexDavis,TamarKrishnamurti","        During the perinatal period, psychosocial health risks, including depression and intimate partner violence, are associated with serious adverse health outcomes for parents and children. To appropriately intervene, healthcare professionals must first identify those at risk, yet stigma often prevents people from directly disclosing the information needed to prompt an assessment. We examine indirect methods of eliciting and analyzing information that could indicate psychosocial risks. Short diary entries by peripartum women exhibit thematic patterns, extracted by topic modeling, and emotional perspective, drawn from dictionary-informed sentiment features. Using these features, we use regularized regression to predict screening measures of depression and psychological aggression by an intimate partner. Journal text entries quantified through topic models and sentiment features show promise for depression prediction, with performance almost as good as closed-form questions. Text-based features were less useful for prediction of intimate partner violence, but moderately indirect multiple-choice questioning allowed for detection without explicit disclosure. Both methods may serve as an initial or complementary screening approach to detecting stigmatized risks.        △ Less","29 April, 2020","cs.CL,cs.CY",
              Calibrating Healthcare AI: Towards Reliable and Interpretable Deep Predictive Models          ,2004.14480,https://arxiv.org/abs/2004.14480,https://arxiv.org/pdf/2004.14480,"Authors:JayaramanJ.Thiagarajan,PrasannaSattigeri,DeeptaRajan,BindyaVenkatesh","        The wide-spread adoption of representation learning technologies in clinical decision making strongly emphasizes the need for characterizing model reliability and enabling rigorous introspection of model behavior. While the former need is often addressed by incorporating uncertainty quantification strategies, the latter challenge is addressed using a broad class of interpretability techniques. In this paper, we argue that these two objectives are not necessarily disparate and propose to utilize prediction calibration to meet both objectives. More specifically, our approach is comprised of a calibration-driven learning method, which is also used to design an interpretability technique based on counterfactual reasoning. Furthermore, we introduce \textit{reliability plots}, a holistic evaluation mechanism for model reliability. Using a lesion classification problem with dermoscopy images, we demonstrate the effectiveness of our approach and infer interesting insights about the model behavior.        △ Less","27 April, 2020","cs.LG,stat.ML",
              Advancing computerized cognitive training for early Alzheimer's disease in a Covid-19 pandemic and post-pandemic world          ,2004.14344,https://arxiv.org/abs/2004.14344,https://arxiv.org/pdf/2004.14344,"Authors:KayleeA.Bodner,TerryE.Goldberg,D.P.Devanand,P.MuraliDoraiswamy","        The COVID-19 pandemic has transformed mobile health applications and telemedicine from nice to have tools into essential healthcare infrastructure. This need is particularly great for the elderly who, due to their greater risk for infection, may avoid medical facilities or be required to self-isolate. These are also the very groups at highest risk for cognitive decline. For example, during the COVID-19 pandemic artificially intelligent conversational agents were employed by hospitals and government agencies (such as the CDC) to field queries from patients about symptoms and treatments. Digital health tools also proved invaluable to provide neuropsychiatric and psychological self-help to people isolated at home or in retirement centers and nursing homes.        △ Less","15 May, 2020",cs.CY,
              Inf-Net: Automatic COVID-19 Lung Infection Segmentation from CT Images          ,2004.14133,https://arxiv.org/abs/2004.14133,https://arxiv.org/pdf/2004.14133,"Authors:Deng-PingFan,TaoZhou,Ge-PengJi,YiZhou,GengChen,HuazhuFu,JianbingShen,LingShao","        Coronavirus Disease 2019 (COVID-19) spread globally in early 2020, causing the world to face an existential health crisis. Automated detection of lung infections from computed tomography (CT) images offers a great potential to augment the traditional healthcare strategy for tackling COVID-19. However, segmenting infected regions from CT slices faces several challenges, including high variation in infection characteristics, and low intensity contrast between infections and normal tissues. Further, collecting a large amount of data is impractical within a short time period, inhibiting the training of a deep model. To address these challenges, a novel COVID-19 Lung Infection Segmentation Deep Network (Inf-Net) is proposed to automatically identify infected regions from chest CT slices. In our Inf-Net, a parallel partial decoder is used to aggregate the high-level features and generate a global map. Then, the implicit reverse attention and explicit edge-attention are utilized to model the boundaries and enhance the representations. Moreover, to alleviate the shortage of labeled data, we present a semi-supervised segmentation framework based on a randomly selected propagation strategy, which only requires a few labeled images and leverages primarily unlabeled data. Our semi-supervised framework can improve the learning ability and achieve a higher performance. Extensive experiments on our COVID-SemiSeg and real CT volumes demonstrate that the proposed Inf-Net outperforms most cutting-edge segmentation models and advances the state-of-the-art performance.        △ Less","21 May, 2020","eess.IV,cs.CV,cs.LG",
              FitChat: Conversational Artificial Intelligence Interventions for Encouraging Physical Activity in Older Adults          ,2004.14067,https://arxiv.org/abs/2004.14067,https://arxiv.org/pdf/2004.14067,"Authors:NirmalieWiratunga,KayCooper,AnjanaWijekoon,ChamathPalihawadana,VanessaMendham,EhudReiter,KyleMartin","        Delivery of digital behaviour change interventions which encourage physical activity has been tried in many forms. Most often interventions are delivered as text notifications, but these do not promote interaction. Advances in conversational AI have improved natural language understanding and generation, allowing AI chatbots to provide an engaging experience with the user. For this reason, chatbots have recently been seen in healthcare delivering digital interventions through free text or choice selection. In this work, we explore the use of voice-based AI chatbots as a novel mode of intervention delivery, specifically targeting older adults to encourage physical activity. We co-created ""FitChat"", an AI chatbot, with older adults and we evaluate the first prototype using Think Aloud Sessions. Our thematic evaluation suggests that older adults prefer voice-based chat over text notifications or free text entry and that voice is a powerful mode for encouraging motivation.        △ Less","29 April, 2020","cs.HC,cs.AI,cs.CY",
              Neural Additive Models: Interpretable Machine Learning with Neural Nets          ,2004.13912,https://arxiv.org/abs/2004.13912,https://arxiv.org/pdf/2004.13912,"Authors:RishabhAgarwal,NicholasFrosst,XuezhouZhang,RichCaruana,GeoffreyE.Hinton","        Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but can be more easily applied to real-world problems.        △ Less","28 April, 2020","cs.LG,cs.AI,stat.ML",
              Unifying Neural Learning and Symbolic Reasoning for Spinal Medical Report Generation          ,2004.13577,https://arxiv.org/abs/2004.13577,https://arxiv.org/pdf/2004.13577,"Authors:ZhongyiHan,BenzhengWei,YilongYin,ShuoLi","        Automated medical report generation in spine radiology, i.e., given spinal medical images and directly create radiologist-level diagnosis reports to support clinical decision making, is a novel yet fundamental study in the domain of artificial intelligence in healthcare. However, it is incredibly challenging because it is an extremely complicated task that involves visual perception and high-level reasoning processes. In this paper, we propose the neural-symbolic learning (NSL) framework that performs human-like learning by unifying deep neural learning and symbolic logical reasoning for the spinal medical report generation. Generally speaking, the NSL framework firstly employs deep neural learning to imitate human visual perception for detecting abnormalities of target spinal structures. Concretely, we design an adversarial graph network that interpolates a symbolic graph reasoning module into a generative adversarial network through embedding prior domain knowledge, achieving semantic segmentation of spinal structures with high complexity and variability. NSL secondly conducts human-like symbolic logical reasoning that realizes unsupervised causal effect analysis of detected entities of abnormalities through meta-interpretive learning. NSL finally fills these discoveries of target diseases into a unified template, successfully achieving a comprehensive medical report generation. When it employed in a real-world clinical dataset, a series of empirical studies demonstrate its capacity on spinal medical report generation as well as show that our algorithm remarkably exceeds existing methods in the detection of spinal structures. These indicate its potential as a clinical tool that contributes to computer-aided diagnosis.        △ Less","28 April, 2020","cs.CV,cs.AI,cs.LG,eess.IV",
              A dynamic modeling tool for estimating healthcare demand from the COVID19 epidemic and evaluating population-wide interventions          ,2004.13544,https://arxiv.org/abs/2004.13544,https://arxiv.org/pdf/2004.13544,"Authors:GabrielRainisch,EduardoA.Undurraga,GerardoChowell","        Objectives. Public health officials need tools to assist with anticipating the healthcare resources required to confront the SARS-COV-2 pandemic. We built a modeling tool to aid practicing public health officials with estimating healthcare demand from the pandemic in their jurisdictions and to evaluate the potential impacts of population-wide social-distancing interventions. Methods. The tool uses a SEIR compartmental model to project the local spread of the pandemic. Users input case counts, healthcare resources, and select intervention strategies to evaluate. Outputs include the number of infections and deaths with and without intervention, and the demand for hospital and critical care beds and ventilators relative to existing capacity. We illustrate the tool using data from three regions of Chile. Results. Our scenarios indicate a surge in COVID-19 patients could overwhelm Chilean hospitals by June, peaking in July or August at 6 to 50 times the current supply of beds and ventilators. A lockdown strategy or combination of case isolation, home quarantine, social distancing of individuals greater than 70 years, and telework interventions may keep treatment demand below capacity. Conclusions. Aggressive interventions can avert substantial morbidity and mortality from COVID-19. Our tool permits rapid evaluation of locally-applicable policy scenarios and updating of results as new data become available.        △ Less","13 May, 2020",q-bio.PE,
              Project 1000 x 1000: Centrifugal melt spinning for distributed manufacturing of N95 filtering facepiece respirators          ,2004.13494,https://arxiv.org/abs/2004.13494,https://arxiv.org/pdf/2004.13494,"Authors:AntonMolina,PranavVyas,NikitaKhlystov,ShailabhKumar,AnestaKothari,DaveDeriso,ZhiruLiu,SamhitaBanavar,EliottFlaum,ManuPrakash","        The COVID-19 pandemic has caused a global shortage of personal protective equipment. While existing supply chains are struggling to meet the surge in demand, the limited supply of N95 filtering facepiece respirators (FFRs) has placed healthcare workers at risk. This paper presents a method for scalable and distributed manufacturing of FFR filter material based on a combination of centrifugal melt spinning utilizing readily available cotton candy machines as an example. The proposed method produces nonwoven polypropylene fabric material with filtering efficiency of up to 96% for particles 0.30-0.49 μm in diameter. We additionally demonstrate a scalable means to test for filtration efficiency and pressure drop to ensure a standardized degree of quality in the output material. We perform preliminary optimization of relevant parameters for scale-up and propose that this is a viable method to rapidly produce up to one million N95 FFRs per day in distributed manner with just six machines per site operating across 200 locations. We share this work as a starting point for others to rapidly construct, replicate and develop their own affordable modular processes aimed at producing high quality filtration material to address the current FFR shortage globally.        △ Less","26 April, 2020","physics.app-ph,cond-mat.soft",
              MultiMBNN: Matched and Balanced Causal Inference with Neural Networks          ,2004.13446,https://arxiv.org/abs/2004.13446,https://arxiv.org/pdf/2004.13446,"Authors:AnkitSharma,GarimaGupta,RanjithaPrasad,ArnabChatterjee,LovekeshVig,GautamShroff","        Causal inference (CI) in observational studies has received a lot of attention in healthcare, education, ad attribution, policy evaluation, etc. Confounding is a typical hazard, where the context affects both, the treatment assignment and response. In a multiple treatment scenario, we propose the neural network based MultiMBNN, where we overcome confounding by employing generalized propensity score based matching, and learning balanced representations. We benchmark the performance on synthetic and real-world datasets using PEHE, and mean absolute percentage error over ATE as metrics. MultiMBNN outperforms the state-of-the-art algorithms for CI such as TARNet and Perfect Match (PM).        △ Less","29 April, 2020","stat.ME,cs.LG,cs.MA",
              Low-tech solutions for the COVID19 supply chain crisis          ,2004.13192,https://arxiv.org/abs/2004.13192,https://arxiv.org/pdf/2004.13192,"Authors:AndreaMArmani,DarrellEHurt,DarrylHwang,MeghanCMcCarthy,AlexisScholtz","        A global effort is ongoing in the scientific community and in the Maker Movement, which focuses on creating devices and tinkering with them, to reverse engineer commercial medical equipment and get it to healthcare workers. For these low-tech solutions to have a real impact, it is important for them to coalesce around approved designs.        △ Less","27 April, 2020","physics.soc-ph,physics.med-ph",
              Optimizing AI for Teamwork          ,2004.13102,https://arxiv.org/abs/2004.13102,https://arxiv.org/pdf/2004.13102,"Authors:GaganBansal,BesmiraNushi,EceKamar,EricHorvitz,DanielS.Weld","        In many high-stakes domains such as criminal justice, finance, and healthcare, AI systems may recommend actions to a human expert responsible for final decisions, a context known as AI-advised decision making. When AI practitioners deploy the most accurate system in these domains, they implicitly assume that the system will function alone in the world. We argue that the most accurate AI team-mate is not necessarily the em best teammate; for example, predictable performance is worth a slight sacrifice in AI accuracy. So, we propose training AI systems in a human-centered manner and directly optimizing for team performance. We study this proposal for a specific type of human-AI team, where the human overseer chooses to accept the AI recommendation or solve the task themselves. To optimize the team performance we maximize the team's expected utility, expressed in terms of quality of the final decision, cost of verifying, and individual accuracies. Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the improvements in utility while being small and varying across datasets and parameters (such as cost of mistake), are real and consistent with our definition of team utility. We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on human-centered optimization problems motivated by human-AI collaborations.        △ Less","25 June, 2020","cs.AI,cs.HC,cs.LG",
              Secure Non-public Health Enterprise Networks          ,2004.13085,https://arxiv.org/abs/2004.13085,https://arxiv.org/pdf/2004.13085,"Authors:MonaGhassemian,MaxSmith-Creasey,MaziarNekovee","        Increasing demand for secure remote operation in industry and technology advancements to support delivering efficient services and tele-mentoring have opened a new market in healthcare sector and emergency services based on 5G and Tactile internet capabilities. In a connected world, hospitals would benefit from providing the on-time availability either for continuous health monitoring or critical services to the citizens in need. In this paper, we propose a secure non-public health enterprise network concept to enable an end-to-end secure and location-agnostic communication between a patient and a healthcare service provider, and other contacts with patients consent either in case of an emergency or to be stored in the medical records. We present how applying non-public enterprise networks can address market demand in health care sector for improved end-to-end security and privacy when dealing with personal and critical information. We present the three tier architecture model describing continuous authentication mechanisms based on biometric collection as well as the dynamic network solutions in the healthcare domain. The biometric collection can be done using ambient or IoT sensors as well as wearable or implantable devices to monitor the patient unobtrusively. Furthermore, end-to-end security solutions should adapt dynamically based on the user profile and situation awareness to address the required level of security at the network side. We discuss the related research challenges for developing the presented non-public health enterprise platform and provide suggestions for future work based on the healthcare sector requirements and opportunities.        △ Less","27 April, 2020",cs.NI,
              Trans-NIH/Interagency Workshop on the Use and Development of Assistive Technology for the Aging Population and People with Chronic Disabilities          ,2004.12961,https://arxiv.org/abs/2004.12961,https://arxiv.org/pdf/2004.12961,"Authors:ElizabethMynatt,AliceBorrelli,SaraCzaja,ErinIturriaga,JeffKaye,WendyNilsen,DanSiewiorek,JohnStankovic","        The first baby boomer born in 1946 turned 65 in 2011 and the last baby boomer will turn 65 in 2029. By then, the total U.S. population over 65 is projected to be 71.5 million (compared with 44.6 million in 2013). The current (2014) median cost of a nursing home is 226aday(226 a day (82,490 per year), while assisted living is 3500amonth(3500 a month (42,000 per year). While the elderly population continues to get larger and costs will continue to rise, nearly ninety percent (90%) of people want to grow old in their own home and community and remain out of the hospital, nursing home or other institutional setting. New technologies could potentially allow older adults and people with disabilities to remain in their homes longer, reduce health care costs and enhance the quality of life.  In summary, there is a need for a new generation of research that addresses the complexity of supporting the quality of life and independence of a vast, diverse, and aging population. While there are common themes and needs in this research that we describe shortly, we must start by recognizing that there is more than one needed path and approach to meet these diverse needs. One path includes the tight integration of chronic disease management in the home with existing acute healthcare systems. Another path embraces comprehensive home health for improving nutrition and social connectedness while combating physical, cognitive and psychological ailments. Yet another path emphasizes wellness, consumer technologies and removing basic barriers to meaningful community participation. These paths will intersect in interesting ways for individuals, families, healthcare providers, and communities. However research is critically needed to illuminate these paths and to make measurable strides in our care and support for over 15% of our nation's citizens.        △ Less","27 April, 2020",cs.CY,
"              Sequential Interpretability: Methods, Applications, and Future Direction for Understanding Deep Learning Models in the Context of Sequential Data          ",2004.12524,https://arxiv.org/abs/2004.12524,https://arxiv.org/pdf/2004.12524,"Authors:BenjaminShickel,ParisaRashidi","        Deep learning continues to revolutionize an ever-growing number of critical application areas including healthcare, transportation, finance, and basic sciences. Despite their increased predictive power, model transparency and human explainability remain a significant challenge due to the ""black box"" nature of modern deep learning models. In many cases the desired balance between interpretability and performance is predominately task specific. Human-centric domains such as healthcare necessitate a renewed focus on understanding how and why these frameworks are arriving at critical and potentially life-or-death decisions. Given the quantity of research and empirical successes of deep learning for computer vision, most of the existing interpretability research has focused on image processing techniques. Comparatively, less attention has been paid to interpreting deep learning frameworks using sequential data. Given recent deep learning advancements in highly sequential domains such as natural language processing and physiological signal processing, the need for deep sequential explanations is at an all-time high. In this paper, we review current techniques for interpreting deep learning techniques involving sequential data, identify similarities to non-sequential methods, and discuss current limitations and future avenues of sequential interpretability research.        △ Less","26 April, 2020","cs.LG,stat.ML",
              Authentic Science Experiences with STEM Datasets: Post-secondary Results and Potential Gender Influences          ,2004.12448,https://arxiv.org/abs/2004.12448,https://arxiv.org/pdf/2004.12448,"Authors:AndriaC.Schwortz,AndreaC.Burrows","        Background: Dataset skills are used in STEM fields from healthcare work to astronomy research. Few fields explicitly teach students the skills to analyze datasets, and yet the increasing push for authentic science implies these skills should be taught.  Purpose: The overarching motivation is to understand learning of dataset skills within an astronomy context. Specifically, when participants work with a 200-entry Google Sheets dataset of astronomical data about quasars, what are they learning, how are they learning it, and who is doing the learning?  Sample: The authors studied a matched set of participants (n=87) consisting of 54 university undergraduate students (34 male, 18 female), and 33 science educators (16 male, 17 female).  Design and methods: Participants explored a three-phase dataset activity and were given an eight-question multiple-choice pre/post-test covering skills of analyzing datasets and astronomy content, with questions spanning Bloom's Taxonomy. Pre/post-test scores were compared and a t-test performed for subsamples by population.  Results: Participants exhibited learning of both dataset skills and astronomy content, indicating that dataset skills can be learned through this astronomy activity. Participants exhibited gains in both recall and synthesis questions, indicating learning is non-sequential. Female undergraduate students exhibited lower levels of learning than other populations.  Conclusions: Implications of the study include a stronger dataset focus in post-secondary STEM education and among science educators, and the need for further investigation into how instructors can ameliorate the challenges faced by female undergraduate students.        △ Less","26 April, 2020","physics.ed-ph,astro-ph.GA",10.1080/02635143.2020.1761783 
              AutoHR: A Strong End-to-end Baseline for Remote Heart Rate Measurement with Neural Searching          ,2004.12292,https://arxiv.org/abs/2004.12292,https://arxiv.org/pdf/2004.12292,"Authors:ZitongYu,XiaobaiLi,XuesongNiu,JingangShi,GuoyingZhao","        Remote photoplethysmography (rPPG), which aims at measuring heart activities without any contact, has great potential in many applications (e.g., remote healthcare). Existing end-to-end rPPG and heart rate (HR) measurement methods from facial videos are vulnerable to the less-constrained scenarios (e.g., with head movement and bad illumination). In this letter, we explore the reason why existing end-to-end networks perform poorly in challenging conditions and establish a strong end-to-end baseline (AutoHR) for remote HR measurement with neural architecture search (NAS). The proposed method includes three parts: 1) a powerful searched backbone with novel Temporal Difference Convolution (TDC), intending to capture intrinsic rPPG-aware clues between frames; 2) a hybrid loss function considering constraints from both time and frequency domains; and 3) spatio-temporal data augmentation strategies for better representation learning. Comprehensive experiments are performed on three benchmark datasets to show our superior performance on both intra- and cross-dataset testing.        △ Less","26 April, 2020",cs.CV,10.1109/LSP.2020.3007086 
              Privacy Preserving Distributed Machine Learning with Federated Learning          ,2004.12108,https://arxiv.org/abs/2004.12108,https://arxiv.org/pdf/2004.12108,"Authors:M.A.P.Chamikara,P.Bertok,I.Khalil,D.Liu,S.Camtepe","        Edge computing and distributed machine learning have advanced to a level that can revolutionize a particular organization. Distributed devices such as the Internet of Things (IoT) often produce a large amount of data, eventually resulting in big data that can be vital in uncovering hidden patterns, and other insights in numerous fields such as healthcare, banking, and policing. Data related to areas such as healthcare and banking can contain potentially sensitive data that can become public if they are not appropriately sanitized. Federated learning (FedML) is a recently developed distributed machine learning (DML) approach that tries to preserve privacy by bringing the learning of an ML model to data owners'. However, literature shows different attack methods such as membership inference that exploit the vulnerabilities of ML models as well as the coordinating servers to retrieve private data. Hence, FedML needs additional measures to guarantee data privacy. Furthermore, big data often requires more resources than available in a standard computer. This paper addresses these issues by proposing a distributed perturbation algorithm named as DISTPAB, for privacy preservation of horizontally partitioned data. DISTPAB alleviates computational bottlenecks by distributing the task of privacy preservation utilizing the asymmetry of resources of a distributed environment, which can have resource-constrained devices as well as high-performance computers. Experiments show that DISTPAB provides high accuracy, high efficiency, high scalability, and high attack resistance. Further experiments on privacy-preserving FedML show that DISTPAB is an excellent solution to stop privacy leaks in DML while preserving high data utility.        △ Less","25 April, 2020",cs.DB,
              SAIA: Split Artificial Intelligence Architecture for Mobile Healthcare System          ,2004.12059,https://arxiv.org/abs/2004.12059,https://arxiv.org/pdf/2004.12059,"Authors:DiZhuang,NamNguyen,KeyuChen,J.MorrisChang","        As the advancement of deep learning (DL), the Internet of Things and cloud computing techniques for biomedical and healthcare problems, mobile healthcare systems have received unprecedented attention. Since DL techniques usually require enormous amount of computation, most of them cannot be directly deployed on the resource-constrained mobile and IoT devices. Hence, most of the mobile healthcare systems leverage the cloud computing infrastructure, where the data collected by the mobile and IoT devices would be transmitted to the cloud computing platforms for analysis. However, in the contested environments, relying on the cloud might not be practical at all times. For instance, the satellite communication might be denied or disrupted. We propose SAIA, a Split Artificial Intelligence Architecture for mobile healthcare systems. Unlike traditional approaches for artificial intelligence (AI) which solely exploits the computational power of the cloud server, SAIA could not only relies on the cloud computing infrastructure while the wireless communication is available, but also utilizes the lightweight AI solutions that work locally on the client side, hence, it can work even when the communication is impeded. In SAIA, we propose a meta-information based decision unit, that could tune whether a sample captured by the client should be operated by the embedded AI (i.e., keeping on the client) or the networked AI (i.e., sending to the server), under different conditions. In our experimental evaluation, extensive experiments have been conducted on two popular healthcare datasets. Our results show that SAIA consistently outperforms its baselines in terms of both effectiveness and efficiency.        △ Less","9 May, 2020","cs.AI,cs.CV,cs.LG,cs.MM",
              Improving embedding efficiency for digital steganography by exploiting similarities between secret and cover images          ,2004.11974,https://arxiv.org/abs/2004.11974,https://arxiv.org/pdf/2004.11974,"Authors:AlanA.Abdulla,HarinSellahewa,SabahA.Jassim","        Digital steganography is becoming a common tool for protecting sensitive communications in various applications such as crime(terrorism) prevention whereby law enforcing personals need to remotely compare facial images captured at the scene of crime with faces databases of known criminals(suspects); exchanging military maps or surveillance video in hostile environment(situations); privacy preserving in the healthcare systems when storing or exchanging patient medical images(records); and prevent bank customers accounts(records) from being accessed illegally by unauthorized users. Existing digital steganography schemes for embedding secret images in cover image files tend not to exploit various redundancies in the secret image bit-stream to deal with the various conflicting requirements on embedding capacity, stego-image quality, and un-detectibility. This paper is concerned with the development of innovative image procedures and data hiding schemes that exploit, as well as increase, similarities between secret image bit-stream and the cover image LSB plane. This will be achieved in two novel steps involving manipulating both the secret and the cover images,prior to embedding, to achieve higher 0:1 ratio in both the secret image bit-stream and the cover image LSB plane. The above two steps strategy has been exploited to use a bit-plane(s) mapping technique, instead of bit-plane(s) replacement to make each cover pixel usable for secret embedding. This paper will demonstrate that this strategy produces stego-images that have minimal distortion, high embedding efficiency, reasonably good stego-image quality and robustness against 3 well-known targeted steganalysis tools.        △ Less","24 April, 2020",cs.MM,
              DFUC2020: Analysis Towards Diabetic Foot Ulcer Detection          ,2004.11853,https://arxiv.org/abs/2004.11853,https://arxiv.org/pdf/2004.11853,"Authors:BillCassidy,NeilD.Reeves,PappachanJoseph,DavidGillespie,ClaireO'Shea,SatyanRajbhandari,ArunG.Maiya,EibeFrank,AndrewBoulton,DavidArmstrong,BijanNajafi,JustinaWu,MoiHoonYap","        Every 20 seconds, a limb is amputated somewhere in the world due to diabetes. This is a global health problem that requires a global solution. The MICCAI challenge discussed in this paper, which concerns the automated detection of diabetic foot ulcers using machine learning techniques, will accelerate the development of innovative healthcare technology to address this unmet medical need. In an effort to improve patient care and reduce the strain on healthcare systems, recent research has focused on the creation of cloud-based detection algorithms. These can be consumed as a service by a mobile app that patients (or a carer, partner or family member) could use themselves at home to monitor their condition and to detect the appearance of a diabetic foot ulcer (DFU). Collaborative work between Manchester Metropolitan University, Lancashire Teaching Hospital and the Manchester University NHS Foundation Trust has created a repository of 4,000 DFU images for the purpose of supporting research toward more advanced methods of DFU detection. Based on a joint effort involving the lead scientists of the UK, US, India and New Zealand, this challenge will solicit original work, and promote interactions between researchers and interdisciplinary collaborations. This paper presents a dataset description and analysis, assessment methods, benchmark algorithms and initial evaluation results. It facilitates the challenge by providing useful insights into state-of-the-art and ongoing research. This grand challenge takes on even greater urgency in a peri and post-pandemic period, where stresses on resource utilization will increase the need for technology that allows people to remain active, healthy and intact in their home.        △ Less","16 August, 2020",cs.CV,
              A Hybrid Approach Combining Control Theory and AI for Engineering Self-Adaptive Systems          ,2004.11793,https://arxiv.org/abs/2004.11793,https://arxiv.org/pdf/2004.11793,"Authors:RicardoDinizCaldas,ArthurRodrigues,EricBerndGil,GenaínaNunesRodrigues,ThomasVogel,PatrizioPelliccione","        Control theoretical techniques have been successfully adopted as methods for self-adaptive systems design to provide formal guarantees about the effectiveness and robustness of adaptation mechanisms. However, the computational effort to obtain guarantees poses severe constraints when it comes to dynamic adaptation. In order to solve these limitations, in this paper, we propose a hybrid approach combining software engineering, control theory, and AI to design for software self-adaptation. Our solution proposes a hierarchical and dynamic system manager with performance tuning. Due to the gap between high-level requirements specification and the internal knob behavior of the managed system, a hierarchically composed components architecture seek the separation of concerns towards a dynamic solution. Therefore, a two-layered adaptive manager was designed to satisfy the software requirements with parameters optimization through regression analysis and evolutionary meta-heuristic. The optimization relies on the collection and processing of performance, effectiveness, and robustness metrics w.r.t control theoretical metrics at the offline and online stages. We evaluate our work with a prototype of the Body Sensor Network (BSN) in the healthcare domain, which is largely used as a demonstrator by the community. The BSN was implemented under the Robot Operating System (ROS) architecture, and concerns about the system dependability are taken as adaptation goals. Our results reinforce the necessity of performing well on such a safety-critical domain and contribute with substantial evidence on how hybrid approaches that combine control and AI-based techniques for engineering self-adaptive systems can provide effective adaptation.        △ Less","24 April, 2020",cs.SE,10.1145/3387939.3391595 
              Counterfactual Learning of Continuous Stochastic Policies          ,2004.11722,https://arxiv.org/abs/2004.11722,https://arxiv.org/pdf/2004.11722,"Authors:HoussamZenati,AlbertoBietti,MatthieuMartin,EustacheDiemert,JulienMairal","        Counterfactual reasoning from logged data has become increasingly important for many applications such as web advertising or healthcare. In this paper, we address the problem of counterfactual risk minimization (CRM) for learning a stochastic policy with continuous actions. First, we introduce a new modelling strategy based on a joint kernel embedding of contexts and actions, which overcomes the shortcomings of previous discretization strategies. Second, we empirically show that the optimization perspective of CRM is more important than previously thought, and we demonstrate the benefits of proximal point algorithms and differentiable estimators. Finally, we propose an evaluation protocol for offline policies in real-world logged systems, which is challenging since policies cannot be replayed on test data, and we release a new large-scale dataset along with multiple synthetic, yet realistic, evaluation setups.        △ Less","16 October, 2020","stat.ML,cs.LG",
              Deep Sentiment Classification and Topic Discovery on Novel Coronavirus or COVID-19 Online Discussions: NLP Using LSTM Recurrent Neural Network Approach          ,2004.11695,https://arxiv.org/abs/2004.11695,https://arxiv.org/pdf/2004.11695,"Authors:HamedJelodar,YongliWang,RitaOrji,HuchengHuang","        Internet forums and public social media, such as online healthcare forums, provide a convenient channel for users (people/patients) concerned about health issues to discuss and share information with each other. In late December 2019, an outbreak of a novel coronavirus (infection from which results in the disease named COVID-19) was reported, and, due to the rapid spread of the virus in other parts of the world, the World Health Organization declared a state of emergency. In this paper, we used automated extraction of COVID-19 related discussions from social media and a natural language process (NLP) method based on topic modeling to uncover various issues related to COVID-19 from public opinions. Moreover, we also investigate how to use LSTM recurrent neural network for sentiment classification of COVID-19 comments. Our findings shed light on the importance of using public opinions and suitable computational techniques to understand issues surrounding COVID-19 and to guide related decision-making.        △ Less","24 April, 2020","cs.IR,cs.CL",
              A Systematic Search over Deep Convolutional Neural Network Architectures for Screening Chest Radiographs          ,2004.11693,https://arxiv.org/abs/2004.11693,https://arxiv.org/pdf/2004.11693,"Authors:ArkaMitra,ArunavaChakravarty,NirmalyaGhosh,TandraSarkar,RamanathanSethuraman,DebdootSheet","        Chest radiographs are primarily employed for the screening of pulmonary and cardio-/thoracic conditions. Being undertaken at primary healthcare centers, they require the presence of an on-premise reporting Radiologist, which is a challenge in low and middle income countries. This has inspired the development of machine learning based automation of the screening process. While recent efforts demonstrate a performance benchmark using an ensemble of deep convolutional neural networks (CNN), our systematic search over multiple standard CNN architectures identified single candidate CNN models whose classification performances were found to be at par with ensembles. Over 63 experiments spanning 400 hours, executed on a 11:3 FP32 TensorTFLOPS compute system, we found the Xception and ResNet-18 architectures to be consistent performers in identifying co-existing disease conditions with an average AUC of 0.87 across nine pathologies. We conclude on the reliability of the models by assessing their saliency maps generated using the randomized input sampling for explanation (RISE) method and qualitatively validating them against manual annotations locally sourced from an experienced Radiologist. We also draw a critical note on the limitations of the publicly available CheXpert dataset primarily on account of disparity in class distribution in training vs. testing sets, and unavailability of sufficient samples for few classes, which hampers quantitative reporting due to sample insufficiency.        △ Less","24 April, 2020","cs.CV,cs.LG",
              Inside the Mind of Investors During the COVID-19 Pandemic: Evidence from the StockTwits Data          ,2004.11686,https://arxiv.org/abs/2004.11686,https://arxiv.org/pdf/2004.11686,Authors:HasanFallahgoul,"        We study the investor beliefs, sentiment and disagreement, about stock market returns during the COVID-19 pandemic using a large number of messages of investors on a social media investing platform, \textit{StockTwits}. The rich and multimodal features of StockTwits data allow us to explore the evolution of sentiment and disagreement within and across investors, sectors, and even industries. We find that the sentiment (disagreement) has a sharp decrease (increase) across all investors with any investment philosophy, horizon, and experience between February 19, 2020, and March 23, 2020, where a historical market high followed by a record drop. Surprisingly, these measures have a sharp reverse toward the end of March. However, the performance of these measures across various sectors is heterogeneous. Financial and healthcare sectors are the most pessimistic and optimistic divisions, respectively.        △ Less","8 May, 2020","q-fin.ST,econ.GN,q-fin.TR",
              Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray images using fine-tuned deep neural networks          ,2004.11676,https://arxiv.org/abs/2004.11676,https://arxiv.org/pdf/2004.11676,"Authors:NarinderSinghPunn,SonaliAgarwal","        The novel coronavirus 2019 (COVID-19) is a respiratory syndrome that resembles pneumonia. The current diagnostic procedure of COVID-19 follows reverse-transcriptase polymerase chain reaction (RT-PCR) based approach which however is less sensitive to identify the virus at the initial stage. Hence, a more robust and alternate diagnosis technique is desirable. Recently, with the release of publicly available datasets of corona positive patients comprising of computed tomography (CT) and chest X-ray (CXR) imaging; scientists, researchers and healthcare experts are contributing for faster and automated diagnosis of COVID-19 by identifying pulmonary infections using deep learning approaches to achieve better cure and treatment. These datasets have limited samples concerned with the positive COVID-19 cases, which raise the challenge for unbiased learning. Following from this context, this article presents the random oversampling and weighted class loss function approach for unbiased fine-tuned learning (transfer learning) in various state-of-the-art deep learning approaches such as baseline ResNet, Inception-v3, Inception ResNet-v2, DenseNet169, and NASNetLarge to perform binary classification (as normal and COVID-19 cases) and also multi-class classification (as COVID-19, pneumonia, and normal case) of posteroanterior CXR images. Accuracy, precision, recall, loss, and area under the curve (AUC) are utilized to evaluate the performance of the models. Considering the experimental results, the performance of each model is scenario dependent; however, NASNetLarge displayed better scores in contrast to other architectures, which is further compared with other recently proposed approaches. This article also added the visual explanation to illustrate the basis of model classification and perception of COVID-19 in CXR images.        △ Less","21 July, 2020","eess.IV,cs.CV,cs.LG",
              Having our omic cake and eating it too: Evaluating User Response to using Blockchain Technology for Private & Secure Health Data Management and Sharing          ,2004.11502,https://arxiv.org/abs/2004.11502,https://arxiv.org/pdf/2004.11502,"Authors:VictoriaL.Lemieux,DarraHofman,HodaHamouda,DanielleBatista,RavneetKaur,WenPan,IanCostanzo,DeanRegier,SamanthaPollard,DeirdreWeymann,RobFraser","        This paper reports on the development and evaluation of a prototype blockchain solution for private and secure individual omics health data management and sharing. This solution is one output of a multidisciplinary project investigating the social, data and technical issues surrounding application of blockchain technology in the context of personalized healthcare research. The project studies potential ethical, legal, social and cognitive constraints of self-sovereign healthcare data management and sharing, and whether such constraints can be addressed through careful user interface design of a blockchain solution.        △ Less","23 April, 2020","cs.CR,cs.CY",
              SoQal: Selective Oracle Questioning in Active Learning          ,2004.10468,https://arxiv.org/abs/2004.10468,https://arxiv.org/pdf/2004.10468,"Authors:DaniKiyasseh,TingtingZhu,DavidA.Clifton","        Large sets of unlabelled data within the healthcare domain remain underutilized. Active learning offers a way to exploit these datasets by iteratively requesting an oracle (e.g. medical professional) to label instances. This process, which can be costly and time-consuming is overly-dependent upon an oracle. To alleviate this burden, we propose SoQal, a questioning strategy that dynamically determines when a label should be requested from an oracle. We perform experiments on five publically-available datasets and illustrate SoQal's superiority relative to baseline approaches, including its ability to reduce oracle label requests by up to 35%. SoQal also performs competitively in the presence of label noise: a scenario that simulates clinicians' uncertain diagnoses when faced with difficult classification tasks.        △ Less","22 April, 2020","cs.LG,stat.ML",
              Managing COVID-19 Pandemic without Destructing the Economy          ,2004.10324,https://arxiv.org/abs/2004.10324,https://arxiv.org/pdf/2004.10324,"Authors:DavidGershon,AlexanderLipton,HagaiLevine","        We analyze an approach to managing the COVID-19 pandemic without shutting down the economy while staying within the capacity of the healthcare system. We base our analysis on a detailed heterogeneous epidemiological model, which takes into account different population groups and phases of the disease, including incubation, infection period, hospitalization, and treatment in the intensive care unit (ICU). We model the healthcare capacity as the total number of hospital and ICU beds for the whole country. We calibrate the model parameters to data reported in several recent research papers. For high- and low-risk population groups, we calculate the number of total and intensive care hospitalizations, and deaths as functions of time. The main conclusion is that countries, which enforce reasonable hygienic measures on time can avoid lockdowns throughout the pandemic provided that the number of spare ICU beds per million is above the threshold of about 100. In countries where the total number of ICU beds is below this threshold, a limited period quarantine to specific high-risk groups of the population suffices. Furthermore, in the case of an inadequate capacity of the healthcare system, we incorporate a feedback loop and demonstrate that quantitative impact of the lack of ICU units on the death curve. In the case of inadequate ICU beds, full- and partial-quarantine scenarios outcomes are almost identical, making it unnecessary to shut down the whole economy. We conclude that only a limited-time quarantine of the high-risk group might be necessary, while the rest of the economy can remain operational.        △ Less","9 May, 2020","q-bio.PE,q-fin.MF",
"              SARS-CoV-2, a Threat to Privacy?          ",2004.10305,https://arxiv.org/abs/2004.10305,https://arxiv.org/pdf/2004.10305,"Authors:TimDaubenschuetz,OksanaKulyk,StephanNeumann,IsabellaHinterleitner,PaulaRamosDelgado,CarmenHoffmann,FlorianScheible","        The global SARS-CoV-2 pandemic is currently putting a massive strain on the world's critical infrastructures. With healthcare systems and internet service providers already struggling to provide reliable service, some operators may, intentionally or unintentionally, lever out privacy-protecting measures to increase their system's efficiency in fighting the virus. Moreover, though it may seem all encouraging to see the effectiveness of authoritarian states in battling the crisis, we, the authors of this paper, would like to raise the community's awareness towards developing more effective means in battling the crisis without the need to limit fundamental human rights. To analyze the current situation, we are discussing and evaluating the steps corporations and governments are taking to condemn the virus by applying established privacy research.        △ Less","21 April, 2020",cs.CY,
              EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness against Adversarial Attacks          ,2004.10162,https://arxiv.org/abs/2004.10162,https://arxiv.org/pdf/2004.10162,"Authors:SanchariSen,BalaramanRavindran,AnandRaghunathan","        Ensuring robustness of Deep Neural Networks (DNNs) is crucial to their adoption in safety-critical applications such as self-driving cars, drones, and healthcare. Notably, DNNs are vulnerable to adversarial attacks in which small input perturbations can produce catastrophic misclassifications. In this work, we propose EMPIR, ensembles of quantized DNN models with different numerical precisions, as a new approach to increase robustness against adversarial attacks. EMPIR is based on the observation that quantized neural networks often demonstrate much higher robustness to adversarial attacks than full precision networks, but at the cost of a substantial loss in accuracy on the original (unperturbed) inputs. EMPIR overcomes this limitation to achieve the 'best of both worlds', i.e., the higher unperturbed accuracies of the full precision models combined with the higher robustness of the low precision models, by composing them in an ensemble. Further, as low precision DNN models have significantly lower computational and storage requirements than full precision models, EMPIR models only incur modest compute and memory overheads compared to a single full-precision model (<25% in our evaluations). We evaluate EMPIR across a suite of DNNs for 3 different image recognition tasks (MNIST, CIFAR-10 and ImageNet) and under 4 different adversarial attacks. Our results indicate that EMPIR boosts the average adversarial accuracies by 42.6%, 15.2% and 10.5% for the DNN models trained on the MNIST, CIFAR-10 and ImageNet datasets respectively, when compared to single full-precision models, without sacrificing accuracy on the unperturbed inputs.        △ Less","21 April, 2020","cs.LG,cs.CV,stat.ML",
              CovidAID: COVID-19 Detection Using Chest X-Ray          ,2004.09803,https://arxiv.org/abs/2004.09803,https://arxiv.org/pdf/2004.09803,"Authors:ArpanMangal,SuryaKalia,HarishRajgopal,KrithikaRangarajan,VinayNamboodiri,SubhashisBanerjee,ChetanArora","        The exponential increase in COVID-19 patients is overwhelming healthcare systems across the world. With limited testing kits, it is impossible for every patient with respiratory illness to be tested using conventional techniques (RT-PCR). The tests also have long turn-around time, and limited sensitivity. Detecting possible COVID-19 infections on Chest X-Ray may help quarantine high risk patients while test results are awaited. X-Ray machines are already available in most healthcare systems, and with most modern X-Ray systems already digitized, there is no transportation time involved for the samples either. In this work we propose the use of chest X-Ray to prioritize the selection of patients for further RT-PCR testing. This may be useful in an inpatient setting where the present systems are struggling to decide whether to keep the patient in the ward along with other patients or isolate them in COVID-19 areas. It would also help in identifying patients with high likelihood of COVID with a false negative RT-PCR who would need repeat testing. Further, we propose the use of modern AI techniques to detect the COVID-19 patients using X-Ray images in an automated manner, particularly in settings where radiologists are not available, and help make the proposed testing technology scalable. We present CovidAID: COVID-19 AI Detector, a novel deep neural network based model to triage patients for appropriate testing. On the publicly available covid-chestxray-dataset [2], our model gives 90.5% accuracy with 100% sensitivity (recall) for the COVID-19 infection. We significantly improve upon the results of Covid-Net [10] on the same dataset.        △ Less","21 April, 2020","eess.IV,cs.CV,cs.LG",
              Flattening the Curve: Insights From Queueing Theory          ,2004.09645,https://arxiv.org/abs/2004.09645,https://arxiv.org/pdf/2004.09645,"Authors:SergioPalomo,JamolPender,WilliamMassey,RobertC.Hampshire","        The worldwide outbreak of the coronavirus was first identified in 2019 in Wuhan, China. Since then, the disease has spread worldwide. As it currently spreading in the United States, policy makers, public health officials and citizens are racing to understand the impact of this virus on the United States healthcare system. They fear that the rapid influx of patients will overwhelm the healthcare system leading to unnecessary fatalities. Most countries and states in America have introduced mitigation strategies, such as social distancing, to decrease the rate of newly infected people, i.e. flattening the curve.In this paper, we analyze the time evolution of the number of people hospitalized due to the coronavirus using the methods of queueing theory. Given that the rate of new infections varies over time as the pandemic evolves, we model the number of coronavirus patients as a dynamical system based on the theory of infinite server queues with non-stationary Poisson arrival rates. With this model we are able to quantify how flattening the curve affects the peak demand for hospital resources. This allows us to characterize how aggressively society must flatten the curve in order to avoid overwhelming the capacity of healthcare system. We also demonstrate how flattening the curve impacts the elapsed time between the peak rate of hospitalizations and the time of the peak demand for the hospital resources. Finally, we present empirical evidence from China, South Korea, Italy and the United States that supports the insights from the model.        △ Less","20 April, 2020","cs.PF,math.DS,math.PR",
              On The Problem of Relevance in Statistical Inference          ,2004.09588,https://arxiv.org/abs/2004.09588,https://arxiv.org/pdf/2004.09588,"Authors:SubhadeepMukhopadhyay,KaijunWang","        How many statistical inference tools we have for inference from massive data? A huge number, but only when we are ready to assume the given database is homogenous, consisting of a large cohort of ""similar"" cases. Why we need the homogeneity assumption? To make `learning from the experience of others' or `borrowing strength' possible. But, what if, we are dealing with a massive database of heterogeneous cases (which is a norm in almost all modern data-science applications including neuroscience, genomics, healthcare, and astronomy)? How many methods we have in this situation? Not much, if not ZERO. Why? It's not obvious how to go about gathering strength when each piece of information is fuzzy. The danger is that, if we include irrelevant cases, borrowing information might heavily damage the quality of the inference! This raises some fundamental questions for big data inference: When (not) to borrow? Whom (not) to borrow? How (not) to borrow? These questions are at the heart of the ""Problem of Relevance"" in statistical inference -- a puzzle that has remained too little addressed since its inception nearly half a century ago.  Here we offer the first practical theory of relevance with precisely describable statistical formulation and algorithm. Through examples, we demonstrate how our new statistical perspective answers previously unanswerable questions in a realistic and feasible way.        △ Less","29 April, 2020","stat.ME,math.ST,stat.ML",
              CLOPS: Continual Learning of Physiological Signals          ,2004.09578,https://arxiv.org/abs/2004.09578,https://arxiv.org/pdf/2004.09578,"Authors:DaniKiyasseh,TingtingZhu,DavidA.Clifton","        Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a healthcare-specific replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform its multi-task learning counterpart. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.        △ Less","20 April, 2020","cs.LG,stat.ML",
              ALPS: Active Learning via Perturbations          ,2004.09557,https://arxiv.org/abs/2004.09557,https://arxiv.org/pdf/2004.09557,"Authors:DaniKiyasseh,TingtingZhu,DavidA.Clifton","        Small, labelled datasets in the presence of larger, unlabelled datasets pose challenges to data-hungry deep learning algorithms. Such scenarios are prevalent in healthcare where labelling is expensive, time-consuming, and requires expert medical professionals. To tackle this challenge, we propose a family of active learning methodologies and acquisition functions dependent upon input and parameter perturbations which we call Active Learning via Perturbations (ALPS). We test our methods on six diverse time-series and image datasets and illustrate their benefit in the presence and absence of an oracle. We also show that acquisition functions that incorporate temporal information have the potential to predict the ability of networks to generalize.        △ Less","20 April, 2020","cs.LG,stat.ML",
              Investigating Coordination of Hospital Departments in Delivering Healthcare for Acute Coronary Syndrome Patients using Data-Driven Network Analysis          ,2004.09290,https://arxiv.org/abs/2004.09290,https://arxiv.org/pdf/2004.09290,"Authors:TesfamariamMAbuhay,YemisrachGGetinet,OlegGMetsker,AlexeyNYakovlev,SergeyVKovalchuk","Healthcare systems are challenged to deliver high-quality and efficient care. Studying patient flow in a hospital is particularly fundamental as it demonstrates effectiveness and efficiency of a hospital. Since hospital is a collection of physically nearby services under one administration, its performance and outcome are shaped by the interaction of its discrete components. Coordination of processes at different levels of organizational structure of a hospital can be studied using network analysis. Hence, this article presents a data-driven static and temporal network of departments. Both networks are directed and weighted and constructed using seven years' (2010-2016) empirical data of 24902 Acute Coronary Syndrome (ACS) patients. The ties reflect an episode-based transfer of ACS patients from department to department in a hospital. The weight represents the number of patients transferred among departments. As a result, the underlying structure of a network of departments that deliver healthcare for ACS patients is described, the main departments and their role in the diagnosis and treatment process of ACS patients are identified, the role of departments over seven years is analyzed and communities of departments are discovered. The results of this study may help hospital administration to effectively organize and manage the coordination of departments based on their significance, strategic positioning and role in the diagnosis and treatment process which, in turn, nurtures value-based and precision healthcare.        △ Less","15 April, 2020","cs.SI,cs.CY",
              X-Ray: Mechanical Search for an Occluded Object by Minimizing Support of Learned Occupancy Distributions          ,2004.09039,https://arxiv.org/abs/2004.09039,https://arxiv.org/pdf/2004.09039,"Authors:MichaelDanielczuk,AneliaAngelova,VincentVanhoucke,KenGoldberg","        For applications in e-commerce, warehouses, healthcare, and home service, robots are often required to search through heaps of objects to grasp a specific target object. For mechanical search, we introduce X-Ray, an algorithm based on learned occupancy distributions. We train a neural network using a synthetic dataset of RGBD heap images labeled for a set of standard bounding box targets with varying aspect ratios. X-Ray minimizes support of the learned distribution as part of a mechanical search policy in both simulated and real environments. We benchmark these policies against two baseline policies on 1,000 heaps of 15 objects in simulation where the target object is partially or fully occluded. Results suggest that X-Ray is significantly more efficient, as it succeeds in extracting the target object 82% of the time, 15% more often than the best-performing baseline. Experiments on an ABB YuMi robot with 20 heaps of 25 household objects suggest that the learned policy transfers easily to a physical system, where it outperforms baseline policies by 15% in success rate with 17% fewer actions. Datasets, videos, and experiments are available at https://sites.google.com/berkeley.edu/x-ray.        △ Less","10 October, 2020","cs.RO,cs.CV",
"              Leveraging Big Data Analytics in Healthcare Enhancement: Trends, Challenges and Opportunities          ",2004.09010,https://arxiv.org/abs/2004.09010,https://arxiv.org/pdf/2004.09010,"Authors:ArshiaRehman,SaeedaNaz,ImranRazzak","        Clinicians decisions are becoming more and more evidence-based meaning in no other field the big data analytics so promising as in healthcare. Due to the sheer size and availability of healthcare data, big data analytics has revolutionized this industry and promises us a world of opportunities. It promises us the power of early detection, prediction, prevention and helps us to improve the quality of life. Researchers and clinicians are working to inhibit big data from having a positive impact on health in the future. Different tools and techniques are being used to analyze, process, accumulate, assimilate and manage large amount of healthcare data either in structured or unstructured form. In this paper, we would like to address the need of big data analytics in healthcare: why and how can it help to improve life?. We present the emerging landscape of big data and analytical techniques in the five sub-disciplines of healthcare i.e.medical image analysis and imaging informatics, bioinformatics, clinical informatics, public health informatics and medical signal analytics. We presents different architectures, advantages and repositories of each discipline that draws an integrated depiction of how distinct healthcare activities are accomplished in the pipeline to facilitate individual patients from multiple perspectives. Finally the paper ends with the notable applications and challenges in adoption of big data analytics in healthcare.        △ Less","5 April, 2020","stat.OT,cs.LG,stat.ML",
              Infection arbitrage          ,2004.08701,https://arxiv.org/abs/2004.08701,https://arxiv.org/pdf/2004.08701,Authors:SanderHeinsalu,"        Increasing the infection risk early in an epidemic is individually and socially optimal under some parameter values. The reason is that the early patients recover or die before the peak of the epidemic, which flattens the peak. This improves welfare if the peak exceeds the capacity of the healthcare system and the social loss rises rapidly enough in the number infected. The individual incentive to get infected early comes from the greater likelihood of receiving treatment than at the peak when the disease has overwhelmed healthcare capacity. Calibration to the Covid-19 pandemic data suggests that catching the infection at the start was individually optimal and for some loss functions would have reduced the aggregate loss.        △ Less","26 April, 2020","q-bio.PE,econ.TH",
              Deep Neural Network for Respiratory Sound Classification in Wearable Devices Enabled by Patient Specific Model Tuning          ,2004.08287,https://arxiv.org/abs/2004.08287,https://arxiv.org/pdf/2004.08287,"Authors:JyotibdhaAcharya,ArindamBasu","        The primary objective of this paper is to build classification models and strategies to identify breathing sound anomalies (wheeze, crackle) for automated diagnosis of respiratory and pulmonary diseases. In this work we propose a deep CNN-RNN model that classifies respiratory sounds based on Mel-spectrograms. We also implement a patient specific model tuning strategy that first screens respiratory patients and then builds patient specific classification models using limited patient data for reliable anomaly detection. Moreover, we devise a local log quantization strategy for model weights to reduce the memory footprint for deployment in memory constrained systems such as wearable devices. The proposed hybrid CNN-RNN model achieves a score of 66.31% on four-class classification of breathing cycles for ICBHI'17 scientific challenge respiratory sound database. When the model is re-trained with patient specific data, it produces a score of 71.81% for leave-one-out validation. The proposed weight quantization technique achieves ~4X reduction in total memory cost without loss of performance. The main contribution of the paper is as follows: Firstly, the proposed model is able to achieve state of the art score on the ICBHI'17 dataset. Secondly, deep learning models are shown to successfully learn domain specific knowledge when pre-trained with breathing data and produce significantly superior performance compared to generalized models. Finally, local log quantization of trained weights is shown to be able to reduce the memory requirement significantly. This type of patient-specific re-training strategy can be very useful in developing reliable long-term automated patient monitoring systems particularly in wearable healthcare solutions.        △ Less","16 April, 2020","eess.AS,cs.LG,cs.SD,stat.ML",10.1109/TBCAS.2020.2981172 
              Simulation of Covid-19 epidemic evolution: are compartmental models really predictive?          ,2004.08207,https://arxiv.org/abs/2004.08207,https://arxiv.org/pdf/2004.08207,Authors:MarcoPaggi,"        Computational models for the simulation of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) epidemic evolution would be extremely useful to support authorities in designing healthcare policies and lockdown measures to contain its impact on public health and economy. In Italy, the devised forecasts have been mostly based on a pure data-driven approach, by fitting and extrapolating open data on the epidemic evolution collected by the Italian Civil Protection Center. In this respect, SIR epidemiological models, which start from the description of the nonlinear interactions between population compartments, would be a much more desirable approach to understand and predict the collective emergent response. The present contribution addresses the fundamental question whether a SIR epidemiological model, suitably enriched with asymptomatic and dead individual compartments, could be able to provide reliable predictions on the epidemic evolution. To this aim, a machine learning approach based on particle swarm optimization (PSO) is proposed to automatically identify the model parameters based on a training set of data of progressive increasing size, considering Lombardy in Italy as a case study. The analysis of the scatter in the forecasts shows that model predictions are quite sensitive to the size of the dataset used for training, and that further data are still required to achieve convergent -- and therefore reliable -- predictions.        △ Less","14 April, 2020","q-bio.PE,cs.LG,physics.soc-ph",
              Coronavirus (COVID-19): ARIMA based time-series analysis to forecast near future          ,2004.07859,https://arxiv.org/abs/2004.07859,https://arxiv.org/pdf/2004.07859,"Authors:HiteshiTandon,PrabhatRanjan,TanmoyChakraborty,VandanaSuhag","        COVID-19, a novel coronavirus, is currently a major worldwide threat. It has infected more than a million people globally leading to hundred-thousands of deaths. In such grave circumstances, it is very important to predict the future infected cases to support prevention of the disease and aid in the healthcare service preparation. Following that notion, we have developed a model and then employed it for forecasting future COVID-19 cases in India. The study indicates an ascending trend for the cases in the coming days. A time series analysis also presents an exponential increase in the number of cases. It is supposed that the present prediction models will assist the government and medical personnel to be prepared for the upcoming conditions and have more readiness in healthcare systems.        △ Less","16 April, 2020","q-bio.PE,q-bio.QM,stat.CO,stat.OT",
              COVID-19 Antibody Test / Vaccination Certification: There's an app for that          ,2004.07376,https://arxiv.org/abs/2004.07376,https://arxiv.org/pdf/2004.07376,"Authors:MarcEisenstadt,ManoharanRamachandran,NiazChowdhury,AllanThird,JohnDomingue","        Goal: As the Coronavirus Pandemic of 2019/2020 unfolds, a COVID-19 'Immunity Passport' has been mooted as a way to enable individuals to return back to work. While the quality of antibody testing, the availability of vaccines, and the likelihood of even attaining COVID-19 immunity continue to be researched, we address the issues involved in providing tamper-proof and privacy-preserving certification for test results and vaccinations. Methods: We developed a prototype mobile phone app and requisite decentralized server architecture that facilitates instant verification of tamper-proof test results. Personally identifiable information is only stored at the user's discretion, and the app allows the end-user selectively to present only the specific test result with no other personal information revealed. The architecture, designed for scalability, relies upon (a) the 2019 World Wide Web Consortium standard called 'Verifiable Credentials', (b) Tim Berners-Lee's decentralized personal data platform 'Solid', and (c) a Consortium Ethereum-based blockchain. Results: Our mobile phone app and decentralized server architecture enable the mixture of verifiability and privacy in a manner derived from public/private key pairs and digital signatures, generalized to avoid restrictive ownership of sensitive digital keys and/or data. Benchmark performance tests show it to scale linearly in the worst case, as significant processing is done locally on each app. For the test certificate Holder, Issuer (e.g. healthcare staff, pharmacy) and Verifier (e.g. employer), it is 'just another app' which takes only minutes to use. Conclusions: The app and decentralized server architecture offer a prototype proof of concept that is readily scalable, applicable generically, and in effect 'waiting in the wings' for the biological issues, plus key ethical issues raised in the discussion section, to be resolved.        △ Less","28 June, 2020","cs.CR,cs.CY,cs.NI,cs.SI",10.1109/OJEMB.2020.2999214 
"              Edge Computing For Smart Health: Context-aware Approaches, Opportunities, and Challenges          ",2004.07311,https://arxiv.org/abs/2004.07311,https://arxiv.org/pdf/2004.07311,"Authors:AlaaAwadAbdellatif,AmrMohamed,CarlaFabianaChiasserini,MouniraTlili,AimanErbad","        Improving efficiency of healthcare systems is a top national interest worldwide. However, the need of delivering scalable healthcare services to the patients while reducing costs is a challenging issue. Among the most promising approaches for enabling smart healthcare (s-health) are edge-computing capabilities and next-generation wireless networking technologies that can provide real-time and cost-effective patient remote monitoring. In this paper, we present our vision of exploiting multi-access edge computing (MEC) for s-health applications. We envision a MEC-based architecture and discuss the benefits that it can bring to realize in-network and context-aware processing so that the s-health requirements are met. We then present two main functionalities that can be implemented leveraging such an architecture to provide efficient data delivery, namely, multimodal data compression and edge-based feature extraction for event detection. The former allows efficient and low distortion compression, while the latter ensures high-reliability and fast response in case of emergency applications. Finally, we discuss the main challenges and opportunities that edge computing could provide and possible directions for future research.        △ Less","15 April, 2020","eess.SP,cs.CY,cs.NI",10.1109/MNET.2019.1800083 
              SenseCare: A Research Platform for Medical Image Informatics and Interactive 3D Visualization          ,2004.07031,https://arxiv.org/abs/2004.07031,https://arxiv.org/pdf/2004.07031,"Authors:QiDuan,GuotaiWang,RuiWang,ChaoFu,XinjunLi,MaoliangGong,XinglongLiu,QingXia,XiaodiHuang,ZhiqiangHu,NingHuang,ShaotingZhang","        Clinical research on smart healthcare has an increasing demand for intelligent and clinic-oriented medical image computing algorithms and platforms that support various applications. To this end, we have developed SenseCare research platform for smart healthcare, which is designed to boost translational research on intelligent diagnosis and treatment planning in various clinical scenarios. To facilitate clinical research with Artificial Intelligence (AI), SenseCare provides a range of AI toolkits for different tasks, including image segmentation, registration, lesion and landmark detection from various image modalities ranging from radiology to pathology. In addition, SenseCare is clinic-oriented and supports a wide range of clinical applications such as diagnosis and surgical planning for lung cancer, pelvic tumor, coronary artery disease, etc. SenseCare provides several appealing functions and features such as advanced 3D visualization, concurrent and efficient web-based access, fast data synchronization and high data security, multi-center deployment, support for collaborative research, etc. In this paper, we will present an overview of SenseCare as an efficient platform providing comprehensive toolkits and high extensibility for intelligent image analysis and clinical research in different application scenarios.        △ Less","2 April, 2020","cs.HC,eess.IV",
"              Blockchain in Healthcare and Medicine: A Contemporary Research of Applications, Challenges, and Future Perspectives          ",2004.06795,https://arxiv.org/abs/2004.06795,https://arxiv.org/pdf/2004.06795,"Authors:H.SamiUllah,S.Aslam,N.Arjomand","        Blockchain technology is one of the most contemporary and disruptive technologies in the world. It has gained considerable attention in numerous applications such as financial services, cybersecurity applications, Internet of Things (IoT), network data management. Now its range of applications is beyond the financial services as the healthcare industry has also adopted blockchain technology in its various subdomains such as Electronic Health Records (EHR), medical supply chain management system, genomic market, neuroscience technology, clinical research, and pharmaceutical medicine. Blockchain is considered a secure and viable solution for storing and accessing patients medical records and the patients can diagnosed and treated with safe and secure data sharing. Blockchain technology will revolutionize the healthcare systems with personalized, authentic, and secure access to the clinical data of patients and that data can be used for further health improvements and clinical researches. In this paper, we conduct a contemporary research on existing applications and developments in healthcare industry with the use of blockchain technology. We also discuss some robust applications and various existing companies that are using blockchain solutions for securing their data along with some current challenges and future perspectives.        △ Less","21 April, 2020","cs.CR,cs.CY",
              Agent-Based Modelling of Malaria Transmission Dynamics          ,2004.06477,https://arxiv.org/abs/2004.06477,https://arxiv.org/pdf/2004.06477,"Authors:BabaganaModu,NereidaPolovina,SavasKonur","        Recent statistics of malaria shows that over 200 million cases and estimated deaths of nearly half a million occur globally. Africa alone accounts for almost 90% of the cases. Several studies have been conducted to understand the disease transmission dynamics. In particular, mathematical methods have been frequently used to model and understand the disease dynamics and outbreak patterns. Although, mathematical methods have provided good results for homogeneous populations, these methods impose significant limitations for studying malaria dynamics in heterogeneous populations, a result of various factors, e.g. spatial and temporal fluctuations, social networks, human movements pattern etc. This paper proposes an agent-based modelling approach that permits modelling and analysing malaria dynamics for heterogenous populations. Our approach is illustrated using the climate and demographic data for the Tripura, Limpopo and Benin cities. Our agent-based simulation has been validated against the reported cases of malaria collected in the cities mentioned. Furthermore, the efficiency of the proposed model has been compared with the mathematical model used as benchmark. A statistical test confirms the proposed model is robust and has potential for predicting the peak seasons of malaria. This potentially makes our methods a useful tool as an intervention mechanism, which will have impact on hospitals, healthcare providers, health organisations.        △ Less","9 April, 2020","q-bio.PE,stat.CO",
              Using Reports of Own and Others' Symptoms and Diagnosis on Social Media to Predict COVID-19 Case Counts: Observational Infoveillance Study in Mainland China          ,2004.06169,https://arxiv.org/abs/2004.06169,https://arxiv.org/pdf/2004.06169,"Authors:CuihuaShen,AnfanChen,ChenLuo,JingwenZhang,BoFeng,WangLiao","        Can public social media data be harnessed to predict COVID-19 case counts? We analyzed approximately 15 million COVID-19 related posts on Weibo, a popular Twitter-like social media platform in China, from November 1, 2019 to March 31, 2020. We developed a machine learning classifier to identify ""sick posts,"" which are reports of one's own and other people's symptoms and diagnosis related to COVID-19. We then modeled the predictive power of sick posts and other COVID-19 posts on daily case counts. We found that reports of symptoms and diagnosis of COVID-19 significantly predicted daily case counts, up to 14 days ahead of official statistics. But other COVID-19 posts did not have similar predictive power. For a subset of geotagged posts (3.10% of all retrieved posts), we found that the predictive pattern held true for both Hubei province and the rest of mainland China, regardless of unequal distribution of healthcare resources and outbreak timeline. Researchers and disease control agencies should pay close attention to the social media infosphere regarding COVID-19. On top of monitoring overall search and posting activities, it is crucial to sift through the contents and efficiently identify true signals from noise.        △ Less","4 August, 2020","cs.SI,physics.soc-ph,q-bio.PE",10.2196/19421 
              Data augmentation using generative networks to identify dementia          ,2004.05989,https://arxiv.org/abs/2004.05989,https://arxiv.org/pdf/2004.05989,"Authors:BahmanMirheidari,YilinPan,DanielBlackburn,RonanO'Malley,TraciWalker,AnnalenaVenneri,MarkusReuber,HeidiChristensen","        Data limitation is one of the most common issues in training machine learning classifiers for medical applications. Due to ethical concerns and data privacy, the number of people that can be recruited to such experiments is generally smaller than the number of participants contributing to non-healthcare datasets. Recent research showed that generative models can be used as an effective approach for data augmentation, which can ultimately help to train more robust classifiers sparse data domains. A number of studies proved that this data augmentation technique works for image and audio data sets. In this paper, we investigate the application of a similar approach to different types of speech and audio-based features extracted from interactions recorded with our automatic dementia detection system. Using two generative models we show how the generated synthesized samples can improve the performance of a DNN based classifier. The variational autoencoder increased the F-score of a four-way classifier distinguishing the typical patient groups seen in memory clinics from 58% to around 74%, a 16% improvement        △ Less","13 April, 2020","eess.AS,cs.CL,cs.LG,cs.SD",
              Federated Machine Learning for Intelligent IoT via Reconfigurable Intelligent Surface          ,2004.05843,https://arxiv.org/abs/2004.05843,https://arxiv.org/pdf/2004.05843,"Authors:KaiYang,YuanmingShi,YongZhou,ZhanpengYang,LiqunFu,WeiChen","        Intelligent Internet-of-Things (IoT) will be transformative with the advancement of artificial intelligence and high-dimensional data analysis, shifting from ""connected things"" to ""connected intelligence"". This shall unleash the full potential of intelligent IoT in a plethora of exciting applications, such as self-driving cars, unmanned aerial vehicles, healthcare, robotics, and supply chain finance. These applications drive the need of developing revolutionary computation, communication and artificial intelligence technologies that can make low-latency decisions with massive real-time data. To this end, federated machine learning, as a disruptive technology, is emerged to distill intelligence from the data at network edge, while guaranteeing device privacy and data security. However, the limited communication bandwidth is a key bottleneck of model aggregation for federated machine learning over radio channels. In this article, we shall develop an over-the-air computation based communication-efficient federated machine learning framework for intelligent IoT networks via exploiting the waveform superposition property of a multi-access channel. Reconfigurable intelligent surface is further leveraged to reduce the model aggregation error via enhancing the signal strength by reconfiguring the wireless propagation environments.        △ Less","13 April, 2020","eess.SP,cs.LG,cs.NI",
              COVID-19 identification in chest X-ray images on flat and hierarchical classification scenarios          ,2004.05835,https://arxiv.org/abs/2004.05835,https://arxiv.org/pdf/2004.05835,"Authors:RodolfoM.Pereira,DiegoBertolini,LucasO.Teixeira,CarlosN.SillaJr.,YandreM.G.Costa","        The COVID-19 can cause severe pneumonia and is estimated to have a high impact on the healthcare system. The standard image diagnosis tests for pneumonia are chest X-ray (CXR) and computed tomography (CT) scan. CXR are useful in because it is cheaper, faster and more widespread than CT. This study aims to identify pneumonia caused by COVID-19 from other types and also healthy lungs using only CXR images. In order to achieve the objectives, we have proposed a classification schema considering the multi-class and hierarchical perspectives, since pneumonia can be structured as a hierarchy. Given the natural data imbalance in this domain, we also proposed the use of resampling algorithms in order to re-balance the classes distribution. Our classification schema extract features using some well-known texture descriptors and also using a pre-trained CNN model. We also explored early and late fusion techniques in order to leverage the strength of multiple texture descriptors and base classifiers at once. To evaluate the approach, we composed a database, named RYDLS-20, containing CXR images of pneumonia caused by different pathogens as well as CXR images of healthy lungs. The classes distribution follows a real-world scenario in which some pathogens are more common than others. The proposed approach achieved a macro-avg F1-Score of 0.65 using a multi-class approach and a F1-Score of 0.89 for the COVID-19 identification in the hierarchical classification scenario. As far as we know, we achieved the best nominal rate obtained for COVID-19 identification in an unbalanced environment with more than three classes. We must also highlight the novel proposed hierarchical classification approach for this task, which considers the types of pneumonia caused by the different pathogens and lead us to the best COVID-19 recognition rate obtained here.        △ Less","6 May, 2020","cs.LG,stat.ML",10.1016/j.cmpb.2020.105532 
              Uncertainty estimation for classification and risk prediction on medical tabular data          ,2004.05824,https://arxiv.org/abs/2004.05824,https://arxiv.org/pdf/2004.05824,"Authors:LottaMeijerink,GiovanniCinà,MicheleTonutti","        In a data-scarce field such as healthcare, where models often deliver predictions on patients with rare conditions, the ability to measure the uncertainty of a model's prediction could potentially lead to improved effectiveness of decision support tools and increased user trust. This work advances the understanding of uncertainty estimation for classification and risk prediction on medical tabular data, in a two-fold way. First, we expand and refine the set of heuristics to select an uncertainty estimation technique, introducing tests for clinically-relevant scenarios such as generalization to uncommon pathologies, changes in clinical protocol and simulations of corrupted data. We furthermore differentiate these heuristics depending on the clinical use-case. Second, we observe that ensembles and related techniques perform poorly when it comes to detecting out-of-domain examples, a critical task which is carried out more successfully by auto-encoders. These remarks are enriched by considerations of the interplay of uncertainty estimation with class imbalance, post-modeling calibration and other modeling procedures. Our findings are supported by an array of experiments on toy and real-world data.        △ Less","23 May, 2020","stat.ML,cs.LG",
              Detection of Covid-19 From Chest X-ray Images Using Artificial Intelligence: An Early Review          ,2004.05436,https://arxiv.org/abs/2004.05436,https://arxiv.org/pdf/2004.05436,"Authors:MuhammadIlyas,HinaRehman,AmineNait-ali","        In 2019, the entire world is facing a situation of health emergency due to a newly emerged coronavirus (COVID-19). Almost 196 countries are affected by covid-19, while USA, Italy, China, Spain, Iran, and France have the maximum active cases of COVID-19. The issues, medical and healthcare departments are facing in delay of detecting the COVID-19. Several artificial intelligence based system are designed for the automatic detection of COVID-19 using chest x-rays. In this article we will discuss the different approaches used for the detection of COVID-19 and the challenges we are facing. It is mandatory to develop an automatic detection system to prevent the transfer of the virus through contact. Several deep learning architecture are deployed for the detection of COVID-19 such as ResNet, Inception, Googlenet etc. All these approaches are detecting the subjects suffering with pneumonia while its hard to decide whether the pneumonia is caused by COVID-19 or due to any other bacterial or fungal attack.        △ Less","11 April, 2020","eess.IV,cs.CV,cs.LG",
              Multi-task Learning via Adaptation to Similar Tasks for Mortality Prediction of Diverse Rare Diseases          ,2004.05318,https://arxiv.org/abs/2004.05318,https://arxiv.org/pdf/2004.05318,"Authors:LuchenLiu,ZequnLiu,HaoxianWu,ZichangWang,JianhaoShen,YipingSong,MingZhang","        Mortality prediction of diverse rare diseases using electronic health record (EHR) data is a crucial task for intelligent healthcare. However, data insufficiency and the clinical diversity of rare diseases make it hard for directly training deep learning models on individual disease data or all the data from different diseases. Mortality prediction for these patients with different diseases can be viewed as a multi-task learning problem with insufficient data and large task number. But the tasks with little training data also make it hard to train task-specific modules in multi-task learning models. To address the challenges of data insufficiency and task diversity, we propose an initialization-sharing multi-task learning method (Ada-Sit) which learns the parameter initialization for fast adaptation to dynamically measured similar tasks. We use Ada-Sit to train long short-term memory networks (LSTM) based prediction models on longitudinal EHR data. And experimental results demonstrate that the proposed model is effective for mortality prediction of diverse rare diseases.        △ Less","11 May, 2020","cs.LG,stat.ML",
              One Model to Recognize Them All: Marginal Distillation from NER Models with Different Tag Sets          ,2004.05140,https://arxiv.org/abs/2004.05140,https://arxiv.org/pdf/2004.05140,"Authors:KeunwooPeterYu,YiYang","        Named entity recognition (NER) is a fundamental component in the modern language understanding pipeline. Public NER resources such as annotated data and model services are available in many domains. However, given a particular downstream application, there is often no single NER resource that supports all the desired entity types, so users must leverage multiple resources with different tag sets. This paper presents a marginal distillation (MARDI) approach for training a unified NER model from resources with disjoint or heterogeneous tag sets. In contrast to recent works, MARDI merely requires access to pre-trained models rather than the original training datasets. This flexibility makes it easier to work with sensitive domains like healthcare and finance. Furthermore, our approach is general enough to integrate with different NER architectures, including local models (e.g., BiLSTM) and global models (e.g., CRF). Experiments on two benchmark datasets show that MARDI performs on par with a strong marginal CRF baseline, while being more flexible in the form of required NER resources. MARDI also sets a new state of the art on the progressive NER task. MARDI significantly outperforms the start-of-the-art model on the task of progressive NER.        △ Less","17 April, 2020",cs.CL,
              Towards Automatic Generation of Questions from Long Answers          ,2004.05109,https://arxiv.org/abs/2004.05109,https://arxiv.org/pdf/2004.05109,"Authors:ShlokKumarMishra,PranavGoel,AbhishekSharma,AbhyudayJagannatha,DavidJacobs,HalDauméIII","        Automatic question generation (AQG) has broad applicability in domains such as tutoring systems, conversational agents, healthcare literacy, and information retrieval. Existing efforts at AQG have been limited to short answer lengths of up to two or three sentences. However, several real-world applications require question generation from answers that span several sentences. Therefore, we propose a novel evaluation benchmark to assess the performance of existing AQG systems for long-text answers. We leverage the large-scale open-source Google Natural Questions dataset to create the aforementioned long-answer AQG benchmark. We empirically demonstrate that the performance of existing AQG methods significantly degrades as the length of the answer increases. Transformer-based methods outperform other existing AQG methods on long answers in terms of automatic as well as human evaluation. However, we still observe degradation in the performance of our best performing models with increasing sentence length, suggesting that long answer QA is a challenging benchmark task for future research.        △ Less","15 April, 2020",cs.CL,
              Engagement Patterns of Peer-to-Peer Interactions on Mental Health Platforms          ,2004.04999,https://arxiv.org/abs/2004.04999,https://arxiv.org/pdf/2004.04999,"Authors:AshishSharma,MonojitChoudhury,TimAlthoff,AmitSharma","        Mental illness is a global health problem, but access to mental healthcare resources remain poor worldwide. Online peer-to-peer support platforms attempt to alleviate this fundamental gap by enabling those who struggle with mental illness to provide and receive social support from their peers. However, successful social support requires users to engage with each other and failures may have serious consequences for users in need. Our understanding of engagement patterns on mental health platforms is limited but critical to inform the role, limitations, and design of these platforms. Here, we present a large-scale analysis of engagement patterns of 35 million posts on two popular online mental health platforms, TalkLife and Reddit. Leveraging communication models in human-computer interaction and communication theory, we operationalize a set of four engagement indicators based on attention and interaction. We then propose a generative model to jointly model these indicators of engagement, the output of which is synthesized into a novel set of eleven distinct, interpretable patterns. We demonstrate that this framework of engagement patterns enables informative evaluations and analysis of online support platforms. Specifically, we find that mutual back-and-forth interactions are associated with significantly higher user retention rates on TalkLife. Such back-and-forth interactions, in turn, are associated with early response times and the sentiment of posts.        △ Less","10 April, 2020",cs.SI,
              Analyze and Development System with Multiple Biometric Identification          ,2004.04911,https://arxiv.org/abs/2004.04911,https://arxiv.org/pdf/2004.04911,Authors:SherDadakhanov,"        Cause of a rapid increase in technological development, increasing identity theft, consumer fraud, the threat to personal data is also increasing every day. Methods developed earlier to ensure personal the information from the thefts was not effective and safe. Biometrics were introduced when it was needed technology for more efficient security of personal information. Old-fashioned traditional approaches like Personal identification number( PIN), passwords, keys, login ID can be forgotten, stolen or lost. In biometric authentication system, user may not remember any passwords or carry any keys. As people they recognize each other by the physical appearance and behavioral characteristics that biometric systems use physical characteristics, such as fingerprints, facial recognition, voice recognition, in order to distinguish between the actual user and scammer. In order to increase safety in 2005, biometric identification methods were developed government and business sectors, but today it has reached almost all private sectors as Banking, Finance, home security and protection, healthcare, business security and security etc. Since biometric samples and templates of a biometric system having one biometric character to detect and the user can be replaced and duplicated, the new idea of merging multiple biometric identification technologies has so-called multimodal biometric recognition systems have been introduced that use two or more biometric data characteristics of the individual that can be identified as a real user or not.        △ Less","10 April, 2020",cs.CV,
              Full Law Identification In Graphical Models Of Missing Data: Completeness Results          ,2004.04872,https://arxiv.org/abs/2004.04872,https://arxiv.org/pdf/2004.04872,"Authors:RaziehNabi,RohitBhattacharya,IlyaShpitser","        Missing data has the potential to affect analyses conducted in all fields of scientific study, including healthcare, economics, and the social sciences. Several approaches to unbiased inference in the presence of non-ignorable missingness rely on the specification of the target distribution and its missingness process as a probability distribution that factorizes with respect to a directed acyclic graph. In this paper, we address the longstanding question of the characterization of models that are identifiable within this class of missing data distributions. We provide the first completeness result in this field of study -- necessary and sufficient graphical conditions under which, the full data distribution can be recovered from the observed data distribution. We then simultaneously address issues that may arise due to the presence of both missing data and unmeasured confounding, by extending these graphical conditions and proofs of completeness, to settings where some variables are not just missing, but completely unobserved.        △ Less","31 August, 2020","stat.ME,cs.LG",
              The Benefits and Costs of Social Distancing in Rich and Poor Countries          ,2004.04867,https://arxiv.org/abs/2004.04867,https://arxiv.org/pdf/2004.04867,"Authors:ZacharyBarnett-Howell,AhmedMushfiqMobarak","        Social distancing is the primary policy prescription for combating the COVID-19 pandemic, and has been widely adopted in Europe and North America. We estimate the value of disease avoidance using an epidemiological model that projects the spread of COVID-19 across rich and poor countries. Social distancing measures that ""flatten the curve"" of the disease to bring demand within the capacity of healthcare systems are predicted to save many lives in high-income countries, such that practically any economic cost is worth bearing. These social distancing policies are estimated to be less effective in poor countries with younger populations less susceptible to COVID-19, and more limited healthcare systems, which were overwhelmed before the pandemic. Moreover, social distancing lowers disease risk by limiting people's economic opportunities. Poorer people are less willing to make those economic sacrifices. They place relatively greater value on their livelihood concerns compared to contracting COVID-19. Not only are the epidemiological and economic benefits of social distancing much smaller in poorer countries, such policies may exact a heavy toll on the poorest and most vulnerable. Workers in the informal sector lack the resources and social protections to isolate themselves and sacrifice economic opportunities until the virus passes. By limiting their ability to earn a living, social distancing can lead to an increase in hunger, deprivation, and related mortality and morbidity. Rather than a blanket adoption of social distancing measures, we advocate for the exploration of alternative harm-reduction strategies, including universal mask adoption and increased hygiene measures.        △ Less","9 April, 2020",econ.GN,
              Use of Available Data To Inform The COVID-19 Outbreak in South Africa: A Case Study          ,2004.04813,https://arxiv.org/abs/2004.04813,https://arxiv.org/pdf/2004.04813,"Authors:VukosiMarivate,HerkulaasMvECombrink","        The coronavirus disease (COVID-19), caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organization (WHO) in February 2020. Currently, there are no vaccines or treatments that have been approved after clinical trials. Social distancing measures, including travel bans, school closure, and quarantine applied to countries or regions are being used to limit the spread of the disease and the demand on the healthcare infrastructure. The seclusion of groups and individuals has led to limited access to accurate information. To update the public, especially in South Africa, announcements are made by the minister of health daily. These announcements narrate the confirmed COVID-19 cases and include the age, gender, and travel history of people who have tested positive for the disease. Additionally, the South African National Institute for Communicable Diseases updates a daily infographic summarising the number of tests performed, confirmed cases, mortality rate, and the regions affected. However, the age of the patient and other nuanced data regarding the transmission is only shared in the daily announcements and not on the updated infographic. To disseminate this information, the Data Science for Social Impact research group at the University of Pretoria, South Africa, has worked on curating and applying publicly available data in a way that is computer-readable so that information can be shared to the public - using both a data repository and a dashboard. Through collaborative practices, a variety of challenges related to publicly available data in South Africa came to the fore. These include shortcomings in the accessibility, integrity, and data management practices between governmental departments and the South African public. In this paper, solutions to these problems will be shared by using a publicly available data repository and dashboard as a case study.        △ Less","29 April, 2020","cs.CY,stat.AP",
              Health Information Standardisation as a basis for Learning Health Systems          ,2004.04811,https://arxiv.org/abs/2004.04811,https://arxiv.org/pdf/2004.04811,Authors:ScottMcLachlan,"        Standardisation of healthcare has been the focus of hospital management and clinicians since the 1990's. Electronic health records were already intended to provide clinicians with real-time access to clinical knowledge and care plans while also recording and storing vast amounts of patient data. It took more than three decades for electronic health records to start to become ubiquitous in all aspects of healthcare. Learning health systems are the next stage in health information systems whose potential benefits have been promoted for more than a decade - yet few are seen in clinical practice. Clinical care process specifications are a primary form of clinical documentation used in all aspects of healthcare, but they lack standardisation. This thesis contends that this lack of standardisation was inherited by electronic health records and that this is a significant issue holding back the development and adoption of learning health systems. Standardisation of clinical documents is used to mitigate issues in electronic health records as a basis for enabling learning health systems. One type of clinical document, the caremap, is standardised in order to achieve an effective approach to containing resources and ensuring consistency and quality. This led not only to improved clinicians' comprehension and acceptance of the clinical document, but also to reduced time expended in developing complicated learning health systems built using the input of clinical experts.        △ Less","30 March, 2020","cs.CY,cs.IT",
              Using random testing to manage a safe exit from the COVID-19 lockdown          ,2004.04614,https://arxiv.org/abs/2004.04614,https://arxiv.org/pdf/2004.04614,"Authors:MarkusMüller,PeterM.Derlet,ChristopherMudry,GabrielAeppli","        We argue that frequent sampling of the fraction of infected people (either by random testing or by analysis of sewage water), is central to managing the COVID-19 pandemic because it both measures in real time the key variable controlled by restrictive measures, and anticipates the load on the healthcare system due to progression of the disease. Knowledge of random testing outcomes will (i) significantly improve the predictability of the pandemic, (ii) allow informed and optimized decisions on how to modify restrictive measures, with much shorter delay times than the present ones, and (iii) enable the real-time assessment of the efficiency of new means to reduce transmission rates.  Here we suggest, irrespective of the size of a suitably homogeneous population, a conservative estimate of 15000 for the number of randomly tested people per day which will suffice to obtain reliable data about the current fraction of infections and its evolution in time, thus enabling close to real-time assessment of the quantitative effect of restrictive measures. Still higher testing capacity permits detection of geographical differences in spreading rates. Furthermore and most importantly, with daily sampling in place, a reboot could be attempted while the fraction of infected people is still an order of magnitude higher than the level required for a relaxation of restrictions with testing focused on symptomatic individuals. This is demonstrated by considering a feedback and control model of mitigation where the feed-back is derived from noisy sampling data.        △ Less","16 April, 2020","q-bio.PE,cond-mat.other,cond-mat.stat-mech,physics.med-ph",10.1088/1478-3975/aba6d0 
              Care Robots with Sexual Assistance Functions          ,2004.04428,https://arxiv.org/abs/2004.04428,https://arxiv.org/pdf/2004.04428,Authors:OliverBendel,"        Residents in retirement and nursing homes have sexual needs just like other people. However, the semi-public situation makes it difficult for them to satisfy these existential concerns. In addition, they may not be able to meet a suitable partner or find it difficult to have a relationship for mental or physical reasons. People who live or are cared for at home can also be affected by this problem. Perhaps they can host someone more easily and discreetly than the residents of a health facility, but some elderly and disabled people may be restricted in some ways. This article examines the opportunities and risks that arise with regard to care robots with sexual assistance functions. First of all, it deals with sexual well-being. Then it presents robotic systems ranging from sex robots to care robots. Finally, the focus is on care robots, with the author exploring technical and design issues. A brief ethical discussion completes the article. The result is that care robots with sexual assistance functions could be an enrichment of the everyday life of people in need of care, but that we also have to consider some technical, design and moral aspects.        △ Less","9 April, 2020","cs.RO,cs.CY,cs.HC",
              Co-Robots as Care Robots          ,2004.04374,https://arxiv.org/abs/2004.04374,https://arxiv.org/pdf/2004.04374,Authors:OliverBendel,"        Cooperation and collaboration robots, co-robots or cobots for short, are an integral part of factories. For example, they work closely with the fitters in the automotive sector, and everyone does what they do best. However, the novel robots are not only relevant in production and logistics, but also in the service sector, especially where proximity between them and the users is desired or unavoidable. For decades, individual solutions of a very different kind have been developed in care. Now experts are increasingly relying on co-robots and teaching them the special tasks that are involved in care or therapy. This article presents the advantages, but also the disadvantages of co-robots in care and support, and provides information with regard to human-robot interaction and communication. The article is based on a model that has already been tested in various nursing and retirement homes, namely Lio from F&P Robotics, and uses results from accompanying studies. The authors can show that co-robots are ideal for care and support in many ways. Of course, it is also important to consider a few points in order to guarantee functionality and acceptance.        △ Less","9 April, 2020","cs.RO,cs.AI,cs.HC",
              Calibrating Structured Output Predictors for Natural Language Processing          ,2004.04361,https://arxiv.org/abs/2004.04361,https://arxiv.org/pdf/2004.04361,"Authors:AbhyudayJagannatha,HongYu","        We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the system is to be deployed in a safety-critical domain such as healthcare. However, the output space of such structured prediction models is often too large to adapt binary or multi-class calibration methods directly. In this study, we propose a general calibration scheme for output entities of interest in neural-network based structured prediction models. Our proposed method can be used with any binary class calibration scheme and a neural network model. Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. We show that our method outperforms current calibration techniques for named-entity-recognition, part-of-speech and question answering. We also improve our model's performance from our decoding step across several tasks and benchmark datasets. Our method improves the calibration and model performance on out-of-domain test scenarios as well.        △ Less","5 May, 2020","cs.CL,cs.LG",
              Towards Highly Scalable Runtime Models with History          ,2004.03727,https://arxiv.org/abs/2004.03727,https://arxiv.org/pdf/2004.03727,"Authors:LucasSakizloglou,SonaGhahremani,ThomasBrand,MatthiasBarkowsky,HolgerGiese","        Advanced systems such as IoT comprise many heterogeneous, interconnected, and autonomous entities operating in often highly dynamic environments. Due to their large scale and complexity, large volumes of monitoring data are generated and need to be stored, retrieved, and mined in a time- and resource-efficient manner. Architectural self-adaptation automates the control, orchestration, and operation of such systems. This can only be achieved via sophisticated decision-making schemes supported by monitoring data that fully captures the system behavior and its history.  Employing model-driven engineering techniques we propose a highly scalable, history-aware approach to store and retrieve monitoring data in form of enriched runtime models. We take advantage of rule-based adaptation where change events in the system trigger adaptation rules. We first present a scheme to incrementally check model queries in the form of temporal logic formulas which represent the conditions of adaptation rules against a runtime model with history. Then we enhance the model to retain only information that is temporally relevant to the queries, therefore reducing the accumulation of information to a required minimum. Finally, we demonstrate the feasibility and scalability of our approach via experiments on a simulated smart healthcare system employing a real-world medical guideline.        △ Less","7 April, 2020",cs.SE,10.1145/3387939.3388614 
              Causal Relational Learning          ,2004.03644,https://arxiv.org/abs/2004.03644,https://arxiv.org/pdf/2004.03644,"Authors:BabakSalimi,HarshParikh,MoeKayali,SudeepaRoy,LiseGetoor,DanSuciu","        Causal inference is at the heart of empirical research in natural and social sciences and is critical for scientific discovery and informed decision making. The gold standard in causal inference is performing randomized controlled trials; unfortunately these are not always feasible due to ethical, legal, or cost constraints. As an alternative, methodologies for causal inference from observational data have been developed in statistical studies and social sciences. However, existing methods critically rely on restrictive assumptions such as the study population consisting of homogeneous elements that can be represented in a single flat table, where each row is referred to as a unit. In contrast, in many real-world settings, the study domain naturally consists of heterogeneous elements with complex relational structure, where the data is naturally represented in multiple related tables. In this paper, we present a formal framework for causal inference from such relational data. We propose a declarative language called CaRL for capturing causal background knowledge and assumptions and specifying causal queries using simple Datalog-like rules.CaRL provides a foundation for inferring causality and reasoning about the effect of complex interventions in relational domains. We present an extensive experimental evaluation on real relational data to illustrate the applicability of CaRL in social sciences and healthcare.        △ Less","7 April, 2020","cs.DB,cs.AI,cs.LG",
              Automated Smartphone based System for Diagnosis of Diabetic Retinopathy          ,2004.03408,https://arxiv.org/abs/2004.03408,https://arxiv.org/pdf/2004.03408,"Authors:MisginaTsigheHagos,ShriKant,SurayyaAdoBala","        Early diagnosis of diabetic retinopathy for treatment of the disease has been failing to reach diabetic people living in rural areas. Shortage of trained ophthalmologists, limited availability of healthcare centers, and expensiveness of diagnostic equipment are among the reasons. Although many deep learning-based automatic diagnosis of diabetic retinopathy techniques have been implemented in the literature, these methods still fail to provide a point-of-care diagnosis. This raises the need for an independent diagnostic of diabetic retinopathy that can be used by a non-expert. Recently the usage of smartphones has been increasing across the world. Automated diagnoses of diabetic retinopathy can be deployed on smartphones in order to provide an instant diagnosis to diabetic people residing in remote areas. In this paper, inception based convolutional neural network and binary decision tree-based ensemble of classifiers have been proposed and implemented to detect and classify diabetic retinopathy. The proposed method was further imported into a smartphone application for mobile-based classification, which provides an offline and automatic system for diagnosis of diabetic retinopathy.        △ Less","7 April, 2020","eess.IV,cs.CV,cs.HC",10.1109/ICCCIS48478.2019.8974492 
              A Machine Learning Based Framework for the Smart Healthcare Monitoring          ,2004.03360,https://arxiv.org/abs/2004.03360,https://arxiv.org/pdf/2004.03360,"Authors:AbrarZahin,LeThanhTan,RoseQingyangHu","        In this paper, we propose a novel framework for the smart healthcare system, where we employ the compressed sensing (CS) and the combination of the state-of-the-art machine learning based denoiser as well as the alternating direction of method of multipliers (ADMM) structure. This integration significantly simplifies the software implementation for the lowcomplexity encoder, thanks to the modular structure of ADMM. Furthermore, we focus on detecting fall down actions from image streams. Thus, teh primary purpose of thus study is to reconstruct the image as visibly clear as possible and hence it helps the detection step at the trained classifier. For this efficient smart health monitoring framework, we employ the trained binary convolutional neural network (CNN) classifier for the fall-action classifier, because this scheme is a part of surveillance scenario. In this scenario, we deal with the fallimages, thus, we compress, transmit and reconstruct the fallimages. Experimental results demonstrate the impacts of network parameters and the significant performance gain of the proposal compared to traditional methods.        △ Less","4 April, 2020","cs.CV,cs.HC,cs.LG,eess.IV",
              MedDialog: Two Large-scale Medical Dialogue Datasets          ,2004.03329,https://arxiv.org/abs/2004.03329,https://arxiv.org/pdf/2004.03329,"Authors:XuehaiHe,ShuChen,ZeqianJu,XiangyuDong,HongchaoFang,SichengWang,YueYang,JiaqiZeng,RuisiZhang,RuoyuZhang,MengZhou,PenghuiZhu,PengtaoXie","        Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we build two large-scale medical dialogue datasets: MedDialog-EN and MedDialog-CN. MedDialog-EN is an English dataset containing 0.3 million conversations between patients and doctors and 0.5 million utterances. MedDialog-CN is an Chinese dataset containing 1.1 million conversations and 4 million utterances. To our best knowledge, MedDialog-(EN,CN) are the largest medical dialogue datasets to date. The dataset is available at https://github.com/UCSD-AI4H/Medical-Dialogue-System        △ Less","7 July, 2020","cs.LG,cs.AI,cs.CL,stat.ML",
              Exemplar Auditing for Multi-Label Biomedical Text Classification          ,2004.03093,https://arxiv.org/abs/2004.03093,https://arxiv.org/pdf/2004.03093,"Authors:AllenSchmaltz,AndrewBeam","        Many practical applications of AI in medicine consist of semi-supervised discovery: The investigator aims to identify features of interest at a resolution more fine-grained than that of the available human labels. This is often the scenario faced in healthcare applications as coarse, high-level labels (e.g., billing codes) are often the only sources that are readily available. These challenges are compounded for modalities such as text, where the feature space is very high-dimensional, and often contains considerable amounts of noise.  In this work, we generalize a recently proposed zero-shot sequence labeling method, ""binary labeling via a convolutional decomposition"", to the case where the available document-level human labels are themselves relatively high-dimensional. The approach yields classification with ""introspection"", relating the fine-grained features of an inference-time prediction to their nearest neighbors from the training set, under the model. The approach is effective, yet parsimonious, as demonstrated on a well-studied MIMIC-III multi-label classification task of electronic health record data, and is useful as a tool for organizing the analysis of neural model predictions and high-dimensional datasets. Our proposed approach yields both a competitively effective classification model and an interrogation mechanism to aid healthcare workers in understanding the salient features that drive the model's predictions.        △ Less","6 April, 2020",cs.CL,
              Temporal rise in the proportion of younger adults and older adolescents among COVID-19 cases in Germany: evidence of lesser adherence to social distancing practices?          ,2004.02817,https://arxiv.org/abs/2004.02817,https://arxiv.org/pdf/2004.02817,Authors:EdwardGoldstein,"        Background: There is uncertainty about the role of different age groups in propagating the SARS-CoV-2 epidemics in different countries. Methods: We used the Koch Institute data on COVID-19 cases in Germany. To minimize the effect of changes in healthcare seeking behavior and testing practices, we included the following 5-year age groups in the analyses: 10-14y through 45-49y. For each age group g, we considered the proportion PL(g) of individuals in age group g among all detected cases aged 10-49y during weeks 13-14, 2020 (later period), as well as corresponding proportion PE(g) for weeks 10-11, 2020 (early period), and the relative risk RR(g)=PL(g)/PE(g). For each pair of age groups g1,g2, a higher value of RR(g1) compared to RR(g2) is interpreted as the relative increase in the population incidence of SARS-Cov-2 for g1 compared to g2 for the later vs. early period. Results: The relative risk was highest for individuals aged 20-24y (RR=1.4(95% CI (1.27,1.55))), followed by individuals aged 15-19y (RR=1.14(0.99,1.32)), aged 30-34y (RR= 1.07(0.99,1.16)), aged 25-29y (RR= 1.06(0.98,1.15)), aged 35-39y (RR=0.95(0.87,1.03)), aged 40-44y (RR=0.9(0.83,0.98)), aged 45-49y (RR=0.83(0.77,0.89)) and aged 10-14y (RR=0.78(0.64,0.95)). Conclusions: The observed relative increase with time in the prevalence of individuals aged 15-34y (particularly those aged 20-24y) among COVID-19 cases is unlikely to be explained by increases in the likelihood of seeking medical care/being tested for individuals in those age groups compared to individuals aged 35-49y or 10-14y, suggesting an actual increase in the prevalence of individuals aged 15-34y among SARS-CoV-2 infections in the German population. That increase likely reflects elevated mixing among individuals aged 15-34y (particularly those aged 20-24y) compared to other age groups, possibly due to lesser adherence to social distancing practices.        △ Less","6 April, 2020",q-bio.PE,
              Evaluating the Communication Efficiency in Federated Learning Algorithms          ,2004.02738,https://arxiv.org/abs/2004.02738,https://arxiv.org/pdf/2004.02738,"Authors:MuhammadAsad,AhmedMoustafa,TakayukiIto,MuhammadAslam","        In the era of advanced technologies, mobile devices are equipped with computing and sensing capabilities that gather excessive amounts of data. These amounts of data are suitable for training different learning models. Cooperated with advancements in Deep Learning (DL), these learning models empower numerous useful applications, e.g., image processing, speech recognition, healthcare, vehicular network and many more. Traditionally, Machine Learning (ML) approaches require data to be centralised in cloud-based data-centres. However, this data is often large in quantity and privacy-sensitive which prevents logging into these data-centres for training the learning models. In turn, this results in critical issues of high latency and communication inefficiency. Recently, in light of new privacy legislations in many countries, the concept of Federated Learning (FL) has been introduced. In FL, mobile users are empowered to learn a global model by aggregating their local models, without sharing the privacy-sensitive data. Usually, these mobile users have slow network connections to the data-centre where the global model is maintained. Moreover, in a complex and large scale network, heterogeneous devices that have various energy constraints are involved. This raises the challenge of communication cost when implementing FL at large scale. To this end, in this research, we begin with the fundamentals of FL, and then, we highlight the recent FL algorithms and evaluate their communication efficiency with detailed comparisons. Furthermore, we propose a set of solutions to alleviate the existing FL problems both from communication perspective and privacy perspective.        △ Less","6 April, 2020","cs.LG,cs.DC,eess.SP",
              SmartCoAuth: Smart-Contract privacy preservation mechanism on querying sensitive records in the cloud          ,2004.02543,https://arxiv.org/abs/2004.02543,https://arxiv.org/pdf/2004.02543,"Authors:MuhammedSiraj,Mohd.IzuanHafezHj.Ninggal,NurIzuraUdzir,MuhammadDanielHafizAbdullah,AziahAsmawi","        Sensitive records stored in the cloud such as healthcare records, private conversation and credit card information are targets of hackers and privacy abuse. Current information and record management systems have difficulties achieving privacy protection of such sensitive records in a secure, transparent, decentralized and trustless environment. The Blockchain technology is a nascent and a promising technology that facilitates data sharing and access in a secure, decentralized and trustless environment. The technology enables the use of smart contracts that can be leveraged to complement existing traditional systems to achieve security objectives that were never possible before. In this paper, we propose a framework based on Blockchain technology to enable privacy-preservation in a secured, decentralized, transparent and trustless environment. We name our framework SmartCoAuth. It is based on Ethereum Smart Contract functions as the secure, decentralized, transparent authentication and authorization mechanism in the framework. It also enables tamper-proof auditing of access to the protected records. We analysed how SmartCoAuth could be integrated into a cloud application to provide reliable privacy-preservation among stakeholders of healthcare records stored in the cloud. The proposed framework provides a satisfactory level of data utility and privacy preservation.        △ Less","6 April, 2020","cs.CR,cs.DC",
              ReRe: A Lightweight Real-time Ready-to-Go Anomaly Detection Approach for Time Series          ,2004.02319,https://arxiv.org/abs/2004.02319,https://arxiv.org/pdf/2004.02319,"Authors:Ming-ChangLee,Jia-ChunLin,ErnstGunnarGran","        Anomaly detection is an active research topic in many different fields such as intrusion detection, network monitoring, system health monitoring, IoT healthcare, etc. However, many existing anomaly detection approaches require either human intervention or domain knowledge, and may suffer from high computation complexity, consequently hindering their applicability in real-world scenarios. Therefore, a lightweight and ready-to-go approach that is able to detect anomalies in real-time is highly sought-after. Such an approach could be easily and immediately applied to perform time series anomaly detection on any commodity machine. The approach could provide timely anomaly alerts and by that enable appropriate countermeasures to be undertaken as early as possible. With these goals in mind, this paper introduces ReRe, which is a Real-time Ready-to-go proactive Anomaly Detection algorithm for streaming time series. ReRe employs two lightweight Long Short-Term Memory (LSTM) models to predict and jointly determine whether or not an upcoming data point is anomalous based on short-term historical data points and two long-term self-adaptive thresholds. Experiments based on real-world time-series datasets demonstrate the good performance of ReRe in real-time anomaly detection without requiring human intervention or domain knowledge.        △ Less","3 June, 2020","cs.LG,stat.ML",
              DNA Methylation Data to Predict Suicidal and Non-Suicidal Deaths: A Machine Learning Approach          ,2004.01819,https://arxiv.org/abs/2004.01819,https://arxiv.org/pdf/2004.01819,"Authors:RifatZahan,IanMcQuillan,NathanielD.Osgood","        The objective of this study is to predict suicidal and non-suicidal deaths from DNA methylation data using a modern machine learning algorithm. We used support vector machines to classify existing secondary data consisting of normalized values of methylated DNA probe intensities from tissues of two cortical brain regions to distinguish suicide cases from control cases. Before classification, we employed Principal component analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) to reduce the dimension of the data. In comparison to PCA, the modern data visualization method t-SNE performs better in dimensionality reduction. t-SNE accounts for the possible non-linear patterns in low-dimensional data. We applied four-fold cross-validation in which the resulting output from t-SNE was used as training data for the Support Vector Machine (SVM). Despite the use of cross-validation, the nominally perfect prediction of suicidal deaths for BA11 data suggests possible over-fitting of the model. The study also may have suffered from 'spectrum bias' since the individuals were only studied from two extreme scenarios. This research constitutes a baseline study for classifying suicidal and non-suicidal deaths from DNA methylation data. Future studies with larger sample size, while possibly incorporating methylation data from living individuals, may reduce the bias and improve the accuracy of the results.        △ Less","3 April, 2020","q-bio.GN,cs.LG,stat.ML",10.1109/ICHI.2018.00057 
              Application of Intelligent Multi Agent Based Systems For E-Healthcare Security          ,2004.01256,https://arxiv.org/abs/2004.01256,https://arxiv.org/pdf/2004.01256,"Authors:FaizalKhan,OmarReyad","        In recent years, availability and usage of extensive systems for Electronic Healthcare Record (EHR) is increased. In medical centers such hospitals and other laboratories, more health data sets were formed during the treatment process. In order to enhance the standard of the services provided in healthcare, these records where shared and can be used by various users depends on their requirements. As a result, notable issues in the security and privacy where obtained which should be monitored and removed in order to make the use of EHR more effectively. Various researches have been done in the past literature for improving the standards of the security and privacy in E-health systems. In spite of this, it is not completely enhanced. In this paper, a comprehensive analysis is done by selecting the existing approaches and models which were proposed for the security and privacy of the E-healthcare systems. Also, a novel Intelligent-based Access Control Security Model (IBAC) based on multi agents is proposed to maintain and support the security and privacy of E-healthcare systems. This system uses agents in order to maintain security and privacy while accessing the E-health data between the users.        △ Less","2 April, 2020","cs.CY,cs.CR",10.18576/isl/080204 
              Predicting Injectable Medication Adherence via a Smart Sharps Bin and Machine Learning          ,2004.01144,https://arxiv.org/abs/2004.01144,https://arxiv.org/pdf/2004.01144,"Authors:YingqiGu,AkshayZalkikar,LaraKelly,KieranDaly,TomasE.Ward","        Medication non-adherence is a widespread problem affecting over 50% of people who have chronic illness and need chronic treatment. Non-adherence exacerbates health risks and drives significant increases in treatment costs. In order to address these challenges, the importance of predicting patients' adherence has been recognised. In other words, it is important to improve the efficiency of interventions of the current healthcare system by prioritizing resources to the patients who are most likely to be non-adherent. Our objective in this work is to make predictions regarding individual patients' behaviour in terms of taking their medication on time during their next scheduled medication opportunity. We do this by leveraging a number of machine learning models. In particular, we demonstrate the use of a connected IoT device; a ""Smart Sharps Bin"", invented by HealthBeacon Ltd.; to monitor and track injection disposal of patients in their home environment. Using extensive data collected from these devices, five machine learning models, namely Extra Trees Classifier, Random Forest, XGBoost, Gradient Boosting and Multilayer Perception were trained and evaluated on a large dataset comprising 165,223 historic injection disposal records collected from 5,915 HealthBeacon units over the course of 3 years. The testing work was conducted on real-time data generated by the smart device over a time period after the model training was complete, i.e. true future data. The proposed machine learning approach demonstrated very good predictive performance exhibiting an Area Under the Receiver Operating Characteristic Curve (ROC AUC) of 0.86.        △ Less","2 April, 2020","cs.LG,stat.ML",
              Learning Longterm Representations for Person Re-Identification Using Radio Signals          ,2004.01091,https://arxiv.org/abs/2004.01091,https://arxiv.org/pdf/2004.01091,"Authors:LijieFan,TianhongLi,RongyaoFang,RumenHristov,YuanYuan,DinaKatabi","        Person Re-Identification (ReID) aims to recognize a person-of-interest across different places and times. Existing ReID methods rely on images or videos collected using RGB cameras. They extract appearance features like clothes, shoes, hair, etc. Such features, however, can change drastically from one day to the next, leading to inability to identify people over extended time periods. In this paper, we introduce RF-ReID, a novel approach that harnesses radio frequency (RF) signals for longterm person ReID. RF signals traverse clothes and reflect off the human body; thus they can be used to extract more persistent human-identifying features like body size and shape. We evaluate the performance of RF-ReID on longitudinal datasets that span days and weeks, where the person may wear different clothes across days. Our experiments demonstrate that RF-ReID outperforms state-of-the-art RGB-based ReID approaches for long term person ReID. Our results also reveal two interesting features: First since RF signals work in the presence of occlusions and poor lighting, RF-ReID allows for person ReID in such scenarios. Second, unlike photos and videos which reveal personal and private information, RF signals are more privacy-preserving, and hence can help extend person ReID to privacy-concerned domains, like healthcare.        △ Less","2 April, 2020",cs.CV,
              A County-level Dataset for Informing the United States' Response to COVID-19          ,2004.00756,https://arxiv.org/abs/2004.00756,https://arxiv.org/pdf/2004.00756,"Authors:BenjaminD.Killeen,JieYingWu,KinjalShah,AnnaZapaishchykova,PhilippNikutta,AniruddhaTamhane,ShreyaChakraborty,JinchiWei,TigerGao,MareikeThies,MathiasUnberath","        As the coronavirus disease 2019 (COVID-19) continues to be a global pandemic, policy makers have enacted and reversed non-pharmaceutical interventions with various levels of restrictions to limit its spread. Data driven approaches that analyze temporal characteristics of the pandemic and its dependence on regional conditions might supply information to support the implementation of mitigation and suppression strategies. To facilitate research in this direction on the example of the United States, we present a machine-readable dataset that aggregates relevant data from governmental, journalistic, and academic sources on the U.S. county level. In addition to county-level time-series data from the JHU CSSE COVID-19 Dashboard, our dataset contains more than 300 variables that summarize population estimates, demographics, ethnicity, housing, education, employment and income, climate, transit scores, and healthcare system-related metrics. Furthermore, we present aggregated out-of-home activity information for various points of interest for each county, including grocery stores and hospitals, summarizing data from SafeGraph and Google mobility reports. We compile information from IHME, state and county-level government, and newspapers for dates of the enactment and reversal of non-pharmaceutical interventions. By collecting these data, as well as providing tools to read them, we hope to accelerate research that investigates how the disease spreads and why spread may be different across regions. Our dataset and associated code are available at github.com/JieYingWu/COVID-19_US_County-level_Summaries.        △ Less","10 September, 2020","cs.CY,cs.DB,physics.soc-ph,q-bio.PE",
              Drug-disease Graph: Predicting Adverse Drug Reaction Signals via Graph Neural Network with Clinical Data          ,2004.00407,https://arxiv.org/abs/2004.00407,https://arxiv.org/pdf/2004.00407,"Authors:HeeyoungKwak,MinwooLee,SeunghyunYoon,JooyoungChang,SangminPark,KyominJung","        Adverse Drug Reaction (ADR) is a significant public health concern world-wide. Numerous graph-based methods have been applied to biomedical graphs for predicting ADRs in pre-marketing phases. ADR detection in post-market surveillance is no less important than pre-marketing assessment, and ADR detection with large-scale clinical data have attracted much attention in recent years. However, there are not many studies considering graph structures from clinical data for detecting an ADR signal, which is a pair of a prescription and a diagnosis that might be a potential ADR. In this study, we develop a novel graph-based framework for ADR signal detection using healthcare claims data. We construct a Drug-disease graph with nodes representing the medical codes. The edges are given as the relationships between two codes, computed using the data. We apply Graph Neural Network to predict ADR signals, using labels from the Side Effect Resource database. The model shows improved AUROC and AUPRC performance of 0.795 and 0.775, compared to other algorithms, showing that it successfully learns node representations expressive of those relationships. Furthermore, our model predicts ADR pairs that do not exist in the established ADR database, showing its capability to supplement the ADR database.        △ Less","1 April, 2020","cs.LG,stat.ML",
              Enriching Consumer Health Vocabulary Using Enhanced GloVe Word Embedding          ,2004.00150,https://arxiv.org/abs/2004.00150,https://arxiv.org/pdf/2004.00150,"Authors:MohammedIbrahim,SusanGauch,OmarSalman,MohammedAlqahatani","        Open-Access and Collaborative Consumer Health Vocabulary (OAC CHV, or CHV for short), is a collection of medical terms written in plain English. It provides a list of simple, easy, and clear terms that laymen prefer to use rather than an equivalent professional medical term. The National Library of Medicine (NLM) has integrated and mapped the CHV terms to their Unified Medical Language System (UMLS). These CHV terms mapped to 56000 professional concepts on the UMLS. We found that about 48% of these laymen's terms are still jargon and matched with the professional terms on the UMLS. In this paper, we present an enhanced word embedding technique that generates new CHV terms from a consumer-generated text. We downloaded our corpus from a healthcare social media and evaluated our new method based on iterative feedback to word embedding using ground truth built from the existing CHV terms. Our feedback algorithm outperformed unmodified GLoVe and new CHV terms have been detected.        △ Less","13 April, 2020","cs.CL,cs.IR,cs.LG,stat.ML",
              Diagnosing COVID-19 Pneumonia from X-Ray and CT Images using Deep Learning and Transfer Learning Algorithms          ,2004.00038,https://arxiv.org/abs/2004.00038,https://arxiv.org/pdf/2004.00038,"Authors:HalgurdS.Maghdid,ArasT.Asaad,KayhanZrarGhafoor,AliSafaaSadiq,MuhammadKhurramKhan","        COVID-19 (also known as 2019 Novel Coronavirus) first emerged in Wuhan, China and spread across the globe with unprecedented effect and has now become the greatest crisis of the modern era. The COVID-19 has proved much more pervasive demands for diagnosis that has driven researchers to develop more intelligent, highly responsive and efficient detection methods. In this work, we focus on proposing AI tools that can be used by radiologists or healthcare professionals to diagnose COVID-19 cases in a quick and accurate manner. However, the lack of a publicly available dataset of X-ray and CT images makes the design of such AI tools a challenging task. To this end, this study aims to build a comprehensive dataset of X-rays and CT scan images from multiple sources as well as provides a simple but an effective COVID-19 detection technique using deep learning and transfer learning algorithms. In this vein, a simple convolution neural network (CNN) and modified pre-trained AlexNet model are applied on the prepared X-rays and CT scan images dataset. The result of the experiments shows that the utilized models can provide accuracy up to 98 % via pre-trained network and 94.1 % accuracy by using the modified CNN.        △ Less","31 March, 2020","eess.IV,cs.CV,cs.LG",
              COVID-ResNet: A Deep Learning Framework for Screening of COVID19 from Radiographs          ,2003.14395,https://arxiv.org/abs/2003.14395,https://arxiv.org/pdf/2003.14395,"Authors:MuhammadFarooq,AbdulHafeez","        In the last few months, the novel COVID19 pandemic has spread all over the world. Due to its easy transmission, developing techniques to accurately and easily identify the presence of COVID19 and distinguish it from other forms of flu and pneumonia is crucial. Recent research has shown that the chest Xrays of patients suffering from COVID19 depicts certain abnormalities in the radiography. However, those approaches are closed source and not made available to the research community for re-producibility and gaining deeper insight. The goal of this work is to build open source and open access datasets and present an accurate Convolutional Neural Network framework for differentiating COVID19 cases from other pneumonia cases. Our work utilizes state of the art training techniques including progressive resizing, cyclical learning rate finding and discriminative learning rates to training fast and accurate residual neural networks. Using these techniques, we showed the state of the art results on the open-access COVID-19 dataset. This work presents a 3-step technique to fine-tune a pre-trained ResNet-50 architecture to improve model performance and reduce training time. We call it COVIDResNet. This is achieved through progressively re-sizing of input images to 128x128x3, 224x224x3, and 229x229x3 pixels and fine-tuning the network at each stage. This approach along with the automatic learning rate selection enabled us to achieve the state of the art accuracy of 96.23% (on all the classes) on the COVIDx dataset with only 41 epochs. This work presented a computationally efficient and highly accurate model for multi-class classification of three different infection types from along with Normal individuals. This model can help in the early screening of COVID19 cases and help reduce the burden on healthcare systems.        △ Less","31 March, 2020","eess.IV,cs.CV,cs.LG",
              Security Assurance Cases for Road Vehicles: an Industry Perspective          ,2003.14106,https://arxiv.org/abs/2003.14106,https://arxiv.org/pdf/2003.14106,"Authors:MazenMohamad,AlexanderÅström,ÖrjanAskerdal,JörgenBorg,RiccardoScandariato","        Assurance cases are structured arguments that are commonly used to reason about the safety of a product or service. Currently, there is an ongoing push towards using assurance cases for also cybersecurity, especially in safety-critical domains, like automotive. While the industry is faced with the challenge of defining a sound methodology to build security assurance cases, the state of the art is rather immature. Therefore, we have conducted a thorough investigation of the (external) constraints and (internal) needs that security assurance cases have to satisfy in the context of the automotive industry. This has been done in the context of two large automotive companies in Sweden. The end result is a set of recommendations that automotive companies can apply in order to define security assurance cases that are (i) aligned with the constraints imposed by the existing and upcoming standards and regulations and (ii)harmonized with the internal product development processes and organizational practices. We expect the results to be also of interest for product companies in other safety-critical domains, like healthcare, transportation, and so on        △ Less","31 March, 2020",cs.SE,
              Static vs accumulating priorities in healthcare queues under heavy loads          ,2003.14087,https://arxiv.org/abs/2003.14087,https://arxiv.org/pdf/2003.14087,"Authors:BinyaminOz,SevaShneer,IlzeZiedins","        Amid unprecedented times caused by COVID-19, healthcare systems all over the world are strained to the limits of, or even beyond, capacity. A similar event is experienced by some healthcare systems regularly, due to for instance seasonal spikes in the number of patients. We model this as a queueing system in heavy traffic (where the arrival rate is approaching the service rate from below) or in overload (where the arrival rate exceeds the service rate). In both cases we assume that customers (patients) may have different priorities and we consider two popular service disciplines: static priorities and accumulating priorities. It has been shown that the latter allows for patients of all classes to be seen in a timely manner as long as the system is stable. We demonstrate however that if accumulating priorities are used in the heavy traffic or overload regime, then all patients, including those with the highest priority, will experience very long waiting times. If on the other hand static priorities are applied, then one can ensure that the highest-priority patients will be seen in a timely manner even in overloaded systems.        △ Less","29 April, 2020","cs.PF,math.PR",
              Using VERA to explain the impact of social distancing on the spread of COVID-19          ,2003.13762,https://arxiv.org/abs/2003.13762,https://arxiv.org/pdf/2003.13762,"Authors:WilliamBroniec,SungeunAn,SpencerRugaber,AshokK.Goel","        COVID-19 continues to spread across the country and around the world. Current strategies for managing the spread of COVID-19 include social distancing. We present VERA, an interactive AI tool, that first enables users to specify conceptual models of the impact of social distancing on the spread of COVID-19. Then, VERA automatically spawns agent-based simulations from the conceptual models, and, given a data set, automatically fills in the values of the simulation parameters from the data. Next, the user can view the simulation results, and, if needed, revise the simulation parameters and run another experimental trial, or build an alternative conceptual model. We describe the use VERA to develop a SIR model for the spread of COVID-19 and its relationship with healthcare capacity.        △ Less","30 March, 2020","cs.CY,cs.HC,q-bio.PE",
              Anonymous Collocation Discovery: Harnessing Privacy to Tame the Coronavirus          ,2003.13670,https://arxiv.org/abs/2003.13670,https://arxiv.org/pdf/2003.13670,"Authors:RanCanetti,AriTrachtenberg,MayankVaria","        Successful containment of the Coronavirus pandemic rests on the ability to quickly and reliably identify those who have been in close proximity to a contagious individual. Existing tools for doing so rely on the collection of exact location information of individuals over lengthy time periods, and combining this information with other personal information. This unprecedented encroachment on individual privacy at national scales has created an outcry and risks rejection of these tools.  We propose an alternative: an extremely simple scheme for providing fine-grained and timely alerts to users who have been in the close vicinity of an infected individual. Crucially, this is done while preserving the anonymity of all individuals, and without collecting or storing any personal information or location history. Our approach is based on using short-range communication mechanisms, like Bluetooth, that are available in all modern cell phones. It can be deployed with very little infrastructure, and incurs a relatively low false-positive rate compared to other collocation methods. We also describe a number of extensions and tradeoffs.  We believe that the privacy guarantees provided by the scheme will encourage quick and broad voluntary adoption. When combined with sufficient testing capacity and existing best practices from healthcare professionals, we hope that this may significantly reduce the infection rate.        △ Less","3 April, 2020","cs.CY,cs.CR",
              Can AI help in screening Viral and COVID-19 pneumonia?          ,2003.13145,https://arxiv.org/abs/2003.13145,https://arxiv.org/pdf/2003.13145,"Authors:MuhammadE.H.Chowdhury,TawsifurRahman,AmithKhandakar,RashidMazhar,MuhammadAbdulKadir,ZaidBinMahbub,KhandakerReajulIslam,MuhammadSalmanKhan,AtifIqbal,NasserAl-Emadi,MamunBinIbneReaz,T.I.Islam","        Coronavirus disease (COVID-19) is a pandemic disease, which has already caused thousands of causalities and infected several millions of people worldwide. Any technological tool enabling rapid screening of the COVID-19 infection with high accuracy can be crucially helpful to healthcare professionals. The main clinical tool currently in use for the diagnosis of COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which is expensive, less-sensitive and requires specialized medical personnel. X-ray imaging is an easily accessible tool that can be an excellent alternative in the COVID-19 diagnosis. This research was taken to investigate the utility of artificial intelligence (AI) in the rapid and accurate detection of COVID-19 from chest X-ray images. The aim of this paper is to propose a robust technique for automatic detection of COVID-19 pneumonia from digital chest X-ray images applying pre-trained deep-learning algorithms while maximizing the detection accuracy. A public database was created by the authors combining several public databases and also by collecting images from recently published articles. The database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579 normal chest X-ray images. Transfer learning technique was used with the help of image augmentation to train and validate several pre-trained deep Convolutional Neural Networks (CNNs). The networks were trained to classify two different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and COVID-19 pneumonia with and without image augmentation. The classification accuracy, precision, sensitivity, and specificity for both the schemes were 99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%, respectively.        △ Less","15 June, 2020","cs.LG,cs.CV",10.1109/ACCESS.2020.3010287 
              IoT Blockchain Solution for Air Quality Monitoring in SmartCities          ,2003.12920,https://arxiv.org/abs/2003.12920,https://arxiv.org/pdf/2003.12920,"Authors:ShajulinBenedict,RumaizeP.,JaspreetKaur","        IoT cloud enabled societal applications have dramatically increased in the recent past due to the thrust for innovations, notably through startup initiatives, in various sectors such as agriculture, healthcare, industry, and so forth. The existing IoT cloud solutions have led practitioners or researchers to a haphazard clutter of serious security hazards and performance inefficiencies. This paper proposes a blockchain enabled IoT cloud implementation to tackle the existing issues in smart cities. It particularly highlights the implementation of chaincodes for air quality monitoring systems in SmartCities; the proposed architecture named as IoT enabled Blockchain for Air Quality Monitoring System (IB-AQMS) is illustrated using experiments. Experimental results were carried out and the findings were disclosed in the paper.        △ Less","28 March, 2020","cs.NI,cs.CR",
              Build-at-home UV-C disinfection system for healthcare settings          ,2003.12916,https://arxiv.org/abs/2003.12916,https://arxiv.org/pdf/2003.12916,"Authors:RosemaryC.She,DongyuChen,PilPak,DenizK.Armani,AndreasSchubert,AndreaM.Armani","        Significant research has shown that UV-C exposure is an effective disinfectant for a range of bacteria and viruses, including coronaviruses. As such, a UV-C treatment in combination with a chemical wipe, such as EPA hydrogen peroxide, is a common cleaning protocol in a medical setting, and such disinfection protocols have gained in importance during the current COVID-19 pandemic due to the need to reuse PPE. However, given the substantial increase in patient volume, the quantity of materials requiring disinfection exceeds the UV-C equipment throughput capabilities at many medical facilities. Therefore, there is a need for a UV-C disinfection system that can be rapidly deployed. In response to this demand, we designed, constructed, and validated a UV-C disinfection system from readily accessible components; specifically, a plastic bin, UV-C light bulb and conventional light housing. To further improve the performance, the interior of the tub was spray-painted with chrome paint, forming a low quality-factor (Q) Fabry-Perot optical cavity. As part of this work, a set of modular design criteria which allows for flexibility in component selection without degradation of UV-C dose performance is established. This flexibility is critical given the current fluctuating availability of source materials. The disinfection capabilities of the system are validated using Bacillus cereus, a gram-positive endospore-forming bacteria.        △ Less","1 April, 2020","physics.med-ph,physics.soc-ph,q-bio.OT",
              Learning medical triage from clinicians using Deep Q-Learning          ,2003.12828,https://arxiv.org/abs/2003.12828,https://arxiv.org/pdf/2003.12828,"Authors:AlbertBuchard,BaptisteBouvier,GiuliaPrando,RoryBeard,MichailLivieratos,DanBusbridge,DanielThompson,JonathanRichens,YuanzhaoZhang,AdamBaker,YuraPerov,KostisGourgoulias,SaurabhJohri","        Medical Triage is of paramount importance to healthcare systems, allowing for the correct orientation of patients and allocation of the necessary resources to treat them adequately. While reliable decision-tree methods exist to triage patients based on their presentation, those trees implicitly require human inference and are not immediately applicable in a fully automated setting. On the other hand, learning triage policies directly from experts may correct for some of the limitations of hard-coded decision-trees. In this work, we present a Deep Reinforcement Learning approach (a variant of DeepQ-Learning) to triage patients using curated clinical vignettes. The dataset, consisting of 1374 clinical vignettes, was created by medical doctors to represent real-life cases. Each vignette is associated with an average of 3.8 expert triage decisions given by medical doctors relying solely on medical history. We show that this approach is on a par with human performance, yielding safe triage decisions in 94% of cases, and matching expert decisions in 85% of cases. The trained agent learns when to stop asking questions, acquires optimized decision policies requiring less evidence than supervised approaches, and adapts to the novelty of a situation by asking for more information. Overall, we demonstrate that a Deep Reinforcement Learning approach can learn effective medical triage policies directly from expert decisions, without requiring expert knowledge engineering. This approach is scalable and can be deployed in healthcare settings or geographical regions with distinct triage specifications, or where trained experts are scarce, to improve decision making in the early stage of care.        △ Less","24 June, 2020",cs.AI,
              Radar networks: A review of features and challenges          ,2003.12746,https://arxiv.org/abs/2003.12746,https://arxiv.org/pdf/2003.12746,"Authors:S.HamedJavadi,AlfonsoFarina","        Networks of multiple radars are typically used for improving the coverage and tracking accuracy. Recently, such networks have facilitated deployment of commercial radars for civilian applications such as healthcare, gesture recognition, home security, and autonomous automobiles. They exploit advanced signal processing techniques together with efficient data fusion methods in order to yield high performance of event detection and tracking. This paper reviews outstanding features of radar networks, their challenges, and their state-of-the-art solutions from the perspective of signal processing. Each discussed subject can be evolved as a hot research topic.        △ Less","28 March, 2020",eess.SP,10.1016/j.inffus.2020.03.005 
              Can We Use Split Learning on 1D CNN Models for Privacy Preserving Training?          ,2003.12365,https://arxiv.org/abs/2003.12365,https://arxiv.org/pdf/2003.12365,"Authors:SharifAbuadbba,KyuyeonKim,MinkiKim,ChandraThapa,SeyitA.Camtepe,YansongGao,HyoungshickKim,SuryaNepal","        A new collaborative learning, called split learning, was recently introduced, aiming to protect user data privacy without revealing raw input data to a server. It collaboratively runs a deep neural network model where the model is split into two parts, one for the client and the other for the server. Therefore, the server has no direct access to raw data processed at the client. Until now, the split learning is believed to be a promising approach to protect the client's raw data; for example, the client's data was protected in healthcare image applications using 2D convolutional neural network (CNN) models. However, it is still unclear whether the split learning can be applied to other deep learning models, in particular, 1D CNN.  In this paper, we examine whether split learning can be used to perform privacy-preserving training for 1D CNN models. To answer this, we first design and implement an 1D CNN model under split learning and validate its efficacy in detecting heart abnormalities using medical ECG data. We observed that the 1D CNN model under split learning can achieve the same accuracy of 98.9\% like the original (non-split) model. However, our evaluation demonstrates that split learning may fail to protect the raw data privacy on 1D CNN models. To address the observed privacy leakage in split learning, we adopt two privacy leakage mitigation techniques: 1) adding more hidden layers to the client side and 2) applying differential privacy. Although those mitigation techniques are helpful in reducing privacy leakage, they have a significant impact on model accuracy. Hence, based on those results, we conclude that split learning alone would not be sufficient to maintain the confidentiality of raw sequential data in 1D CNN models.        △ Less","16 March, 2020","cs.CR,cs.LG,cs.NE,stat.ML",
              Optimization of Genomic Classifiers for Clinical Deployment: Evaluation of Bayesian Optimization to Select Predictive Models of Acute Infection and In-Hospital Mortality          ,2003.12310,https://arxiv.org/abs/2003.12310,https://arxiv.org/pdf/2003.12310,"Authors:MichaelB.Mayhew,ElizabethTran,KirindiChoi,UrosMidic,RolandLuethy,NanditaDamaraju,LjubomirButurovic","        Acute infection, if not rapidly and accurately detected, can lead to sepsis, organ failure and even death. Current detection of acute infection as well as assessment of a patient's severity of illness are imperfect. Characterization of a patient's immune response by quantifying expression levels of specific genes from blood represents a potentially more timely and precise means of accomplishing both tasks. Machine learning methods provide a platform to leverage this 'host response' for development of deployment-ready classification models. Prioritization of promising classifiers is dependent, in part, on hyperparameter optimization for which a number of approaches including grid search, random sampling and Bayesian optimization have been shown to be effective. We compare HO approaches for the development of diagnostic classifiers of acute infection and in-hospital mortality from gene expression of 29 diagnostic markers. We take a deployment-centered approach to our comprehensive analysis, accounting for heterogeneity in our multi-study patient cohort with our choices of dataset partitioning and hyperparameter optimization objective as well as assessing selected classifiers in external (as well as internal) validation. We find that classifiers selected by Bayesian optimization for in-hospital mortality can outperform those selected by grid search or random sampling. However, in contrast to previous research: 1) Bayesian optimization is not more efficient in selecting classifiers in all instances compared to grid search or random sampling-based methods and 2) we note marginal gains in classifier performance in only specific circumstances when using a common variant of Bayesian optimization (i.e. automatic relevance determination). Our analysis highlights the need for further practical, deployment-centered benchmarking of HO approaches in the healthcare context.        △ Less","13 October, 2020","cs.LG,q-bio.QM,stat.ML",
              TRACER: A Framework for Facilitating Accurate and Interpretable Analytics for High Stakes Applications          ,2003.12012,https://arxiv.org/abs/2003.12012,https://arxiv.org/pdf/2003.12012,"Authors:KaipingZheng,ShaofengCai,HorngRueyChua,WeiWang,KeeYuanNgiam,BengChinOoi","        In high stakes applications such as healthcare and finance analytics, the interpretability of predictive models is required and necessary for domain practitioners to trust the predictions. Traditional machine learning models, e.g., logistic regression (LR), are easy to interpret in nature. However, many of these models aggregate time-series data without considering the temporal correlations and variations. Therefore, their performance cannot match up to recurrent neural network (RNN) based models, which are nonetheless difficult to interpret. In this paper, we propose a general framework TRACER to facilitate accurate and interpretable predictions, with a novel model TITV devised for healthcare analytics and other high stakes applications such as financial investment and risk management. Different from LR and other existing RNN-based models, TITV is designed to capture both the time-invariant and the time-variant feature importance using a feature-wise transformation subnetwork and a self-attention subnetwork, for the feature influence shared over the entire time series and the time-related importance respectively. Healthcare analytics is adopted as a driving use case, and we note that the proposed TRACER is also applicable to other domains, e.g., fintech. We evaluate the accuracy of TRACER extensively in two real-world hospital datasets, and our doctors/clinicians further validate the interpretability of TRACER in both the patient level and the feature level. Besides, TRACER is also validated in a high stakes financial application and a critical temperature forecasting application. The experimental results confirm that TRACER facilitates both accurate and interpretable analytics for high stakes applications.        △ Less","24 March, 2020","eess.SP,cs.AI,cs.LG,stat.AP,stat.ML",
              Prior Adaptive Semi-supervised Learning with Application to EHR Phenotyping          ,2003.11744,https://arxiv.org/abs/2003.11744,https://arxiv.org/pdf/2003.11744,"Authors:YichiZhang,MoleiLiu,MateyNeykov,TianxiCai","        Electronic Health Records (EHR) data, a rich source for biomedical research, have been successfully used to gain novel insight into a wide range of diseases. Despite its potential, EHR is currently underutilized for discovery research due to it's major limitation in the lack of precise phenotype information. To overcome such difficulties, recent efforts have been devoted to developing supervised algorithms to accurately predict phenotypes based on relatively small training datasets with gold standard labels extracted via chart review. However, supervised methods typically require a sizable training set to yield generalizable algorithms especially when the number of candidate features, pp, is large. In this paper, we propose a semi-supervised (SS) EHR phenotyping method that borrows information from both a small labeled data where both the label YY and the feature set XX are observed and a much larger unlabeled data with observations on XX only as well as a surrogate variable SS that is predictive of YY and available for all patients, under a high dimensional setting. Under a working prior assumption that SS is related to XX only through YY and allowing it to hold approximately, we propose a prior adaptive semi-supervised (PASS) estimator that adaptively incorporates the prior knowledge by shrinking the estimator towards a direction derived under the prior. We derive asymptotic theory for the proposed estimator and demonstrate its superiority over existing estimators via simulation studies. The proposed method is applied to an EHR phenotyping study of rheumatoid arthritis at Partner's Healthcare.        △ Less","26 March, 2020",stat.ME,
              Susceptible-Infected-Recovered (SIR) Dynamics of COVID-19 and Economic Impact          ,2003.11221,https://arxiv.org/abs/2003.11221,https://arxiv.org/pdf/2003.11221,Authors:AlexisAkiraToda,"        I estimate the Susceptible-Infected-Recovered (SIR) epidemic model for Coronavirus Disease 2019 (COVID-19). The transmission rate is heterogeneous across countries and far exceeds the recovery rate, which enables a fast spread. In the benchmark model, 28% of the population may be simultaneously infected at the peak, potentially overwhelming the healthcare system. The peak reduces to 6.2% under the optimal mitigation policy that controls the timing and intensity of social distancing. A stylized asset pricing model suggests that the stock price temporarily decreases by 50% in the benchmark case but shows a W-shaped, moderate but longer bear market under the optimal policy.        △ Less","26 March, 2020","q-bio.PE,econ.GN",
              Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning          ,2003.11126,https://arxiv.org/abs/2003.11126,https://arxiv.org/pdf/2003.11126,"Authors:AliMousavi,LihongLi,QiangLiu,DennyZhou","        Off-policy estimation for long-horizon problems is important in many real-life applications such as healthcare and robotics, where high-fidelity simulators may not be available and on-policy evaluation is expensive or impossible. Recently, \cite{liu18breaking} proposed an approach that avoids the \emph{curse of horizon} suffered by typical importance-sampling-based methods. While showing promising results, this approach is limited in practice as it requires data be drawn from the \emph{stationary distribution} of a \emph{known} behavior policy. In this work, we propose a novel approach that eliminates such limitations. In particular, we formulate the problem as solving for the fixed point of a certain operator. Using tools from Reproducing Kernel Hilbert Spaces (RKHSs), we develop a new estimator that computes importance ratios of stationary distributions, without knowledge of how the off-policy data are collected. We analyze its asymptotic consistency and finite-sample generalization. Experiments on benchmarks verify the effectiveness of our approach.        △ Less","24 March, 2020","cs.LG,cs.AI,stat.ML",
              Covid-19 Tweeting in English: Gender Differences          ,2003.11090,https://arxiv.org/abs/2003.11090,https://arxiv.org/pdf/2003.11090,"Authors:MikeThelwall,SaheedaThelwall","        At the start of 2020, COVID-19 became the most urgent threat to global public health. Uniquely in recent times, governments have imposed partly voluntary, partly compulsory restrictions on the population to slow the spread of the virus. In this context, public attitudes and behaviors are vitally important for reducing the death rate. Analyzing tweets about the disease may therefore give insights into public reactions that may help guide public information campaigns. This article analyses 3,038,026 English tweets about COVID-19 from March 10 to 23, 2020. It focuses on one relevant aspect of public reaction: gender differences. The results show that females are more likely to tweet about the virus in the context of family, social distancing and healthcare whereas males are more likely to tweet about sports cancellations, the global spread of the virus and political reactions. Thus, women seem to be taking a disproportionate share of the responsibility for directly keeping the population safe. The detailed results may be useful to inform public information announcements and to help understand the spread of the virus. For example, failure to impose a sporting bans whilst encouraging social distancing may send mixed messages to males.        △ Less","24 March, 2020","cs.DL,cs.SI",
              Is a COVID19 Quarantine Justified in Chile or USA Right Now?          ,2003.10879,https://arxiv.org/abs/2003.10879,https://arxiv.org/pdf/2003.10879,"Authors:R.I.Gonzalez,F.Munoz,P.S.Moya,M.Kiwi","        During the current COVID-19 pandemic it is imperative to give early warnings to reduce mortality. However, non-specialist such as authorities and the general population face several problems to understand the real thread of this pandemic, and under/overestimation of its risk are a commonplace in the press and social media. Here we define an index, which we call the COVID-19 Burden Index, that relates the capacities of the healthcare system of a given country to treat severe and critical cases. Its value is 0 if there is no extra strain in the healthcare system, and it reaches 1.0 when the collapse is imminent. As of 23 March 2020, we show that Chile, the USA, UK, among other countries, must reduce the rate of infections right now, otherwise, in less than 7 days they could be in a catastrophic situation such as Italy, Spain and Iran.        △ Less","24 March, 2020","physics.med-ph,physics.soc-ph,q-bio.PE",
"              PanNuke Dataset Extension, Insights and Baselines          ",2003.10778,https://arxiv.org/abs/2003.10778,https://arxiv.org/pdf/2003.10778,"Authors:JevgenijGamper,NavidAlemiKoohbanani,KsenijaBenes,SimonGraham,MostafaJahanifar,SyedAliKhurram,AyeshaAzam,KatherineHewitt,NasirRajpoot","        The emerging area of computational pathology (CPath) is ripe ground for the application of deep learning (DL) methods to healthcare due to the sheer volume of raw pixel data in whole-slide images (WSIs) of cancerous tissue slides. However, it is imperative for the DL algorithms relying on nuclei-level details to be able to cope with data from `the clinical wild', which tends to be quite challenging.  We study, and extend recently released PanNuke dataset consisting of ~200,000 nuclei categorized into 5 clinically important classes for the challenging tasks of segmenting and classifying nuclei in WSIs. Previous pan-cancer datasets consisted of only up to 9 different tissues and up to 21,000 unlabeled nuclei and just over 24,000 labeled nuclei with segmentation masks. PanNuke consists of 19 different tissue types that have been semi-automatically annotated and quality controlled by clinical pathologists, leading to a dataset with statistics similar to the clinical wild and with minimal selection bias. We study the performance of segmentation and classification models when applied to the proposed dataset and demonstrate the application of models trained on PanNuke to whole-slide images. We provide comprehensive statistics about the dataset and outline recommendations and research directions to address the limitations of existing DL tools when applied to real-world CPath applications.        △ Less","22 April, 2020","eess.IV,cs.CV,q-bio.QM",
              Estimating Uncertainty and Interpretability in Deep Learning for Coronavirus (COVID-19) Detection          ,2003.10769,https://arxiv.org/abs/2003.10769,https://arxiv.org/pdf/2003.10769,"Authors:BirajaGhoshal,AllanTucker","        Deep Learning has achieved state of the art performance in medical imaging. However, these methods for disease detection focus exclusively on improving the accuracy of classification or predictions without quantifying uncertainty in a decision. Knowing how much confidence there is in a computer-based medical diagnosis is essential for gaining clinicians trust in the technology and therefore improve treatment. Today, the 2019 Coronavirus (SARS-CoV-2) infections are a major healthcare challenge around the world. Detecting COVID-19 in X-ray images is crucial for diagnosis, assessment and treatment. However, diagnostic uncertainty in the report is a challenging and yet inevitable task for radiologist. In this paper, we investigate how drop-weights based Bayesian Convolutional Neural Networks (BCNN) can estimate uncertainty in Deep Learning solution to improve the diagnostic performance of the human-machine team using publicly available COVID-19 chest X-ray dataset and show that the uncertainty in prediction is highly correlates with accuracy of prediction. We believe that the availability of uncertainty-aware deep learning solution will enable a wider adoption of Artificial Intelligence (AI) in a clinical setting.        △ Less","27 March, 2020","eess.IV,cs.CV,cs.LG,stat.ML",
              From Bit To Bedside: A Practical Framework For Artificial Intelligence Product Development In Healthcare,2003.10303,https://arxiv.org/abs/2003.10303,https://arxiv.org/pdf/2003.10303,"Authors:DavidHiggins,VinceI.Madai","        Artificial Intelligence (AI) in healthcare holds great potential to expand access to high-quality medical care, whilst reducing overall systemic costs. Despite hitting the headlines regularly and many publications of proofs-of-concept, certified products are failing to breakthrough to the clinic. AI in healthcare is a multi-party process with deep knowledge required in multiple individual domains. The lack of understanding of the specific challenges in the domain is, therefore, the major contributor to the failure to deliver on the big promises. Thus, we present a decision perspective framework, for the development of AI-driven biomedical products, from conception to market launch. Our framework highlights the risks, objectives and key results which are typically required to proceed through a three-phase process to the market launch of a validated medical AI product. We focus on issues related to Clinical validation, Regulatory affairs, Data strategy and Algorithmic development. The development process we propose for AI in healthcare software strongly diverges from modern consumer software development processes. We highlight the key time points to guide founders, investors and key stakeholders throughout their relevant part of the process. Our framework should be seen as a template for innovation frameworks, which can be used to coordinate team communications and responsibilities towards a reasonable product development roadmap, thus unlocking the potential of AI in medicine.        △ Less","23 March, 2020","cs.CY,cs.AI,cs.HC",10.1002/aisy.202000052 
              Boosting test-efficiency by pooled testing strategies for SARS-CoV-2          ,2003.09944,https://arxiv.org/abs/2003.09944,https://arxiv.org/pdf/2003.09944,"Authors:RudolfHanel,StefanThurner","        In the current COVID19 crisis many national healthcare systems are confronted with an acute shortage of tests for confirming SARS-CoV-2 infections. For low overall infection levels in the population, pooling of samples can drastically amplify the testing efficiency. Here we present a formula to estimate the optimal pooling size, the efficiency gain (tested persons per test), and the expected upper bound of missed infections in the pooled testing, all as a function of the populationwide infection levels and the false negative/positive rates of the currently used PCR tests. Assuming an infection level of 0.1 % and a false negative rate of 2 %, the optimal pool size is about 32, the efficiency gain is about 15 tested persons per test. For an infection level of 1 % the optimal pool size is 11, the efficiency gain is 5.1 tested persons per test. For an infection level of 10 % the optimal pool size reduces to about 4, the efficiency gain is about 1.7 tested persons per test. For infection levels of 30 % and higher there is no more benefit from pooling. To see to what extent replicates of the pooled tests improve the estimate of the maximal number of missed infections, we present all results for 1, 3, and 5 replicates.        △ Less","22 March, 2020","q-bio.PE,stat.AP",
              A SIDARTHE Model of COVID-19 Epidemic in Italy          ,2003.09861,https://arxiv.org/abs/2003.09861,https://arxiv.org/pdf/2003.09861,"Authors:GiuliaGiordano,FrancoBlanchini,RaffaeleBruno,PatrizioColaneri,AlessandroDiFilippo,AngelaDiMatteo,MartaColaneri,theCOVID19IRCCSSanMatteoPaviaTaskForce","        In late December 2019, a novel strand of Coronavirus (SARS-CoV-2) causing a severe, potentially fatal respiratory syndrome (COVID-19) was identified in Wuhan, Hubei Province, China and is causing outbreaks in multiple world countries, soon becoming a pandemic. Italy has now become the most hit country outside of Asia: on March 16, 2020, the Italian Civil Protection documented a total of 27980 confirmed cases and 2158 deaths of people tested positive for SARS-CoV-2. In the context of an emerging infectious disease outbreak, it is of paramount importance to predict the trend of the epidemic in order to plan an effective control strategy and to determine its impact. This paper proposes a new epidemic model that discriminates between infected individuals depending on whether they have been diagnosed and on the severity of their symptoms. The distinction between diagnosed and non-diagnosed is important because non-diagnosed individuals are more likely to spread the infection than diagnosed ones, since the latter are typically isolated, and can explain misperceptions of the case fatality rate and of the seriousness of the epidemic phenomenon. Being able to predict the amount of patients that will develop life-threatening symptoms is important since the disease frequently requires hospitalisation (and even Intensive Care Unit admission) and challenges the healthcare system capacity. We show how the basic reproduction number can be redefined in the new framework, thus capturing the potential for epidemic containment. Simulation results are compared with real data on the COVID-19 epidemic in Italy, to show the validity of the model and compare different possible predicted scenarios depending on the adopted countermeasures.        △ Less","22 March, 2020","q-bio.PE,eess.SY,math.DS",10.1038/s41591-020-0883-7 
              NeuCrowd: Neural Sampling Network for Representation Learning with Crowdsourced Labels          ,2003.09660,https://arxiv.org/abs/2003.09660,https://arxiv.org/pdf/2003.09660,"Authors:YangHao,WenbiaoDing,ZhongqinWu,ZitaoLiu","        Representation learning approaches require a massive amount of discriminative training data, which is unavailable in many scenarios, such as healthcare, smart city, education, etc. In practice, people refer to crowdsourcing to get annotated labels. However, due to issues like data privacy, budget limitation, shortage of domain-specific annotators, the number of crowdsourced labels is still very limited. Moreover, because of annotators' diverse expertises, crowdsourced labels are often inconsistent. Thus, directly applying existing supervised representation learning (SRL) algorithms may easily get the overfitting problem and yield suboptimal solutions. In this paper, we propose \emph{NeuCrowd}, a unified framework for SRL from crowdsourced labels. The proposed framework (1) creates a sufficient number of high-quality \emph{n}-tuplet training samples by utilizing safety-aware sampling and robust anchor generation; and (2) automatically learns a neural sampling network that adaptively learns to select effective samples for SRL networks. The proposed framework is evaluated on both one synthetic and three real-world data sets. The results show that our approach outperforms a wide range of state-of-the-art baselines in terms of prediction accuracy and AUC. To encourage the reproducible results, we make our code publicly available at \url{https://github.com/crowd-data-mining/NeuCrowd}.        △ Less","23 September, 2020","cs.LG,cs.AI,stat.ML",
"              The early phase of the COVID-19 outbreak in Lombardy, Italy          ",2003.09320,https://arxiv.org/abs/2003.09320,https://arxiv.org/pdf/2003.09320,"Authors:DCereda,MTirani,FRovida,VDemicheli,MAjelli,PPoletti,FTrentini,GGuzzetta,VMarziano,ABarone,MMagoni,SDeandrea,GDiurno,MLombardo,MFaccini,APan,RBruno,EPariani,GGrasselli,APiatti,MGramegna,FBaldanti,AMelegaro,SMerler","        In the night of February 20, 2020, the first case of novel coronavirus disease (COVID-19) was confirmed in the Lombardy Region, Italy. In the week that followed, Lombardy experienced a very rapid increase in the number of cases. We analyzed the first 5,830 laboratory-confirmed cases to provide the first epidemiological characterization of a COVID-19 outbreak in a Western Country. Epidemiological data were collected through standardized interviews of confirmed cases and their close contacts. We collected demographic backgrounds, dates of symptom onset, clinical features, respiratory tract specimen results, hospitalization, contact tracing. We provide estimates of the reproduction number and serial interval. The epidemic in Italy started much earlier than February 20, 2020. At the time of detection of the first COVID-19 case, the epidemic had already spread in most municipalities of Southern-Lombardy. The median age for of cases is 69 years (range, 1 month to 101 years). 47% of positive subjects were hospitalized. Among these, 18% required intensive care. The mean serial interval is estimated to be 6.6 days (95% CI, 0.7 to 19). We estimate the basic reproduction number at 3.1 (95% CI, 2.9 to 3.2). We estimated a decreasing trend in the net reproduction number starting around February 20, 2020. We did not observe significantly different viral loads in nasal swabs between symptomatic and asymptomatic. The transmission potential of COVID-19 is very high and the number of critical cases may become largely unsustainable for the healthcare system in a very short-time horizon. We observed a slight decrease of the reproduction number, possibly connected with an increased population awareness and early effect of interventions. Aggressive containment strategies are required to control COVID-19 spread and catastrophic outcomes for the healthcare system.        △ Less","20 March, 2020",q-bio.PE,
              FedNER: Privacy-preserving Medical Named Entity Recognition with Federated Learning          ,2003.09288,https://arxiv.org/abs/2003.09288,https://arxiv.org/pdf/2003.09288,"Authors:SuyuGe,FangzhaoWu,ChuhanWu,TaoQi,YongfengHuang,XingXie","        Medical named entity recognition (NER) has wide applications in intelligent healthcare. Sufficient labeled data is critical for training accurate medical NER model. However, the labeled data in a single medical platform is usually limited. Although labeled datasets may exist in many different medical platforms, they cannot be directly shared since medical data is highly privacy-sensitive. In this paper, we propose a privacy-preserving medical NER method based on federated learning, which can leverage the labeled data in different platforms to boost the training of medical NER model and remove the need of exchanging raw data among different platforms. Since the labeled data in different platforms usually has some differences in entity type and annotation criteria, instead of constraining different platforms to share the same model, we decompose the medical NER model in each platform into a shared module and a private module. The private module is used to capture the characteristics of the local data in each platform, and is updated using local labeled data. The shared module is learned across different medical platform to capture the shared NER knowledge. Its local gradients from different platforms are aggregated to update the global shared module, which is further delivered to each platform to update their local shared modules. Experiments on three publicly available datasets validate the effectiveness of our method.        △ Less","25 March, 2020",cs.CL,
              Teacher-Student chain for efficient semi-supervised histology image classification          ,2003.08797,https://arxiv.org/abs/2003.08797,https://arxiv.org/pdf/2003.08797,"Authors:ShayneShaw,MaciejPajak,AnetaLisowska,SotiriosATsaftaris,AlisonQO'Neil","        Deep learning shows great potential for the domain of digital pathology. An automated digital pathology system could serve as a second reader, perform initial triage in large screening studies, or assist in reporting. However, it is expensive to exhaustively annotate large histology image databases, since medical specialists are a scarce resource. In this paper, we apply the semi-supervised teacher-student knowledge distillation technique proposed by Yalniz et al. (2019) to the task of quantifying prognostic features in colorectal cancer. We obtain accuracy improvements through extending this approach to a chain of students, where each student's predictions are used to train the next student i.e. the student becomes the teacher. Using the chain approach, and only 0.5% labelled data (the remaining 99.5% in the unlabelled pool), we match the accuracy of training on 100% labelled data. At lower percentages of labelled data, similar gains in accuracy are seen, allowing some recovery of accuracy even from a poor initial choice of labelled training set. In conclusion, this approach shows promise for reducing the annotation burden, thus increasing the affordability of automated digital pathology systems.        △ Less","20 March, 2020","cs.CV,cs.LG,eess.IV,stat.ML",
              A Review of Computational Approaches for Evaluation of Rehabilitation Exercises          ,2003.08767,https://arxiv.org/abs/2003.08767,https://arxiv.org/pdf/2003.08767,"Authors:YalinLiao,AleksandarVakanski,MinXian,DavidPaul,RussellBaker","        Recent advances in data analytics and computer-aided diagnostics stimulate the vision of patient-centric precision healthcare, where treatment plans are customized based on the health records and needs of every patient. In physical rehabilitation, the progress in machine learning and the advent of affordable and reliable motion capture sensors have been conducive to the development of approaches for automated assessment of patient performance and progress toward functional recovery. The presented study reviews computational approaches for evaluating patient performance in rehabilitation programs using motion capture systems. Such approaches will play an important role in supplementing traditional rehabilitation assessment performed by trained clinicians, and in assisting patients participating in home-based rehabilitation. The reviewed computational methods for exercise evaluation are grouped into three main categories: discrete movement score, rule-based, and template-based approaches. The review places an emphasis on the application of machine learning methods for movement evaluation in rehabilitation. Related work in the literature on data representation, feature engineering, movement segmentation, and scoring functions is presented. The study also reviews existing sensors for capturing rehabilitation movements and provides an informative listing of pertinent benchmark datasets. The significance of this paper is in being the first to provide a comprehensive review of computational methods for evaluation of patient performance in rehabilitation programs.        △ Less","19 March, 2020","cs.CV,cs.LG",10.1016/j.compbiomed.2020.103687 
              Super Low Resolution RF Powered Accelerometers for Alerting on Hospitalized Patient Bed Exits          ,2003.08530,https://arxiv.org/abs/2003.08530,https://arxiv.org/pdf/2003.08530,"Authors:MichaelChesser,AsangiJayatilaka,RenukaVisvanathan,ChristopheFumeaux,AlansonSample,DamithC.Ranasinghe","        Falls have serious consequences and are prevalent in acute hospitals and nursing homes caring for older people. Most falls occur in bedrooms and near the bed. Technological interventions to mitigate the risk of falling aim to automatically monitor bed-exit events and subsequently alert healthcare personnel to provide timely supervisions. We observe that frequency-domain information related to patient activities exist predominantly in very low frequencies. Therefore, we recognise the potential to employ a low resolution acceleration sensing modality in contrast to powering and sensing with a conventional MEMS (Micro Electro Mechanical System) accelerometer. Consequently, we investigate a batteryless sensing modality with low cost wirelessly powered Radio Frequency Identification (RFID) technology with the potential for convenient integration into clothing, such as hospital gowns. We design and build a passive accelerometer-based RFID sensor embodiment---ID-Sensor---for our study. The sensor design allows deriving ultra low resolution acceleration data from the rate of change of unique RFID tag identifiers in accordance with the movement of a patient's upper body. We investigate two convolutional neural network architectures for learning from raw RFID-only data streams and compare performance with a traditional shallow classifier with engineered features. We evaluate performance with 23 hospitalized older patients. We demonstrate, for the first time and to the best of knowledge, that: i) the low resolution acceleration data embedded in the RF powered ID-Sensor data stream can provide a practicable method for activity recognition; and ii) highly discriminative features can be efficiently learned from the raw RFID-only data stream using a fully convolutional network architecture.        △ Less","18 March, 2020","cs.CY,cs.LG,eess.SP",10.1109/PERCOM.2019.8767398 
              Patient-centric HetNets Powered by Machine Learning and Big Data Analytics for 6G Networks          ,2003.08239,https://arxiv.org/abs/2003.08239,https://arxiv.org/pdf/2003.08239,"Authors:MohammedS.Hadi,AhmedQ.Lawey,TaisirE.H.El-Gorashi,JaafarM.H.Elmirghani","        Having a cognitive and self-optimizing network that proactively adapts not only to channel conditions, but also according to its users needs can be one of the highest forthcoming priorities of future 6G Heterogeneous Networks (HetNets). In this paper, we introduce an interdisciplinary approach linking the concepts of e-healthcare, priority, big data analytics (BDA) and radio resource optimization in a multi-tier 5G network. We employ three machine learning (ML) algorithms, namely, naive Bayesian (NB) classifier, logistic regression (LR), and decision tree (DT), working as an ensemble system to analyze historical medical records of stroke out-patients (OPs) and readings from body-attached internet-of-things (IoT) sensors to predict the likelihood of an imminent stroke. We convert the stroke likelihood into a risk factor functioning as a priority in a mixed integer linear programming (MILP) optimization model. Hence, the task is to optimally allocate physical resource blocks (PRBs) to HetNet users while prioritizing OPs by granting them high gain PRBs according to the severity of their medical state. Thus, empowering the OPs to send their critical data to their healthcare provider with minimized delay. To that end, two optimization approaches are proposed, a weighted sum rate maximization (WSRMax) approach and a proportional fairness (PF) approach. The proposed approaches increased the OPs average signal to interference plus noise (SINR) by 57% and 95%, respectively. The WSRMax approach increased the system total SINR to a level higher than that of the PF approach, nevertheless, the PF approach yielded higher SINRs for the OPs, better fairness and a lower margin of error.        △ Less","18 March, 2020","cs.NI,eess.SP",
              The Future of Digital Health with Federated Learning          ,2003.08119,https://arxiv.org/abs/2003.08119,https://arxiv.org/pdf/2003.08119,"Authors:NicolaRieke,JonnyHancox,WenqiLi,FaustoMilletari,HolgerRoth,ShadiAlbarqouni,SpyridonBakas,MathieuN.Galtier,BennettLandman,KlausMaier-Hein,SebastienOurselin,MicahSheller,RonaldM.Summers,AndrewTrask,DaguangXu,MaximilianBaust,M.JorgeCardoso","        Data-driven Machine Learning has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing medical data is not fully exploited by ML primarily because it sits in data silos and privacy concerns restrict access to this data. However, without access to sufficient data, ML will be prevented from reaching its full potential and, ultimately, from making the transition from research to clinical practice. This paper considers key factors contributing to this issue, explores how Federated Learning (FL) may provide a solution for the future of digital health and highlights the challenges and considerations that need to be addressed.        △ Less","18 March, 2020","cs.CY,cs.LG",
              Assessing Robustness to Noise: Low-Cost Head CT Triage          ,2003.07977,https://arxiv.org/abs/2003.07977,https://arxiv.org/pdf/2003.07977,"Authors:SarahM.Hooper,JaredA.Dunnmon,MatthewP.Lungren,SanjivSamGambhir,ChristopherRé,AdamS.Wang,BhavikN.Patel","        Automated medical image classification with convolutional neural networks (CNNs) has great potential to impact healthcare, particularly in resource-constrained healthcare systems where fewer trained radiologists are available. However, little is known about how well a trained CNN can perform on images with the increased noise levels, different acquisition protocols, or additional artifacts that may arise when using low-cost scanners, which can be underrepresented in datasets collected from well-funded hospitals. In this work, we investigate how a model trained to triage head computed tomography (CT) scans performs on images acquired with reduced x-ray tube current, fewer projections per gantry rotation, and limited angle scans. These changes can reduce the cost of the scanner and demands on electrical power but come at the expense of increased image noise and artifacts. We first develop a model to triage head CTs and report an area under the receiver operating characteristic curve (AUROC) of 0.77. We then show that the trained model is robust to reduced tube current and fewer projections, with the AUROC dropping only 0.65% for images acquired with a 16x reduction in tube current and 0.22% for images acquired with 8x fewer projections. Finally, for significantly degraded images acquired by a limited angle scan, we show that a model trained specifically to classify such images can overcome the technological limitations to reconstruction and maintain an AUROC within 0.09% of the original model.        △ Less","28 March, 2020","eess.IV,cs.LG,stat.ML",
              An Overview and Case Study of the Clinical AI Model Development Life Cycle for Healthcare Systems          ,2003.07678,https://arxiv.org/abs/2003.07678,https://arxiv.org/pdf/2003.07678,"Authors:CharlesLu,JuliaStrout,RomaneGauriau,BradWright,FabiolaBezerraDeCarvalhoMarcruz,VarunBuch,KatherineAndriole","Healthcare is one of the most promising areas for machine learning models to make a positive impact. However, successful adoption of AI-based systems in healthcare depends on engaging and educating stakeholders from diverse backgrounds about the development process of AI models. We present a broadly accessible overview of the development life cycle of clinical AI models that is general enough to be adapted to most machine learning projects, and then give an in-depth case study of the development process of a deep learning based system to detect aortic aneurysms in Computed Tomography (CT) exams. We hope other healthcare institutions and clinical practitioners find the insights we share about the development process useful in informing their own model development efforts and to increase the likelihood of successful deployment and integration of AI in healthcare.        △ Less","26 March, 2020","cs.CY,cs.LG,eess.IV",
              Fair inference on error-prone outcomes          ,2003.07621,https://arxiv.org/abs/2003.07621,https://arxiv.org/pdf/2003.07621,"Authors:LauraBoeschoten,Erik-JanvanKesteren,AyoubBagheri,DanielL.Oberski","        Fair inference in supervised learning is an important and active area of research, yielding a range of useful methods to assess and account for fairness criteria when predicting ground truth targets. As shown in recent work, however, when target labels are error-prone, potential prediction unfairness can arise from measurement error. In this paper, we show that, when an error-prone proxy target is used, existing methods to assess and calibrate fairness criteria do not extend to the true target variable of interest. To remedy this problem, we suggest a framework resulting from the combination of two existing literatures: fair ML methods, such as those found in the counterfactual fairness literature on the one hand, and, on the other, measurement models found in the statistical literature. We discuss these approaches and their connection resulting in our framework. In a healthcare decision problem, we find that using a latent variable model to account for measurement error removes the unfairness detected previously.        △ Less","17 March, 2020","stat.ML,cs.CY,cs.LG",
              Image Quality Transfer Enhances Contrast and Resolution of Low-Field Brain MRI in African Paediatric Epilepsy Patients          ,2003.07216,https://arxiv.org/abs/2003.07216,https://arxiv.org/pdf/2003.07216,"Authors:MatteoFigini,HongxiangLin,GodwinOgbole,FeliceDArco,StefanoB.Blumberg,DavidW.Carmichael,RyutaroTanno,EnricoKaden,BiobeleJ.Brown,IkeoluwaLagunju,HelenJ.Cross,DelmiroFernandez-Reyes,DanielC.Alexander","        1.5T or 3T scanners are the current standard for clinical MRI, but low-field (<1T) scanners are still common in many lower- and middle-income countries for reasons of cost and robustness to power failures. Compared to modern high-field scanners, low-field scanners provide images with lower signal-to-noise ratio at equivalent resolution, leaving practitioners to compensate by using large slice thickness and incomplete spatial coverage. Furthermore, the contrast between different types of brain tissue may be substantially reduced even at equal signal-to-noise ratio, which limits diagnostic value. Recently the paradigm of Image Quality Transfer has been applied to enhance 0.36T structural images aiming to approximate the resolution, spatial coverage, and contrast of typical 1.5T or 3T images. A variant of the neural network U-Net was trained using low-field images simulated from the publicly available 3T Human Connectome Project dataset. Here we present qualitative results from real and simulated clinical low-field brain images showing the potential value of IQT to enhance the clinical utility of readily accessible low-field MRIs in the management of epilepsy.        △ Less","18 March, 2020","eess.IV,cs.CV,physics.med-ph",
              Causal datasheet: An approximate guide to practically assess Bayesian networks in the real world          ,2003.07182,https://arxiv.org/abs/2003.07182,https://arxiv.org/pdf/2003.07182,"Authors:BradleyButcher,VincentS.Huang,JeremyReffin,SemaK.Sgaier,GraceCharles,NoviQuadrianto","        In solving real-world problems like changing healthcare-seeking behaviors, designing interventions to improve downstream outcomes requires an understanding of the causal links within the system. Causal Bayesian Networks (BN) have been proposed as one such powerful method. In real-world applications, however, confidence in the results of BNs are often moderate at best. This is due in part to the inability to validate against some ground truth, as the DAG is not available. This is especially problematic if the learned DAG conflicts with pre-existing domain doctrine. At the policy level, one must justify insights generated by such analysis, preferably accompanying them with uncertainty estimation. Here we propose a causal extension to the datasheet concept proposed by Gebru et al (2018) to include approximate BN performance expectations for any given dataset. To generate the results for a prototype Causal Datasheet, we constructed over 30,000 synthetic datasets with properties mirroring characteristics of real data. We then recorded the results given by state-of-the-art structure learning algorithms. These results were used to populate the Causal Datasheet, and recommendations were automatically generated dependent on expected performance. As a proof of concept, we used our Causal Datasheet Generation Tool (CDG-T) to assign expected performance expectations to a maternal health survey we conducted in Uttar Pradesh, India.        △ Less","12 March, 2020","cs.AI,cs.LG,cs.PF,stat.ML",
              Using Data Assimilation of Mechanistic Models to Estimate Glucose and Insulin Metabolism          ,2003.06541,https://arxiv.org/abs/2003.06541,https://arxiv.org/pdf/2003.06541,"Authors:JamiJ.Mulgrave,MatthewE.Levine,DavidJ.Albers,JoonHa,ArthurSherman,GeorgeHripcsak",        Motivation: There is a growing need to integrate mechanistic models of biological processes with computational methods in healthcare in order to improve prediction. We apply data assimilation in the context of Type 2 diabetes to understand parameters associated with the disease.  Results: The data assimilation method captures how well patients improve glucose tolerance after their surgery. Data assimilation has the potential to improve phenotyping in Type 2 diabetes.        △ Less,"13 March, 2020","stat.AP,physics.med-ph",
              Optimizing Medical Treatment for Sepsis in Intensive Care: from Reinforcement Learning to Pre-Trial Evaluation          ,2003.06474,https://arxiv.org/abs/2003.06474,https://arxiv.org/pdf/2003.06474,"Authors:LuchenLi,IgnacioAlbert-Smet,AldoA.Faisal","        Our aim is to establish a framework where reinforcement learning (RL) of optimizing interventions retrospectively allows us a regulatory compliant pathway to prospective clinical testing of the learned policies in a clinical deployment. We focus on infections in intensive care units which are one of the major causes of death and difficult to treat because of the complex and opaque patient dynamics, and the clinically debated, highly-divergent set of intervention policies required by each individual patient, yet intensive care units are naturally data rich. In our work, we build on RL approaches in healthcare (""AI Clinicians""), and learn off-policy continuous dosing policy of pharmaceuticals for sepsis treatment using historical intensive care data under partially observable MDPs (POMDPs). POMPDs capture uncertainty in patient state better by taking in all historical information, yielding an efficient representation, which we investigate through ablations. We compensate for the lack of exploration in our retrospective data by evaluating each encountered state with a best-first tree search. We mitigate state distributional shift by optimizing our policy in the vicinity of the clinicians' compound policy. Crucially, we evaluate our model recommendations using not only conventional policy evaluations but a novel framework that incorporates human experts: a model-agnostic pre-clinical evaluation method to estimate the accuracy and uncertainty of clinician's decisions versus our system recommendations when confronted with the same individual patient history (""shadow mode"").        △ Less","18 March, 2020","cs.LG,cs.AI",
              Predictive Analysis for Detection of Human Neck Postures using a robust integration of kinetics and kinematics          ,2003.06311,https://arxiv.org/abs/2003.06311,https://arxiv.org/pdf/2003.06311,"Authors:KorupalliVRajeshKumar,SusanElias","        Human neck postures and movements need to be monitored, measured, quantified and analyzed, as a preventive measure in healthcare applications. Improper neck postures are an increasing source of neck musculoskeletal disorders, requiring therapy and rehabilitation. The motivation for the research presented in this paper was the need to develop a notification mechanism for improper neck usage. Kinematic data captured by sensors have limitations in accurately classifying the neck postures. Hence, we propose an integrated use of kinematic and kinetic data to efficiently classify neck postures. Using machine learning algorithms we obtained 100% accuracy in the predictive analysis of this data. The research analysis and discussions show that the kinetic data of the Hyoid muscles can accurately detect the neck posture given the corresponding kinematic data captured by the neck-band. The proposed robust platform for the integration of kinematic and kinetic data has enabled the design of a smart neck-band for the prevention of neck musculoskeletal disorders.        △ Less","12 March, 2020","eess.SP,cs.LG,stat.ML",
              Bayesian Posterior Interval Calibration to Improve the Interpretability of Observational Studies          ,2003.06002,https://arxiv.org/abs/2003.06002,https://arxiv.org/pdf/2003.06002,"Authors:JamiJ.Mulgrave,DavidMadigan,GeorgeHripcsak","        Observational healthcare data offer the potential to estimate causal effects of medical products on a large scale. However, the confidence intervals and p-values produced by observational studies only account for random error and fail to account for systematic error. As a consequence, operating characteristics such as confidence interval coverage and Type I error rates often deviate sharply from their nominal values and render interpretation impossible. While there is longstanding awareness of systematic error in observational studies, analytic approaches to empirically account for systematic error are relatively new. Several authors have proposed approaches using negative controls (also known as ""falsification hypotheses"") and positive controls. The basic idea is to adjust confidence intervals and p-values in light of the bias (if any) detected in the analyses of the negative and positive control. In this work, we propose a Bayesian statistical procedure for posterior interval calibration that uses negative and positive controls. We show that the posterior interval calibration procedure restores nominal characteristics, such as 95% coverage of the true effect size by the 95% posterior interval.        △ Less","12 March, 2020",stat.AP,
              Reconfiguring health services to reduce the workload of caregivers during the COVID-19 outbreak using an open-source scalable platform for remote digital monitoring and coordination of care in hospital Command Centres          ,2003.05873,https://arxiv.org/abs/2003.05873,https://arxiv.org/pdf/2003.05873,"Authors:PhilippeRavaud,FranckleOuay,EtienneDepaulis,AlexandreHuckert,BrunoVegreville,Viet-ThiTran","        The Covid-19 outbreak threatens to saturate healthcare systems in most Western countries. We describe how digital technologies may be used to automatically and remotely monitor patients at home. Patients answer simple self-reported questionnaires and their data is transmitted, in real time, to a Command Centre in the nearest reference hospital. Patient reported data are automatically filtered by algorithms to identify those with early warning signs. Open-source code of all software components required to deploy the remote digital monitoring platform and Command Centres is available.        △ Less","12 March, 2020",cs.CY,
              SUOD: Accelerating Large-scare Unsupervised Heterogeneous Outlier Detection          ,2003.05731,https://arxiv.org/abs/2003.05731,https://arxiv.org/pdf/2003.05731,"Authors:YueZhao,XiyangHu,ChengCheng,CongWang,ChanglinWan,WenWang,JianingYang,HaopingBai,ZhengLi,CaoXiao,YunlongWang,ZhiQiao,JimengSun,LemanAkoglu","        Outlier detection (OD) is a key data mining task for identifying abnormal objects from general samples with numerous high-stake applications including fraud detection and intrusion detection. Due to the lack of ground truth labels, practitioners often have to build a large number of unsupervised models that are heterogeneous (i.e., different algorithms and hyperparameters) for further combination and analysis with ensemble learning, rather than relying on a single model. However, this yields severe scalability issues on high-dimensional, large datasets.  How to accelerate the training and predicting with a large number of heterogeneous unsupervised OD models? How to ensure the acceleration does not deteriorate detection models' accuracy? How to accommodate the acceleration need for both a single worker setting and a distributed system with multiple workers? In this study, we propose a three-module acceleration system called SUOD (scalable unsupervised outlier detection) to address these questions. It focuses on three complementary aspects to accelerate (dimensionality reduction for high-dimensional data, model approximation for complex models, and execution efficiency improvement for taskload imbalance within distributed systems), while controlling detection performance degradation. Extensive experiments on more than 20 benchmark datasets demonstrate SUOD's effectiveness in heterogeneous OD acceleration. By the submission time, the released open-source system has been widely used with more than 700,000 times downloads. A real-world deployment case on fraudulent claim analysis at IQVIA, a leading healthcare firm, is also provided.        △ Less","11 October, 2020","cs.LG,cs.DC,cs.IR,stat.ML",
              Off-policy Policy Evaluation For Sequential Decisions Under Unobserved Confounding          ,2003.05623,https://arxiv.org/abs/2003.05623,https://arxiv.org/pdf/2003.05623,"Authors:HongseokNamkoong,RamtinKeramati,SteveYadlowsky,EmmaBrunskill","        When observed decisions depend only on observed features, off-policy policy evaluation (OPE) methods for sequential decision making problems can estimate the performance of evaluation policies before deploying them. This assumption is frequently violated due to unobserved confounders, unrecorded variables that impact both the decisions and their outcomes. We assess robustness of OPE methods under unobserved confounding by developing worst-case bounds on the performance of an evaluation policy. When unobserved confounders can affect every decision in an episode, we demonstrate that even small amounts of per-decision confounding can heavily bias OPE methods. Fortunately, in a number of important settings found in healthcare, policy-making, operations, and technology, unobserved confounders may primarily affect only one of the many decisions made. Under this less pessimistic model of one-decision confounding, we propose an efficient loss-minimization-based procedure for computing worst-case bounds, and prove its statistical consistency. On two simulated healthcare examples---management of sepsis patients and developmental interventions for autistic children---where this is a reasonable model of confounding, we demonstrate that our method invalidates non-robust results and provides meaningful certificates of robustness, allowing reliable selection of policies even under unobserved confounding.        △ Less","12 March, 2020","stat.ML,cs.LG",
              High Temperature and High Humidity Reduce the Transmission of COVID-19          ,2003.05003,https://arxiv.org/abs/2003.05003,https://arxiv.org/pdf/2003.05003,"Authors:JingyuanWang,KeTang,KaiFeng,XinLi,WeifengLv,KunChen,FeiWang","        With the ongoing global pandemic of COVID-19, a question is whether the coming summer in the northern hemisphere will reduce the transmission intensity of COVID-19 with increased humidity and temperature. In this paper, we investigate this problem using the data from the cases with symptom-onset dates from January 19 to February 10, 2020 for 100 Chinese cities, and cases with confirmed dates from March 15 to April 25 for 1,005 U.S. counties. Statistical analysis is performed to assess the relationship between the transmissibility of COVID-19 and the temperature/humidity, by controlling for various demographic, socio-economic, geographic, healthcare and policy factors and correcting for cross-sectional correlation. We find a similar influence of the temperature and relative humidity on effective reproductive number (R values) of COVID-19 for both China and the U.S. before lockdown in both countries: one-degree Celsius increase in temperature reduces R value by about 0.023 (0.026 (95% CI [-0.0395,-0.0125]) in China and 0.020 (95% CI [-0.0311, -0.0096]) in the U.S.), and one percent relative humidity rise reduces R value by 0.0078 (0.0076 (95% CI [-0.0108,-0.0045]) in China and 0.0080 (95% CI [-0.0150,-0.0010]) in the U.S.). If assuming a 30 degree and 25 percent increase in temperature and relative humidity from winter to summer in the northern hemisphere, we expect the R values to decline about 0.89 (0.69 by temperature and 0.20 by humidity). Given the notion that the non-intervened R values are around 2.5 to 3, only weather factors cannot make the R values below their critical condition of R<1, under which the epidemic diminishes gradually. Therefore, public health intervention such as social distancing is crucial to block the transmission of COVID-19 even in summer.        △ Less","22 May, 2020",q-bio.PE,
              Generating Emotionally Aligned Responses in Dialogues using Affect Control Theory          ,2003.03645,https://arxiv.org/abs/2003.03645,https://arxiv.org/pdf/2003.03645,"Authors:NabihaAsghar,IvanKobyzev,JesseHoey,PascalPoupart,MuhammadBilalSheikh","        State-of-the-art neural dialogue systems excel at syntactic and semantic modelling of language, but often have a hard time establishing emotional alignment with the human interactant during a conversation. In this work, we bring Affect Control Theory (ACT), a socio-mathematical model of emotions for human-human interactions, to the neural dialogue generation setting. ACT makes predictions about how humans respond to emotional stimuli in social situations. Due to this property, ACT and its derivative probabilistic models have been successfully deployed in several applications of Human-Computer Interaction, including empathetic tutoring systems, assistive healthcare devices and two-person social dilemma games. We investigate how ACT can be used to develop affect-aware neural conversational agents, which produce emotionally aligned responses to prompts and take into consideration the affective identities of the interactants.        △ Less","16 April, 2020","cs.CL,cs.AI,cs.HC",
"              Honeycomb Layered Oxides: Structure, Energy Storage, Transport, Topology and Relevant Insights          ",2003.03555,https://arxiv.org/abs/2003.03555,https://arxiv.org/pdf/2003.03555,"Authors:GodwillMbitiKanyolo,TitusMasese,NamiMatsubara,Chih-YaoChen,JosefRizell,OlaKenjiForslund,ElisabettaNocerino,KonstantinosPapadopoulos,AntonZubayer,MinamiKato,KoheiTada,KeigoKubota,HiroshiSenoh,Zhen-DongHuang,YasmineSassa,MartinMansson,HajimeMatsumoto","        The advent of nanotechnology has hurtled the discovery and development of nanostructured materials with stellar chemical and physical functionalities in a bid to address issues in energy, environment, telecommunications and healthcare. In this quest, a class of two-dimensional layered materials consisting of alkali or coinage metal atoms sandwiched between slabs exclusively made of transition metal and chalcogen (or pnictogen) atoms arranged in a honeycomb fashion have emerged as materials exhibiting fascinatingly rich crystal chemistry, high-voltage electrochemistry, fast cation diffusion besides playing host to varied exotic electromagnetic and topological phenomena. Currently, with a niche application in energy storage as high-voltage materials, this class of honeycomb layered oxides serves as ideal pedagogical exemplars of the innumerable capabilities of nanomaterials drawing immense interest in multiple fields ranging from materials science, solid-state chemistry, electrochemistry and condensed matter physics. In this review, we delineate the relevant chemistry and physics of honeycomb layered oxides, and discuss their functionalities for tunable electrochemistry, superfast ionic conduction, electromagnetism and topology. Moreover, we elucidate the unexplored albeit vastly promising crystal chemistry space whilst outlining effective ways to identify regions within this compositional space, particularly where interesting electromagnetic and topological properties could be lurking within the aforementioned alkali and coinage-metal honeycomb layered oxide structures. We conclude by pointing towards possible future research directions, particularly the prospective realisation of Kitaev-Heisenberg-Dzyaloshinskii-Moriya interactions with single crystals and Floquet theory in closely-related honeycomb layered oxide materials.        △ Less","9 September, 2020",cond-mat.mtrl-sci,
              Deep learning for prediction of population health costs          ,2003.03466,https://arxiv.org/abs/2003.03466,https://arxiv.org/pdf/2003.03466,"Authors:PhilippDrewe-Boss,DirkEnders,JochenWalker,UweOhler","        Accurate prediction of healthcare costs is important for optimally managing health costs. However, methods leveraging the medical richness from data such as health insurance claims or electronic health records are missing. Here, we developed a deep neural network to predict future cost from health insurance claims records. We applied the deep network and a ridge regression model to a sample of 1.4 million German insurants to predict total one-year health care costs. Both methods were compared to Morbi-RSA models with various performance measures and were also used to predict patients with a change in costs and to identify relevant codes for this prediction. We showed that the neural network outperformed the ridge regression as well as all Morbi-RSA models for cost prediction. Further, the neural network was superior to ridge regression in predicting patients with cost change and identified more specific codes. In summary, we showed that our deep neural network can leverage the full complexity of the patient records and outperforms standard approaches. We suggest that the better performance is due to the ability to incorporate complex interactions in the model and that the model might also be used for predicting other health phenotypes.        △ Less","6 March, 2020","cs.LG,cs.CY,stat.ML",
              Heterogeneity Loss to Handle Intersubject and Intrasubject Variability in Cancer          ,2003.03295,https://arxiv.org/abs/2003.03295,https://arxiv.org/pdf/2003.03295,"Authors:ShubhamGoswami,SurilMehta,DhruvaSahrawat,AnubhaGupta,RituGupta","        Developing nations lack adequate number of hospitals with modern equipment and skilled doctors. Hence, a significant proportion of these nations' population, particularly in rural areas, is not able to avail specialized and timely healthcare facilities. In recent years, deep learning (DL) models, a class of artificial intelligence (AI) methods, have shown impressive results in medical domain. These AI methods can provide immense support to developing nations as affordable healthcare solutions. This work is focused on one such application of blood cancer diagnosis. However, there are some challenges to DL models in cancer research because of the unavailability of a large data for adequate training and the difficulty of capturing heterogeneity in data at different levels ranging from acquisition characteristics, session, to subject-level (within subjects and across subjects). These challenges render DL models prone to overfitting and hence, models lack generalization on prospective subjects' data. In this work, we address these problems in the application of B-cell Acute Lymphoblastic Leukemia (B-ALL) diagnosis using deep learning. We propose heterogeneity loss that captures subject-level heterogeneity, thereby, forcing the neural network to learn subject-independent features. We also propose an unorthodox ensemble strategy that helps us in providing improved classification over models trained on 7-folds giving a weighted-F1F_1 score of 95.26% on unseen (test) subjects' data that are, so far, the best results on the C-NMC 2019 dataset for B-ALL classification.        △ Less","18 March, 2020","cs.CV,cs.LG,eess.IV",
              Towards a predictive spatio-temporal representation of brain data          ,2003.03290,https://arxiv.org/abs/2003.03290,https://arxiv.org/pdf/2003.03290,"Authors:TiagoAzevedo,LucaPassamonti,PietroLiò,NicolaToschi","        The characterisation of the brain as a ""connectome"", in which the connections are represented by correlational values across timeseries and as summary measures derived from graph theory analyses, has been very popular in the last years. However, although this representation has advanced our understanding of the brain function, it may represent an oversimplified model. This is because the typical fMRI datasets are constituted by complex and highly heterogeneous timeseries that vary across space (i.e., location of brain regions). We compare various modelling techniques from deep learning and geometric deep learning to pave the way for future research in effectively leveraging the rich spatial and temporal domains of typical fMRI datasets, as well as of other similar datasets. As a proof-of-concept, we compare our approaches in the homogeneous and publicly available Human Connectome Project (HCP) dataset on a supervised binary classification task. We hope that our methodological advances relative to previous ""connectomic"" measures can ultimately be clinically and computationally relevant by leading to a more nuanced understanding of the brain dynamics in health and disease. Such understanding of the brain can fundamentally reduce the constant specialised clinical expertise in order to accurately understand brain variability.        △ Less","29 February, 2020","cs.LG,eess.IV,q-bio.NC,stat.ML",
              What went wrong and when? Instance-wise Feature Importance for Time-series Models          ,2003.02821,https://arxiv.org/abs/2003.02821,https://arxiv.org/pdf/2003.02821,"Authors:SanaTonekaboni,ShalmaliJoshi,KieranCampbell,DavidDuvenaud,AnnaGoldenberg","        Explanations of time series models are useful for high stakes applications like healthcare but have received little attention in machine learning literature. We propose FIT, a framework that evaluates the importance of observations for a multivariate time-series black-box model, by quantifying the shift in the predictive distribution over time. FIT defines the importance of an observation based on its contribution to the distributional shift under a KL-divergence that contrasts the predictive distribution against a counterfactual where the rest of the features are unobserved. We also demonstrate the need to control for time-dependent distribution shifts. We compare with state-of-the-art baselines on simulated and real-world clinical data and demonstrate that our approach is superior in identifying important time points and observations over the course of the time series.        △ Less","13 July, 2020","cs.LG,stat.ML",
              Application of Deep Neural Networks to assess corporate Credit Rating          ,2003.02334,https://arxiv.org/abs/2003.02334,https://arxiv.org/pdf/2003.02334,"Authors:ParisaGolbayani,DanWang,IonutFlorescu","        Recent literature implements machine learning techniques to assess corporate credit rating based on financial statement reports. In this work, we analyze the performance of four neural network architectures (MLP, CNN, CNN2D, LSTM) in predicting corporate credit rating as issued by Standard and Poor's. We analyze companies from the energy, financial and healthcare sectors in US. The goal of the analysis is to improve application of machine learning algorithms to credit assessment. To this end, we focus on three questions. First, we investigate if the algorithms perform better when using a selected subset of features, or if it is better to allow the algorithms to select features themselves. Second, is the temporal aspect inherent in financial data important for the results obtained by a machine learning algorithm? Third, is there a particular neural network architecture that consistently outperforms others with respect to input features, sectors and holdout set? We create several case studies to answer these questions and analyze the results using ANOVA and multiple comparison testing procedure.        △ Less","4 March, 2020","q-fin.RM,cs.LG,stat.AP,stat.ML",
              Mortality and Healthcare: a Stochastic Control Analysis under Epstein-Zin Preferences          ,2003.01783,https://arxiv.org/abs/2003.01783,https://arxiv.org/pdf/2003.01783,"Authors:JoshuaAurand,Yu-JuiHuang","        This paper studies optimal consumption, investment, and healthcare spending under Epstein-Zin preferences. Given consumption and healthcare spending plans, Epstein-Zin utilities are defined over an agent's random lifetime, partially controllable by the agent as healthcare reduces mortality growth. To the best of our knowledge, this is the first time Epstein-Zin utilities are formulated on a controllable random horizon, via an infinite-horizon backward stochastic differential equation with superlinear growth. A new comparison result is established for the uniqueness of associated utility value processes. In a Black-Scholes market, the stochastic control problem is solved through the related Hamilton-Jacobi-Bellman (HJB) equation. The verification argument features a delicate containment of the growth of the controlled morality process, which is unique to our framework, relying on a combination of probabilistic arguments and analysis of the HJB equation. In contrast to prior work under time-separable utilities, Epstein-Zin preferences largely facilitate calibration. In four countries we examined, the model-generated mortality closely approximates actual mortality data; moreover, the calibrated efficacy of healthcare is in broad agreement with empirical studies on healthcare across countries.        △ Less","21 April, 2020","q-fin.MF,math.OC",
              Marketplace for AI Models          ,2003.01593,https://arxiv.org/abs/2003.01593,https://arxiv.org/pdf/2003.01593,"Authors:AbhishekKumar,BenjaminFinley,TristanBraud,SasuTarkoma,PanHui","        Artificial intelligence shows promise for solving many practical societal problems in areas such as healthcare and transportation. However, the current mechanisms for AI model diffusion such as Github code repositories, academic project webpages, and commercial AI marketplaces have some limitations; for example, a lack of monetization methods, model traceability, and model auditabilty. In this work, we sketch guidelines for a new AI diffusion method based on a decentralized online marketplace. We consider the technical, economic, and regulatory aspects of such a marketplace including a discussion of solutions for problems in these areas. Finally, we include a comparative analysis of several current AI marketplaces that are already available or in development. We find that most of these marketplaces are centralized commercial marketplaces with relatively few models.        △ Less","3 March, 2020","cs.CY,cs.AI",
              On the Effectiveness of Virtual Reality-based Training for Robotic Setup          ,2003.01540,https://arxiv.org/abs/2003.01540,https://arxiv.org/pdf/2003.01540,"Authors:ArianMehrfard,JavadFotouhi,TessForster,GiacomoTaylor,DanyalFer,DeborahNagle,NassirNavab,BernhardFuerst","        Virtual Reality (VR) is rapidly increasing in popularity as a teaching tool. It allows for the creation of a highly immersive, three-dimensional virtual environment intended to simulate real-life environments. With more robots saturating the industry - from manufacturing to healthcare, there is a need to train end-users on how to set up, operate, tear down, and troubleshoot the robot. Even though VR has become widely used in training surgeons on the psychomotor skills associated with operating the robot, little research has been done to see how the benefits of VR could translate to teaching the bedside staff, tasked with supporting the robot during the full end-to-end surgical procedure. We trained 30 participants on how to set up a robotic arm in an environment mimicking clinical setup. We divided these participants equally into 3 groups with one group trained with paper-based instructions, one with video-based instructions and one with VR-based instructions. We then compared and contrasted these three different training methods. VR and paper-based were highly favored training mediums over video-based. VR-trained participants achieved slightly higher fidelity of individual robotic joint angles, suggesting better comprehension of the spatial awareness skills necessary to achieve desired arm positioning. In addition, VR resulted in higher reproducibility of setup fidelity and more consistency in user confidence levels as compared to paper and video-based training.        △ Less","3 March, 2020",cs.RO,
              Few-Shot Relation Learning with Attention for EEG-based Motor Imagery Classification          ,2003.01300,https://arxiv.org/abs/2003.01300,https://arxiv.org/pdf/2003.01300,"Authors:SionAn,SoopilKim,PhilipChikontwe,SangHyunPark","        Brain-Computer Interfaces (BCI) based on Electroencephalography (EEG) signals, in particular motor imagery (MI) data have received a lot of attention and show the potential towards the design of key technologies both in healthcare and other industries. MI data is generated when a subject imagines movement of limbs and can be used to aid rehabilitation as well as in autonomous driving scenarios. Thus, classification of MI signals is vital for EEG-based BCI systems. Recently, MI EEG classification techniques using deep learning have shown improved performance over conventional techniques. However, due to inter-subject variability, the scarcity of unseen subject data, and low signal-to-noise ratio, extracting robust features and improving accuracy is still challenging. In this context, we propose a novel two-way few shot network that is able to efficiently learn how to learn representative features of unseen subject categories and how to classify them with limited MI EEG data. The pipeline includes an embedding module that learns feature representations from a set of samples, an attention mechanism for key signal feature discovery, and a relation module for final classification based on relation scores between a support set and a query signal. In addition to the unified learning of feature similarity and a few shot classifier, our method leads to emphasize informative features in support data relevant to the query data, which generalizes better on unseen subjects. For evaluation, we used the BCI competition IV 2b dataset and achieved an 9.3% accuracy improvement in the 20-shot classification task with state-of-the-art performance. Experimental results demonstrate the effectiveness of employing attention and the overall generality of our method.        △ Less","19 August, 2020","eess.SP,cs.LG",
              Deep Survival Machines: Fully Parametric Survival Regression and Representation Learning for Censored Data with Competing Risks          ,2003.01176,https://arxiv.org/abs/2003.01176,https://arxiv.org/pdf/2003.01176,"Authors:ChiragNagpal,XinyuLi,ArturDubrawski","        We describe a new approach to estimating relative risks in time-to-event prediction problems with censored data in a fully parametric manner. Our approach does not require making strong assumptions of constant baseline hazard of the underlying survival distribution, as required by the Cox-proportional hazard model. By jointly learning deep nonlinear representations of the input covariates, we demonstrate the benefits of our approach when used to estimate survival risks through extensive experimentation on multiple real world datasets with different levels of censoring. We further demonstrate advantages of our model in the competing risks scenario. To the best of our knowledge, this is the first work involving fully parametric estimation of survival times with competing risks in the presence of censoring.        △ Less","2 March, 2020","cs.LG,stat.AP,stat.ML",
              Cluster-Based Social Reinforcement Learning          ,2003.00627,https://arxiv.org/abs/2003.00627,https://arxiv.org/pdf/2003.00627,"Authors:MahakGoindani,JenniferNeville","        Social Reinforcement Learning methods, which model agents in large networks, are useful for fake news mitigation, personalized teaching/healthcare, and viral marketing, but it is challenging to incorporate inter-agent dependencies into the models effectively due to network size and sparse interaction data. Previous social RL approaches either ignore agents dependencies or model them in a computationally intensive manner. In this work, we incorporate agent dependencies efficiently in a compact model by clustering users (based on their payoff and contribution to the goal) and combine this with a method to easily derive personalized agent-level policies from cluster-level policies. We also propose a dynamic clustering approach that captures changing user behavior. Experiments on real-world datasets illustrate that our proposed approach learns more accurate policy estimates and converges more quickly, compared to several baselines that do not use agent correlations or only use static clusters.        △ Less","23 March, 2020","cs.LG,stat.ML",
              Dynamic Skyline Queries on Encrypted Data Using Result Materialization          ,2003.00051,https://arxiv.org/abs/2003.00051,https://arxiv.org/pdf/2003.00051,"Authors:SepantaZeighami,GabrielGhinita,CyrusShahabi","        Skyline computation is an increasingly popular query, with broad applicability in domains such as healthcare, travel and finance. Given the recent trend to outsource databases and query evaluation, and due to the proprietary and sometimes highly sensitivity nature of the data (e.g., in healthcare), it is essential to evaluate skylines on encrypted datasets. Several research efforts acknowledged the importance of secure skyline computation, but existing solutions suffer from at least one of the following shortcomings: (i) they only provide ad-hoc security; (ii) they are prohibitively expensive; or (iii) they rely on unrealistic assumptions, such as the presence of multiple non-colluding parties in the protocol.  Inspired from solutions for secure nearest-neighbors (NN) computation, we conjecture that the most secure and efficient way to compute skylines is through result materialization. However, this approach is significantly more challenging for skylines than for NN queries. We exhaustively study and provide algorithms for pre-computation of skyline results, and we perform an in-depth theoretical analysis of this process. We show that pre-computing results while minimizing storage overhead is NP-hard, and we provide dynamic programming and greedy heuristics that solve the problem more efficiently, while maintaining storage at reasonable levels. Our algorithms are novel and applicable to plain-text skyline computation, but we focus on the encrypted setting where materialization reduces the cost of skyline computation from hours to seconds. Extensive experiments show that we clearly outperform existing work in terms of performance, and our security analysis proves that we obtain a smaller (and quantifiable) data leakage than competitors.        △ Less","28 February, 2020",cs.DB,
              Simpler handling of clinical concepts in R with clinconcept          ,2002.11431,https://arxiv.org/abs/2002.11431,https://arxiv.org/pdf/2002.11431,Authors:RobertC.Free,"        Routinely collected data in electronic healthcare records are often underpinned by clinical concept dictionaries. Increasingly data sets from these sources are being made available and used for research purposes, but without additional tooling it can be difficult to work effectively with these dictionaries due to their design, size and complex nature. In an effort to improve this situation the clinconcept package was created to provide a straightforward way for researchers to build, manage and interrogate databases containing commmonly used clinical concept dictionaries. This article describes the rationale behind the package, how to install it and use it and how it can be extended to support other data sources.        △ Less","26 February, 2020","cs.CY,stat.AP",
              EmbPred30: Assessing 30-days Readmission for Diabetic Patients using Categorical Embeddings          ,2002.11215,https://arxiv.org/abs/2002.11215,https://arxiv.org/pdf/2002.11215,"Authors:Sarthak,ShikharShukla,SuryaPrakashTripathi","        Hospital readmission is a crucial healthcare quality measure that helps in determining the level of quality of care that a hospital offers to a patient and has proven to be immensely expensive. It is estimated that more than $25 billion are spent yearly due to readmission of diabetic patients in the USA. This paper benchmarks existing models and proposes a new embedding based state-of-the-art deep neural network(DNN). The model can identify whether a hospitalized diabetic patient will be readmitted within 30 days or not with an accuracy of 95.2% and Area Under the Receiver Operating Characteristics(AUROC) of 97.4% on data collected from 130 US hospitals between 1999-2008. The results are encouraging with patients having changes in medication while admitted having a high chance of getting readmitted. Identifying prospective patients for readmission could help the hospital systems in improving their inpatient care, thereby saving them from unnecessary expenditures.        △ Less","25 February, 2020","cs.LG,stat.AP,stat.ML",
              Design of Breathing-states Detector for m-Health Platform using Seismocardiographic Signal          ,2002.10510,https://arxiv.org/abs/2002.10510,https://arxiv.org/pdf/2002.10510,"Authors:TilendraChoudhary,M.K.Bhuyan,KangkanaBora,L.N.Sharma","        In this work, a seismocardiogram (SCG) based breathing-state measuring method is proposed for m-health applications. The aim of the proposed framework is to assess human respiratory system by identifying degree-of-breathings, such as breathlessness, normal breathing, and long and labored breathing. For this, it is needed to measure cardiac-induced chest-wall vibrations, reflected in the SCG signal. Orthogonal subspace projection is employed to extract the SCG cycles with the help of a concurrent ECG signal. Subsequently, fifteen statistically significant morphological-features are extracted from each of the SCG cycles. These features can efficiently characterize physiological changes due to varying respiratory rates. Stacked autoencoder (SAE) deep learning architecture is employed for the identification of different respiratory-effort levels. The performance of the proposed method is evaluated and compared with other standard classifiers for 1147 analyzed SCGbeats. The proposed method gives an overall average accuracy of 91.45% in recognizing three different breathing states. The proposed framework has a great potential for different healthcare applications, and it may be commercially fabricated for IoT based remote health-monitoring systems for consumer electronics market.        △ Less","24 February, 2020",eess.SP,
              Survey Bandits with Regret Guarantees          ,2002.09814,https://arxiv.org/abs/2002.09814,https://arxiv.org/pdf/2002.09814,"Authors:SanathKumarKrishnamurthy,SusanAthey","        We consider a variant of the contextual bandit problem. In standard contextual bandits, when a user arrives we get the user's complete feature vector and then assign a treatment (arm) to that user. In a number of applications (like healthcare), collecting features from users can be costly. To address this issue, we propose algorithms that avoid needless feature collection while maintaining strong regret guarantees.        △ Less","22 February, 2020","cs.LG,econ.EM,stat.ML",
              Transformer Hawkes Process          ,2002.09291,https://arxiv.org/abs/2002.09291,https://arxiv.org/pdf/2002.09291,"Authors:SimiaoZuo,HaomingJiang,ZichongLi,TuoZhao,HongyuanZha","        Modern data acquisition routinely produce massive amounts of event sequence data in various domains, such as social media, healthcare, and financial markets. These data often exhibit complicated short-term and long-term temporal dependencies. However, most of the existing recurrent neural network based point process models fail to capture such dependencies, and yield unreliable prediction performance. To address this issue, we propose a Transformer Hawkes Process (THP) model, which leverages the self-attention mechanism to capture long-term dependencies and meanwhile enjoys computational efficiency. Numerical experiments on various datasets show that THP outperforms existing models in terms of both likelihood and event prediction accuracy by a notable margin. Moreover, THP is quite general and can incorporate additional structural knowledge. We provide a concrete example, where THP achieves improved prediction performance for learning multiple point processes when incorporating their relational information.        △ Less","14 August, 2020","cs.LG,stat.ML",
              Anonymizing Data for Privacy-Preserving Federated Learning          ,2002.09096,https://arxiv.org/abs/2002.09096,https://arxiv.org/pdf/2002.09096,"Authors:OliviaChoudhury,ArisGkoulalas-Divanis,TheodorosSalonidis,IssaSylla,YoonyoungPark,GraceHsu,AmarDas","        Federated learning enables training a global machine learning model from data distributed across multiple sites, without having to move the data. This is particularly relevant in healthcare applications, where data is rife with personal, highly-sensitive information, and data analysis methods must provably comply with regulatory guidelines. Although federated learning prevents sharing raw data, it is still possible to launch privacy attacks on the model parameters that are exposed during the training process, or on the generated machine learning model. In this paper, we propose the first syntactic approach for offering privacy in the context of federated learning. Unlike the state-of-the-art differential privacy-based frameworks, our approach aims to maximize utility or model performance, while supporting a defensible level of privacy, as demanded by GDPR and HIPAA. We perform a comprehensive empirical evaluation on two important problems in the healthcare domain, using real-world electronic health data of 1 million patients. The results demonstrate the effectiveness of our approach in achieving high model performance, while offering the desired level of privacy. Through comparative studies, we also show that, for varying datasets, experimental setups, and privacy budgets, our approach offers higher model performance than differential privacy-based techniques in federated learning.        △ Less","20 February, 2020","cs.CR,cs.AI,cs.LG",
"              Phylogenetic analyses of the severe acute respiratory syndrome coronavirus 2 reflected the several routes of introduction to Taiwan, the United States, and Japan          ",2002.08802,https://arxiv.org/abs/2002.08802,https://arxiv.org/pdf/2002.08802,"Authors:TomokoMatsuda,HikoyuSuzuki,NorichikaOgata","        Worldwide Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection is disrupting in the economy and anxiety of people. The public anxiety has increased the psychological burden on government and healthcare professionals, resulting in a government worker suicide in Japan. The terrified people are asking the government for border measures. However, are border measures possible for this virus? By analyzing 48 almost complete virus genome sequences, we found out that the viruses that invaded Taiwan, the United States, and Japan were introduced independently. We identified thirteen parsimony-informative sites and three groups (CTC, TCC, and TCT). Viruses found outside China did not form a monophyletic clade, opposite to previous study. These results suggest the difficulty of implementing effective border measures against this virus.        △ Less","28 February, 2020",q-bio.GN,
"              A Comprehensive Scoping Review of Bayesian Networks in Healthcare: Past, Present and Future          ",2002.08627,https://arxiv.org/abs/2002.08627,https://arxiv.org/pdf/2002.08627,"Authors:EvangeliaKyrimi,ScottMcLachlan,KudakwasheDube,MarianaR.Neves,AliFahmi,NormanFenton","        No comprehensive review of Bayesian networks (BNs) in healthcare has been published in the past, making it difficult to organize the research contributions in the present and identify challenges and neglected areas that need to be addressed in the future. This unique and novel scoping review of BNs in healthcare provides an analytical framework for comprehensively characterizing the domain and its current state. The review shows that: (1) BNs in healthcare are not used to their full potential; (2) a generic BN development process is lacking; (3) limitations exists in the way BNs in healthcare are presented in the literature, which impacts understanding, consensus towards systematic methodologies, practice and adoption of BNs; and (4) a gap exists between having an accurate BN and a useful BN that impacts clinical practice. This review empowers researchers and clinicians with an analytical framework and findings that will enable understanding of the need to address the problems of restricted aims of BNs, ad hoc BN development methods, and the lack of BN adoption in practice. To map the way forward, the paper proposes future research directions and makes recommendations regarding BN development methods and adoption in practice.        △ Less","28 February, 2020",cs.AI,
              Interpretability of machine learning based prediction models in healthcare,2002.08596,https://arxiv.org/abs/2002.08596,https://arxiv.org/pdf/2002.08596,"Authors:GregorStiglic,PrimozKocbek,NinoFijacko,MarinkaZitnik,KatrienVerbert,LeonaCilar","        There is a need of ensuring machine learning models that are interpretable. Higher interpretability of the model means easier comprehension and explanation of future predictions for end-users. Further, interpretable machine learning models allow healthcare experts to make reasonable and data-driven decisions to provide personalized decisions that can ultimately lead to higher quality of service in healthcare. Generally, we can classify interpretability approaches in two groups where the first focuses on personalized interpretation (local interpretability) while the second summarizes prediction models on a population level (global interpretability). Alternatively, we can group interpretability methods into model-specific techniques, which are designed to interpret predictions generated by a specific model, such as a neural network, and model-agnostic approaches, which provide easy-to-understand explanations of predictions made by any machine learning model. Here, we give an overview of interpretability approaches and provide examples of practical interpretability of machine learning in different areas of healthcare, including prediction of health-related outcomes, optimizing treatments or improving the efficiency of screening for specific conditions. Further, we outline future directions for interpretable machine learning and highlight the importance of developing algorithmic solutions that can enable machine-learning driven decision making in high-stakes healthcare problems.        △ Less","14 August, 2020","cs.LG,stat.ML",10.1002/widm.1379 
              Federated pretraining and fine tuning of BERT using clinical notes from multiple silos          ,2002.08562,https://arxiv.org/abs/2002.08562,https://arxiv.org/pdf/2002.08562,"Authors:DianboLiu,TimMiller","        Large scale contextual representation models, such as BERT, have significantly advanced natural language processing (NLP) in recently years. However, in certain area like healthcare, accessing diverse large scale text data from multiple institutions is extremely challenging due to privacy and regulatory reasons. In this article, we show that it is possible to both pretrain and fine tune BERT models in a federated manner using clinical texts from different silos without moving the data.        △ Less","19 February, 2020","cs.CL,cs.LG",
              Comparative Visual Analytics for Assessing Medical Records with Sequence Embedding          ,2002.08356,https://arxiv.org/abs/2002.08356,https://arxiv.org/pdf/2002.08356,"Authors:RongchenGuo,TakanoriFujiwara,YiranLi,KellyM.Lima,SomanSen,NamK.Tran,Kwan-LiuMa","        Machine learning for data-driven diagnosis has been actively studied in medicine to provide better healthcare. Supporting analysis of a patient cohort similar to a patient under treatment is a key task for clinicians to make decisions with high confidence. However, such analysis is not straightforward due to the characteristics of medical records: high dimensionality, irregularity in time, and sparsity. To address this challenge, we introduce a method for similarity calculation of medical records. Our method employs event and sequence embeddings. While we use an autoencoder for the event embedding, we apply its variant with the self-attention mechanism for the sequence embedding. Moreover, in order to better handle the irregularity of data, we enhance the self-attention mechanism with consideration of different time intervals. We have developed a visual analytics system to support comparative studies of patient records. To make a comparison of sequences with different lengths easier, our system incorporates a sequence alignment method. Through its interactive interface, the user can quickly identify patients of interest and conveniently review both the temporal and multivariate aspects of the patient records. We demonstrate the effectiveness of our design and system with case studies using a real-world dataset from the neonatal intensive care unit of UC Davis.        △ Less","23 March, 2020","physics.med-ph,cs.HC,cs.LG,stat.ML",
              A New Methodology for Information Security Risk Assessment for Medical Devices and Its Evaluation          ,2002.06938,https://arxiv.org/abs/2002.06938,https://arxiv.org/pdf/2002.06938,"Authors:TomMahler,YuvalElovici,YuvalShahar","        As technology advances towards more connected and digital environments, medical devices are becoming increasingly connected to hospital networks and to the Internet, which exposes them, and thus the patients using them, to new cybersecurity threats. Currently, there is a lack of a methodology dedicated to information security risk assessment for medical devices.  In this study, we present the Threat identification, ontology-based Likelihood, severity Decomposition, and Risk integration (TLDR) methodology for information security risk assessment for medical devices. The TLDR methodology uses the following steps: (1) identifying the potentially vulnerable components of medical devices, in this case, four different medical imaging devices (MIDs); (2) identifying the potential attacks, in this case, 23 potential attacks on MIDs; (3) mapping the discovered attacks into a known attack ontology - in this case, the Common Attack Pattern Enumeration and Classifications (CAPECs); (4) estimating the likelihood of the mapped CAPECs in the medical domain with the assistance of a panel of senior healthcare Information Security Experts (ISEs); (5) computing the CAPEC-based likelihood estimates of each attack; (6) decomposing each attack into several severity aspects and assigning them weights; (7) assessing the magnitude of the impact of each of the severity aspects for each attack with the assistance of a panel of senior Medical Experts (MEs); (8) computing the composite severity assessments for each attack; and finally, (9) integrating the likelihood and severity of each attack into its risk, and thus prioritizing it. The details of steps six to eight are beyond the scope of the current study; in the current study, we had replaced them by a single step that included asking the panel of MEs [in this case, radiologists], to assess the overall severity for each attack and use it as its severity...        △ Less","17 February, 2020","cs.CR,cs.CY",
              Trustworthy AI in the Age of Pervasive Computing and Big Data          ,2002.05657,https://arxiv.org/abs/2002.05657,https://arxiv.org/pdf/2002.05657,"Authors:AbhishekKumar,TristanBraud,SasuTarkoma,PanHui","        The era of pervasive computing has resulted in countless devices that continuously monitor users and their environment, generating an abundance of user behavioural data. Such data may support improving the quality of service, but may also lead to adverse usages such as surveillance and advertisement. In parallel, Artificial Intelligence (AI) systems are being applied to sensitive fields such as healthcare, justice, or human resources, raising multiple concerns on the trustworthiness of such systems. Trust in AI systems is thus intrinsically linked to ethics, including the ethics of algorithms, the ethics of data, or the ethics of practice. In this paper, we formalise the requirements of trustworthy AI systems through an ethics perspective. We specifically focus on the aspects that can be integrated into the design and development of AI systems. After discussing the state of research and the remaining challenges, we show how a concrete use-case in smart cities can benefit from these methods.        △ Less","30 January, 2020",cs.CY,
              Learning Occupational Task-Shares Dynamics for the Future of Work          ,2002.05655,https://arxiv.org/abs/2002.05655,https://arxiv.org/pdf/2002.05655,"Authors:SubhroDas,SebastianSteffen,WyattClarke,PrabhatReddy,ErikBrynjolfsson,MartinFleming","        The recent wave of AI and automation has been argued to differ from previous General Purpose Technologies (GPTs), in that it may lead to rapid change in occupations' underlying task requirements and persistent technological unemployment. In this paper, we apply a novel methodology of dynamic task shares to a large dataset of online job postings to explore how exactly occupational task demands have changed over the past decade of AI innovation, especially across high, mid and low wage occupations. Notably, big data and AI have risen significantly among high wage occupations since 2012 and 2016, respectively. We built an ARIMA model to predict future occupational task demands and showcase several relevant examples in Healthcare, Administration, and IT. Such task demands predictions across occupations will play a pivotal role in retraining the workforce of the future.        △ Less","28 January, 2020","cs.CY,cs.LG,stat.AP,stat.ML",10.1145/3375627.3375826 
              HAN-ECG: An Interpretable Atrial Fibrillation Detection Model Using Hierarchical Attention Networks          ,2002.05262,https://arxiv.org/abs/2002.05262,https://arxiv.org/pdf/2002.05262,"Authors:SajadMousavi,FatemehAfghah,U.RajendraAcharya","        Atrial fibrillation (AF) is one of the most prevalent cardiac arrhythmias that affects the lives of more than 3 million people in the U.S. and over 33 million people around the world and is associated with a five-fold increased risk of stroke and mortality. like other problems in healthcare domain, artificial intelligence (AI)-based algorithms have been used to reliably detect AF from patients' physiological signals. The cardiologist level performance in detecting this arrhythmia is often achieved by deep learning-based methods, however, they suffer from the lack of interpretability. In other words, these approaches are unable to explain the reasons behind their decisions. The lack of interpretability is a common challenge toward a wide application of machine learning-based approaches in the healthcare which limits the trust of clinicians in such methods. To address this challenge, we propose HAN-ECG, an interpretable bidirectional-recurrent-neural-network-based approach for the AF detection task. The HAN-ECG employs three attention mechanism levels to provide a multi-resolution analysis of the patterns in ECG leading to AF. The first level, wave level, computes the wave weights, the second level, heartbeat level, calculates the heartbeat weights, and third level, window (i.e., multiple heartbeats) level, produces the window weights in triggering a class of interest. The detected patterns by this hierarchical attention model facilitate the interpretation of the neural network decision process in identifying the patterns in the signal which contributed the most to the final prediction. Experimental results on two AF databases demonstrate that our proposed model performs significantly better than the existing algorithms. Visualization of these attention layers illustrates that our model decides upon the important waves and heartbeats which are clinically meaningful in the detection task.        △ Less","12 February, 2020","q-bio.QM,cs.LG,eess.SP",
              Differentiable Graph Module (DGM) for Graph Convolutional Networks          ,2002.04999,https://arxiv.org/abs/2002.04999,https://arxiv.org/pdf/2002.04999,"Authors:AneesKazi,LucaCosmo,NassirNavab,MichaelBronstein","        Graph deep learning has recently emerged as a powerful ML concept allowing to generalize successful deep neural architectures to non-Euclidean structured data. Such methods have shown promising results on a broad spectrum of applications ranging from social science, biomedicine, and particle physics to computer vision, graphics, and chemistry. One of the limitations of the majority of the current graph neural network architectures is that they are often restricted to the transductive setting and rely on the assumption that the underlying graph is known and fixed. In many settings, such as those arising in medical and healthcare applications, this assumption is not necessarily true since the graph may be noisy, partially- or even completely unknown, and one is thus interested in inferring it from the data. This is especially important in inductive settings when dealing with nodes not present in the graph at training time. Furthermore, sometimes such a graph itself may convey insights that are even more important than the downstream task. In this paper, we introduce Differentiable Graph Module (DGM), a learnable function predicting the edge probability in the graph relevant for the task, that can be combined with convolutional graph neural network layers and trained in an end-to-end fashion. We provide an extensive evaluation of applications from the domains of healthcare (disease prediction), brain imaging (gender and age prediction), computer graphics (3D point cloud segmentation), and computer vision (zero-shot learning). We show that our model provides a significant improvement over baselines both in transductive and inductive settings and achieves state-of-the-art results.        △ Less","17 June, 2020","cs.LG,stat.ML",
              Deep Transfer Learning for Physiological Signals          ,2002.04770,https://arxiv.org/abs/2002.04770,https://arxiv.org/pdf/2002.04770,"Authors:HughChen,ScottLundberg,GabeErion,JerryH.Kim,Su-InLee","        Deep learning is increasingly common in healthcare, yet transfer learning for physiological signals (e.g., temperature, heart rate, etc.) is under-explored. Here, we present a straightforward, yet performant framework for transferring knowledge about physiological signals. Our framework is called PHASE (PHysiologicAl Signal Embeddings). It i) learns deep embeddings of physiological signals and ii) predicts adverse outcomes based on the embeddings. PHASE is the first instance of deep transfer learning in a cross-hospital, cross-department setting for physiological signals. We show that PHASE's per-signal (one for each signal) LSTM embedding functions confer a number of benefits including improved performance, successful transference between hospitals, and lower computational cost.        △ Less","11 February, 2020","cs.LG,eess.SP,stat.ML",
              A Single RGB Camera Based Gait Analysis with a Mobile Tele-Robot for Healthcare,2002.04700,https://arxiv.org/abs/2002.04700,https://arxiv.org/pdf/2002.04700,Authors:ZiyangWang,"        With the increasing awareness of high-quality life, there is a growing need for health monitoring devices running robust algorithms in home environment. Health monitoring technologies enable real-time analysis of users' health status, offering long-term healthcare support and reducing hospitalization time. The purpose of this work is twofold, the software focuses on the analysis of gait, which is widely adopted for joint correction and assessing any lower limb or spinal problem. On the hardware side, we design a novel marker-less gait analysis device using a low-cost RGB camera mounted on a mobile tele-robot. As gait analysis with a single camera is much more challenging compared to previous works utilizing multi-cameras, a RGB-D camera or wearable sensors, we propose using vision-based human pose estimation approaches. More specifically, based on the output of two state-of-the-art human pose estimation models (Openpose and VNect), we devise measurements for four bespoke gait parameters: inversion/eversion, dorsiflexion/plantarflexion, ankle and foot progression angles. We thereby classify walking patterns into normal, supination, pronation and limp. We also illustrate how to run the purposed machine learning models in low-resource environments such as a single entry-level CPU. Experiments show that our single RGB camera method achieves competitive performance compared to state-of-the-art methods based on depth cameras or multi-camera motion capture system, at smaller hardware costs.        △ Less","14 March, 2020","cs.RO,cs.CV,cs.HC,cs.LG",
              Confounding-Robust Policy Evaluation in Infinite-Horizon Reinforcement Learning          ,2002.04518,https://arxiv.org/abs/2002.04518,https://arxiv.org/pdf/2002.04518,"Authors:NathanKallus,AngelaZhou","        Off-policy evaluation of sequential decision policies from observational data is necessary in applications of batch reinforcement learning such as education and healthcare. In such settings, however, unobserved variables confound observed actions, rendering exact evaluation of new policies impossible, i.e., unidentifiable. We develop a robust approach that estimates sharp bounds on the (unidentifiable) value of a given policy in an infinite-horizon problem given data from another policy with unobserved confounding, subject to a sensitivity model. We consider stationary or baseline unobserved confounding and compute bounds by optimizing over the set of all stationary state-occupancy ratios that agree with a new partially identified estimating equation and the sensitivity model. We prove convergence to the sharp bounds as we collect more confounded data. Although checking set membership is a linear program, the support function is given by a difficult nonconvex optimization problem. We develop approximations based on nonconvex projected gradient descent and demonstrate the resulting bounds empirically.        △ Less","13 July, 2020","cs.LG,math.OC,stat.ML",
              Privacy-preserving collaborative machine learning on genomic data using TensorFlow          ,2002.04344,https://arxiv.org/abs/2002.04344,https://arxiv.org/pdf/2002.04344,"Authors:ChengHong,ZhicongHuang,Wen-jieLu,HunterQu,LiMa,MortenDahl,JasonMancuso","        Machine learning (ML) methods have been widely used in genomic studies. However, genomic data are often held by different stakeholders (e.g. hospitals, universities, and healthcare companies) who consider the data as sensitive information, even though they desire to collaborate. To address this issue, recent works have proposed solutions using Secure Multi-party Computation (MPC), which train on the decentralized data in a way that the participants could learn nothing from each other beyond the final trained model.  We design and implement several MPC-friendly ML primitives, including class weight adjustment and parallelizable approximation of activation function. In addition, we develop the solution as an extension to TF Encrypted~\citep{dahl2018private}, enabling us to quickly experiment with enhancements of both machine learning techniques and cryptographic protocols while leveraging the advantages of TensorFlow's optimizations. Our implementation compares favorably with state-of-the-art methods, winning first place in Track IV of the iDASH2019 secure genome analysis competition.        △ Less","29 February, 2020",cs.CR,
              Point-of-Care Diabetic Retinopathy Diagnosis: A Standalone Mobile Application Approach          ,2002.04066,https://arxiv.org/abs/2002.04066,https://arxiv.org/pdf/2002.04066,Authors:MisginaTsigheHagos,"        Although deep learning research and applications have grown rapidly over the past decade, it has shown limitation in healthcare applications and its reachability to people in remote areas. One of the challenges of incorporating deep learning in medical data classification or prediction is the shortage of annotated training data in the healthcare industry. Medical data sharing privacy issues and limited patient population size can be stated as some of the reasons for training data insufficiency in healthcare. Methods to exploit deep learning applications in healthcare have been proposed and implemented in this dissertation.  Traditional diagnosis of diabetic retinopathy requires trained ophthalmologists and expensive imaging equipment to reach healthcare centres in order to provide facilities for treatment of preventable blindness. Diabetic people residing in remote areas with shortage of healthcare services and ophthalmologists usually fail to get periodical diagnosis of diabetic retinopathy thereby facing the probability of vision loss or impairment. Deep learning and mobile application development have been integrated in this dissertation to provide an easy to use point-of-care smartphone based diagnosis of diabetic retinopathy. In order to solve the challenge of shortage of healthcare centres and trained ophthalmologists, the standalone diagnostic service was built so as to be operated by a non-expert without an internet connection. This approach could be transferred to other areas of medical image classification.        △ Less","26 January, 2020","eess.IV,cs.CV",
              AI-oriented Medical Workload Allocation for Hierarchical Cloud/Edge/Device Computing          ,2002.03493,https://arxiv.org/abs/2002.03493,https://arxiv.org/pdf/2002.03493,"Authors:TianshuHao,JianfengZhan,KaiHwang,WanlingGao,XuWen","        In a hierarchically-structured cloud/edge/device computing environment, workload allocation can greatly affect the overall system performance. This paper deals with AI-oriented medical workload generated in emergency rooms (ER) or intensive care units (ICU) in metropolitan areas. The goal is to optimize AI-workload allocation to cloud clusters, edge servers, and end devices so that minimum response time can be achieved in life-saving emergency applications.  In particular, we developed a new workload allocation method for the AI workload in distributed cloud/edge/device computing systems. An efficient scheduling and allocation strategy is developed in order to reduce the overall response time to satisfy multi-patient demands. We apply several ICU AI workloads from a comprehensive edge computing benchmark Edge AIBench. The healthcare AI applications involved are short-of-breath alerts, patient phenotype classification, and life-death threats. Our experimental results demonstrate the high efficiency and effectiveness in real-life health-care and emergency applications.        △ Less","9 February, 2020","cs.DC,cs.PF",
              Interpretable Off-Policy Evaluation in Reinforcement Learning by Highlighting Influential Transitions          ,2002.03478,https://arxiv.org/abs/2002.03478,https://arxiv.org/pdf/2002.03478,"Authors:OmerGottesman,JosephFutoma,YaoLiu,SonaliParbhoo,LeoAnthonyCeli,EmmaBrunskill,FinaleDoshi-Velez","        Off-policy evaluation in reinforcement learning offers the chance of using observational data to improve future outcomes in domains such as healthcare and education, but safe deployment in high stakes settings requires ways of assessing its validity. Traditional measures such as confidence intervals may be insufficient due to noise, limited data and confounding. In this paper we develop a method that could serve as a hybrid human-AI system, to enable human experts to analyze the validity of policy evaluation estimates. This is accomplished by highlighting observations in the data whose removal will have a large effect on the OPE estimate, and formulating a set of rules for choosing which ones to present to domain experts for validation. We develop methods to compute exactly the influence functions for fitted Q-evaluation with two different function classes: kernel-based and linear least squares, as well as importance sampling methods. Experiments on medical simulations and real-world intensive care unit data demonstrate that our method can be used to identify limitations in the evaluation process and make evaluation more robust.        △ Less","11 August, 2020","cs.LG,stat.ML",
              Improved Algorithms for Conservative Exploration in Bandits          ,2002.03221,https://arxiv.org/abs/2002.03221,https://arxiv.org/pdf/2002.03221,"Authors:EvrardGarcelon,MohammadGhavamzadeh,AlessandroLazaric,MatteoPirotta","        In many fields such as digital marketing, healthcare, finance, and robotics, it is common to have a well-tested and reliable baseline policy running in production (e.g., a recommender system). Nonetheless, the baseline policy is often suboptimal. In this case, it is desirable to deploy online learning algorithms (e.g., a multi-armed bandit algorithm) that interact with the system to learn a better/optimal policy under the constraint that during the learning process the performance is almost never worse than the performance of the baseline itself. In this paper, we study the conservative learning problem in the contextual linear bandit setting and introduce a novel algorithm, the Conservative Constrained LinUCB (CLUCB2). We derive regret bounds for CLUCB2 that match existing results and empirically show that it outperforms state-of-the-art conservative bandit algorithms in a number of synthetic and real-world problems. Finally, we consider a more realistic constraint where the performance is verified only at predefined checkpoints (instead of at every step) and show how this relaxed constraint favorably impacts the regret and empirical performance of CLUCB2.        △ Less","8 February, 2020","cs.LG,stat.ML",
              HHH: An Online Medical Chatbot System based on Knowledge Graph and Hierarchical Bi-Directional Attention          ,2002.03140,https://arxiv.org/abs/2002.03140,https://arxiv.org/pdf/2002.03140,"Authors:QimingBao,LinNi,JiamouLiu","        This paper proposes a chatbot framework that adopts a hybrid model which consists of a knowledge graph and a text similarity model. Based on this chatbot framework, we build HHH, an online question-and-answer (QA) Healthcare Helper system for answering complex medical questions. HHH maintains a knowledge graph constructed from medical data collected from the Internet. HHH also implements a novel text representation and similarity deep learning model, Hierarchical BiLSTM Attention Model (HBAM), to find the most similar question from a large QA dataset. We compare HBAM with other state-of-the-art language models such as bidirectional encoder representation from transformers (BERT) and Manhattan LSTM Model (MaLSTM). We train and test the models with a subset of the Quora duplicate questions dataset in the medical area. The experimental results show that our model is able to achieve a superior performance than these existing methods.        △ Less","8 February, 2020",cs.CL,10.1145/3373017.3373049 
              Assistive robots for the social management of health: a framework for robot design and human-robot interaction research          ,2002.03062,https://arxiv.org/abs/2002.03062,https://arxiv.org/pdf/2002.03062,"Authors:MeiaChita-Tegmark,MatthiasScheutz","        There is a close connection between health and the quality of one's social life. Strong social bonds are essential for health and wellbeing, but often health conditions can detrimentally affect a person's ability to interact with others. This can become a vicious cycle resulting in further decline in health. For this reason, the social management of health is an important aspect of healthcare. We propose that socially assistive robots (SARs) could help people with health conditions maintain positive social lives by supporting them in social interactions. This paper makes three contributions, as detailed below. We develop a framework of social mediation functions that robots could perform, motivated by the special social needs that people with health conditions have. In this framework we identify five types of functions that SARs could perform: a) changing how the person is perceived, b) enhancing the social behavior of the person, c) modifying the social behavior of others, d) providing structure for interactions, and e) changing how the person feels. We thematically organize and review the existing literature on robots supporting human-human interactions, in both clinical and non-clinical settings, and explain how the findings and design ideas from these studies can be applied to the functions identified in the framework. Finally, we point out and discuss challenges in designing SARs for supporting social interactions, and highlight opportunities for future robot design and HRI research on the mediator role of robots.        △ Less","29 March, 2020","cs.HC,cs.CY,cs.RO",10.1007/s12369-020-00634-z 
              MDLdroid: a ChainSGD-reduce Approach to Mobile Deep Learning for Personal Mobile Sensing          ,2002.02897,https://arxiv.org/abs/2002.02897,https://arxiv.org/pdf/2002.02897,"Authors:YuZhang,TaoGu,XiZhang","        Personal mobile sensing is fast permeating our daily lives to enable activity monitoring, healthcare and rehabilitation. Combined with deep learning, these applications have achieved significant success in recent years. Different from conventional cloud-based paradigms, running deep learning on devices offers several advantages including data privacy preservation and low-latency response for both model inference and update. Since data collection is costly in reality, Google's Federated Learning offers not only complete data privacy but also better model robustness based on multiple user data. However, personal mobile sensing applications are mostly user-specific and highly affected by environment. As a result, continuous local changes may seriously affect the performance of a global model generated by Federated Learning. In addition, deploying Federated Learning on a local server, e.g., edge server, may quickly reach the bottleneck due to resource constraint and serious failure by attacks. Towards pushing deep learning on devices, we present MDLdroid, a novel decentralized mobile deep learning framework to enable resource-aware on-device collaborative learning for personal mobile sensing applications. To address resource limitation, we propose a ChainSGD-reduce approach which includes a novel chain-directed Synchronous Stochastic Gradient Descent algorithm to effectively reduce overhead among multiple devices. We also design an agent-based multi-goal reinforcement learning mechanism to balance resources in a fair and efficient manner. Our evaluations show that our model training on off-the-shelf mobile devices achieves 2x to 3.5x faster than single-device training, and 1.5x faster than the master-slave approach.        △ Less","15 February, 2020","cs.LG,cs.NI,stat.ML",
              Machine Learning for Predicting Epileptic Seizures Using EEG Signals: A Review          ,2002.01925,https://arxiv.org/abs/2002.01925,https://arxiv.org/pdf/2002.01925,"Authors:KhansaRasheed,AdnanQayyum,JunaidQadir,ShobiSivathamboo,PatrickKwan,LevinKuhlmann,TerenceO'Brien,AdeelRazi","        With the advancement in artificial intelligence (AI) and machine learning (ML) techniques, researchers are striving towards employing these techniques for advancing clinical practice. One of the key objectives in healthcare is the early detection and prediction of disease to timely provide preventive interventions. This is especially the case for epilepsy, which is characterized by recurrent and unpredictable seizures. Patients can be relieved from the adverse consequences of epileptic seizures if it could somehow be predicted in advance. Despite decades of research, seizure prediction remains an unsolved problem. This is likely to remain at least partly because of the inadequate amount of data to resolve the problem. There have been exciting new developments in ML-based algorithms that have the potential to deliver a paradigm shift in the early and accurate prediction of epileptic seizures. Here we provide a comprehensive review of state-of-the-art ML techniques in early prediction of seizures using EEG signals. We will identify the gaps, challenges, and pitfalls in the current research and recommend future directions.        △ Less","4 February, 2020","cs.LG,eess.SP,q-bio.NC",
              Control of Fork-Join Processing Networks with Multiple Job Types and Parallel Shared Resources          ,2002.01496,https://arxiv.org/abs/2002.01496,https://arxiv.org/pdf/2002.01496,Authors:ErhunOzkan,"        A fork-join processing network is a queueing network in which tasks associated with a job can be processed simultaneously. Fork-join processing networks are prevalent in computer systems, healthcare, manufacturing, project management, justice system, etc. Unlike the conventional queueing networks, fork-join processing networks have synchronization constraints that arise due to the parallel processing of tasks and can cause significant job delays. We study scheduling control in fork-join processing networks with multiple job types and parallel shared resources. Jobs arriving in the system fork into arbitrary number of tasks, then those tasks are processed in parallel, and then they join and leave the network. There are shared resources processing multiple job types. We study the scheduling problem for those shared resources (that is, which type of job to prioritize at any given time) and propose an asymptotically optimal scheduling policy in diffusion scale.        △ Less","4 February, 2020","math.PR,math.OC",
              A Generalized Signal Quality Estimation Method for IoT Sensors          ,2002.01279,https://arxiv.org/abs/2002.01279,https://arxiv.org/pdf/2002.01279,"Authors:ArleneJohn,BarryCardiff,DeepuJohn","        IoT wearable devices are widely expected to reduce the cost and risk of personal healthcare. However, ambulatory data collected from such devices are often corrupted or contaminated with severe noises. Signal Quality Indicators (SQIs) can be used to assess the quality of data obtained from wearable devices, such that transmission/ storage of unusable data can be prevented. This article introduces a novel and generalized SQI which can be implemented on an edge device for detecting the quality of any quasi-periodic signal under observation, regardless of the type of noise present. The application of this SQI on Electrocardiogram (ECG) signals is investigated. From the analysis carried out, it was found that the proposed generalized SQI is suitable for quality assessment of ECG signals and exhibits a linear behavior in the medium to high SNR regions under all noise conditions considered. The proposed SQI was used for acceptability testing of ECG records in CinC Physionet 2011 challenge dataset and found to be accurate for 90.4% of the records while having minimal computational complexity.        △ Less","4 February, 2020",eess.SP,
              Learning Contextualized Document Representations for Healthcare Answer Retrieval          ,2002.00835,https://arxiv.org/abs/2002.00835,https://arxiv.org/pdf/2002.00835,"Authors:SebastianArnold,BettyvanAken,PaulGrundmann,FelixA.Gers,AlexanderLöser","        We present Contextual Discourse Vectors (CDV), a distributed document representation for efficient answer retrieval from long healthcare documents. Our approach is based on structured query tuples of entities and aspects from free text and medical taxonomies. Our model leverages a dual encoder architecture with hierarchical LSTM layers and multi-task training to encode the position of clinical entities and aspects alongside the document discourse. We use our continuous representations to resolve queries with short latency using approximate nearest neighbor search on sentence level. We apply the CDV model for retrieving coherent answer passages from nine English public health resources from the Web, addressing both patients and medical professionals. Because there is no end-to-end training data available for all application scenarios, we train our model with self-supervised data from Wikipedia. We show that our generalized model significantly outperforms several state-of-the-art baselines for healthcare passage ranking and is able to adapt to heterogeneous domains without additional fine-tuning.        △ Less","3 February, 2020",cs.CL,10.1145/3366423.3380208 
              Bayesian Networks in Healthcare: Distribution by Medical Condition          ,2002.00224,https://arxiv.org/abs/2002.00224,https://arxiv.org/pdf/2002.00224,"Authors:ScottMcLachlan,KudakwasheDube,GrahamAHitman,NormanEFenton,EvangeliaKyrimi","        Bayesian networks (BNs) have received increasing research attention that is not matched by adoption in practice and yet have potential to significantly benefit healthcare. Hitherto, research works have not investigated the types of medical conditions being modelled with BNs, nor whether any differences exist in how and why they are applied to different conditions. This research seeks to identify and quantify the range of medical conditions for which healthcare-related BN models have been proposed, and the differences in approach between the most common medical conditions to which they have been applied. We found that almost two-thirds of all healthcare BNs are focused on four conditions: cardiac, cancer, psychological and lung disorders. We believe that a lack of understanding regarding how BNs work and what they are capable of exists, and that it is only with greater understanding and promotion that we may ever realise the full potential of BNs to effect positive change in daily healthcare practice.        △ Less","4 February, 2020","cs.CY,cs.AI",
              Rapid Detection of Hot-spot by Tensor Decomposition with Application to Weekly Gonorrhea Data          ,2001.11685,https://arxiv.org/abs/2001.11685,https://arxiv.org/pdf/2001.11685,"Authors:YujieZhao,HaoYan,SarahE.Holte,RoxanneP.Kerani,YajunMei","        In many bio-surveillance and healthcare applications, data sources are measured from many spatial locations repeatedly over time, say, daily/weekly/monthly. In these applications, we are typically interested in detecting hot-spots, which are defined as some structured outliers that are sparse over the spatial domain but persistent over time. In this paper, we propose a tensor decomposition method to detect when and where the hot-spots occur. Our proposed methods represent the observed raw data as a three-dimensional tensor including a circular time dimension for daily/weekly/monthly patterns, and then decompose the tensor into three components: smooth global trend, local hot-spots, and residuals. A combination of LASSO and fused LASSO is used to estimate the model parameters, and a CUSUM procedure is applied to detect when and where the hot-spots might occur. The usefulness of our proposed methodology is validated through numerical simulation and a real-world dataset in the weekly number of gonorrhea cases from 20062006 to 20182018 for 5050 states in the United States.        △ Less","7 April, 2020","stat.AP,stat.ME",
              REST: Robust and Efficient Neural Networks for Sleep Monitoring in the Wild          ,2001.11363,https://arxiv.org/abs/2001.11363,https://arxiv.org/pdf/2001.11363,"Authors:RahulDuggal,ScottFreitas,CaoXiao,DuenHorngChau,JimengSun","        In recent years, significant attention has been devoted towards integrating deep learning technologies in the healthcare domain. However, to safely and practically deploy deep learning models for home health monitoring, two significant challenges must be addressed: the models should be (1) robust against noise; and (2) compact and energy-efficient. We propose REST, a new method that simultaneously tackles both issues via 1) adversarial training and controlling the Lipschitz constant of the neural network through spectral regularization while 2) enabling neural network compression through sparsity regularization. We demonstrate that REST produces highly-robust and efficient models that substantially outperform the original full-sized models in the presence of noise. For the sleep staging task over single-channel electroencephalogram (EEG), the REST model achieves a macro-F1 score of 0.67 vs. 0.39 achieved by a state-of-the-art model in the presence of Gaussian noise while obtaining 19x parameter reduction and 15x MFLOPS reduction on two large, real-world EEG datasets. By deploying these models to an Android application on a smartphone, we quantitatively observe that REST allows models to achieve up to 17x energy reduction and 9x faster inference. We open-source the code repository with this paper: https://github.com/duggalrahul/REST.        △ Less","29 January, 2020","eess.SP,cs.LG,stat.ML",10.1145/3366423.3380241 
              FOCUS: Dealing with Label Quality Disparity in Federated Learning          ,2001.11359,https://arxiv.org/abs/2001.11359,https://arxiv.org/pdf/2001.11359,"Authors:YiqiangChen,XiaodongYang,XinQin,HanYu,BiaoChen,ZhiqiShen","        Ubiquitous systems with End-Edge-Cloud architecture are increasingly being used in healthcare applications. Federated Learning (FL) is highly useful for such applications, due to silo effect and privacy preserving. Existing FL approaches generally do not account for disparities in the quality of local data labels. However, the clients in ubiquitous systems tend to suffer from label noise due to varying skill-levels, biases or malicious tampering of the annotators. In this paper, we propose Federated Opportunistic Computing for Ubiquitous Systems (FOCUS) to address this challenge. It maintains a small set of benchmark samples on the FL server and quantifies the credibility of the client local data without directly observing them by computing the mutual cross-entropy between performance of the FL model on the local datasets and that of the client local FL model on the benchmark dataset. Then, a credit weighted orchestration is performed to adjust the weight assigned to clients in the FL model based on their credibility values. FOCUS has been experimentally evaluated on both synthetic data and real-world data. The results show that it effectively identifies clients with noisy labels and reduces their impact on the model performance, thereby significantly outperforming existing FL approaches.        △ Less","29 January, 2020","cs.LG,stat.ML",
              EEG-based Brain-Computer Interfaces (BCIs): A Survey of Recent Studies on Signal Sensing Technologies and Computational Intelligence Approaches and their Applications          ,2001.11337,https://arxiv.org/abs/2001.11337,https://arxiv.org/pdf/2001.11337,"Authors:XiaotongGu,ZehongCao,AlirezaJolfaei,PengXu,DongruiWu,Tzyy-PingJung,Chin-TengLin","        Brain-Computer Interface (BCI) is a powerful communication tool between users and systems, which enhances the capability of the human brain in communicating and interacting with the environment directly. Advances in neuroscience and computer science in the past decades have led to exciting developments in BCI, thereby making BCI a top interdisciplinary research area in computational neuroscience and intelligence. Recent technological advances such as wearable sensing devices, real-time data streaming, machine learning, and deep learning approaches have increased interest in electroencephalographic (EEG) based BCI for translational and healthcare applications. Many people benefit from EEG-based BCIs, which facilitate continuous monitoring of fluctuations in cognitive states under monotonous tasks in the workplace or at home. In this study, we survey the recent literature of EEG signal sensing technologies and computational intelligence approaches in BCI applications, compensated for the gaps in the systematic summary of the past five years (2015-2019). In specific, we first review the current status of BCI and its significant obstacles. Then, we present advanced signal sensing and enhancement technologies to collect and clean EEG signals, respectively. Furthermore, we demonstrate state-of-art computational intelligence techniques, including interpretable fuzzy models, transfer learning, deep learning, and combinations, to monitor, maintain, or track human cognitive states and operating performance in prevalent applications. Finally, we deliver a couple of innovative BCI-inspired healthcare applications and discuss some future research directions in EEG-based BCIs.        △ Less","28 January, 2020","eess.SP,cs.AI,cs.HC",
              Interpreting Machine Learning Malware Detectors Which Leverage N-gram Analysis          ,2001.10916,https://arxiv.org/abs/2001.10916,https://arxiv.org/pdf/2001.10916,"Authors:WilliamBriguglio,SherifSaad","        In cyberattack detection and prevention systems, cybersecurity analysts always prefer solutions that are as interpretable and understandable as rule-based or signature-based detection. This is because of the need to tune and optimize these solutions to mitigate and control the effect of false positives and false negatives. Interpreting machine learning models is a new and open challenge. However, it is expected that an interpretable machine learning solution will be domain-specific. For instance, interpretable solutions for machine learning models in healthcare are different than solutions in malware detection. This is because the models are complex, and most of them work as a black-box. Recently, the increased ability for malware authors to bypass antimalware systems has forced security specialists to look to machine learning for creating robust detection systems. If these systems are to be relied on in the industry, then, among other challenges, they must also explain their predictions. The objective of this paper is to evaluate the current state-of-the-art ML models interpretability techniques when applied to ML-based malware detectors. We demonstrate interpretability techniques in practice and evaluate the effectiveness of existing interpretability techniques in the malware analysis domain.        △ Less","27 January, 2020","cs.CR,cs.AI,cs.LG",
              Deep Reinforcement Learning for Backscatter-Aided Data Offloading in Mobile Edge Computing          ,2001.10183,https://arxiv.org/abs/2001.10183,https://arxiv.org/pdf/2001.10183,"Authors:ShiminGong,YutongXie,JingXu,DusitNiyato,Ying-ChangLiang","        Wireless network optimization has been becoming very challenging as the problem size and complexity increase tremendously, due to close couplings among network entities with heterogeneous service and resource requirements. By continuously interacting with the environment, deep reinforcement learning (DRL) provides a mechanism for different network entities to build knowledge and make autonomous decisions to improve network performance. In this article, we first review typical DRL approaches and recent enhancements. We then discuss the applications of DRL for mobile edge computing (MEC), which can be used for the low-power IoT devices, e.g., wireless sensors in healthcare monitoring, to offload computation workload to nearby MEC servers. To balance power consumption in offloading and computation, we propose a novel hybrid offloading model that exploits the complement operations of RF communications and low-power backscatter communications. The DRL framework is then customized to optimize the transmission scheduling and workload allocation in two communications technologies, which is shown to enhance the offloading performance significantly compared with existing schemes.        △ Less","28 January, 2020","cs.IT,eess.SP",
              SeMA: Extending and Analyzing Storyboards to Develop Secure Android Apps          ,2001.10052,https://arxiv.org/abs/2001.10052,https://arxiv.org/pdf/2001.10052,"Authors:JoydeepMitra,Venkatesh-PrasadRanganath,TorbenAmtoft,MikeHiggins","        Mobile apps provide various critical services, such as banking, communication, and healthcare. To this end, they have access to our personal information and have the ability to perform actions on our behalf. Hence, securing mobile apps is crucial to ensuring the privacy and safety of its users.  Recent research efforts have focused on developing solutions to secure mobile ecosystems (i.e., app platforms, apps, and app stores), specifically in the context of detecting vulnerabilities in Android apps. Despite this attention, known vulnerabilities are often found in mobile apps, which can be exploited by malicious apps to harm the user. Further, fixing vulnerabilities after developing an app has downsides in terms of time, resources, user inconvenience, and information loss.  In an attempt to address this concern, we have developed SeMA, a mobile app development methodology that builds on existing mobile app design artifacts such as storyboards. With SeMA, security is a first-class citizen in an app's design -- app designers and developers can collaborate to specify and reason about the security properties of an app at an abstract level without being distracted by implementation level details. Our realization of SeMA using Android Studio tooling demonstrates the methodology is complementary to existing design and development practices. An evaluation of the effectiveness of SeMA shows the methodology can detect and help prevent 49 vulnerabilities known to occur in Android apps. Further, a usability study of the methodology involving ten real-world developers shows the methodology is likely to reduce the development time and help developers uncover and prevent known vulnerabilities while designing apps.        △ Less","20 July, 2020","cs.SE,cs.CR,cs.PL",
              Algorithmic Fairness          ,2001.09784,https://arxiv.org/abs/2001.09784,https://arxiv.org/pdf/2001.09784,"Authors:DanaPessach,ErezShmueli","        An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence (AI) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop AI algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision-making may be inherently prone to unfairness, even when there is no intention for it. This paper presents an overview of the main concepts of identifying, measuring and improving algorithmic fairness when using AI algorithms. The paper begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, towards a better understanding of which mechanisms should be used in different scenarios. The paper then describes the most commonly used fairness-related datasets in this field. Finally, the paper ends by reviewing several emerging research sub-fields of algorithmic fairness.        △ Less","21 January, 2020","cs.CY,cs.AI,cs.LG,stat.ML",
              A Federated Learning Framework for Privacy-preserving and Parallel Training          ,2001.09782,https://arxiv.org/abs/2001.09782,https://arxiv.org/pdf/2001.09782,"Authors:Tien-DungCao,TramTruong-Huu,HienTran,KhanhTran","        The deployment of such deep learning in practice has been hurdled by two issues: the computational cost of model training and the privacy issue of training data such as medical or healthcare records. The large size of both learning models and datasets incurs a massive computational cost, requiring efficient approaches to speed up the training phase. While parallel and distributed learning can address the issue of computational overhead, preserving the privacy of training data and intermediate results (e.g., gradients) remains a hard problem. Enabling parallel training of deep learning models on distributed datasets while preserving data privacy is even more complex and challenging. In this paper, we develop and implement FEDF, a distributed deep learning framework for privacy-preserving and parallel training. The framework allows a model to be learned on multiple geographically-distributed training datasets (which may belong to different owners) while do not reveal any information of each dataset as well as the intermediate results. We formally prove the convergence of the learning model when training with the developed framework and its privacy-preserving property. We carry out extensive experiments to evaluate the performance of the framework in terms of speedup ratio, the approximation to the upper-bound performance (when training centrally) and communication overhead between the master and training workers. The results show that the developed framework achieves a speedup of up to 4.8x compared to the centralized training approach and maintaining the performance approximation of the models within 4.5% of the centrally-trained models. The proposed framework also significantly reduces the amount of data exchanged between the master and training workers by up to 34% compared to existing work.        △ Less","29 May, 2020","cs.DC,cs.CR,cs.LG",
              Artificial intelligence in medicine and healthcare: a review and classification of current and near-future applications and their ethical and social Impact          ,2001.09778,https://arxiv.org/abs/2001.09778,https://arxiv.org/pdf/2001.09778,"Authors:EmilioGómez-González,EmiliaGomez,JavierMárquez-Rivas,ManuelGuerrero-Claro,IsabelFernández-Lizaranzu,MaríaIsabelRelimpio-López,ManuelE.Dorado,MaríaJoséMayorga-Buiza,GuillermoIzquierdo-Ayuso,LuisCapitán-Morales","        This paper provides an overview of the current and near-future applications of Artificial Intelligence (AI) in Medicine and Health Care and presents a classification according to their ethical and societal aspects, potential benefits and pitfalls, and issues that can be considered controversial and are not deeply discussed in the literature.  This work is based on an analysis of the state of the art of research and technology, including existing software, personal monitoring devices, genetic tests and editing tools, personalized digital models, online platforms, augmented reality devices, and surgical and companion robotics. Motivated by our review, we present and describe the notion of 'extended personalized medicine', we then review existing applications of AI in medicine and healthcare and explore the public perception of medical AI systems, and how they show, simultaneously, extraordinary opportunities and drawbacks that even question fundamental medical concepts. Many of these topics coincide with urgent priorities recently defined by the World Health Organization for the coming decade. In addition, we study the transformations of the roles of doctors and patients in an age of ubiquitous information, identify the risk of a division of Medicine into 'fake-based', 'patient-generated', and 'scientifically tailored', and draw the attention of some aspects that need further thorough analysis and public debate.        △ Less","6 February, 2020","cs.CY,cs.AI",
              Fairness and Decision-making in Collaborative Shift Scheduling Systems          ,2001.09755,https://arxiv.org/abs/2001.09755,https://arxiv.org/pdf/2001.09755,"Authors:AlarithUhde,NadineSchlicker,DieterP.Wallach,MarcHassenzahl","        The strains associated with shift work decrease healthcare workers' well-being. However, shift schedules adapted to their individual needs can partially mitigate these problems. From a computing perspective, shift scheduling was so far mainly treated as an optimization problem with little attention given to the preferences, thoughts, and feelings of the healthcare workers involved. In the present study, we explore fairness as a central, human-oriented attribute of shift schedules as well as the scheduling process. Three in-depth qualitative interviews and a validating vignette study revealed that while on an abstract level healthcare workers agree on equality as the guiding norm for a fair schedule, specific scheduling conflicts should foremost be resolved by negotiating the importance of individual needs. We discuss elements of organizational fairness, including transparency and team spirit. Finally, we present a sketch for fair scheduling systems, summarizing key findings for designers in a readily usable way.        △ Less","6 February, 2020",cs.CY,10.1145/3313831.3376656 
              Can an Algorithm be My Healthcare Proxy?          ,2001.09742,https://arxiv.org/abs/2001.09742,https://arxiv.org/pdf/2001.09742,"Authors:DuncanCMcElfresh,SamuelDooley,YuanCui,KendraGriesman,WeiqinWang,TylerWill,NeilSehgal,JohnPDickerson","        Planning for death is not a process in which everyone participates. Yet a lack of planning can have vast impacts on a patient's well-being, the well-being of her family, and the medical community as a whole. Advance Care Planning (ACP) has been a field in the United States for a half-century. Many modern techniques prompting patients to think about end of life (EOL) involve short surveys or questionnaires. Different surveys are targeted to different populations (based off of likely disease progression or cultural factors, for instance), are designed with different intentions, and are administered in different ways. There has been recent work using technology to increase the number of people using advance care planning tools. However, modern techniques from machine learning and artificial intelligence could be employed to make additional changes to the current ACP process. In this paper we will discuss some possible ways in which these tools could be applied. We will discuss possible implications of these applications through vignettes of patient scenarios. We hope that this paper will encourage thought about appropriate applications of artificial intelligence in ACP as well as implementation of AI in order to ensure intentions are honored.        △ Less","7 January, 2020",cs.CY,
              The Ground Truth Trade-Off in Wearable Sensing Studies          ,2001.09738,https://arxiv.org/abs/2001.09738,https://arxiv.org/pdf/2001.09738,"Authors:DaniyalLiaqat,RobertWu,SalaarLiaqat,EyaldeLara,AndreaGershon,FrankRudzicz","        Perez et al's study using the Apple Watch to identify atrial fibrillation (AF) is a watershed moment in large-scale machine learning for wearable computing. Identifying relevant patients will be tremendously important to research in healthcare. For a condition like AF, this could reduce stroke risk by two thirds. In the study by Perez et al, only 450 out of 420,000 individuals had ground truth data. Their study excluded 417,000 participants using the irregular pulse notification. This design decision means their study was only able to report positive predictive value (PPV) and unable to explore sensitivity or specificity. In this editorial, we explore the difficulty of obtaining ground truth data and its implications for study design.        △ Less","6 January, 2020",cs.CY,
              Secondary Use of Electronic Health Record: Opportunities and Challenges          ,2001.09479,https://arxiv.org/abs/2001.09479,https://arxiv.org/pdf/2001.09479,"Authors:ShahidMunirShah,RizwanAhmedKhan","        In present technological era, healthcare providers generate huge amount of clinical data on daily basis. Generated clinical data is stored digitally in the form of Electronic Health Records (EHR) as a central data repository of hospitals. Data contained in EHR is not only used for the patients' primary care but also for various secondary purposes such as clinical research, automated disease surveillance and clinical audits for quality enhancement. Using EHR data for secondary purposes without consent or in some cases even with consent creates privacy issues for individuals. Secondly, EHR data is also made accessible to various stake holders including different government agencies at various geographical sites through wired or wireless networks. Sharing of EHR across multiples agencies makes it vulnerable to cyber attacks and also makes it difficult to implement strict privacy laws as in some cases data is shared with organization that is governed by specific regional law. Privacy of an individual could be severely affected when their sensitive private information contained in EHR is leaked or exposed to public. Data leak can cause financial losses or an individuals may encounter social boycott if their medical condition is exposed in public. To protect patients personal data from such threats, there exists different privacy regulations such as GDPR, HIPAA and MHR. However, continually evolving state-of-the-art techniques in machine learning, data analytics and hacking are making it even more difficult to completely protect individual's / patient's privacy. In this article, we have systematically examined various secondary uses of EHR with the aim to highlight how these secondary uses effect patients' privacy. Secondly, we have critically analyzed GDPR and highlighted possible areas of improvement, considering escalating use of technology and different secondary uses of EHR.        △ Less","26 January, 2020","cs.CY,cs.CR",10.1109/ACCESS.2020.3011099 
              CorGAN: Correlation-Capturing Convolutional Generative Adversarial Networks for Generating Synthetic Healthcare Records          ,2001.09346,https://arxiv.org/abs/2001.09346,https://arxiv.org/pdf/2001.09346,"Authors:AmirsinaTorfi,EdwardA.Fox","        Deep learning models have demonstrated high-quality performance in areas such as image classification and speech processing. However, creating a deep learning model using electronic health record (EHR) data, requires addressing particular privacy challenges that are unique to researchers in this domain. This matter focuses attention on generating realistic synthetic data while ensuring privacy. In this paper, we propose a novel framework called correlation-capturing Generative Adversarial Network (CorGAN), to generate synthetic healthcare records. In CorGAN we utilize Convolutional Neural Networks to capture the correlations between adjacent medical features in the data representation space by combining Convolutional Generative Adversarial Networks and Convolutional Autoencoders. To demonstrate the model fidelity, we show that CorGAN generates synthetic data with performance similar to that of real data in various Machine Learning settings such as classification and prediction. We also give a privacy assessment and report on statistical analysis regarding realistic characteristics of the synthetic data. The software of this work is open-source and is available at: https://github.com/astorfi/cor-gan.        △ Less","4 March, 2020","cs.LG,stat.ML",
"              iGLU 2.0: A new non-invasive, accurate serum glucometer for smart healthcare",2001.09182,https://arxiv.org/abs/2001.09182,https://arxiv.org/pdf/2001.09182,"Authors:PrateekJain,AmitMJoshi,NavneetAgrawal,SarajuMohanty","        To best of the authors knowledge, this article presents the first-ever non-invasive glucometer that takes into account serum glucose for high accuracy. In case of blood glucose measurement, serum glucose value has always been considered precise blood glucose value during prandial modes. Serum glucose can be measured in laboratory and more stable glucose level compare to capillary glucose. However, this invasive approach is not convenient for frequent measurement. Sometimes, Conventional invasive blood glucose measurement may be responsible for cause of trauma and chance of blood related infections. To overcome this issue, in the current paper, we propose a novel Internet-of-Medical (IoMT) enabled glucometer for non-invasive precise serum glucose measurement. In this work, a near-infrared (NIR) spectroscopic technique has been used for glucose measurement. The novel device called iGLU 2.0 is based on optical detection and precise machine learning (ML) regression models. The optimal multiple polynomial regression and deep neural network models have been presented to analyze the precise measurement. The glucose values of serum are saved on cloud through open IoT platform for endocrinologist at remote location. To validate iGLU 2.0, Mean Absolute Relative Difference (mARD) and Average Error (AvgE) are obtained 6.07% and 6.09%, respectively from predicted blood glucose values for capillary glucose. For serum glucose, mARD and AvgE are found 4.86% and 4.88%, respectively. These results represent that the proposed non-invasive glucose measurement device is more precise for serum glucose compared to capillary glucose.        △ Less","24 January, 2020","eess.SP,q-bio.QM",
              A Formal Development Cycle for Security Engineering in Isabelle          ,2001.08983,https://arxiv.org/abs/2001.08983,https://arxiv.org/pdf/2001.08983,Authors:FlorianKammüller,"        In this paper, we show a security engineering process based on a formal notion of refinement fully formalized in the proof assistant Isabelle. This Refinement-Risk Cycle focuses on attack analysis and security refinement supported by interactive theorem proving. Since we use a fully formalized model of infrastructures with actors and policies we can support a novel way of formal security refinement for system specifications. This formal process is built practically as an extension to the Isabelle Infrastructure framework with attack trees. We define a formal notion of refinement on infrastructure models. Thanks to the formal foundation of Kripke structures and branching time temporal logic in the Isabelle Infrastructure framework, these stepwise transformations can be interleaved with attack tree analysis thus providing a fully formal security engineering framework. The process is illustrated on an IoT healthcare case study introducing GDPR requirements and blockchain.        △ Less","4 January, 2020","cs.CR,cs.SE",
              Localization of Critical Findings in Chest X-Ray without Local Annotations Using Multi-Instance Learning          ,2001.08817,https://arxiv.org/abs/2001.08817,https://arxiv.org/pdf/2001.08817,"Authors:EvanSchwab,AndréGooßen,HrishikeshDeshpande,AxelSaalbach","        The automatic detection of critical findings in chest X-rays (CXR), such as pneumothorax, is important for assisting radiologists in their clinical workflow like triaging time-sensitive cases and screening for incidental findings. While deep learning (DL) models has become a promising predictive technology with near-human accuracy, they commonly suffer from a lack of explainability, which is an important aspect for clinical deployment of DL models in the highly regulated healthcare industry. For example, localizing critical findings in an image is useful for explaining the predictions of DL classification algorithms. While there have been a host of joint classification and localization methods for computer vision, the state-of-the-art DL models require locally annotated training data in the form of pixel level labels or bounding box coordinates. In the medical domain, this requires an expensive amount of manual annotation by medical experts for each critical finding. This requirement becomes a major barrier for training models that can rapidly scale to various findings. In this work, we address these shortcomings with an interpretable DL algorithm based on multi-instance learning that jointly classifies and localizes critical findings in CXR without the need for local annotations. We show competitive classification results on three different critical findings (pneumothorax, pneumonia, and pulmonary edema) from three different CXR datasets.        △ Less","23 January, 2020","cs.LG,cs.CV,eess.IV,stat.ML",
              Best Principal Submatrix Selection for the Maximum Entropy Sampling Problem: Scalable Algorithms and Performance Guarantees          ,2001.08537,https://arxiv.org/abs/2001.08537,https://arxiv.org/pdf/2001.08537,"Authors:YongchunLi,WeijunXie","        This paper studies a classic maximum entropy sampling problem (MESP), which aims to select the most informative principal submatrix of a prespecified size from a covariance matrix. MESP has been widely applied to many areas, including healthcare, power system, manufacturing and data science. By investigating its Lagrangian dual and primal characterization, we derive a novel convex integer program for MESP and show that its continuous relaxation yields a near-optimal solution. The results motivate us to study an efficient sampling algorithm and develop its approximation bound for MESP, which improves the best-known bound in literature. We then provide an efficient deterministic implementation of the sampling algorithm with the same approximation bound. By developing new mathematical tools for the singular matrices and analyzing the Lagrangian dual of the proposed convex integer program, we investigate the widely-used local search algorithm and prove its first-known approximation bound for MESP. The proof techniques further inspire us with an efficient implementation of the local search algorithm. Our numerical experiments demonstrate that these approximation algorithms can efficiently solve medium-sized and large-scale instances to near-optimality. Our proposed algorithms are coded and released as open-source software. Finally, we extend the analyses to the A-Optimal MESP (A-MESP), where the objective is to minimize the trace of the inverse of the selected principal submatrix.        △ Less","23 January, 2020","stat.ML,cs.LG,math.OC",
              Leveraging Blockchain for Immutable Logging and Querying Across Multiple Sites          ,2001.08529,https://arxiv.org/abs/2001.08529,https://arxiv.org/pdf/2001.08529,"Authors:MustafaSafaOzdayi,MuratKantarcioglu,BradleyMalin","        Blockchain has emerged as a decentralized and distributed framework that enables tamper-resilience and, thus, practical immutability for stored data. This immutability property is important in scenarios where auditability is desired, such as in maintaining access logs for sensitive healthcare and biomedical data.However, the underlying data structure of blockchain, by default, does not provide capabilities to efficiently query the stored data. In this investigation, we show that it is possible to efficiently run complex audit queries over the access log data stored on blockchains by using additional key-value stores. This paper specifically reports on the approach we designed for the blockchain track of iDASH Privacy & Security Workshop 2018 competition.Particularly, we implemented our solution and compared its loading and query-response performance with SQLite, a commonly used relational database, using the data provided by the iDASH 2018 organizers. Depending on the query type and the data size, the run time difference between blockchain based query-response and SQLite based query-response ranged from 0.2 seconds to 6 seconds. A deeper inspection revealed that range queries were the bottleneck of our solution which, nevertheless, scales up linearly. Concretely, this investigation demonstrates that blockchain-based systems can provide reasonable query-response times to complex queries even if they only use simple key-value stores to manage their data. Consequently, we show that blockchains may be useful for maintaining data with auditability and immutability requirements across multiple sites.        △ Less","5 March, 2020","cs.DB,cs.CR,cs.DC",
              Secure and Robust Machine Learning for Healthcare: A Survey          ,2001.08103,https://arxiv.org/abs/2001.08103,https://arxiv.org/pdf/2001.08103,"Authors:AdnanQayyum,JunaidQadir,MuhammadBilal,AlaAl-Fuqaha","        Recent years have witnessed widespread adoption of machine learning (ML)/deep learning (DL) techniques due to their superior performance for a variety of healthcare applications ranging from the prediction of cardiac arrest from one-dimensional heart signals to computer-aided diagnosis (CADx) using multi-dimensional medical images. Notwithstanding the impressive performance of ML/DL, there are still lingering doubts regarding the robustness of ML/DL in healthcare settings (which is traditionally considered quite challenging due to the myriad security and privacy issues involved), especially in light of recent results that have shown that ML/DL are vulnerable to adversarial attacks. In this paper, we present an overview of various application areas in healthcare that leverage such techniques from security and privacy point of view and present associated challenges. In addition, we present potential methods to ensure secure and privacy-preserving ML for healthcare applications. Finally, we provide insight into the current research challenges and promising directions for future research.        △ Less","21 January, 2020","cs.LG,eess.IV,stat.ML",
              PDS: Deduce Elder Privacy from Smart Homes          ,2001.08099,https://arxiv.org/abs/2001.08099,https://arxiv.org/pdf/2001.08099,"Authors:Ming-ChangLee,Jia-ChunLin,OlafOwe","        With the development of IoT technologies in the past few years, a wide range of smart devices are deployed in a variety of environments aiming to improve the quality of human life in a cost efficient way. Due to the increasingly serious aging problem around the world, smart homes for elder healthcare have become an important IoT-based application, which not only enables elders' health to be properly monitored and taken care of, but also allows them to live more comfortably and independently in their houses. However, elders' privacy might be disclosed from smart homes due to non-fully protected network communication. To show that elders' privacy could be substantially exposed, in this paper we develop a Privacy Deduction Scheme (PDS for short) by eavesdropping sensor traffic from a smart home to identify elders' movement activities and speculating sensor locations in the smart home based on a series of deductions from the viewpoint of an attacker. The experimental results based on sensor datasets from real smart homes demonstrate the effectiveness of PDS in deducing and disclosing elders' privacy, which might be maliciously exploited by attackers to endanger elders and their properties.        △ Less","21 January, 2020","cs.CR,cs.LG",10.1016/j.iot.2019.100072 
              Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning          ,2001.07769,https://arxiv.org/abs/2001.07769,https://arxiv.org/pdf/2001.07769,"Authors:NilakshDas,HaekyuPark,ZijieJ.Wang,FredHohman,RobertFirstman,EmilyRogers,DuenHorngChau","        Deep neural networks (DNNs) are increasingly powering high-stakes applications such as autonomous cars and healthcare; however, DNNs are often treated as ""black boxes"" in such applications. Recent research has also revealed that DNNs are highly vulnerable to adversarial attacks, raising serious concerns over deploying DNNs in the real world. To overcome these deficiencies, we are developing Massif, an interactive tool for deciphering adversarial attacks. Massif identifies and interactively visualizes neurons and their connections inside a DNN that are strongly activated or suppressed by an adversarial attack. Massif provides both a high-level, interpretable overview of the effect of an attack on a DNN, and a low-level, detailed description of the affected neurons. These tightly coupled views in Massif help people better understand which input features are most vulnerable or important for correct predictions.        △ Less","16 February, 2020","cs.LG,cs.CR,stat.ML",
              Generalization Bounds and Representation Learning for Estimation of Potential Outcomes and Causal Effects          ,2001.07426,https://arxiv.org/abs/2001.07426,https://arxiv.org/pdf/2001.07426,"Authors:FredrikD.Johansson,UriShalit,NathanKallus,DavidSontag","        Practitioners in diverse fields such as healthcare, economics and education are eager to apply machine learning to improve decision making. The cost and impracticality of performing experiments and a recent monumental increase in electronic record keeping has brought attention to the problem of evaluating decisions based on non-experimental observational data. This is the setting of this work. In particular, we study estimation of individual-level causal effects, such as a single patient's response to alternative medication, from recorded contexts, decisions and outcomes. We give generalization bounds on the error in estimated effects based on distance measures between groups receiving different treatments, allowing for sample re-weighting. We provide conditions under which our bound is tight and show how it relates to results for unsupervised domain adaptation. Led by our theoretical results, we devise representation learning algorithms that minimize our bound, by regularizing the representation's induced treatment group distance, and encourage sharing of information between treatment groups. We extend these algorithms to simultaneously learn a weighted representation to further reduce treatment group distances. Finally, an experimental evaluation on real and synthetic data shows the value of our proposed representation architecture and regularization scheme.        △ Less","21 January, 2020","cs.LG,stat.ML",
              Ranking Significant Discrepancies in Clinical Reports          ,2001.06674,https://arxiv.org/abs/2001.06674,https://arxiv.org/pdf/2001.06674,"Authors:SeanMacAvaney,ArmanCohan,NazliGoharian,RossFilice","        Medical errors are a major public health concern and a leading cause of death worldwide. Many healthcare centers and hospitals use reporting systems where medical practitioners write a preliminary medical report and the report is later reviewed, revised, and finalized by a more experienced physician. The revisions range from stylistic to corrections of critical errors or misinterpretations of the case. Due to the large quantity of reports written daily, it is often difficult to manually and thoroughly review all the finalized reports to find such errors and learn from them. To address this challenge, we propose a novel ranking approach, consisting of textual and ontological overlaps between the preliminary and final versions of reports. The approach learns to rank the reports based on the degree of discrepancy between the versions. This allows medical practitioners to easily identify and learn from the reports in which their interpretation most substantially differed from that of the attending physician (who finalized the report). This is a crucial step towards uncovering potential errors and helping medical practitioners to learn from such errors, thus improving patient-care in the long run. We evaluate our model on a dataset of radiology reports and show that our approach outperforms both previously-proposed approaches and more recent language models by 4.5% to 15.4%.        △ Less","18 January, 2020","cs.IR,cs.CL",10.1007/978-3-030-45442-5_30 
              An adversarial learning framework for preserving users' anonymity in face-based emotion recognition          ,2001.06103,https://arxiv.org/abs/2001.06103,https://arxiv.org/pdf/2001.06103,"Authors:VanshNarula,Zhangyang,Wang,TheodoraChaspari","        Image and video-capturing technologies have permeated our every-day life. Such technologies can continuously monitor individuals' expressions in real-life settings, affording us new insights into their emotional states and transitions, thus paving the way to novel well-being and healthcare applications. Yet, due to the strong privacy concerns, the use of such technologies is met with strong skepticism, since current face-based emotion recognition systems relying on deep learning techniques tend to preserve substantial information related to the identity of the user, apart from the emotion-specific information. This paper proposes an adversarial learning framework which relies on a convolutional neural network (CNN) architecture trained through an iterative procedure for minimizing identity-specific information and maximizing emotion-dependent information. The proposed approach is evaluated through emotion classification and face identification metrics, and is compared against two CNNs, one trained solely for emotion recognition and the other trained solely for face identification. Experiments are performed using the Yale Face Dataset and Japanese Female Facial Expression Database. Results indicate that the proposed approach can learn a convolutional transformation for preserving emotion recognition accuracy and degrading face identity recognition, providing a foundation toward privacy-aware emotion recognition technologies.        △ Less","16 January, 2020","cs.LG,cs.CV,stat.ML",
              Average Waiting Times in the Two-Class M/G/1 Delayed Accumulating Priority Queue          ,2001.06054,https://arxiv.org/abs/2001.06054,https://arxiv.org/pdf/2001.06054,"Authors:BlairBilodeau,DavidA.Stanford","        Previously, Mojalal et al. (2019) gave an expression for the waiting time distribution of low priority customers in the Delayed Accumulating Priority Queue, but with no quantification of the effect on others in system. We provide an analytical expression for the expected waiting time of both high and low priority customers by exploiting a conservation law for work conserving queues. Our expression can be efficiently implemented numerically, requiring only the truncation of sums which converge exponentially quickly. This enables us to use common key performance indicators to demonstrate how the accumulation rate and delay level should be chosen by health care practitioners.        △ Less","20 January, 2020","math.PR,math.OC",
              Modelling pathogen spread in a healthcare network: indirect patient movements          ,2001.05875,https://arxiv.org/abs/2001.05875,https://arxiv.org/pdf/2001.05875,"Authors:M.J.Piotrowska,K.Sakowski,A.Karch,H.Tahir,J.Horn,M.E.Kretzschmar,R.T.Mikolajczyk","        A hybrid network--deterministic model for simulation of multiresistant pathogen spread in a healthcare system is presented. The model accounts for two paths of pathogen transmission between the healthcare facilities: inter-hospital patient transfers (direct transfers) and readmission of colonized patients (indirect transfers). In the latter case, the patients may be readmitted to the same facility or to a different one. Intra-hospital pathogen transmission is governed by a SIS model expressed by a system of ordinary differential equations.  Using a network model created for a Lower Saxony region (Germany), we showed that the proposed model reproduces the basic properties of healthcare-associated pathogen spread. Moreover, it shows the important contribution of the readmission of colonized patients on the prevalence of individual hospitals as well as of overall healthcare system: it can increase the overall prevalence by the factor of 4 as compared to inter-hospital transfers only. The final prevalence in individual healthcare facilities was shown to depend on average length of stay by a non-linear concave function.  Finally, we demonstrated that the network parameters of the model may be derived from administrative admission/discharge records. In particular, they are sufficient to obtain inter-hospital transfer probabilities, and to express the patients' transfer as a Markov process.        △ Less","15 January, 2020","q-bio.PE,math.NA",
              A Technology-aided Multi-modal Training Approach to Assist Abdominal Palpation Training and its Assessment in Medical Education          ,2001.05745,https://arxiv.org/abs/2001.05745,https://arxiv.org/pdf/2001.05745,"Authors:A.Asadipour,K.Debattista,V.Patel,A.Chalmers","        Computer-assisted multimodal training is an effective way of learning complex motor skills in various applications. In particular disciplines (eg. healthcare) incompetency in performing dexterous hands-on examinations (clinical palpation) may result in misdiagnosis of symptoms, serious injuries or even death. Furthermore, a high quality clinical examination can help to exclude significant pathology, and reduce time and cost of diagnosis by eliminating the need for unnecessary medical imaging. Medical palpation is used regularly as an effective preliminary diagnosis method all around the world but years of training are required currently to achieve competency. This paper focuses on a multimodal palpation training system to teach and improve clinical examination skills in relation to the abdomen. It is our aim to shorten significantly the palpation training duration by increasing the frequency of rehearsals as well as providing essential augmented feedback on how to perform various abdominal palpation techniques which has been captured and modelled from medical experts. Twenty three first year medical students divided into a control group (n=8), a semi-visually trained group (n=8), and a fully visually trained group (n=7) were invited to perform three palpation tasks (superficial, deep and liver). The medical students performances were assessed using both computer-based and human-based methods where a positive correlation was shown between the generated scores, r=.62, p(one-tailed)<.05. The visually-trained group significantly outperformed the control group in which abstract visualisation of applied forces and their palmar locations were provided to the students during each palpation examination (p<.05). Moreover, a positive trend was observed between groups when visual feedback was presented, J=132, z=2.62, r=0.55.        △ Less","16 January, 2020","cs.HC,cs.CV,eess.IV",10.1016/j.ijhcs.2020.102394 
"              Comparing Rule-based, Feature-based and Deep Neural Methods for De-identification of Dutch Medical Records          ",2001.05714,https://arxiv.org/abs/2001.05714,https://arxiv.org/pdf/2001.05714,"Authors:JanTrienes,DolfTrieschnigg,ChristinSeifert,DjoerdHiemstra","        Unstructured information in electronic health records provide an invaluable resource for medical research. To protect the confidentiality of patients and to conform to privacy regulations, de-identification methods automatically remove personally identifying information from these medical records. However, due to the unavailability of labeled data, most existing research is constrained to English medical text and little is known about the generalizability of de-identification methods across languages and domains. In this study, we construct a varied dataset consisting of the medical records of 1260 patients by sampling data from 9 institutes and three domains of Dutch healthcare. We test the generalizability of three de-identification methods across languages and domains. Our experiments show that an existing rule-based method specifically developed for the Dutch language fails to generalize to this new data. Furthermore, a state-of-the-art neural architecture performs strongly across languages and domains, even with limited training data. Compared to feature-based and rule-based methods the neural method requires significantly less configuration effort and domain-knowledge. We make all code and pre-trained de-identification models available to the research community, allowing practitioners to apply them to their datasets and to enable future benchmarks.        △ Less","16 January, 2020",cs.CL,
              Scaling Blockchains to Support Electronic Health Records for Hospital Systems          ,2001.05525,https://arxiv.org/abs/2001.05525,https://arxiv.org/pdf/2001.05525,"Authors:AlyssaDonawa,InemaOrukari,CoreyE.Baker","        Electronic Health Records (EHRs) have improved many aspects of healthcare and allowed for easier patient management for medical providers. Blockchains have been proposed as a promising solution for supporting Electronic Health Records (EHRs), but have also been linked to scalability concerns about supporting real-world healthcare systems. This paper quantifies the scalability issues and bottlenecks related to current blockchains and puts into perspective the limitations blockchains have with supporting healthcare systems. Particularly we show that well known blockchains such as Bitcoin, Ethereum, and IOTA cannot support transactions of a large scale hospital system such as the University of Kentucky HealthCare system and leave over 7.5M unsealed transactions per day. We then discuss how bottlenecks of blockchains can be relieved with sidechains, enabling well-known blockchains to support even larger hospital systems of over 30M transactions per day. We then introduce the Patient-Healthchain architecture to provide future direction on how scaling blockchains for EHR systems with sidechains can be achieved.        △ Less","24 February, 2020",cs.CR,
              Autoencoders as Weight Initialization of Deep Classification Networks for Cancer versus Cancer Studies          ,2001.05253,https://arxiv.org/abs/2001.05253,https://arxiv.org/pdf/2001.05253,"Authors:MafaldaFalcaoFerreira,RuiCamacho,LuisF.Teixeira","        Cancer is still one of the most devastating diseases of our time. One way of automatically classifying tumor samples is by analyzing its derived molecular information (i.e., its genes expression signatures). In this work, we aim to distinguish three different types of cancer: thyroid, skin, and stomach. For that, we compare the performance of a Denoising Autoencoder (DAE) used as weight initialization of a deep neural network. Although we address a different domain problem in this work, we have adopted the same methodology of Ferreira et al.. In our experiments, we assess two different approaches when training the classification model: (a) fixing the weights, after pre-training the DAE, and (b) allowing fine-tuning of the entire classification network. Additionally, we apply two different strategies for embedding the DAE into the classification network: (1) by only importing the encoding layers, and (2) by inserting the complete autoencoder. Our best result was the combination of unsupervised feature learning through a DAE, followed by its full import into the classification network, and subsequent fine-tuning through supervised training, achieving an F1 score of 98.04% +/- 1.09 when identifying cancerous thyroid samples.        △ Less","15 January, 2020","cs.LG,stat.ML",
              POPCORN: Partially Observed Prediction COnstrained ReiNforcement Learning          ,2001.04032,https://arxiv.org/abs/2001.04032,https://arxiv.org/pdf/2001.04032,"Authors:JosephFutoma,MichaelC.Hughes,FinaleDoshi-Velez","        Many medical decision-making tasks can be framed as partially observed Markov decision processes (POMDPs). However, prevailing two-stage approaches that first learn a POMDP and then solve it often fail because the model that best fits the data may not be well suited for planning. We introduce a new optimization objective that (a) produces both high-performing policies and high-quality generative models, even when some observations are irrelevant for planning, and (b) does so in batch off-policy settings that are typical in healthcare, when only retrospective data is available. We demonstrate our approach on synthetic examples and a challenging medical decision-making problem.        △ Less","31 March, 2020","stat.ML,cs.LG",
              Bridging the gap between AI and Healthcare sides: towards developing clinically relevant AI-powered diagnosis systems          ,2001.03923,https://arxiv.org/abs/2001.03923,https://arxiv.org/pdf/2001.03923,"Authors:ChangheeHan,LeonardoRundo,KoheiMurao,TakafumiNemoto,HidekiNakayama","        Despite the success of Convolutional Neural Network-based Computer-Aided Diagnosis research, its clinical applications remain challenging. Accordingly, developing medical Artificial Intelligence (AI) fitting into a clinical environment requires identifying/bridging the gap between AI and Healthcare sides. Since the biggest problem in Medical Imaging lies in data paucity, confirming the clinical relevance for diagnosis of research-proven image augmentation techniques is essential. Therefore, we hold a clinically valuable AI-envisioning workshop among Japanese Medical Imaging experts, physicians, and generalists in Healthcare/Informatics. Then, a questionnaire survey for physicians evaluates our pathology-aware Generative Adversarial Network (GAN)-based image augmentation projects in terms of Data Augmentation and physician training. The workshop reveals the intrinsic gap between AI/Healthcare sides and solutions on Why (i.e., clinical significance/interpretation) and How (i.e., data acquisition, commercial deployment, and safety/feeling safe). This analysis confirms our pathology-aware GANs' clinical relevance as a clinical decision support system and non-expert physician training tool. Our findings would play a key role in connecting inter-disciplinary research and clinical applications, not limited to the Japanese medical context and pathology-aware GANs.        △ Less","6 April, 2020","cs.CV,cs.LG,eess.IV",
              Reward Engineering for Object Pick and Place Training          ,2001.03792,https://arxiv.org/abs/2001.03792,https://arxiv.org/pdf/2001.03792,"Authors:RaghavNagpal,AchyuthanUnniKrishnan,HanshenYu","        Robotic grasping is a crucial area of research as it can result in the acceleration of the automation of several Industries utilizing robots ranging from manufacturing to healthcare. Reinforcement learning is the field of study where an agent learns a policy to execute an action by exploring and exploiting rewards from an environment. Reinforcement learning can thus be used by the agent to learn how to execute a certain task, in our case grasping an object. We have used the Pick and Place environment provided by OpenAI's Gym to engineer rewards. Hindsight Experience Replay (HER) has shown promising results with problems having a sparse reward. In the default configuration of the OpenAI baseline and environment the reward function is calculated using the distance between the target location and the robot end-effector. By weighting the cost based on the distance of the end-effector from the goal in the x,y and z-axes we were able to almost halve the learning time compared to the baselines provided by OpenAI, an intuitive strategy that further reduced learning time. In this project, we were also able to introduce certain user desired trajectories in the learnt policies (city-block / Manhattan trajectories). This helps us understand that by engineering the rewards we can tune the agent to learn policies in a certain way even if it might not be the most optimal but is the desired manner.        △ Less","11 January, 2020","cs.AI,cs.RO",
              Optimizing Investments in Cyber Hygiene for Protecting Healthcare Users          ,2001.03782,https://arxiv.org/abs/2001.03782,https://arxiv.org/pdf/2001.03782,"Authors:SakshyamPanda,EmmanouilPanaousis,GeorgeLoukas,ChristosLaoudias","        Cyber hygiene measures are often recommended for strengthening an organization's security posture, especially for protecting against social engineering attacks that target the human element. However, the related recommendations are typically the same for all organizations and their employees, regardless of the nature and the level of risk for different groups of users. Building upon an existing cybersecurity investment model, this paper presents a tool for optimal selection of cyber hygiene safeguards, which we refer as the Optimal Safeguards Tool. The model combines game theory and combinatorial optimization taking into account the probability of each user group to being attacked, the value of assets accessible by each group, and the efficacy of each control for a particular group. The model considers indirect cost as the time employees could require for learning and training against an implemented control. Utilizing a game-theoretic framework to support the Knapsack optimization problem permits us to optimally select safeguards' application levels minimizing the aggregated expected damage within a security investment budget. We evaluate OST in a healthcare domain use case. The Critical Internet Security Control group 17 for implementing security awareness and training programs for employees belonging to the ICT, clinical and administration personnel of a hospital. We compare the strategies implemented by OST against alternative common-sense defending approaches for three different types of attackers: Nash, Weighted and Opportunistic. Nash defending strategies are consistently better than the competing strategies for all attacker types with a minor exception where the Nash defending strategy performs at least as good as other common-sense approaches.        △ Less","11 January, 2020",cs.CR,
              Convolutional-Recurrent Neural Networks on Low-Power Wearable Platforms for Cardiac Arrhythmia Detection          ,2001.03538,https://arxiv.org/abs/2001.03538,https://arxiv.org/pdf/2001.03538,"Authors:AntoninoFaraone,RicardDelgado-Gonzalo","        Low-power sensing technologies, such as wearables, have emerged in the healthcare domain since they enable continuous and non-invasive monitoring of physiological signals. In order to endow such devices with clinical value, classical signal processing has encountered numerous challenges. However, data-driven methods, such as machine learning, offer attractive accuracies at the expense of being resource and memory demanding. In this paper, we focus on the inference of neural networks running in microcontrollers and low-power processors which wearable sensors and devices are generally equipped with. In particular, we adapted an existing convolutional-recurrent neural network, designed to detect and classify cardiac arrhythmias from a single-lead electrocardiogram, to the low-power embedded System-on-Chip nRF52 from Nordic Semiconductor with an ARM's Cortex-M4 processing core. We show our implementation in fixed-point precision, using the CMSIS-NN libraries, yields a drop of F1F_1 score from 0.8 to 0.784, from the original implementation, with a memory footprint of 195.6KB, and a throughput of 33.98MOps/s.        △ Less","8 January, 2020","eess.SP,cs.LG,stat.ML",10.1109/AICAS48895.2020.9073950 
              Compressive sensing based privacy for fall detection          ,2001.03463,https://arxiv.org/abs/2001.03463,https://arxiv.org/pdf/2001.03463,"Authors:RonakGupta,PrashantAnand,SantanuChaudhury,BrejeshLall,SanjaySingh","        Fall detection holds immense importance in the field of healthcare, where timely detection allows for instant medical assistance. In this context, we propose a 3D ConvNet architecture which consists of 3D Inception modules for fall detection. The proposed architecture is a custom version of Inflated 3D (I3D) architecture, that takes compressed measurements of video sequence as spatio-temporal input, obtained from compressive sensing framework, rather than video sequence as input, as in the case of I3D convolutional neural network. This is adopted since privacy raises a huge concern for patients being monitored through these RGB cameras. The proposed framework for fall detection is flexible enough with respect to a wide variety of measurement matrices. Ten action classes randomly selected from Kinetics-400 with no fall examples, are employed to train our 3D ConvNet post compressive sensing with different types of sensing matrices on the original video clips. Our results show that 3D ConvNet performance remains unchanged with different sensing matrices. Also, the performance obtained with Kinetics pre-trained 3D ConvNet on compressively sensed fall videos from benchmark datasets is better than the state-of-the-art techniques.        △ Less","10 January, 2020",cs.CV,
              Trace Clustering on Very Large Event Data in Healthcare Using Frequent Sequence Patterns          ,2001.03411,https://arxiv.org/abs/2001.03411,https://arxiv.org/pdf/2001.03411,"Authors:XixiLu,SeyedAminTabatabaei,MarkHoogendoorn,HajoA.Reijers","        Trace clustering has increasingly been applied to find homogenous process executions. However, current techniques have difficulties in finding a meaningful and insightful clustering of patients on the basis of healthcare data. The resulting clusters are often not in line with those of medical experts, nor do the clusters guarantee to help return meaningful process maps of patients' clinical pathways. After all, a single hospital may conduct thousands of distinct activities and generate millions of events per year. In this paper, we propose a novel trace clustering approach by using sample sets of patients provided by medical experts. More specifically, we learn frequent sequence patterns on a sample set, rank each patient based on the patterns, and use an automated approach to determine the corresponding cluster. We find each cluster separately, while the frequent sequence patterns are used to discover a process map. The approach is implemented in ProM and evaluated using a large data set obtained from a university medical center. The evaluation shows F1-scores of 0.7 for grouping kidney injury, 0.9 for diabetes, and 0.64 for head/neck tumor, while the process maps show meaningful behavioral patterns of the clinical pathways of these groups, according to the domain experts.        △ Less","10 January, 2020",cs.DB,10.1007/978-3-030-26619-6_14 
              Emo-CNN for Perceiving Stress from Audio Signals: A Brain Chemistry Approach          ,2001.02329,https://arxiv.org/abs/2001.02329,https://arxiv.org/pdf/2001.02329,"Authors:AnupAnandDeshmukh,CatherineSoladie,RenaudSeguier","        Emotion plays a key role in many applications like healthcare, to gather patients emotional behavior. There are certain emotions which are given more importance due to their effectiveness in understanding human feelings. In this paper, we propose an approach that models human stress from audio signals. The research challenge in speech emotion detection is defining the very meaning of stress and being able to categorize it in a precise manner. Supervised Machine Learning models, including state of the art Deep Learning classification methods, rely on the availability of clean and labelled data. One of the problems in affective computation and emotion detection is the limited amount of annotated data of stress. The existing labelled stress emotion datasets are highly subjective to the perception of the annotator.  We address the first issue of feature selection by exploiting the use of traditional MFCC features in Convolutional Neural Network. Our experiments show that Emo-CNN consistently and significantly outperforms the popular existing methods over multiple datasets. It achieves 90.2% categorical accuracy on the Emo-DB dataset. To tackle the second and the more significant problem of subjectivity in stress labels, we use Lovheim's cube, which is a 3-dimensional projection of emotions. The cube aims at explaining the relationship between these neurotransmitters and the positions of emotions in 3D space. The learnt emotion representations from the Emo-CNN are mapped to the cube using three component PCA (Principal Component Analysis) which is then used to model human stress. This proposed approach not only circumvents the need for labelled stress data but also complies with the psychological theory of emotions given by Lovheim's cube. We believe that this work is the first step towards creating a connection between Artificial Intelligence and the chemistry of human emotions.        △ Less","7 January, 2020","cs.HC,cs.AI,cs.SD,eess.AS",
              Prediction of MRI Hardware Failures based on Image Features using Time Series Classification          ,2001.02127,https://arxiv.org/abs/2001.02127,https://arxiv.org/pdf/2001.02127,"Authors:NadineKuhnert,LeaPflüger,AndreasMaier","        Already before systems malfunction one has to know if hardware components will fail in near future in order to counteract in time. Thus, unplanned downtime is ought to be avoided. In medical imaging, maximizing the system's uptime is crucial for patients' health and healthcare provider's daily business. We aim to predict failures of Head/Neck coils used in Magnetic Resonance Imaging (MRI) by training a statistical model on sequential data collected over time. As image features depend on the coil's condition, their deviations from the normal range already hint to future failure. Thus, we used image features and their variation over time to predict coil damage. After comparison of different time series classification methods we found Long Short Term Memorys (LSTMs) to achieve the highest F-score of 86.43% and to tell with 98.33% accuracy if hardware should be replaced.        △ Less","5 January, 2020","cs.LG,eess.IV,stat.ML",
              State Transition Modeling of the Smoking Behavior using LSTM Recurrent Neural Networks          ,2001.02101,https://arxiv.org/abs/2001.02101,https://arxiv.org/pdf/2001.02101,"Authors:ChrisogonasO.Odhiambo,CaseyA.Cole,AlalehTorkjazi,HomayounValafar","        The use of sensors has pervaded everyday life in several applications including human activity monitoring, healthcare, and social networks. In this study, we focus on the use of smartwatch sensors to recognize smoking activity. More specifically, we have reformulated the previous work in detection of smoking to include in-context recognition of smoking. Our presented reformulation of the smoking gesture as a state-transition model that consists of the mini-gestures hand-to-lip, hand-on-lip, and hand-off-lip, has demonstrated improvement in detection rates nearing 100% using conventional neural networks. In addition, we have begun the utilization of Long-Short-Term Memory (LSTM) neural networks to allow for in-context detection of gestures with accuracy nearing 97%.        △ Less","7 January, 2020","cs.CV,cs.LG",
              Effective Scaling of Blockchain Beyond Consensus Innovations and Moore's Law          ,2001.01865,https://arxiv.org/abs/2001.01865,https://arxiv.org/pdf/2001.01865,"Authors:YinqiuLiu,KaiQian,JianliChen,KunWang,LeiHe","        As an emerging technology, blockchain has achieved great success in numerous application scenarios, from intelligent healthcare to smart cities. However, a long-standing bottleneck hindering its further development is the massive resource consumption attributed to the distributed storage and computation methods. This makes blockchain suffer from insufficient performance and poor scalability. Here, we analyze the recent blockchain techniques and demonstrate that the potential of widely-adopted consensus-based scaling is seriously limited, especially in the current era when Moore's law-based hardware scaling is about to end. We achieve this by developing an open-source benchmarking tool, called Prism, for investigating the key factors causing low resource efficiency and then discuss various topology and hardware innovations which could help to scale up blockchain. To the best of our knowledge, this is the first in-depth study that explores the next-generation scaling strategies by conducting large-scale and comprehensive benchmarking.        △ Less","20 April, 2020","cs.CR,cs.DC",
              PhD Forum: Enabling Autonomic IoT for Smart Urban Services          ,2001.01561,https://arxiv.org/abs/2001.01561,https://arxiv.org/pdf/2001.01561,"Authors:MuhammadJunaidFarooq,QuanyanZhu","        The development of autonomous cyber-physical systems (CPS) and advances towards the fifth generation (5G) of wireless technology is promising to revolutionize many industry verticals such as Healthcare, Transportation, Energy, Retail Services, Building Automation, Education, etc., leading to the realization of the smart city paradigm. The Internet of Things (IoT), enables powerful and unprecedented capabilities for intelligent and autonomous operation. We leverage ideas from Network Science, Optimization & Decision Theory, Incentive Mechanism Design, and Data Science/Machine Learning to achieve key design goals, in IoT-enabled urban systems, such as efficiency, security & resilience, and economics.        △ Less","24 December, 2019","eess.SP,cs.NI",
              Opportunities and Challenges of Deep Learning Methods for Electrocardiogram Data: A Systematic Review          ,2001.01550,https://arxiv.org/abs/2001.01550,https://arxiv.org/pdf/2001.01550,"Authors:ShendaHong,YuxiZhou,JunyuanShang,CaoXiao,JimengSun","        Background:The electrocardiogram (ECG) is one of the most commonly used diagnostic tools in medicine and healthcare. Deep learning methods have achieved promising results on predictive healthcare tasks using ECG signals. Objective:This paper presents a systematic review of deep learning methods for ECG data from both modeling and application perspectives. Methods:We extracted papers that applied deep learning (deep neural network) models to ECG data that were published between Jan. 1st of 2010 and Feb. 29th of 2020 from Google Scholar, PubMed, and the DBLP. We then analyzed each article according to three factors: tasks, models, and data. Finally, we discuss open challenges and unsolved problems in this area. Results: The total number of papers extracted was 191. Among these papers, 108 were published after 2019. Different deep learning architectures have been used in various ECG analytics tasks, such as disease detection/classification, annotation/localization, sleep staging, biometric human identification, and denoising. Conclusion: The number of works on deep learning for ECG data has grown explosively in recent years. Such works have achieved accuracy comparable to that of traditional feature-based approaches and ensembles of multiple approaches can achieve even better results. Specifically, we found that a hybrid architecture of a convolutional neural network and recurrent neural network ensemble using expert features yields the best results. However, there are some new challenges and problems related to interpretability, scalability, and efficiency that must be addressed. Furthermore, it is also worth investigating new applications from the perspectives of datasets and methods. Significance: This paper summarizes existing deep learning research using ECG data from multiple perspectives and highlights existing challenges and problems to identify potential future research directions.        △ Less","30 April, 2020","eess.SP,cs.CV,cs.LG",
"              Big Data Architecture in Czech Republic Healthcare Service: Requirements, TPC-H Benchmarks and Vertica          ",2001.01192,https://arxiv.org/abs/2001.01192,https://arxiv.org/pdf/2001.01192,"Authors:MartinŠtufi,BorisBačić,LeonidStoimenov","        Big data in healthcare has made a positive difference in advancing analytical capabilities and lowering the costs of medical care. In addition to providing analytical capabilities on platforms supporting current and near-future AI with machine-learning and data-mining algorithms, there is also a need for ethical considerations mandating new ways to preserve privacy, all of which are preconditioned by the growing body of regulations and expectations. The purpose of this study is to improve existing clinical care by implementing a big data platform for the Czech Republic National Health Service. Based on the achieved performance and its compliance with mandatory guidelines, the reported big-data platform was selected as the winning solution from the Czech Republic national tender (Tender Id. VZ0036628, No. Z2017-035520). The platform, based on analytical Vertica NoSQL database for massive data processing, complies with the TPC-H1 for decision support benchmark, the European Union (EU) and the Czech Republic requirements, well-exceeding defined system performance thresholds. The reported artefacts and concepts are transferrable to healthcare systems in other countries and are intended to provide personalised autonomous assessment from big data in a cost-effective, scalable and high-performance manner. The implemented platform allows: (1) scalability; (2) further implementations of newly-developed machine learning algorithms for classification and predictive analytics; (3) security improvements related to Electronic Health Records (EHR) by using automated functions for data encryption and decryption; and (4) the use of big data to allow strategic planning in healthcare.        △ Less","5 January, 2020",cs.DC,
              Privacy in Data Service Composition          ,2001.00975,https://arxiv.org/abs/2001.00975,https://arxiv.org/pdf/2001.00975,"Authors:MahmoudBarhamgi,CharithPerera,Chia-MuYu,DjamalBenslimane,DavidCamacho,ChristineBonnet","        In modern information systems different information features, about the same individual, are often collected and managed by autonomous data collection services that may have different privacy policies. Answering many end-users' legitimate queries requires the integration of data from multiple such services. However, data integration is often hindered by the lack of a trusted entity, often called a mediator, with which the services can share their data and delegate the enforcement of their privacy policies. In this paper, we propose a flexible privacy-preserving data integration approach for answering data integration queries without the need for a trusted mediator. In our approach, services are allowed to enforce their privacy policies locally. The mediator is considered to be untrusted, and only has access to encrypted information to allow it to link data subjects across the different services. Services, by virtue of a new privacy requirement, dubbed k-Protection, limiting privacy leaks, cannot infer information about the data held by each other. End-users, in turn, have access to privacy-sanitized data only. We evaluated our approach using an example and a real dataset from the healthcare application domain. The results are promising from both the privacy preservation and the performance perspectives.        △ Less","3 January, 2020","cs.DB,cs.CR,cs.DC",10.1109/TSC.2019.2963309 
              A Parallel Sparse Tensor Benchmark Suite on CPUs and GPUs          ,2001.00660,https://arxiv.org/abs/2001.00660,https://arxiv.org/pdf/2001.00660,"Authors:JiajiaLi,MaheshLakshminarasimhan,XiaolongWu,AngLi,CatherineOlschanowsky,KevinBarker","        Tensor computations present significant performance challenges that impact a wide spectrum of applications ranging from machine learning, healthcare analytics, social network analysis, data mining to quantum chemistry and signal processing. Efforts to improve the performance of tensor computations include exploring data layout, execution scheduling, and parallelism in common tensor kernels. This work presents a benchmark suite for arbitrary-order sparse tensor kernels using state-of-the-art tensor formats: coordinate (COO) and hierarchical coordinate (HiCOO) on CPUs and GPUs. It presents a set of reference tensor kernel implementations that are compatible with real-world tensors and power law tensors extended from synthetic graph generation techniques. We also propose Roofline performance models for these kernels to provide insights of computer platforms from sparse tensor view.        △ Less","2 January, 2020","cs.DC,cs.PF",
              A Voice Interactive Multilingual Student Support System using IBM Watson          ,2001.00471,https://arxiv.org/abs/2001.00471,https://arxiv.org/pdf/2001.00471,"Authors:KennedyRalston,YuhaoChen,HarunaIsah,FarhanaZulkernine","        Systems powered by artificial intelligence are being developed to be more user-friendly by communicating with users in a progressively human-like conversational way. Chatbots, also known as dialogue systems, interactive conversational agents, or virtual agents are an example of such systems used in a wide variety of applications ranging from customer support in the business domain to companionship in the healthcare sector. It is becoming increasingly important to develop chatbots that can best respond to the personalized needs of their users so that they can be as helpful to the user as possible in a real human way. This paper investigates and compares three popular existing chatbots API offerings and then propose and develop a voice interactive and multilingual chatbot that can effectively respond to users mood, tone, and language using IBM Watson Assistant, Tone Analyzer, and Language Translator. The chatbot was evaluated using a use case that was targeted at responding to users needs regarding exam stress based on university students survey data generated using Google Forms. The results of measuring the chatbot effectiveness at analyzing responses regarding exam stress indicate that the chatbot responding appropriately to the user queries regarding how they are feeling about exams 76.5%. The chatbot could also be adapted for use in other application areas such as student info-centers, government kiosks, and mental health support systems.        △ Less","20 December, 2019","cs.HC,cs.AI,cs.CL,cs.IR",
"              ""Happy and Assured that life will be easy 10years from now."": Perceptions of Artificial Intelligence in 8 Countries          ",2001.00081,https://arxiv.org/abs/2001.00081,https://arxiv.org/pdf/2001.00081,"Authors:PatrickGageKelley,YongweiYang,CourtneyHeldreth,ChristopherMoessner,AaronSedley,AndreasKramm,DavidNewman,AllisonWoodruff","        As the influence and use of artificial intelligence (AI) have grown and its transformative potential has become more apparent, many questions have been raised regarding the economic, political, and social implications of its use. Public opinion plays an important role in these discussions, influencing product adoption, commercial development, research funding, and regulation. In this paper we present results of a survey of public opinion of artificial intelligence conducted with 10,005 respondents spanning eight countries and six continents. We report widespread perception that AI will have significant impact on society, and highlight public optimism about applications for healthcare as well as public concerns about privacy and job loss. We also identify four key sentiments (exciting, useful, worrying, and futuristic) whose prevalence distinguishes response to AI in different countries.        △ Less","27 December, 2019",cs.CY,
              User Acceptance of Usable Blockchain-Based Research Data Sharing System: An Extended TAM Based Study          ,2001.00079,https://arxiv.org/abs/2001.00079,https://arxiv.org/pdf/2001.00079,"Authors:AjayKumarShrestha,JulitaVassileva","        Blockchain technology has evolved as a promising means to transform data management models in many domains including healthcare, agricultural research, tourism domains etc. In the research community, a usable blockchain-based system can allow users to create a proof of ownership and provenance of the research work, share research data without losing control and ownership of it, provide incentives for sharing and give users full transparency and control over who access their data, when and for what purpose. The initial adoption of such blockchain-based systems is necessary for continued use of the services, but their user acceptance behavioral model has not been well investigated in the literature. In this paper, we take the Technology Acceptance Model (TAM) as a foundation and extend the external constructs to uncover how the perceived ease of use, perceived usability, quality of the system and perceived enjoyment influence the intention to use the blockchain-based system. We based our study on user evaluation of a prototype of a blockchain-based research data sharing framework using a TAM validated questionnaire. Our results show that, overall, all the individual constructs of the behavior model significantly influence the intention to use the system while their collective effect is found to be insignificant. The quality of the system and the perceived enjoyment have stronger influence on the perceived usefulness. However, the effect of perceived ease of use on the perceived usefulness is not supported. Finally, we discuss the implications of our findings.        △ Less","14 December, 2019","cs.CY,cs.CR,cs.GT",
"              I'm a doctor, not a mathematician! Homeostasis as a proportional-integral control system          ",1912.13148,https://arxiv.org/abs/1912.13148,https://arxiv.org/pdf/1912.13148,"Authors:LennaertvanVeen,JacobMorra,AdamPalanica,YanFossat","        The distinction between ""healthy"" and ""unhealthy"" patients is commonly based on single, discrete values taken at an isolated point in time (e.g., blood pressure or core temperature). Perhaps a more robust and insightful diagnosis can be obtained by studying the functional interdependence of such indicators and the homeostasis that controls them. This requires quasi-continuous measurements and a procedure to map the data onto a parsimonious control model with a degree of universality. The current research illustrates this approach using glucose homeostasis as a target. Data were obtained from 41 healthy subjects wearing over-the-counter glucose monitors, and projected onto a simple proportional-integral (PI) controller, widely used in engineering applications. The indicators quantifying the control function are clustered for the great majority of subjects, while a few outliers exhibit less efficient homeostasis. Practical implications for healthcare and education are further discussed.        △ Less","30 December, 2019","physics.med-ph,q-bio.TO",
              Using massive health insurance claims data to predict very high-cost claimants: a machine learning approach          ,1912.13032,https://arxiv.org/abs/1912.13032,https://arxiv.org/pdf/1912.13032,"Authors:JoséM.Maisog,WenhongLi,YanchunXu,BrianHurley,HetalShah,RyanLemberg,TinaBorden,StephenBandeian,MelissaSchline,RoxannaCross,AlanSpiro,RussMichael,AlexanderGutfraind","        Due to escalating healthcare costs, accurately predicting which patients will incur high costs is an important task for payers and providers of healthcare. High-cost claimants (HiCCs) are patients who have annual costs above $\$250,000andwhorepresentjust0.16 and who represent just 0.16% of the insured population but currently account for 9% of all healthcare costs. In this study, we aimed to develop a high-performance algorithm to predict HiCCs to inform a novel care management system. Using health insurance claims from 48 million people and augmented with census data, we applied machine learning to train binary classification models to calculate the personal risk of HiCC. To train the models, we developed a platform starting with 6,006 variables across all clinical and demographic dimensions and constructed over one hundred candidate models. The best model achieved an area under the receiver operating characteristic curve of 91.2%. The model exceeds the highest published performance (84%) and remains high for patients with no prior history of high-cost status (89%), who have less than a full year of enrollment (87%), or lack pharmacy claims data (88%). It attains an area under the precision-recall curve of 23.1%, and precision of 74% at a threshold of 0.99. A care management program enrolling 500 people with the highest HiCC risk is expected to treat 199 true HiCCs and generate a net savings of $7.3millionperyear.Ourresultsdemonstratethathigh−performingpredictivemodelscanbeconstructedusingclaimsdataandpubliclyavailabledataalone,evenforrarehigh−costclaimantsexceeding million per year. Our results demonstrate that high-performing predictive models can be constructed using claims data and publicly available data alone, even for rare high-cost claimants exceeding $250,000$. Our model demonstrates the transformational power of machine learning and artificial intelligence in care management, which would allow healthcare payers and providers to introduce the next generation of care management programs.        △ Less","30 December, 2019","cs.LG,stat.ML",
              AutoDiscern: Rating the Quality of Online Health Information with Hierarchical Encoder Attention-based Neural Networks          ,1912.12999,https://arxiv.org/abs/1912.12999,https://arxiv.org/pdf/1912.12999,"Authors:LauraKinkead,AhmedAllam,MichaelKrauthammer","        Patients increasingly turn to search engines and online content before, or in place of, talking with a health professional. Low quality health information, which is common on the internet, presents risks to the patient in the form of misinformation and a possibly poorer relationship with their physician. To address this, the DISCERN criteria (developed at University of Oxford) are used to evaluate the quality of online health information. However, patients are unlikely to take the time to apply these criteria to the health websites they visit. We built an automated implementation of the DISCERN instrument (Brief version) using machine learning models. We compared the performance of a traditional model (Random Forest) with that of a hierarchical encoder attention-based neural network (HEA) model using two language embeddings, BERT and BioBERT. The HEA BERT and BioBERT models achieved average F1-macro scores across all criteria of 0.75 and 0.74, respectively, outperforming the Random Forest model (average F1-macro = 0.69). Overall, the neural network based models achieved 81% and 86% average accuracy at 100% and 80% coverage, respectively, compared to 94% manual rating accuracy. The attention mechanism implemented in the HEA architectures not only provided 'model explainability' by identifying reasonable supporting sentences for the documents fulfilling the Brief DISCERN criteria, but also boosted F1 performance by 0.05 compared to the same architecture without an attention mechanism. Our research suggests that it is feasible to automate online health information quality assessment, which is an important step towards empowering patients to become informed partners in the healthcare process.        △ Less","26 May, 2020","cs.LG,cs.CL,cs.CY,stat.ML",
              Approval policies for modifications to Machine Learning-Based Software as a Medical Device: A study of bio-creep          ,1912.12413,https://arxiv.org/abs/1912.12413,https://arxiv.org/pdf/1912.12413,"Authors:JeanFeng,ScottEmerson,NoahSimon","        Successful deployment of machine learning algorithms in healthcare requires careful assessments of their performance and safety. To date, the FDA approves locked algorithms prior to marketing and requires future updates to undergo separate premarket reviews. However, this negates a key feature of machine learning--the ability to learn from a growing dataset and improve over time. This paper frames the design of an approval policy, which we refer to as an automatic algorithmic change protocol (aACP), as an online hypothesis testing problem. As this process has obvious analogy with noninferiority testing of new drugs, we investigate how repeated testing and adoption of modifications might lead to gradual deterioration in prediction accuracy, also known as ``biocreep'' in the drug development literature. We consider simple policies that one might consider but do not necessarily offer any error-rate guarantees, as well as policies that do provide error-rate control. For the latter, we define two online error-rates appropriate for this context: Bad Approval Count (BAC) and Bad Approval and Benchmark Ratios (BABR). We control these rates in the simple setting of a constant population and data source using policies aACP-BAC and aACP-BABR, which combine alpha-investing, group-sequential, and gate-keeping methods. In simulation studies, bio-creep regularly occurred when using policies with no error-rate guarantees, whereas aACP-BAC and -BABR controlled the rate of bio-creep without substantially impacting our ability to approve beneficial modifications.        △ Less","28 December, 2019","stat.ML,cs.LG",
              Natural language processing of MIMIC-III clinical notes for identifying diagnosis and procedures with neural networks          ,1912.12397,https://arxiv.org/abs/1912.12397,https://arxiv.org/pdf/1912.12397,"Authors:SiddharthaNuthakki,SunilNeela,JudyW.Gichoya,SaptarshiPurkayastha","        Coding diagnosis and procedures in medical records is a crucial process in the healthcare industry, which includes the creation of accurate billings, receiving reimbursements from payers, and creating standardized patient care records. In the United States, Billing and Insurance related activities cost around $471 billion in 2012 which constitutes about 25% of all the U.S hospital spending. In this paper, we report the performance of a natural language processing model that can map clinical notes to medical codes, and predict final diagnosis from unstructured entries of history of present illness, symptoms at the time of admission, etc. Previous studies have demonstrated that deep learning models perform better at such mapping when compared to conventional machine learning models. Therefore, we employed state-of-the-art deep learning method, ULMFiT on the largest emergency department clinical notes dataset MIMIC III which has 1.2M clinical notes to select for the top-10 and top-50 diagnosis and procedure codes. Our models were able to predict the top-10 diagnoses and procedures with 80.3% and 80.5% accuracy, whereas the top-50 ICD-9 codes of diagnosis and procedures are predicted with 70.7% and 63.9% accuracy. Prediction of diagnosis and procedures from unstructured clinical notes benefit human coders to save time, eliminate errors and minimize costs. With promising scores from our present model, the next step would be to deploy this on a small-scale real-world scenario and compare it with human coders as the gold standard. We believe that further research of this approach can create highly accurate predictions that can ease the workflow in a clinical setting.        △ Less","27 December, 2019","cs.CL,cs.LG",
              A Secure Authentication Technique in Internet of Medical Things through Machine Learning          ,1912.12143,https://arxiv.org/abs/1912.12143,/search/?searchtype=author&query=Mawgoud%2C+A+A,"Authors:AhmedA.Mawgoud,AhmedI.Karadawy,BenbellaS.Tawfik","        The rapid growth of the Internet of Things technology in healthcare domain led to the appearance of many security threats and risks. It became very challenging to provide full protection with the expansion in using sensor objects in medical field, this led to the Internet of Medical Things definition, the security part in IoMT poses a perilous problem that keeps growing, because of the data sensitivity and critical information. The lack of providing a secure environment in IoMT may lead to patients privacy issues, not only leaving the data privacy of the patients at risk but also their lives can be in danger. In this paper, we provide a discussion on both definition and architecture of the Internet of Medical Things and Propose a new authentication approach through machine learning, to enhance the security level.        △ Less","13 April, 2020",cs.CR,
              Split Learning for collaborative deep learning in healthcare,1912.12115,https://arxiv.org/abs/1912.12115,https://arxiv.org/pdf/1912.12115,"Authors:MaartenG.Poirot,PraneethVepakomma,KenChang,JayashreeKalpathy-Cramer,RajivGupta,RameshRaskar","        Shortage of labeled data has been holding the surge of deep learning in healthcare back, as sample sizes are often small, patient information cannot be shared openly, and multi-center collaborative studies are a burden to set up. Distributed machine learning methods promise to mitigate these problems. We argue for a split learning based approach and apply this distributed learning method for the first time in the medical field to compare performance against (1) centrally hosted and (2) non collaborative configurations for a range of participants. Two medical deep learning tasks are used to compare split learning to conventional single and multi center approaches: a binary classification problem of a data set of 9000 fundus photos, and multi-label classification problem of a data set of 156,535 chest X-rays. The several distributed learning setups are compared for a range of 1-50 distributed participants. Performance of the split learning configuration remained constant for any number of clients compared to a single center study, showing a marked difference compared to the non collaborative configuration after 2 clients (p < 0.001) for both sets. Our results affirm the benefits of collaborative training of deep neural networks in health care. Our work proves the significant benefit of distributed learning in healthcare, and paves the way for future real-world implementations.        △ Less","27 December, 2019","cs.LG,cs.DC,stat.ML",
              Anomalous Communications Detection in IoT Networks Using Sparse Autoencoders          ,1912.11831,https://arxiv.org/abs/1912.11831,https://arxiv.org/pdf/1912.11831,"Authors:MustafizurRahmanShahid,GregoryBlanc,ZonghuaZhang,HervéDebar","        Nowadays, IoT devices have been widely deployed for enabling various smart services, such as, smart home or e-healthcare. However, security remains as one of the paramount concern as many IoT devices are vulnerable. Moreover, IoT malware are constantly evolving and getting more sophisticated. IoT devices are intended to perform very specific tasks, so their networking behavior is expected to be reasonably stable and predictable. Any significant behavioral deviation from the normal patterns would indicate anomalous events. In this paper, we present a method to detect anomalous network communications in IoT networks using a set of sparse autoencoders. The proposed approach allows us to differentiate malicious communications from legitimate ones. So that, if a device is compromised only malicious communications can be dropped while the service provided by the device is not totally interrupted. To characterize network behavior, bidirectional TCP flows are extracted and described using statistics on the size of the first N packets sent and received, along with statistics on the corresponding inter-arrival times between packets. A set of sparse autoencoders is then trained to learn the profile of the legitimate communications generated by an experimental smart home network. Depending on the value of N, the developed model achieves attack detection rates ranging from 86.9% to 91.2%, and false positive rates ranging from 0.1% to 0.5%.        △ Less","26 December, 2019","cs.CR,cs.LG,cs.NE,eess.SP",10.1109/NCA.2019.8935007 
              Joint Annotator-and-Spectrum Allocation in Wireless Networks for Crowd Labelling          ,1912.11678,https://arxiv.org/abs/1912.11678,https://arxiv.org/pdf/1912.11678,"Authors:XiaoyangLi,GuangxuZhu,KaimingShen,WeiYu,YiGong,KaibinHuang","        The massive sensing data generated by Internet-of-Things will provide fuel for ubiquitous artificial intelligence (AI), automating the operations of our society ranging from transportation to healthcare. The realistic adoption of this technique however entails labelling of the enormous data prior to the training of AI models via supervised learning. To tackle this challenge, we explore a new perspective of wireless crowd labelling that is capable of downloading data to many imperfect mobile annotators for repetition labelling by exploiting multicasting in wireless networks. In this cross-disciplinary area, the integration of the rate-distortion theory and the principle of repetition labelling for accuracy improvement gives rise to a new tradeoff between radio-and-annotator resources under a constraint on labelling accuracy. Building on the tradeoff and aiming at maximizing the labelling throughput, this work focuses on the joint optimization of encoding rate, annotator clustering, and sub-channel allocation, which results in an NP-hard integer programming problem. To devise an efficient solution approach, we establish an optimal sequential annotator-clustering scheme based on the order of decreasing signal-to-noise ratios. Thereby, the optimal solution can be found by an efficient tree search. Next, the solution is simplified by applying truncated channel inversion. Alternatively, the optimization problem can be recognized as a knapsack problem, which can be efficiently solved in pseudo-polynomial time by means of dynamic programming. In addition, exact polices are derived for the annotators constrained and spectrum constrained cases. Last, simulation results demonstrate the significant throughput gains based on the optimal solution compared with decoupled allocation of the two types of resources.        △ Less","25 December, 2019","cs.IT,eess.SP",
              Healthy Access for Healthy Places: A Multidimensional Food Access Measure          ,1912.11351,https://arxiv.org/abs/1912.11351,https://arxiv.org/pdf/1912.11351,"Authors:IrenaGao,MaryniaKolak","        When it comes to preventive healthcare, place matters. It is increasingly clear that social factors, particularly reliable access to healthy food, are as determinant to health and health equity as medical care. However, food access studies often only present one-dimensional measurements of access. We hypothesize that food access is a multidimensional concept and evaluated Penchansky and Thomas's 1981 definition of access. In our approach, we identify ten variables contributing to food access in the City of Chicago and use principal component analysis to identify vulnerable populations with low access. Our results indicate that within the urban environment of the case study site, affordability is the most important factor in low food accessibility, followed by urban youth, reduced mobility, and higher immigrant population.        △ Less","22 December, 2019",econ.GN,
              MM for Penalized Estimation          ,1912.11119,https://arxiv.org/abs/1912.11119,https://arxiv.org/pdf/1912.11119,Authors:ZhuWang,"        Penalized estimation can conduct variable selection and parameter estimation simultaneously. The general framework is to minimize a loss function subject to a penalty designed to generate sparse variable selection. The majorization-minimization (MM) algorithm is a computational scheme for stability and simplicity, and the MM algorithm has been widely applied in penalized estimation. Much of the previous work have focused on convex loss functions such as generalized linear models. When data are contaminated with outliers, robust loss functions can generate more reliable estimates. Recent literature has witnessed a growing impact of nonconvex loss-based methods, which can generate robust estimation for data contaminated with outliers. This article investigates MM algorithm for penalized estimation, provide innovative optimality conditions and establish convergence theory with both convex and nonconvex loss functions. With respect to applications, we focus on several nonconvex loss functions, which were formerly studied in machine learning for regression and classification problems. Performance of the proposed algorithms are evaluated on simulated and real data including healthcare costs and cancer clinical status. Efficient implementations of the algorithms are available in the R package mpath in CRAN.        △ Less","5 September, 2020","stat.CO,stat.ML",
              Predicting Heart Failure Readmission from Clinical Notes Using Deep Learning          ,1912.10306,https://arxiv.org/abs/1912.10306,https://arxiv.org/pdf/1912.10306,"Authors:XiongLiu,YuChen,JayBae,HuLi,JosephJohnston,ToddSanger","        Heart failure hospitalization is a severe burden on healthcare. How to predict and therefore prevent readmission has been a significant challenge in outcomes research. To address this, we propose a deep learning approach to predict readmission from clinical notes. Unlike conventional methods that use structured data for prediction, we leverage the unstructured clinical notes to train deep learning models based on convolutional neural networks (CNN). We then use the trained models to classify and predict potentially high-risk admissions/patients. For evaluation, we trained CNNs using the discharge summary notes in the MIMIC III database. We also trained regular machine learning models based on random forest using the same datasets. The result shows that deep learning models outperform the regular models in prediction tasks. CNN method achieves a F1 score of 0.756 in general readmission prediction and 0.733 in 30-day readmission prediction, while random forest only achieves a F1 score of 0.674 and 0.656 respectively. We also propose a chi-square test based method to interpret key features associated with deep learning predicted readmissions. It reveals clinical insights about readmission embedded in the clinical notes. Collectively, our method can make the human evaluation process more efficient and potentially facilitate the reduction of readmission rates.        △ Less","21 December, 2019","cs.CL,cs.LG,stat.ML",
              A Model Predictive Approach for Online Mobile Manipulation of Nonholonomic Objects using Learned Dynamics          ,1912.09565,https://arxiv.org/abs/1912.09565,https://arxiv.org/pdf/1912.09565,"Authors:RoyaSabbaghNovin,AmirYazdani,AndrewMerryweather,TuckerHermans","        A particular type of assistive robots designed for physical interaction with objects could play an important role assisting with mobility and fall prevention in healthcare facilities. Autonomous mobile manipulation presents a hurdle prior to safely using robots in real life applications. In this article, we introduce a mobile manipulation framework based on model predictive control using learned dynamics models of objects. We focus on the specific problem of manipulating legged objects such as those commonly found in healthcare environments and personal dwellings (e.g. walkers, tables, chairs). We describe a probabilistic method for autonomous learning of an approximate dynamics model for these objects. In this method, we learn dynamic parameters using a small dataset consisting of force and motion data from interactions between the robot and object. Moreover, we account for multiple manipulation strategies by formulating the manipulation planning as a mixed-integer convex optimization. The proposed framework considers the hybrid control system comprised of i) choosing which leg to grasp, and ii) control of continuous applied forces for manipulation. We formalize our algorithm based on model predictive control to compensate for modeling errors and find an optimal path to manipulate the object from one configuration to another. We show results for several objects with various wheel configurations. Simulation and physical experiments show that the obtained dynamics models are sufficiently accurate for safe and collision-free manipulation. When combined with the proposed manipulation planning algorithm, the robot successfully moves the object to a desired pose while avoiding collision.        △ Less","5 April, 2020",cs.RO,
              Reducing Selection Bias in Counterfactual Reasoning for Individual Treatment Effects Estimation          ,1912.09040,https://arxiv.org/abs/1912.09040,https://arxiv.org/pdf/1912.09040,"Authors:ZichenZhang,QingfengLan,LeiDing,YueWang,NegarHassanpour,RussellGreiner","        Counterfactual reasoning is an important paradigm applicable in many fields, such as healthcare, economics, and education. In this work, we propose a novel method to address the issue of \textit{selection bias}. We learn two groups of latent random variables, where one group corresponds to variables that only cause selection bias, and the other group is relevant for outcome prediction. They are learned by an auto-encoder where an additional regularized loss based on Pearson Correlation Coefficient (PCC) encourages the de-correlation between the two groups of random variables. This allows for explicitly alleviating selection bias by only keeping the latent variables that are relevant for estimating individual treatment effects. Experimental results on a synthetic toy dataset and a benchmark dataset show that our algorithm is able to achieve state-of-the-art performance and improve the result of its counterpart that does not explicitly model the selection bias.        △ Less","19 December, 2019","cs.LG,stat.ML",
              Data Services with Bindaas: RESTful Interfaces for Diverse Data Sources          ,1912.08768,https://arxiv.org/abs/1912.08768,https://arxiv.org/pdf/1912.08768,"Authors:PradeebanKathiravelu,YusufNadirSaghar,TusharAggarwal,AshishSharma","        The diversity of data management systems affords developers the luxury of building systems with heterogeneous systems that address needs that are unique to the data. It allows one to mix-n-match systems that can store, query, update, and process data, based on specific use cases. However, this heterogeneity brings with it the burden of developing custom interfaces for each data management system. Developers are required to build high-performance APIs for data access while adopting best-practices governing security, data privacy, and access control. These include user authentication, data authorization, role-based access control, and audit mechanisms to avoid compromising the security standards mandated by data providers.  In this paper, we present Bindaas, a secure, extensible big data middleware that offers uniform access to diverse data sources. By providing a standard RESTful web service interface to the data sources, Bindaas exposes query, update, store, and delete functionality of the data sources as data service APIs, while providing turn-key support for standard operations involving security, access control, and audit-trails. Bindaas consists of optional features, such as query and response modifiers as well as plugins that implement composable and reusable data operations on the data. The research community has deployed Bindaas in various production environments in healthcare. Our evaluations highlight the efficiency of Bindaas in serving concurrent requests to data source instances. We further observe that the overheads caused by Bindaas on the data sources are negligible.        △ Less","18 December, 2019","cs.DB,cs.SE",
              A Web Page Classifier Library Based on Random Image Content Analysis Using Deep Learning          ,1912.08644,https://arxiv.org/abs/1912.08644,https://arxiv.org/pdf/1912.08644,"Authors:LeonardoEspinosaLeal,Kaj-MikaelBjörk,AmauryLendasse,AntonAkusok","        In this paper, we present a methodology and the corresponding Python library 1 for the classification of webpages. Our method retrieves a fixed number of images from a given webpage, and based on them classifies the webpage into a set of established classes with a given probability. The library trains a random forest model build upon the features extracted from images by a pre-trained deep network. The implementation is tested by recognizing weapon class webpages in a curated list of 3859 websites. The results show that the best method of classifying a webpage into the studies classes is to assign the class according to the maximum probability of any image belonging to this (weapon) class being above the threshold, across all the retrieved images. Further research explores the possibilities for the developed methodology to also apply in image classification for healthcare applications.        △ Less","18 December, 2019",cs.CV,10.1145/3197768.3201525 
              Normalization of breast MRIs using Cycle-Consistent Generative Adversarial Networks          ,1912.08061,https://arxiv.org/abs/1912.08061,https://arxiv.org/pdf/1912.08061,"Authors:GouravModanwal,AdithyaVellal,MaciejA.Mazurowski","        Dynamic Contrast Enhanced-Magnetic Resonance Imaging (DCE-MRI) is widely used to complement ultrasound examinations and x-ray mammography during the early detection and diagnosis of breast cancer. However, images generated by various MRI scanners (e.g. GE Healthcare vs Siemens) differ both in intensity and noise distribution, preventing algorithms trained on MRIs from one scanner to generalize to data from other scanners successfully. We propose a method for image normalization to solve this problem. MRI normalization is challenging because it requires both normalizing intensity values and mapping between the noise distributions of different scanners. We utilize a cycle-consistent generative adversarial network to learn a bidirectional mapping between MRIs produced by GE Healthcare and Siemens scanners. This allows us learning the mapping between two different scanner types without matched data, which is not commonly available. To ensure the preservation of breast shape and structures within the breast, we propose two technical innovations. First, we incorporate a mutual information loss with the CycleGAN architecture to ensure that the structure of the breast is maintained. Second, we propose a modified discriminator architecture which utilizes a smaller field-of-view to ensure the preservation of finer details in the breast tissue. Quantitative and qualitative evaluations show that the second proposed method was able to consistently preserve a high level of detail in the breast structure while also performing the proper intensity normalization and noise mapping. Our results demonstrate that the proposed model can successfully learn a bidirectional mapping between MRIs produced by different vendors, potentially enabling improved accuracy of downstream computational algorithms for diagnosis and detection of breast cancer.        △ Less","16 December, 2019","eess.IV,cs.CV,cs.LG",
              On the Explanation of Machine Learning Predictions in Clinical Gait Analysis          ,1912.07737,https://arxiv.org/abs/1912.07737,https://arxiv.org/pdf/1912.07737,"Authors:DjordjeSlijepcevic,FabianHorst,SebastianLapuschkin,Anna-MariaRaberger,MatthiasZeppelzauer,WojciechSamek,ChristianBreiteneder,WolfgangI.Schöllhorn,BrianHorsak","        Machine learning (ML) is increasingly used to support decision-making in the healthcare sector. While ML approaches provide promising results with regard to their classification performance, most share a central limitation, namely their black-box character. Motivated by the interest to understand the functioning of ML models, methods from the field of Explainable Artificial Intelligence (XAI) have recently become important. This article investigates the usefulness of XAI methods in clinical gait classification. For this purpose, predictions of state-of-the-art classification methods are explained with an established XAI method, i.e., Layer-wise Relevance Propagation (LRP). We propose to evaluate the obtained explanations with two complementary approaches: a statistical analysis of the underlying data using Statistical Parametric Mapping and a qualitative evaluation by a clinical expert. A gait dataset comprising ground reaction force measurements from 132 patients with different lower-body gait disorders and 62 healthy controls is utilized. We investigate several gait classification tasks, employ multiple classification methods, and analyze the impact of data normalization and different signal components for classification performance and explanation quality. Our experiments show that explanations obtained by LRP exhibit promising statistical properties concerning inter-class discriminativity and are also in line with clinically relevant biomechanical gait characteristics.        △ Less","19 August, 2020","cs.LG,stat.ML",
              Training without training data: Improving the generalizability of automated medical abbreviation disambiguation          ,1912.06174,https://arxiv.org/abs/1912.06174,https://arxiv.org/pdf/1912.06174,"Authors:MartaSkreta,AryanArbabi,JixuanWang,MichaelBrudno","        Abbreviation disambiguation is important for automated clinical note processing due to the frequent use of abbreviations in clinical settings. Current models for automated abbreviation disambiguation are restricted by the scarcity and imbalance of labeled training data, decreasing their generalizability to orthogonal sources. In this work we propose a novel data augmentation technique that utilizes information from related medical concepts, which improves our model's ability to generalize. Furthermore, we show that incorporating the global context information within the whole medical note (in addition to the traditional local context window), can significantly improve the model's representation for abbreviations. We train our model on a public dataset (MIMIC III) and test its performance on datasets from different sources (CASI, i2b2). Together, these two techniques boost the accuracy of abbreviation disambiguation by almost 14% on the CASI dataset and 4% on i2b2.        △ Less","12 December, 2019","cs.LG,cs.CL,stat.ML",
              Awareness in Practice: Tensions in Access to Sensitive Attribute Data for Antidiscrimination          ,1912.06171,https://arxiv.org/abs/1912.06171,https://arxiv.org/pdf/1912.06171,"Authors:MirandaBogen,AaronRieke,ShazedaAhmed","        Organizations cannot address demographic disparities that they cannot see. Recent research on machine learning and fairness has emphasized that awareness of sensitive attributes, such as race and sex, is critical to the development of interventions. However, on the ground, the existence of these data cannot be taken for granted.  This paper uses the domains of employment, credit, and healthcare in the United States to surface conditions that have shaped the availability of sensitive attribute data. For each domain, we describe how and when private companies collect or infer sensitive attribute data for antidiscrimination purposes. An inconsistent story emerges: Some companies are required by law to collect sensitive attribute data, while others are prohibited from doing so. Still others, in the absence of legal mandates, have determined that collection and imputation of these data are appropriate to address disparities.  This story has important implications for fairness research and its future applications. If companies that mediate access to life opportunities are unable or hesitant to collect or infer sensitive attribute data, then proposed techniques to detect and mitigate bias in machine learning models might never be implemented outside the lab. We conclude that today's legal requirements and corporate practices, while highly inconsistent across domains, offer lessons for how to approach the collection and inference of sensitive data in appropriate circumstances. We urge stakeholders, including machine learning practitioners, to actively help chart a path forward that takes both policy goals and technical needs into account.        △ Less","12 December, 2019",cs.CY,10.1145/3351095.3372877 
              Content Generation for Workforce Training          ,1912.05606,https://arxiv.org/abs/1912.05606,https://arxiv.org/pdf/1912.05606,"Authors:HollyRushmeier,KapilChalilMadathil,JessicaHodgins,BethMynatt,TonyDerose,BlairMacintyre,otherworkshopparticipants","        Efficient workforce training is needed in today's world in which technology is continually changing the nature of work. Students need to be prepared to enter the workforce. Employees need to become lifelong learners to stay up-to-date in their work and to adapt when job functions are eliminated. The training needs are across all industries - including manufacturing, construction, and healthcare. Computing systems, in particular Virtual/Augmented Reality systems, have been adopted in many training application and show even more promise in the future. However, there are fundamental limitations in today's systems that limit the domains where computing systems can be applied and the extent to which they can be deployed. These limitations need to be addressed by new computing research. In particular research is needed at multiple levels:  - Application Data Collection Level Requiring High Security and Privacy Protections  - Training Material Authoring Level  - Software Systems Level  - Hardware Level  To accomplish these research goals, a training community needs to be established to do research in end-to-end training systems and to create a community of learning and domain experts available for consulting for in depth computing research on individual system components.        △ Less","11 December, 2019",cs.CY,
              Nonparametric Universal Copula Modeling          ,1912.05503,https://arxiv.org/abs/1912.05503,https://arxiv.org/pdf/1912.05503,"Authors:SubhadeepMukhopadhyay,EmanuelParzen","        To handle the ubiquitous problem of ""dependence learning,"" copulas are quickly becoming a pervasive tool across a wide range of data-driven disciplines encompassing neuroscience, finance, econometrics, genomics, social science, machine learning, healthcare and many more. Copula (or connection) functions were invented in 1959 by Abe Sklar in response to a query of Maurice Frechet. After 60 years, where do we stand now? This article provides a history of the key developments and offers a unified perspective.        △ Less","11 December, 2019","stat.ME,stat.AP,stat.CO",
              Severity Detection Tool for Patients with Infectious Disease          ,1912.05345,https://arxiv.org/abs/1912.05345,https://arxiv.org/pdf/1912.05345,"Authors:GirmawAbebeTadesse,TingtingZhu,NhanLeNguyenThanh,NguyenThanhHung,HaThiHaiDuong,TruongHuuKhanh,PhamVanQuang,DucDuongTran,LamMinhYen,HRogierVanDoorn,NguyenVanHao,JohnPrince,HamzaJaved,DaniKiyasseh,LeVanTan,LouiseThwaites,DavidA.Clifton","        Hand, foot and mouth disease (HFMD) and tetanus are serious infectious diseases in low and middle income countries. Tetanus in particular has a high mortality rate and its treatment is resource-demanding. Furthermore, HFMD often affects a large number of infants and young children. As a result, its treatment consumes enormous healthcare resources, especially when outbreaks occur. Autonomic nervous system dysfunction (ANSD) is the main cause of death for both HFMD and tetanus patients. However, early detection of ANSD is a difficult and challenging problem. In this paper, we aim to provide a proof-of-principle to detect the ANSD level automatically by applying machine learning techniques to physiological patient data, such as electrocardiogram (ECG) and photoplethysmogram (PPG) waveforms, which can be collected using low-cost wearable sensors. Efficient features are extracted that encode variations in the waveforms in the time and frequency domains. A support vector machine is employed to classify the ANSD levels. The proposed approach is validated on multiple datasets of HFMD and tetanus patients in Vietnam. Results show that encouraging performance is achieved in classifying ANSD levels. Moreover, the proposed features are simple, more generalisable and outperformed the standard heart rate variability (HRV) analysis. The proposed approach would facilitate both the diagnosis and treatment of infectious diseases in low and middle income countries, and thereby improve overall patient care.        △ Less","10 December, 2019","eess.SP,cs.CV,cs.LG",
              Blockchain for 5G and Beyond Networks: A State of the Art Survey          ,1912.05062,https://arxiv.org/abs/1912.05062,https://arxiv.org/pdf/1912.05062,"Authors:DinhCNguyen,PubuduNPathirana,MingDing,ArunaSeneviratne","        The fifth generation (5G) wireless networks are on the way to be deployed around the world. The 5G technologies target to support diverse vertical applications by connecting heterogeneous devices and machines with drastic improvements in terms of high quality of service, increased network capacity and enhanced system throughput. Despite all these advantages that 5G will bring about, there are still major challenges to be addressed, including decentralization, transparency, risks of data interoperability, network privacy and security vulnerabilities. Blockchain can offer innovative solutions to effectively solve the challenges in 5G networks. Driven by the dramatically increased capacities of the 5G networks and the recent breakthroughs in the blockchain technology, blockchain-based 5G services are expected to witness a rapid development and bring substantial benefits to future society. In this paper, we provide a state-of-art survey on the integration of blockchain with 5G networks and beyond. Our key focus is on the discussions on the potential of blockchain for enabling key 5G technologies, including cloud/edge computing, Software Defined Networks, Network Function Virtualization, Network Slicing, and D2D communications. We then explore the opportunities of blockchain to important 5G services, ranging from spectrum management, network virtualization, resource management to interference management, federated learning, privacy and security provision. The recent advances in the applications of blockchain in 5G Internet of Things are also surveyed in various domains, i.e. smart healthcare, smart city, smart transportation, smart grid and UAVs. The main findings derived from the survey are then summarized, and possible research challenges with open issues are also identified. Lastly, we complete this survey by shedding new light on future directions of research on this newly emerging area.        △ Less","10 December, 2019","cs.NI,cs.IT,eess.SP",
              Is Your Smartband Smart Enough to Know Who You Are: Continuous Physiological Authentication in The Wild          ,1912.04760,https://arxiv.org/abs/1912.04760,https://arxiv.org/pdf/1912.04760,"Authors:DenizEkiz,YektaSaidCan,YagmurCerenDardagan,CemErsoy","        The use of cloud services that process privacy-sensitive information such as digital banking, pervasive healthcare, smart home applications requires an implicit continuous authentication solution which will make these systems less vulnerable to the spoofing attacks. Physiological signals can be used for continuous authentication due to their personal uniqueness. Ubiquitous wrist-worn wearable devices are equipped with photoplethysmogram sensors which enable to extract heart rate variability (HRV) features. In this study, we show that these devices can be used for continuous physiological authentication, for enhancing the security of the cloud, edge services, and IoT devices. A system that is suitable for the smartband framework comes with new challenges such as relatively low signal quality and artifacts due to placement which were not encountered in full lead electrocardiogram systems. After the artifact removal, cleaned physiological signals are fed to the machine learning algorithms. In order to train our machine learning models, we collected physiological data using off-the-shelf smartbands and smartwatches in a real-life event. Performance evaluation of selected machine learning algorithms shows that HRV is a strong candidate for continuous unobtrusive implicit physiological authentication.        △ Less","15 January, 2020","cs.HC,eess.SP",10.1109/ACCESS.2020.2982852 
              Interfacial Load Monitoring and Failure Detection in Total Joint Replacements via Piezoresistive Bone Cement and Electrical Impedance Tomography          ,1912.04723,https://arxiv.org/abs/1912.04723,https://arxiv.org/pdf/1912.04723,"Authors:HamidGhaednia,CrystalE.Owens,RicardoRoberts,TylerN.Tallman,A.JohnHart,KartikM.Varadarajan","        Aseptic loosening, or loss of implant fixation, is a common complication following total joint replacement. Revision surgeries cost the healthcare system over $8 billion annually in the US. Despite the prevalence of aseptic loosening, timely and accurate detection remains a challenge because traditional imaging modalities such as plain radiographs struggle to reliably detect the early stages of implant loosening. Motivated by this challenge, we present a novel approach for in vivo monitoring and failure detection of cemented joint replacements. Poly(methyl methacrylate) (PMMA) bone cement is modified with low volume fractions of chopped carbon fiber (CF) to impart piezoresistive-based self-sensing. Electrical impedance tomography (EIT) is then used to detect and monitor load-induced deformation and fracture of CF/PMMA in a phantom tank. We therefore show that EIT indeed is able to adeptly detect loading force on a prosthetic surrogate, distinguish between increasing load magnitudes, detect failure of implant fixation, and even distinguish between cement cracking and cement de-bonding. Because EIT is a low-cost, physiologically benign, and potentially real-time imaging modality, the feasibility study herein presented has potential to transform the standard of care for post-operative monitoring of implant conditions. Beyond clinical relevance, this technique could positively impact orthopedic researchers by providing, via in vivo monitoring, insight into the factors that initiate aseptic loosening.        △ Less","11 December, 2019",eess.SP,10.1088/1361-665X/ab874f 
              Willingness to Pay for Community-Based Health Insurance among Rural Households of Southwest Ethiopia          ,1912.04281,https://arxiv.org/abs/1912.04281,https://arxiv.org/pdf/1912.04281,"Authors:MelakuHaileLikka,ShimelesOloloSinkie,BerhaneMegerssa","        Use of healthcare services is inadequate in Ethiopia in spite of the high burden of diseases. User-fee charges are the most important factor for this deficiency in healthcare utilization. Hence, the country is introducing community based and social health insurances since 2010 to tackle such problems. This study was conducted cross-sectionally, in March 2013, to assess willingness of rural households to pay for community-based health insurance in Debub Bench district of Southwest Ethiopia. Two-stage sampling technique was used to select 845 households. Selected households were contacted using simple random sampling technique. Double bounded dichotomous choice method was used to illicit the willingness to pay. Data were analyzed with STATA 11. Krinsky and Rob method was used to calculate the mean/median with 95% CI willingness to pay after the predictors have been estimated using Seemingly Unrelated Bivariate Probit Regression. Eight hundred and eight (95.6%) of the sampled households were interviewed. Among them 629(77.8%) households were willing to join the proposed CBHI scheme. About 54% of the households in the district were willing to pay either the initial or second bids presented. On average, these households were willingness to pay was 162.61 Birr per household (8.9 US)annually.Ifthecommunitybasedhealthinsuranceisrolledoutinthedistrict,abouthalfofhouseholdswillcontribute163Birr(8.9US) annually. If the community based health insurance is rolled out in the district, about half of households will contribute 163 Birr (8.9 US) annually. If the premium exceeds the amount specified, majority of the households would not join the scheme. Key words: community based health insurance, willingness to pay, contingent valuation method, double bounded dichotomous choice, Krinsky and Robb, rural households, Ethiopia.        △ Less","9 December, 2019",econ.GN,
              MetaCI: Meta-Learning for Causal Inference in a Heterogeneous Population          ,1912.03960,https://arxiv.org/abs/1912.03960,https://arxiv.org/pdf/1912.03960,"Authors:AnkitSharma,GarimaGupta,RanjithaPrasad,ArnabChatterjee,LovekeshVig,GautamShroff","        Performing inference on data obtained through observational studies is becoming extremely relevant due to the widespread availability of data in fields such as healthcare, education, retail, etc. Furthermore, this data is accrued from multiple homogeneous subgroups of a heterogeneous population, and hence, generalizing the inference mechanism over such data is essential. We propose the MetaCI framework with the goal of answering counterfactual questions in the context of causal inference (CI), where the factual observations are obtained from several homogeneous subgroups. While the CI network is designed to generalize from factual to counterfactual distribution in order to tackle covariate shift, MetaCI employs the meta-learning paradigm to tackle the shift in data distributions between training and test phase due to the presence of heterogeneity in the population, and due to drifts in the target distribution, also known as concept shift. We benchmark the performance of the MetaCI algorithm using the mean absolute percentage error over the average treatment effect as the metric, and demonstrate that meta initialization has significant gains compared to randomly initialized networks, and other methods.        △ Less","1 May, 2020","cs.LG,cs.MA,stat.ML",
              Big Data Security Issues and Challenges in Healthcare,1912.03848,https://arxiv.org/abs/1912.03848,https://arxiv.org/pdf/1912.03848,"Authors:BehnamKianiKalejahi,SaeedMeshgini,AyshanYariyeva,DawdaNdure,UzeyirMaharramov,AliFarzamnia","        This paper embodies the usage of Big Data in Healthcare. It is important to note that big data in terms of Architecture and implementation might be or has already or will continue to assist the continuous growth in the field of healthcare. The main important aspects of this study are the general importance of big data in healthcare, the positives big data will help tackle and enhance in this field and not to also forget to mention the tremendous downside big data has on healthcare that is still needed to improve or putting extensive research on. We believe there is still a long way in which institutions and individuals understand the hidden truth about big data. We have highlighted the various ways one could be confidently relied on big data and on the other hand highlighted the weighted importance of big problem big data and expected solutions.        △ Less","8 December, 2019","cs.CR,cs.NI",
              Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers          ,1912.03277,https://arxiv.org/abs/1912.03277,https://arxiv.org/pdf/1912.03277,"Authors:DivyatMahajan,ChenhaoTan,AmitSharma","        To construct interpretable explanations that are consistent with the original ML model, counterfactual examples---showing how the model's output changes with small perturbations to the input---have been proposed. This paper extends the work in counterfactual explanations by addressing the challenge of feasibility of such examples. For explanations of ML models in critical domains such as healthcare and finance, counterfactual examples are useful for an end-user only to the extent that perturbation of feature inputs is feasible in the real world. We formulate the problem of feasibility as preserving causal relationships among input features and present a method that uses (partial) structural causal models to generate actionable counterfactuals. When feasibility constraints cannot be easily expressed, we consider an alternative mechanism where people can label generated CF examples on feasibility: whether it is feasible to intervene and realize the candidate CF example from the original input. To learn from this labelled feasibility data, we propose a modified variational auto encoder loss for generating CF examples that optimizes for feasibility as people interact with its output. Our experiments on Bayesian networks and the widely used ''Adult-Income'' dataset show that our proposed methods can generate counterfactual explanations that better satisfy feasibility constraints than existing methods.. Code repository can be accessed here: \textit{https://github.com/divyat09/cf-feasibility}        △ Less","12 June, 2020","cs.LG,cs.AI,stat.ML",
              Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning          ,1912.02631,https://arxiv.org/abs/1912.02631,https://arxiv.org/pdf/1912.02631,"Authors:RahulRachuri,AjithSuresh","        Machine learning has started to be deployed in fields such as healthcare and finance, which propelled the need for and growth of privacy-preserving machine learning (PPML). We propose an actively secure four-party protocol (4PC), and a framework for PPML, showcasing its applications on four of the most widely-known machine learning algorithms -- Linear Regression, Logistic Regression, Neural Networks, and Convolutional Neural Networks.  Our 4PC protocol tolerating at most one malicious corruption is practically efficient as compared to the existing works. We use the protocol to build an efficient mixed-world framework (Trident) to switch between the Arithmetic, Boolean, and Garbled worlds. Our framework operates in the offline-online paradigm over rings and is instantiated in an outsourced setting for machine learning. Also, we propose conversions especially relevant to privacy-preserving machine learning.  The highlights of our framework include using a minimal number of expensive circuits overall as compared to ABY3. This can be seen in our technique for truncation, which does not affect the online cost of multiplication and removes the need for any circuits in the offline phase. Our B2A conversion has an improvement of 7×\mathbf{7} \times in rounds and 18×\mathbf{18} \times in the communication complexity. In addition to these, all of the special conversions for machine learning, e.g. Secure Comparison, achieve constant round complexity.  The practicality of our framework is argued through improvements in the benchmarking of the aforementioned algorithms when compared with ABY3. All the protocols are implemented over a 64-bit ring in both LAN and WAN settings. Our improvements go up to 187×\mathbf{187} \times for the training phase and 158×\mathbf{158} \times for the prediction phase when observed over LAN and WAN.        △ Less","5 December, 2019","cs.LG,cs.CR,stat.ML",
              Energy Autonomous Wearable Sensors for Smart Healthcare: A Review          ,1912.02596,https://arxiv.org/abs/1912.02596,https://arxiv.org/pdf/1912.02596,"Authors:AbhishekSinghDahiya,JeromeThireau,JamilaBoudaden,SwatchithLal,UmairGulzar,YanZhang,ThierryGil,NadineAzemard,PeterRamm,TimKiessling,CianO'Murchu,FredrikSebelius,JonasTilly,ColmGlynn,ShaneGeary,ColmO'Dwyer,KafilRazeeb,AlainLacampagne,BenoitCharlot,AidaTodri-Sanial","        Energy Autonomous Wearable Sensors (EAWS) have attracted a large interest due to their potential to provide reliable measurements and continuous bioelectric signals, which help to reduce health risk factors early on, ongoing assessment for disease prevention, and maintaining optimum, lifelong health quality. This review paper presents recent developments and state-of-the-art research related to three critical elements that enable an EAWS. The first element is wearable sensors, which monitor human body physiological signals and activities. Emphasis is given on explaining different types of transduction mechanisms presented, and emerging materials and fabrication techniques. The second element is the flexible and wearable energy storage device to drive low-power electronics and the software needed for automatic detection of unstable physiological parameters. The third is the flexible and stretchable energy harvesting module to recharge batteries for continuous operation of wearable sensors. We conclude by discussing some of the technical challenges in realizing energy-autonomous wearable sensing technologies and possible solutions for overcoming them.        △ Less","5 December, 2019","eess.SP,cs.HC",
              Artificial Intelligence for Low-Resource Communities: Influence Maximization in an Uncertain World          ,1912.02102,https://arxiv.org/abs/1912.02102,https://arxiv.org/pdf/1912.02102,Authors:AmulyaYadav,"        The potential of Artificial Intelligence (AI) to tackle challenging problems that afflict society is enormous, particularly in the areas of healthcare, conservation and public safety and security. Many problems in these domains involve harnessing social networks of under-served communities to enable positive change, e.g., using social networks of homeless youth to raise awareness about Human Immunodeficiency Virus (HIV) and other STDs. Unfortunately, most of these real-world problems are characterized by uncertainties about social network structure and influence models, and previous research in AI fails to sufficiently address these uncertainties. This thesis addresses these shortcomings by advancing the state-of-the-art to a new generation of algorithms for interventions in social networks. In particular, this thesis describes the design and development of new influence maximization algorithms which can handle various uncertainties that commonly exist in real-world social networks. These algorithms utilize techniques from sequential planning problems and social network theory to develop new kinds of AI algorithms. Further, this thesis also demonstrates the real-world impact of these algorithms by describing their deployment in three pilot studies to spread awareness about HIV among actual homeless youth in Los Angeles. This represents one of the first-ever deployments of computer science based influence maximization algorithms in this domain. Our results show that our AI algorithms improved upon the state-of-the-art by 160% in the real-world. We discuss research and implementation challenges faced in deploying these algorithms, and lessons that can be gleaned for future deployment of such algorithms. The positive results from these deployments illustrate the enormous potential of AI in addressing societally relevant problems.        △ Less","2 December, 2019","cs.AI,cs.SI",
              Degenerative Adversarial NeuroImage Nets for 4D Simulations: Application in Longitudinal MRI          ,1912.01526,https://arxiv.org/abs/1912.01526,https://arxiv.org/pdf/1912.01526,"Authors:DanieleRavi,StefanoB.Blumberg,KyriakiMengoudi,MouchengXu,DanielC.Alexander,NeilP.Oxtoby","        Accurate and realistic simulation of medical images is a growing area of research relevant to many healthcare applications. However, current image simulators have been unsuccessful when deployed on longitudinal clinical data --- for example, disease progression modelling designed to generate 3D MRI sequences (4D). Failures are typically due to inability to produce subject-specific simulation, and inefficient implementations incapable of synthesizing spatiotemporal images in high resolution. Memory limitations preclude training of the full-4D model, necessitating techniques that discard spatiotemporal information, such as 2D slice-by-slice implementations or patch-based approaches. Here we introduce a novel technique to address this challenge, called Profile Weight Functions (PWF). We demonstrate the power of PWFs by extending a recent framework for neuroimage simulation from 2D (plus time) to 3D (plus time), which is not currently available. To our knowledge, we are the first to implement a disease progression simulator able to predict accurate sequences of realistic, high-resolution, 3D medical images. We demonstrate our framework by training a model using 9652 T1-weighted MRI from the Alzheimer's Disease Neuroimaging Initiative dataset. We validate our results on a separate test set of 1216 MRI, demonstrating the capability to synthesize a personalized time-series of images given a single-time point and other metadata.        △ Less","6 March, 2020","eess.IV,cs.CV",
              FT-ClipAct: Resilience Analysis of Deep Neural Networks and Improving their Fault Tolerance using Clipped Activation          ,1912.00941,https://arxiv.org/abs/1912.00941,https://arxiv.org/pdf/1912.00941,"Authors:Le-HaHoang,MuhammadAbdullahHanif,MuhammadShafique","        Deep Neural Networks (DNNs) are widely being adopted for safety-critical applications, e.g., healthcare and autonomous driving. Inherently, they are considered to be highly error-tolerant. However, recent studies have shown that hardware faults that impact the parameters of a DNN (e.g., weights) can have drastic impacts on its classification accuracy. In this paper, we perform a comprehensive error resilience analysis of DNNs subjected to hardware faults (e.g., permanent faults) in the weight memory. The outcome of this analysis is leveraged to propose a novel error mitigation technique which squashes the high-intensity faulty activation values to alleviate their impact. We achieve this by replacing the unbounded activation functions with their clipped versions. We also present a method to systematically define the clipping values of the activation functions that result in increased resilience of the networks against faults. We evaluate our technique on the AlexNet and the VGG-16 DNNs trained for the CIFAR-10 dataset. The experimental results show that our mitigation technique significantly improves the resilience of the DNNs to faults. For example, the proposed technique offers on average 68.92% improvement in the classification accuracy of resilience-optimized VGG-16 model at 1e-5 fault rate, when compared to the base network without any fault mitigation.        △ Less","2 December, 2019","cs.LG,stat.ML",
              Dicoogle Framework for Medical Imaging Teaching and Research          ,1912.00877,https://arxiv.org/abs/1912.00877,https://arxiv.org/pdf/1912.00877,"Authors:RuiLebre,EduardoPinho,JorgeMiguelSilva,CarlosCosta","        One of the most noticeable trends in healthcare over the last years is the continuous growth of data volume produced and its heterogeneity. In the medical imaging field, the evolution of digital systems is supported by the PACS concept and the DICOM standard. These technologies are deeply grounded in medical laboratories, supporting the production and providing healthcare practitioners with the ability to set up collaborative work environments with researchers and academia to study and improve healthcare practice. However, the complexity of those systems and protocols makes difficult and time-consuming to prototype new ideas or develop applied research, even for skilled users with training in those environments. Dicoogle emerges as a reference tool to achieve those objectives through a set of resources aggregated in the form of a learning pack. It is an open-source PACS archive that, on the one hand, provides a comprehensive view of the PACS and DICOM technologies and, on the other hand, provides the user with tools to easily expand its core functionalities. This paper describes the Dicoogle framework, with particular emphasis in its Learning Pack package, the resources available and the impact of the platform in research and academia. It starts by presenting an overview of its architectural concept, the most recent research backed up by Dicoogle, some remarks obtained from its use in teaching, and worldwide usage statistics of the software. Moreover, a comparison between the Dicoogle platform and the most popular open-source PACS in the market is presented.        △ Less","2 December, 2019",cs.SE,
              Learning a faceted customer segmentation for discovering new business opportunities at Intel          ,1912.00778,https://arxiv.org/abs/1912.00778,https://arxiv.org/pdf/1912.00778,"Authors:ItayLieder,MeiravSegal,EranAvidan,AsafCohen,TomHope","        For sales and marketing organizations within large enterprises, identifying and understanding new markets, customers and partners is a key challenge. Intel's Sales and Marketing Group (SMG) faces similar challenges while growing in new markets and domains and evolving its existing business. In today's complex technological and commercial landscape, there is need for intelligent automation supporting a fine-grained understanding of businesses in order to help SMG sift through millions of companies across many geographies and languages and identify relevant directions. We present a system developed in our company that mines millions of public business web pages, and extracts a faceted customer representation. We focus on two key customer aspects that are essential for finding relevant opportunities: industry segments (ranging from broad verticals such as healthcare, to more specific fields such as 'video analytics') and functional roles (e.g., 'manufacturer' or 'retail'). To address the challenge of labeled data collection, we enrich our data with external information gleaned from Wikipedia, and develop a semi-supervised multi-label, multi-lingual deep learning model that parses customer website texts and classifies them into their respective facets. Our system scans and indexes companies as part of a large-scale knowledge graph that currently holds tens of millions of connected entities with thousands being fetched, enriched and connected to the graph by the hour in real time, and also supports knowledge and insight discovery. In experiments conducted in our company, we are able to significantly boost the performance of sales personnel in the task of discovering new customers and commercial partnership opportunities.        △ Less","27 November, 2019","cs.IR,cs.CL,cs.LG,stat.ML",
              Time-Guided High-Order Attention Model of Longitudinal Heterogeneous Healthcare Data          ,1912.00773,https://arxiv.org/abs/1912.00773,https://arxiv.org/pdf/1912.00773,"Authors:YiHuang,XiaoshanYang,ChangshengXu","        Due to potential applications in chronic disease management and personalized healthcare, the EHRs data analysis has attracted much attention of both researchers and practitioners. There are three main challenges in modeling longitudinal and heterogeneous EHRs data: heterogeneity, irregular temporality and interpretability. A series of deep learning methods have made remarkable progress in resolving these challenges. Nevertheless, most of existing attention models rely on capturing the 1-order temporal dependencies or 2-order multimodal relationships among feature elements. In this paper, we propose a time-guided high-order attention (TGHOA) model. The proposed method has three major advantages. (1) It can model longitudinal heterogeneous EHRs data via capturing the 3-order correlations of different modalities and the irregular temporal impact of historical events. (2) It can be used to identify the potential concerns of medical features to explain the reasoning process of the healthcare model. (3) It can be easily expanded into cases with more modalities and flexibly applied in different prediction tasks. We evaluate the proposed method in two tasks of mortality prediction and disease ranking on two real world EHRs datasets. Extensive experimental results show the effectiveness of the proposed model.        △ Less","27 November, 2019",cs.LG,
              Semantic Enrichment of Streaming Healthcare Data          ,1912.00423,https://arxiv.org/abs/1912.00423,https://arxiv.org/pdf/1912.00423,"Authors:DanielCotter,V.K.CodyBumgardner","        In the past decade, the healthcare industry has made significant advances in the digitization of patient information. However, a lack of interoperability among healthcare systems still imposes a high cost to patients, hospitals, and insurers. Currently, most systems pass messages using idiosyncratic messaging standards that require specialized knowledge to interpret. This increases the cost of systems integration and often puts more advanced uses of data out of reach. In this project, we demonstrate how two open standards, FHIR and RDF, can be combined both to integrate data from disparate sources in real-time and make that data queryable and susceptible to automated inference. To validate the effectiveness of the semantic engine, we perform simulations of real-time data feeds and demonstrate how they can be combined and used by client-side applications with no knowledge of the underlying sources.        △ Less","1 December, 2019","cs.IR,cs.CL",
              Incremental Clustering Techniques for Multi-Party Privacy-Preserving Record Linkage          ,1911.12930,https://arxiv.org/abs/1911.12930,https://arxiv.org/pdf/1911.12930,"Authors:DinushaVatsalan,PeterChristen,ErhardRahm","        Privacy-Preserving Record Linkage (PPRL) supports the integration of sensitive information from multiple datasets, in particular the privacy-preserving matching of records referring to the same entity. PPRL has gained much attention in many application areas, with the most prominent ones in the healthcare domain. PPRL techniques tackle this problem by conducting linkage on masked (encoded) values. Employing PPRL on records from multiple (more than two) parties/sources (multi-party PPRL, MP-PPRL) is an increasingly important but challenging problem that so far has not been sufficiently solved. Existing MP-PPRL approaches are limited to finding only those entities that are present in all parties thereby missing entities that match only in a subset of parties. Furthermore, previous MP-PPRL approaches face substantial scalability limitations due to the need of a large number of comparisons between masked records. We thus propose and evaluate new MP-PPRL approaches that find matches in any subset of parties and still scale to many parties. Our approaches maintain all matches within clusters, where these clusters are incrementally extended or refined by considering records from one party after the other. An empirical evaluation using multiple real datasets ranging from 3 to 26 parties each containing up to 55 million records validates that our protocols are efficient, and significantly outperform existing MP-PPRL approaches in terms of linkage quality and scalability.        △ Less","28 November, 2019","cs.DB,cs.DC",
              Detecting total hip replacement prosthesis design on preoperative radiographs using deep convolutional neural network          ,1911.12387,https://arxiv.org/abs/1911.12387,https://arxiv.org/pdf/1911.12387,"Authors:AlirezaBorjali,AntoniaF.Chen,OrhunK.Muratoglu,MohammadA.Morid,KartikM.Varadarajan","        Identifying the design of a failed implant is a key step in preoperative planning of revision total joint arthroplasty. Manual identification of the implant design from radiographic images is time consuming and prone to error. Failure to identify the implant design preoperatively can lead to increased operating room time, more complex surgery, increased blood loss, increased bone loss, increased recovery time, and overall increased healthcare costs. In this study, we present a novel, fully automatic and interpretable approach to identify the design of total hip replacement (THR) implants from plain radiographs using deep convolutional neural network (CNN). CNN achieved 100% accuracy in identification of three commonly used THR implant designs. Such CNN can be used to automatically identify the design of a failed THR implant preoperatively in just a few seconds, saving time and improving the identification accuracy. This can potentially improve patient outcomes, free practitioners time, and reduce healthcare costs.        △ Less","27 November, 2019","eess.IV,cs.CV",10.1002/jor.24617 
              A semi-autonomous approach to connecting proprietary EHR standards to FHIR          ,1911.12254,https://arxiv.org/abs/1911.12254,https://arxiv.org/pdf/1911.12254,"Authors:MartinChapman,VasaCurcin,ElizabethISklar","        HL7's Fast Healthcare Interoperability Resources (FHIR) standard is designed to provide a consistent way in which to represent and exchange healthcare data, such as electronic health records (EHRs). SMART--on--FHIR (SoF) technology uses this standard to augment existing healthcare data systems with a standard FHIR interface. While this is an important goal, little attention has been paid to developing mechanisms that convert EHR data structured using proprietary schema to the FHIR standard, in order to be served by such an interface. In this paper, a formal process is proposed that both identifies a set of FHIR resources that best capture the elements of an EHR, and transitions the contents of that EHR to FHIR, with a view to supporting the operation of SoF containers, and the wider interoperability of health records with the FHIR standard. This process relies on a number of techniques that enable us to understand when two terms are equivalent, in particular a set of similarity metrics, which are combined along with a series of parameters in order to enable the approach to be tuned to the different EHR standards encountered. Thus, when realised in software, the translation process is semi-autonomous, requiring only the specification of these parameters before performing an arbitrary number of future conversions. The approach is demonstrated by utilising it as part of the CONSULT project, a wider decision support system that aims to provide intelligent decision support for stroke patients.        △ Less","23 January, 2020",cs.SE,
              ConCare: Personalized Clinical Feature Embedding via Capturing the Healthcare Context          ,1911.12216,https://arxiv.org/abs/1911.12216,https://arxiv.org/pdf/1911.12216,"Authors:LiantaoMa,ChaoheZhang,YashaWang,WenjieRuan,JiantaoWang,WenTang,XinyuMa,XinGao,JunyiGao","        Predicting the patient's clinical outcome from the historical electronic medical records (EMR) is a fundamental research problem in medical informatics. Most deep learning-based solutions for EMR analysis concentrate on learning the clinical visit embedding and exploring the relations between visits. Although those works have shown superior performances in healthcare prediction, they fail to explore the personal characteristics during the clinical visits thoroughly. Moreover, existing works usually assume that the more recent record weights more in the prediction, but this assumption is not suitable for all conditions. In this paper, we propose ConCare to handle the irregular EMR data and extract feature interrelationship to perform individualized healthcare prediction. Our solution can embed the feature sequences separately by modeling the time-aware distribution. ConCare further improves the multi-head self-attention via the cross-head decorrelation, so that the inter-dependencies among dynamic features and static baseline information can be effectively captured to form the personal health context. Experimental results on two real-world EMR datasets demonstrate the effectiveness of ConCare. The medical findings extracted by ConCare are also empirically confirmed by human experts and medical literature.        △ Less","27 November, 2019","cs.LG,stat.ML",
              Explaining Models by Propagating Shapley Values of Local Components          ,1911.11888,https://arxiv.org/abs/1911.11888,https://arxiv.org/pdf/1911.11888,"Authors:HughChen,ScottLundberg,Su-InLee","        In healthcare, making the best possible predictions with complex models (e.g., neural networks, ensembles/stacks of different models) can impact patient welfare. In order to make these complex models explainable, we present DeepSHAP for mixed model types, a framework for layer wise propagation of Shapley values that builds upon DeepLIFT (an existing approach for explaining neural networks). We show that in addition to being able to explain neural networks, this new framework naturally enables attributions for stacks of mixed models (e.g., neural network feature extractor into a tree model) as well as attributions of the loss. Finally, we theoretically justify a method for obtaining attributions with respect to a background distribution (under a Shapley value framework).        △ Less","26 November, 2019","cs.LG,stat.ML",
              Doctor2Vec: Dynamic Doctor Representation Learning for Clinical Trial Recruitment          ,1911.10395,https://arxiv.org/abs/1911.10395,https://arxiv.org/pdf/1911.10395,"Authors:SiddharthBiswal,CaoXiao,LucasM.Glass,ElizabethMilkovits,JimengSun","        Massive electronic health records (EHRs) enable the success of learning accurate patient representations to support various predictive health applications. In contrast, doctor representation was not well studied despite that doctors play pivotal roles in healthcare. How to construct the right doctor representations? How to use doctor representation to solve important health analytic problems? In this work, we study the problem on {\it clinical trial recruitment}, which is about identifying the right doctors to help conduct the trials based on the trial description and patient EHR data of those doctors. We propose doctor2vec which simultaneously learns 1) doctor representations from EHR data and 2) trial representations from the description and categorical information about the trials. In particular, doctor2vec utilizes a dynamic memory network where the doctor's experience with patients are stored in the memory bank and the network will dynamically assign weights based on the trial representation via an attention mechanism. Validated on large real-world trials and EHR data including 2,609 trials, 25K doctors and 430K patients, doctor2vec demonstrated improved performance over the best baseline by up to 8.7%8.7\% in PR-AUC. We also demonstrated that the doctor2vec embedding can be transferred to benefit data insufficiency settings including trial recruitment in less populated/newly explored country with 13.7%13.7\% improvement or for rare diseases with 8.1%8.1\% improvement in PR-AUC.        △ Less","23 November, 2019","cs.LG,cs.CY,stat.ML",
              Towards Quantification of Explainability in Explainable Artificial Intelligence Methods          ,1911.10104,https://arxiv.org/abs/1911.10104,https://arxiv.org/pdf/1911.10104,"Authors:SheikhRabiulIslam,WilliamEberle,SheikhK.Ghafoor","        Artificial Intelligence (AI) has become an integral part of domains such as security, finance, healthcare, medicine, and criminal justice. Explaining the decisions of AI systems in human terms is a key challenge--due to the high complexity of the model, as well as the potential implications on human interests, rights, and lives . While Explainable AI is an emerging field of research, there is no consensus on the definition, quantification, and formalization of explainability. In fact, the quantification of explainability is an open challenge. In our previous work, we incorporated domain knowledge for better explainability, however, we were unable to quantify the extent of explainability. In this work, we (1) briefly analyze the definitions of explainability from the perspective of different disciplines (e.g., psychology, social science), properties of explanation, explanation methods, and human-friendly explanations; and (2) propose and formulate an approach to quantify the extent of explainability. Our experimental result suggests a reasonable and model-agnostic way to quantify explainability        △ Less","22 November, 2019","cs.AI,q-fin.RM",
              Geo-clustered chronic affinity: pathways from socio-economic disadvantages to health disparities          ,1911.09769,https://arxiv.org/abs/1911.09769,https://arxiv.org/pdf/1911.09769,"Authors:EunKyongShin,YoungsangKwon,ArashShaban-Nejad","        Our objective was to develop and test a new concept (affinity) analogous to multimorbidity of chronic conditions for individuals at census tract level in Memphis, TN. The use of affinity will improve the surveillance of multiple chronic conditions and facilitate the design of effective interventions. We used publicly available chronic condition data (Center for Disease Control and Prevention 500 Cities project), socio-demographic data (US Census Bureau), and demographic data (Environmental Systems Research Institute). A geo-distinctive pattern of clustered chronic affinity associated with socio-economic deprivation wasobserved. Statistical results confirmed that neighborhoods with higher rates of crime, poverty, and unemploy-ment were associated with an increased likelihood of having a higher affinity among major chronic conditions.With the inclusion of smoking in the model, however, only the crime prevalence was statistically significantlyassociated with the chronic affinity. Chronic affinity disadvantages were disproportionately accumulated in socially disadvantagedareas. We showed links between commonly co-observed chronic diseases at the population level and systemat-ically explored the complexity of affinity and socio-economic disparities. Our affinity score, based on publiclyavailable datasets, served as a surrogate for multimorbidity at the population level, which may assist policy-makers and public health planners to identify urgent hot spots for chronic disease and allocate clinical, medicaland healthcare resources efficiently.        △ Less","21 November, 2019","cs.CY,math.ST",10.1093/jamiaopen/ooz029 
              An Empirical Study of Sections in Classifying Disease Outbreak Reports          ,1911.09319,https://arxiv.org/abs/1911.09319,https://arxiv.org/pdf/1911.09319,"Authors:SonDoan,MikeConway,NigelCollier","        Identifying articles that relate to infectious diseases is a necessary step for any automatic bio-surveillance system that monitors news articles from the Internet. Unlike scientific articles which are available in a strongly structured form, news articles are usually loosely structured. In this chapter, we investigate the importance of each section and the effect of section weighting on performance of text classification. The experimental results show that (1) classification models using the headline and leading sentence achieve a high performance in terms of F-score compared to other parts of the article; (2) all section with bag-of-word representation (full text) achieves the highest recall; and (3) section weighting information can help to improve accuracy.        △ Less","21 November, 2019",cs.CL,10.1007/978-1-4419-1274-9_4 
              Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments          ,1911.09005,https://arxiv.org/abs/1911.09005,https://arxiv.org/pdf/1911.09005,"Authors:RoelDobbe,ThomasKrendlGilbert,YonatanMintz","        As AI systems become prevalent in high stakes domains such as surveillance and healthcare, researchers now examine how to design and implement them in a safe manner. However, the potential harms caused by systems to stakeholders in complex social contexts and how to address these remains unclear. In this paper, we explain the inherent normative uncertainty in debates about the safety of AI systems. We then address this as a problem of vagueness by examining its place in the design, training, and deployment stages of AI system development. We adopt Ruth Chang's theory of intuitive comparability to illustrate the dilemmas that manifest at each stage. We then discuss how stakeholders can navigate these dilemmas by incorporating distinct forms of dissent into the development pipeline, drawing on Elizabeth Anderson's work on the epistemic powers of democratic institutions. We outline a framework of sociotechnical commitments to formal, substantive and discursive challenges that address normative uncertainty across stakeholders, and propose the cultivation of related virtues by those responsible for development.        △ Less","20 November, 2019","cs.AI,cs.CY,eess.SY",
              Graph-Driven Generative Models for Heterogeneous Multi-Task Learning          ,1911.08709,https://arxiv.org/abs/1911.08709,https://arxiv.org/pdf/1911.08709,"Authors:WenlinWang,HongtengXu,ZheGan,BaiLi,GuoyinWang,LiqunChen,QianYang,WenqiWang,LawrenceCarin","        We propose a novel graph-driven generative model, that unifies multiple heterogeneous learning tasks into the same framework. The proposed model is based on the fact that heterogeneous learning tasks, which correspond to different generative processes, often rely on data with a shared graph structure. Accordingly, our model combines a graph convolutional network (GCN) with multiple variational autoencoders, thus embedding the nodes of the graph i.e., samples for the tasks) in a uniform manner while specializing their organization and usage to different tasks. With a focus on healthcare applications (tasks), including clinical topic modeling, procedure recommendation and admission-type prediction, we demonstrate that our method successfully leverages information across different tasks, boosting performance in all tasks and outperforming existing state-of-the-art approaches.        △ Less","20 November, 2019","cs.LG,stat.ML",
              Towards unstructured mortality prediction with free-text clinical notes          ,1911.08437,https://arxiv.org/abs/1911.08437,https://arxiv.org/pdf/1911.08437,"Authors:MohammadHashir,RapinderSawhney","Healthcare data continues to flourish yet a relatively small portion, mostly structured, is being utilized effectively for predicting clinical outcomes. The rich subjective information available in unstructured clinical notes can possibly facilitate higher discrimination but tends to be under-utilized in mortality prediction. This work attempts to assess the gain in performance when multiple notes that have been minimally preprocessed are used as an input for prediction. A hierarchical architecture consisting of both convolutional and recurrent layers is used to concurrently model the different notes compiled in an individual hospital stay. This approach is evaluated on predicting in-hospital mortality on the MIMIC-III dataset. On comparison to approaches utilizing structured data, it achieved higher metrics despite requiring less cleaning and preprocessing. This demonstrates the potential of unstructured data in enhancing mortality prediction and signifies the need to incorporate more raw unstructured data into current clinical prediction methods.        △ Less","19 November, 2019","cs.LG,cs.AI,cs.CL,stat.ML",
"              iGateLink: A Gateway Library for Linking IoT, Edge, Fog and Cloud Computing Environments          ",1911.08413,https://arxiv.org/abs/1911.08413,https://arxiv.org/pdf/1911.08413,"Authors:RiccardoMancini,ShreshthTuli,TommasoCucinotta,RajkumarBuyya","        In recent years, the Internet of Things (IoT) has been growing in popularity, along with the increasingly important role played by IoT gateways, mediating the interactions among a plethora of heterogeneous IoT devices and cloud services. In this paper, we present iGateLink, an open-source Android library easing the development of Android applications acting as a gateway between IoT devices and Edge/Fog/Cloud Computing environments. Thanks to its pluggable design, modules providing connectivity with a number of devices acting as data sources or Fog/Cloud frameworks can be easily reused for different applications. Using iGateLink in two case-studies replicating previous works in the healthcare and image processing domains, the library proved to be effective in adapting to different scenarios and speeding up the development of gateway applications, as compared to the use of conventional methods.        △ Less","16 November, 2019","eess.SP,cs.DC,cs.SE",
              Exploring the added value of blockchain technology for the healthcare domain          ,1911.08277,https://arxiv.org/abs/1911.08277,https://arxiv.org/pdf/1911.08277,"Authors:BasR.J.Bolmer,MoniqueTaverne,MarcoScherer","        In this report, the University Medical Center Groningen (UMCG) has written down lessons learned on how blockchain technology can have an impact on the healthcare domain. By looking at two use-cases, the hospital challenged several teams, participating in an open innovation program and blockchain hackathon, to find a solution that showed the added value of the technology for patient care and scientific research. Besides this practical perspective, the report also considers literature discussing the current state of blockchain technology in regard to developments in the healthcare domain (touching on patient empowerment, data management, regulations, and interoperability between healthcare systems).        △ Less","15 November, 2019","cs.CY,cs.DB,cs.HC",
"              ""The Human Body is a Black Box"": Supporting Clinical Decision-Making with Deep Learning          ",1911.08089,https://arxiv.org/abs/1911.08089,https://arxiv.org/pdf/1911.08089,"Authors:MarkSendak,MadeleineElish,MichaelGao,JosephFutoma,WilliamRatliff,MarshallNichols,ArmandoBedoya,SureshBalu,CaraO'Brien","        Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to accuracy, fairness, accountability, and transparency that come from actual, situated use. Serious questions remain under examined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing on model interpretability to ensure a fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.        △ Less","6 December, 2019","cs.CY,cs.AI,cs.HC,cs.LG",
              Drug Repurposing for Cancer: An NLP Approach to Identify Low-Cost Therapies          ,1911.07819,https://arxiv.org/abs/1911.07819,https://arxiv.org/pdf/1911.07819,"Authors:ShivashankarSubramanian,IoanaBaldini,SushmaRavichandran,DmitriyA.Katz-Rogozhnikov,KarthikeyanNatesanRamamurthy,PrasannaSattigeri,KushR.Varshney,AnnmarieWang,PradeepMangalath,LauraB.Kleiman","        More than 200 generic drugs approved by the U.S. Food and Drug Administration for non-cancer indications have shown promise for treating cancer. Due to their long history of safe patient use, low cost, and widespread availability, repurposing of generic drugs represents a major opportunity to rapidly improve outcomes for cancer patients and reduce healthcare costs worldwide. Evidence on the efficacy of non-cancer generic drugs being tested for cancer exists in scientific publications, but trying to manually identify and extract such evidence is intractable. In this paper, we introduce a system to automate this evidence extraction from PubMed abstracts. Our primary contribution is to define the natural language processing pipeline required to obtain such evidence, comprising the following modules: querying, filtering, cancer type entity extraction, therapeutic association classification, and study type classification. Using the subject matter expertise on our team, we create our own datasets for these specialized domain-specific tasks. We obtain promising performance in each of the modules by utilizing modern language modeling techniques and plan to treat them as baseline approaches for future improvement of individual components.        △ Less","5 December, 2019","cs.CL,cs.LG,stat.ML",
              Towards Quantification of Bias in Machine Learning for Healthcare: A Case Study of Renal Failure Prediction          ,1911.07679,https://arxiv.org/abs/1911.07679,https://arxiv.org/pdf/1911.07679,"Authors:JosieWilliams,NargesRazavian","        As machine learning (ML) models, trained on real-world datasets, become common practice, it is critical to measure and quantify their potential biases. In this paper, we focus on renal failure and compare a commonly used traditional risk score, Tangri, with a more powerful machine learning model, which has access to a larger variable set and trained on 1.6 million patients' EHR data. We will compare and discuss the generalization and applicability of these two models, in an attempt to quantify biases of status quo clinical practice, compared to ML-driven models.        △ Less","18 November, 2019","cs.LG,stat.AP,stat.ML",
              Justification-Based Reliability in Machine Learning          ,1911.07391,https://arxiv.org/abs/1911.07391,https://arxiv.org/pdf/1911.07391,"Authors:NuraliVirani,NareshIyer,ZhaoyuanYang","        With the advent of Deep Learning, the field of machine learning (ML) has surpassed human-level performance on diverse classification tasks. At the same time, there is a stark need to characterize and quantify reliability of a model's prediction on individual samples. This is especially true in application of such models in safety-critical domains of industrial control and healthcare. To address this need, we link the question of reliability of a model's individual prediction to the epistemic uncertainty of the model's prediction. More specifically, we extend the theory of Justified True Belief (JTB) in epistemology, created to study the validity and limits of human-acquired knowledge, towards characterizing the validity and limits of knowledge in supervised classifiers. We present an analysis of neural network classifiers linking the reliability of its prediction on an input to characteristics of the support gathered from the input and latent spaces of the network. We hypothesize that the JTB analysis exposes the epistemic uncertainty (or ignorance) of a model with respect to its inference, thereby allowing for the inference to be only as strong as the justification permits. We explore various forms of support (for e.g., k-nearest neighbors (k-NN) and l_p-norm based) generated for an input, using the training data to construct a justification for the prediction with that input. Through experiments conducted on simulated and real datasets, we demonstrate that our approach can provide reliability for individual predictions and characterize regions where such reliability cannot be ascertained.        △ Less","21 January, 2020","cs.LG,stat.ML",
              Missingness as Stability: Understanding the Structure of Missingness in Longitudinal EHR data and its Impact on Reinforcement Learning in Healthcare,1911.07084,https://arxiv.org/abs/1911.07084,https://arxiv.org/pdf/1911.07084,"Authors:ScottL.Fleming,KuhanJeyapragasan,TonyDuan,DaisyDing,SaurabhGombar,NigamShah,EmmaBrunskill","        There is an emerging trend in the reinforcement learning for healthcare literature. In order to prepare longitudinal, irregularly sampled, clinical datasets for reinforcement learning algorithms, many researchers will resample the time series data to short, regular intervals and use last-observation-carried-forward (LOCF) imputation to fill in these gaps. Typically, they will not maintain any explicit information about which values were imputed. In this work, we (1) call attention to this practice and discuss its potential implications; (2) propose an alternative representation of the patient state that addresses some of these issues; and (3) demonstrate in a novel but representative clinical dataset that our alternative representation yields consistently better results for achieving optimal control, as measured by off-policy policy evaluation, compared to representations that do not incorporate missingness information.        △ Less","16 November, 2019","cs.LG,cs.AI,stat.ML",
              Fairness With Minimal Harm: A Pareto-Optimal Approach For Healthcare,1911.06935,https://arxiv.org/abs/1911.06935,https://arxiv.org/pdf/1911.06935,"Authors:NataliaMartinez,MartinBertran,GuillermoSapiro","        Common fairness definitions in machine learning focus on balancing notions of disparity and utility. In this work, we study fairness in the context of risk disparity among sub-populations. We are interested in learning models that minimize performance discrepancies across sensitive groups without causing unnecessary harm. This is relevant to high-stakes domains such as healthcare, where non-maleficence is a core principle. We formalize this objective using Pareto frontiers, and provide analysis, based on recent works in fairness, to exemplify scenarios were perfect fairness might not be feasible without doing unnecessary harm. We present a methodology for training neural networks that achieve our goal by dynamically re-balancing subgroups risks. We argue that even in domains where fairness at cost is required, finding a non-unnecessary-harm fairness model is the optimal initial step. We demonstrate this methodology on real case-studies of predicting ICU patient mortality, and classifying skin lesions from dermatoscopic images.        △ Less","15 November, 2019","cs.LG,stat.ML",
              HealthFog: An Ensemble Deep Learning based Smart Healthcare System for Automatic Diagnosis of Heart Diseases in Integrated IoT and Fog Computing Environments          ,1911.06633,https://arxiv.org/abs/1911.06633,https://arxiv.org/pdf/1911.06633,"Authors:ShreshthTuli,NipamBasumatary,SukhpalSinghGill,MohsenKahani,RajeshChandArya,GurpreetSinghWander,RajkumarBuyya","        Cloud computing provides resources over the Internet and allows a plethora of applications to be deployed to provide services for different industries. The major bottleneck being faced currently in these cloud frameworks is their limited scalability and hence inability to cater to the requirements of centralized Internet of Things (IoT) based compute environments. The main reason for this is that latency-sensitive applications like health monitoring and surveillance systems now require computation over large amounts of data (Big Data) transferred to centralized database and from database to cloud data centers which leads to drop in performance of such systems. The new paradigms of fog and edge computing provide innovative solutions by bringing resources closer to the user and provide low latency and energy-efficient solutions for data processing compared to cloud domains. Still, the current fog models have many limitations and focus from a limited perspective on either accuracy of results or reduced response time but not both. We proposed a novel framework called HealthFog for integrating ensemble deep learning in Edge computing devices and deployed it for a real-life application of automatic Heart Disease analysis. HealthFog delivers healthcare as a fog service using IoT devices and efficiently manages the data of heart patients, which comes as user requests. Fog-enabled cloud framework, FogBus is used to deploy and test the performance of the proposed model in terms of power consumption, network bandwidth, latency, jitter, accuracy and execution time. HealthFog is configurable to various operation modes that provide the best Quality of Service or prediction accuracy, as required, in diverse fog computation scenarios and for different user requirements.        △ Less","15 November, 2019","cs.DC,eess.SP",10.1016/j.future.2019.10.043 
              Using natural language processing to extract health-related causality from Twitter messages          ,1911.06488,https://arxiv.org/abs/1911.06488,https://arxiv.org/pdf/1911.06488,"Authors:SonDoan,EllyWYang,SameerTilak,ManabuTorii","        Twitter messages (tweets) contain various types of information, which include health-related information. Analysis of health-related tweets would help us understand health conditions and concerns encountered in our daily life. In this work, we evaluated an approach to extracting causal relations from tweets using natural language processing (NLP) techniques. We focused on three health-related topics: stress"", ""insomnia"", and ""headache"". We proposed a set of lexico-syntactic patterns based on dependency parser outputs to extract causal information. A large dataset consisting of 24 million tweets were used. The results show that our approach achieved an average precision between 74.59% and 92.27%. Analysis of extracted relations revealed interesting findings about health-related in Twitter.        △ Less","15 November, 2019",cs.CL,10.1109/ICHI-W.2018.00031 
"              ""How do I fool you?"": Manipulating User Trust via Misleading Black Box Explanations          ",1911.06473,https://arxiv.org/abs/1911.06473,https://arxiv.org/pdf/1911.06473,"Authors:HimabinduLakkaraju,OsbertBastani","        As machine learning black boxes are increasingly being deployed in critical domains such as healthcare and criminal justice, there has been a growing emphasis on developing techniques for explaining these black boxes in a human interpretable manner. It has recently become apparent that a high-fidelity explanation of a black box ML model may not accurately reflect the biases in the black box. As a consequence, explanations have the potential to mislead human users into trusting a problematic black box. In this work, we rigorously explore the notion of misleading explanations and how they influence user trust in black-box models. More specifically, we propose a novel theoretical framework for understanding and generating misleading explanations, and carry out a user study with domain experts to demonstrate how these explanations can be used to mislead users. Our work is the first to empirically establish how user trust in black box models can be manipulated via misleading explanations.        △ Less","14 November, 2019",cs.AI,
              Federated Learning for Healthcare Informatics          ,1911.06270,https://arxiv.org/abs/1911.06270,https://arxiv.org/pdf/1911.06270,"Authors:JieXu,BenjaminS.Glicksberg,ChangSu,PeterWalker,JiangBian,FeiWang","        With the rapid development of computer software and hardware technologies, more and more healthcare data are becoming readily available from clinical institutions, patients, insurance companies and pharmaceutical industries, among others. This access provides an unprecedented opportunity for data science technologies to derive data-driven insights and improve the quality of care delivery. Healthcare data, however, are usually fragmented and private making it difficult to generate robust results across populations. For example, different hospitals own the electronic health records (EHR) of different patient populations and these records are difficult to share across hospitals because of their sensitive nature. This creates a big barrier for developing effective analytical approaches that are generalizable, which need diverse, ""big data"". Federated learning, a mechanism of training a shared global model with a central server while keeping all the sensitive data in local institutions where the data belong, provides great promise to connect the fragmented healthcare data sources with privacy-preservation. The goal of this survey is to provide a review for federated learning technologies, particularly within the biomedical space. In particular, we summarize the general solutions to the statistical challenges, system challenges and privacy issues in federated learning, and point out the implications and potentials in healthcare.        △ Less","20 August, 2020",cs.LG,
              SAVEHR: Self Attention Vector Representations for EHR based Personalized Chronic Disease Onset Prediction and Interpretability          ,1911.05370,https://arxiv.org/abs/1911.05370,https://arxiv.org/pdf/1911.05370,"Authors:SunilMallya,MarcOverhage,SravanBodapati,NavneetSrivastava,SahikaGenc","        Chronic disease progression is emerging as an important area of investment for healthcare providers. As the quantity and richness of available clinical data continue to increase along with advances in machine learning, there is great potential to advance our approaches to caring for patients. An ideal approach to this problem should generate good performance on at least three axes namely, a) perform across many clinical conditions without requiring deep clinical expertise or extensive data scientist effort, b) generalization across populations, and c) be explainable (model interpretability). We present SAVEHR, a self-attention based architecture on heterogeneous structured EHR data that achieves >> 0.51 AUC-PR and >> 0.87 AUC-ROC gains on predicting the onset of four clinical conditions (CHF, Kidney Failure, Diabetes and COPD) 15-months in advance, and transfers with high performance onto a new population. We demonstrate that SAVEHR model performs superior to ten baselines on all three axes stated formerly.        △ Less","13 November, 2019","cs.LG,cs.AI",
              Managing access to primary care clinics using robust scheduling templates          ,1911.05129,https://arxiv.org/abs/1911.05129,https://arxiv.org/pdf/1911.05129,"Authors:SinaFaridimehr,SaravananVenkatachalam,RatnaBabuChinnam","        An important challenge confronting healthcare is the effective management of access to primary care. Robust appointment scheduling policies/templates can help strike an effective balance between the lead-time to an appointment (a.k.a. indirect waiting time, measuring the difference between a patient's desired and actual appointment dates) and waiting times at the clinic on the day of the appointment (a.k.a. direct waiting time). We propose methods for identifying effective appointment scheduling templates using a two-stage stochastic mixed-integer linear program model. The model embeds simulation for accurate evaluation of direct waiting times and uses sample average approximation method for computational efficiency. The model accounts for patients' no-show behaviors, provider availability, overbooking, demand uncertainty, and overtime constraints. The model allows the scheduling templates to be potentially updated at regular intervals while minimizing the patient expected waiting times and balancing provider utilization. Proposed methods are validated using data from the U.S. Department of Veterans Affairs (VA) primary care clinics.        △ Less","12 November, 2019",math.OC,
              Harmonic Mean Point Processes: Proportional Rate Error Minimization for Obtundation Prediction          ,1911.05109,https://arxiv.org/abs/1911.05109,https://arxiv.org/pdf/1911.05109,"Authors:YoonjungKim,JeremyC.Weiss","        In healthcare, the highest risk individuals for morbidity and mortality are rarely those with the greatest modifiable risk. By contrast, many machine learning formulations implicitly attend to the highest risk individuals. We focus on this problem in point processes, a popular modeling technique for the analysis of the temporal event sequences in electronic health records (EHR) data with applications in risk stratification and risk score systems. We show that optimization of the log-likelihood function also gives disproportionate attention to high risk individuals and leads to poor prediction results for low risk individuals compared to ones at high risk. We characterize the problem and propose an adjusted log-likelihood formulation as a new objective for point processes. We demonstrate the benefits of our method in simulations and in EHR data of patients admitted to the critical care unit for intracerebral hemorrhage.        △ Less","14 November, 2019","stat.ML,cs.AI,cs.LG",
              Superlubric Nanogenerators with Superb Performances          ,1911.04764,https://arxiv.org/abs/1911.04764,https://arxiv.org/pdf/1911.04764,"Authors:XuanyuHuang,LiLin,QuanshuiZheng","        Nanogenerators promise self-powered sensors and devices for extensive applications in internet of things, sensor networks, big data, personal healthcare systems, artificial intelligence, et al. However, low electric current densities and short product lifespans have blocked nanogenerators' applications. Here we show that structural superlubricity, a state of nearly zero friction and wear between two contacted solid surfaces, provides a revolutionary solution to the above challenge. We investigate three types of superlubric nanogenerators (SLNGs), namely the capacitor-based, triboelectric, and electret-based SLNGs, and systematically analyze the influences of material and structural parameters to these SLNGs' performances. We demonstrate that SLNGs can achieve not only enduring lifespans, but also superb performances - three orders of magnitude in current densities and output powers higher than those of conventional nanogenerators. Furthermore, SLNGs can be driven by very weak external loads (down to ~1 μμN) in very low frequencies (down to ~1 μμHz), and are thus capable to harvest electric energies from an extremely board spectrum of environments and biosystems. Among the three types of SLNGs, the capacitor-based is synthetically most competitive in the senses of performance, fabrication and maintaining. These results can guide designs and accelerate fabrications of SLNGs toward real applications.        △ Less","12 November, 2019",physics.app-ph,
              iGLU 1.0: An Accurate Non-Invasive Near-Infrared Dual Short Wavelengths Spectroscopy based Glucometer for Smart Healthcare,1911.04471,https://arxiv.org/abs/1911.04471,https://arxiv.org/pdf/1911.04471,"Authors:PrateekJain,AmitM.Joshi,SarajuP.Mohanty","        In the case of diabetes, fingertip pricking for a blood sample is inconvenient for glucose measurement. Invasive approaches like laboratory test and one-touch glucometer enhance the risk of blood-related infections. To mitigate this important issue, in the current paper, we propose a novel Internet-of-Medical-Things (IoMT) enabled edge-device for precise, non-invasive blood glucose measurement. In this work, a near-infrared (NIR) spectroscopic technique using two wavelengths (940 nm, 1300 nm) is taken to detect the glucose molecule from human blood. The novel device called iGLU is based on NIR spectroscopy and machine learning (ML) models of high accuracy. An optimal multiple polynomial regression model and deep neural network (DNN) model have been presented for precise measurement. The proposed device is validated and blood glucose values are stored on the cloud using open IoT platform for remote monitoring by an endocrinologist. For device validation, the estimated blood glucose values have been compared with blood glucose values obtained from the invasive device. It has been observed that mean absolute relative difference (MARD) and average error (AvgE) are found 4.66 % and 4.61 % respectively from predicted blood glucose concentration values. The regression coefficient is found 0.81. The proposed spectroscopic non-invasive device provides accurate and cost-effective solution for smart healthcare.        △ Less","10 November, 2019","eess.SP,eess.SY",
              Does hospital cooperation increase the quality of healthcare?          ,1911.04168,https://arxiv.org/abs/1911.04168,https://arxiv.org/pdf/1911.04168,"Authors:PaoloBerta,VeronicaVinciotti,FrancescoMoscone","        Motivated by reasons such as altruism, managers from different hospitals may engage in cooperative behaviours, which shape the networked healthcare economy. In this paper we study the determinants of hospital cooperation and its association with the quality delivered by hospitals, using Italian administrative data. We explore the impact on patient transfers between hospitals (cooperation/network) of a set of demand-supply factors, as well as distance-based centrality measures. We then use this framework to assess how such cooperation is related to the overall quality for the hospital of origin and of destination of the patient transfer. The over-dispersed Poisson mixed model that we propose, inspired by the literature on social relations models, is suitably defined to handle network data, which are rarely used in health economics. The results show that distance plays an important role in hospital cooperation, though there are other factors that matter such as geographical centrality. Another empirical finding is the existence of a positive relationship between hospital cooperation and the overall quality of the connected hospitals. The absence of a source of information on the quality of hospitals accessible to all providers, such as in the form of star ratings, may prevent some hospitals to engage and cooperate with other hospitals of potentially higher quality. This may result in a lower degree of cooperation among hospitals and a reduction in quality overall.        △ Less","11 November, 2019",stat.AP,
"              Estimating length of hospital stay, with regard to associated factors; A model to enhance healthcare service efficiency and reduce healthcare resource consumption          ",1911.03963,https://arxiv.org/abs/1911.03963,https://arxiv.org/pdf/1911.03963,"Authors:SeyedNasserMoosavi,AshkanKhalifeh,AliShojaee,MasoudAbessi","        To assess the efficiency and the resource consumption level in healthcare scope, many economic and social factors have to be considered. An index which has recently been studied by the researchers, is length of hospital stay (LOS) defined as how long each patient is hospitalized. Detecting and controlling the factors affecting this index can help to reduce healthcare costs and also to improve healthcare service efficiency. The effects of three major factors, say, the season during which the patient is hospitalized, the patients age and his/her gender on LOS pertaining to 82718 patients receiving healthcare service from the hospitals under contract to insurance organization in Tehran, have been analyzed, using unbalanced factorial design. The test results imply that the whole model is significant (P-value=0<0.01), the separate effects of all three factors on LOS are significant (P-value=0<0.01) and the only significant interaction at alpha=0.01 is the one between gender and age group (P-value=0<0.01). moreover, no two age groups have the significant equal means and age groups 1-10 and 26-40 years possess the minimum and maximum means of LOS, respectively. LOS means in winter and autumn are equal, being the maximum. Due to the significance of these effects, allocating specified budget and resources with regard to these factors will help hospitals and healthcare centers enhance their efficiency and remain within their budget.        △ Less","10 November, 2019",stat.AP,
              Analyzing the impact of two major factors on medical expenses paid by health insurance organization in Iran          ,1911.03961,https://arxiv.org/abs/1911.03961,https://arxiv.org/pdf/1911.03961,"Authors:SeyedNasserMoosavi,AshkanKhalifeh,AliShojaee,MasoudAbessi","        In healthcare scope, the profound role of insurance companies is undeniable. Health insurance establishments main responsibility is to support public health financially and promote the quality of health services. Governments subsidies to healthcare insurance, insured payments and insurance companies costs must be specified in such a way that both people and insurers mutually benefit. In this research, we propose a model for determining healthcare costs paid by health insurance organization with regard to two major factors, the geographical regions where the patients live and the seasons when they receive service, using two-way ANOVA method. Since both effects are found to be significant, allocating different insurance costs to the people residing in different regions, and also changing the patterns of insurance extension in different seasons with regard to the results derived from the research, can detract healthcare costs and give more satisfaction to the lower income insured patients.        △ Less","10 November, 2019",stat.AP,
              Discovering Invariances in Healthcare Neural Networks          ,1911.03295,https://arxiv.org/abs/1911.03295,https://arxiv.org/pdf/1911.03295,"Authors:MohammadTahaBahadori,LayneC.Price","        We study the invariance characteristics of pre-trained predictive models by empirically learning transformations on the input that leave the prediction function approximately unchanged. To learn invariant transformations, we minimize the Wasserstein distance between the predictive distribution conditioned on the data instances and the predictive distribution conditioned on the transformed data instances. To avoid finding degenerate or perturbative transformations, we add a similarity regularization to discourage similarity between the data and its transformed values. We theoretically analyze the correctness of the algorithm and the structure of the solutions. Applying the proposed technique to clinical time series data, we discover variables that commonly-used LSTM models do not rely on for their prediction, especially when the LSTM is trained to be adversarially robust. We also analyze the invariances of BioBERT on clinical notes and discover words that it is invariant to.        △ Less","3 March, 2020","cs.LG,q-bio.QM,stat.ML",
              AI Aided Noise Processing of Spintronic Based IoT Sensor for Magnetocardiography Application          ,1911.03127,https://arxiv.org/abs/1911.03127,https://arxiv.org/pdf/1911.03127,"Authors:AttayebMohsen,MuftahAl-Mahdawi,MostafaM.Fouda,MikihikoOogane,YasuoAndo,ZubairMdFadlullah","        As we are about to embark upon the highly hyped ""Society 5.0"", powered by the Internet of Things (IoT), traditional ways to monitor human heart signals for tracking cardio-vascular conditions are challenging, particularly in remote healthcare settings. On the merits of low power consumption, portability, and non-intrusiveness, there are no suitable IoT solutions that can provide information comparable to the conventional Electrocardiography (ECG). In this paper, we propose an IoT device utilizing a spintronic ultra-sensitive sensor that measures the magnetic fields produced by cardio-vascular electrical activity, i.e. Magentocardiography (MCG). After that, we treat the low-frequency noise generated by the sensors, which is also a challenge for most other sensors dealing with low-frequency bio-magnetic signals. Instead of relying on generic signal processing techniques such as averaging or filtering, we employ deep-learning training on bio-magnetic signals. Using an existing dataset of ECG records, MCG labels are synthetically constructed. A unique deep learning structure composed of combined Convolutional Neural Network (CNN) with Gated Recurrent Unit (GRU) is trained using the labeled data moving through a striding window, which is able to smartly capture and eliminate the noise features. Simulation results are reported to evaluate the effectiveness of the proposed method that demonstrates encouraging performance.        △ Less","10 June, 2020","eess.SP,cs.LG,physics.app-ph",
              Transfer Learning in 4D for Breast Cancer Diagnosis using Dynamic Contrast-Enhanced Magnetic Resonance Imaging          ,1911.03022,https://arxiv.org/abs/1911.03022,https://arxiv.org/pdf/1911.03022,"Authors:QiyuanHu,HeatherM.Whitney,MaryellenL.Giger","        Deep transfer learning using dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) has shown strong predictive power in characterization of breast lesions. However, pretrained convolutional neural networks (CNNs) require 2D inputs, limiting the ability to exploit the rich 4D (volumetric and temporal) image information inherent in DCE-MRI that is clinically valuable for lesion assessment. Training 3D CNNs from scratch, a common method to utilize high-dimensional information in medical images, is computationally expensive and is not best suited for moderately sized healthcare datasets. Therefore, we propose a novel approach using transfer learning that incorporates the 4D information from DCE-MRI, where volumetric information is collapsed at feature level by max pooling along the projection perpendicular to the transverse slices and the temporal information is contained either in second-post contrast subtraction images. Our methodology yielded an area under the receiver operating characteristic curve of 0.89+/-0.01 on a dataset of 1161 breast lesions, significantly outperforming a previous approach that incorporates the 4D information in DCE-MRI by the use of maximum intensity projection (MIP) images.        △ Less","7 November, 2019","physics.med-ph,cs.CV,cs.LG,eess.IV",
              Priority Quality Attributes for Engineering AI-enabled Systems          ,1911.02912,https://arxiv.org/abs/1911.02912,https://arxiv.org/pdf/1911.02912,"Authors:LenaPons,IpekOzkaya","        Deploying successful software-reliant systems that address their mission goals and user needs within cost, resource, and expected quality constraints require design trade-offs. These trade-offs dictate how systems are structured and how they behave and consequently can effectively be evolved and sustained. Software engineering practices address this challenge by centering system design and evolution around delivering key quality attributes, such as security, privacy, data centricity, sustainability, and explainability. These concerns are more urgent requirements for software-reliant systems that also include AI components due to the uncertainty introduced by data elements. Moreover, systems employed by the public sector exhibit unique design time and runtime challenges due to the regulatory nature of the domains. We assert that the quality attributes of security, privacy, data centricity, sustainability, and explainability pose new challenges to AI engineering and will drive the success of AI-enabled systems in the public sector. In this position paper, we enumerate with examples from healthcare domain concerns related to these requirements to mitigate barriers to architecting and fielding AI-enabled systems in the public sector.        △ Less","15 October, 2019",cs.OH,
              Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods          ,1911.02508,https://arxiv.org/abs/1911.02508,https://arxiv.org/pdf/1911.02508,"Authors:DylanSlack,SophieHilgard,EmilyJia,SameerSingh,HimabinduLakkaraju","        As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real-world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.        △ Less","3 February, 2020","cs.LG,cs.AI,stat.ML",
              A Survey of Blockchain Applications in Different Domains          ,1911.02013,https://arxiv.org/abs/1911.02013,https://arxiv.org/pdf/1911.02013,"Authors:WubingChen,ZhiyingXu,ShuyuShi,YangZhao,JunZhao","        Blockchains have received much attention recently since they provide decentralized approaches to the creation and management of value. Many banks, Internet companies, car manufacturers, and even governments worldwide have incorporated or started considering blockchains to improve the security, scalability, and efficiency of their services. In this paper, we survey blockchain applications in different areas. These areas include cryptocurrency, healthcare, advertising, insurance, copyright protection, energy, and societal applications. Our work provides a timely summary for individuals and organizations interested in blockchains. We envision our study to motivate more blockchain applications.        △ Less","5 November, 2019","cs.CR,cs.CY,cs.DC",10.1145/3301403.3301407 
              Fourth Industrial Revolution for Development: The Relevance of Cloud Federation in Healthcare Support          ,1911.01708,https://arxiv.org/abs/1911.01708,https://arxiv.org/pdf/1911.01708,"Authors:OlasupoO.Ajayi,AntoineB.Bagula,KunMa","        Inefficient healthcare is a major concern among many African nations and can be mitigated by building world-class infrastructure connecting different medical facilities for collaboration and resource sharing. Such infrastructure should support collection and exchange of medical data for the purpose of accessing expertise not available locally. It should be equipped with modern technologies of the fourth industrial revolution, providing decision support to doctors thereby enabling African nations leapfrog from poorly equipped to medically prepared. Sadly, world-class healthcare infrastructure are a missing piece in the African public health ecosystem. Medical facilities are either non-existent or prohibitively expensive when they exist. Federated cloud computing can provide a solution to this challenge. Being a model that allows collaboration between multiple Cloud service providers through resources pooling; it allows for the execution of tasks on computing resources flexibly and cost efficiently. This paper aims to connect unconnected medical facilities in Africa by proposing a Cloud federation for healthcare using cooperative and competitive collaboration models. Simulations were carried out to test the efficacy of these models using five different workload allocation schemes: First-Fit-Descending (FFD), Best-Fit-Descending (BFD), Binary-Search-Best-Fit (BSBF); Genetic Algorithm meta-heuristic and Stable Roommate Allocation economic model for both light and heavy workloads. Results of simulations revealed that the cooperative model resulted in lower delays but higher resource utilisation; while the competitive provided faster service delivery and better quality of service. BSBF and BFD resulted in the best resources utilisation and energy conservation. Finally, deployment considerations and potential business models for federated Cloud for African healthcare were presented.        △ Less","5 November, 2019","cs.DC,cs.CY",
              Understanding racial bias in health using the Medical Expenditure Panel Survey data          ,1911.01509,https://arxiv.org/abs/1911.01509,https://arxiv.org/pdf/1911.01509,"Authors:MoninderSingh,KarthikeyanNatesanRamamurthy","        Over the years, several studies have demonstrated that there exist significant disparities in health indicators in the United States population across various groups. Healthcare expense is used as a proxy for health in algorithms that drive healthcare systems and this exacerbates the existing bias. In this work, we focus on the presence of racial bias in health indicators in the publicly available, and nationally representative Medical Expenditure Panel Survey (MEPS) data. We show that predictive models for care management trained using this data inherit this bias. Finally, we demonstrate that this inherited bias can be reduced significantly using simple mitigation techniques.        △ Less","4 November, 2019","cs.LG,cs.CY,stat.ML",
"              Digital Twin: Enabling Technologies, Challenges and Open Research          ",1911.01276,https://arxiv.org/abs/1911.01276,https://arxiv.org/pdf/1911.01276,"Authors:AidanFuller,ZhongFan,CharlesDay,ChrisBarlow","        Digital Twin technology is an emerging concept that has become the centre of attention for industry and, in more recent years, academia. The advancements in industry 4.0 concepts have facilitated its growth, particularly in the manufacturing industry. The Digital Twin is defined extensively but is best described as the effortless integration of data between a physical and virtual machine in either direction. The challenges, applications, and enabling technologies for Artificial Intelligence, Internet of Things (IoT) and Digital Twins are presented. A review of publications relating to Digital Twins is performed, producing a categorical review of recent papers. The review has categorised them by research areas: manufacturing, healthcare and smart cities, discussing a range of papers that reflect these areas and the current state of research. The paper provides an assessment of the enabling technologies, challenges and open research for Digital Twins.        △ Less","18 June, 2020",cs.CY,10.1109/ACCESS.2020.2998358 
              Validation of a deep learning mammography model in a population with low screening rates          ,1911.00364,https://arxiv.org/abs/1911.00364,https://arxiv.org/pdf/1911.00364,"Authors:KevinWu,EricWu,YapingWu,HongnaTan,GregSorensen,MeiyunWang,BillLotter","        A key promise of AI applications in healthcare is in increasing access to quality medical care in under-served populations and emerging markets. However, deep learning models are often only trained on data from advantaged populations that have the infrastructure and resources required for large-scale data collection. In this paper, we aim to empirically investigate the potential impact of such biases on breast cancer detection in mammograms. We specifically explore how a deep learning algorithm trained on screening mammograms from the US and UK generalizes to mammograms collected at a hospital in China, where screening is not widely implemented. For the evaluation, we use a top-scoring model developed for the Digital Mammography DREAM Challenge. Despite the change in institution and population composition, we find that the model generalizes well, exhibiting similar performance to that achieved in the DREAM Challenge, even when controlling for tumor size. We also illustrate a simple but effective method for filtering predictions based on model variance, which can be particularly useful for deployment in new settings. While there are many components in developing a clinically effective system, these results represent a promising step towards increasing access to life-saving screening mammography in populations where screening rates are currently low.        △ Less","1 November, 2019","eess.IV,cs.CV",
              Towards Robust Deep Neural Networks for Affect and Depression Recognition          ,1911.00310,https://arxiv.org/abs/1911.00310,https://arxiv.org/pdf/1911.00310,"Authors:EmnaRejaibi,DaoudKadoch,KamilBentounes,RomainAlfred,MohamedDaoudi,AbdenourHadid,AliceOthmani","        Intelligent monitoring systems and affective computing applications have emerged in recent years to enhance healthcare. Examples of these applications include assessment of affective states such as Major Depressive Disorder (MDD). MDD describes the constant expression of certain emotions: negative emotions (low Valence) and lack of interest (low Arousal). High-performing intelligent systems would enhance MDD diagnosis in its early stages. In this paper, we present a new deep neural network architecture, called EmoAudioNet, for emotion and depression recognition from speech. Deep EmoAudioNet learns from the time-frequency representation of the audio signal and the visual representation of its spectrum of frequencies. Our model outperforms the state-of-the-art methods for RECOLA and for DAIC-WOZ datasets and it reaches high accuracies of 89.30%, 91.44% and 73.25% in predicting arousal, valence, and depression, respectively.        △ Less","17 April, 2020","cs.HC,cs.SD,eess.AS",
              Implementation of an Index Optimize Technology for Highly Specialized Terms based on the Phonetic Algorithm Metaphone          ,1911.00152,https://arxiv.org/abs/1911.00152,https://arxiv.org/pdf/1911.00152,"Authors:V.Buriachok,M.Hadzhyiev,V.Sokolov,P.Skladannyi,L.Kuzmenko","        When compiling databases, for example to meet the needs of healthcare establishments, there is quite a common problem with the introduction and further processing of names and last names of doctors and patients that are highly specialized both in terms of pronunciation and writing. This is because names and last names of people cannot be unique, their notation is not subject to any rules of phonetics, while their length in different languages may not match. With the advent of the Internet, this situation has become generally critical and can lead to that multiple copies of e-mails are sent to one address. It is possible to solve the specified problem by using phonetic algorithms for comparing words Daitch-Mokotoff, Soundex, NYSIIS, Polyphone, and Metaphone, as well as the Levenshtein and Jaro algorithms, Q-gram-based algorithms, which make it possible to find distances between words. The most widespread among them are the Soundex and Metaphone algorithms, which are designed to index the words based on their sound, taking into consideration the rules of pronunciation. By applying the Metaphone algorithm, an attempt has been made to optimize the phonetic search processes for tasks of fuzzy coincidence, for example, at data deduplication in various databases and registries, in order to reduce the number of errors of incorrect input of last names. An analysis of the most common last names reveals that some of them are of the Ukrainian or Russian origin. At the same time, the rules following which the names are pronounced and written, for example in Ukrainian, differ radically from basic algorithms for English and differ quite significantly for the Russian language. That is why a phonetic algorithm should take into consideration first of all the peculiarities in the formation of Ukrainian last names, which is of special relevance now.        △ Less","31 October, 2019",cs.CL,10.15587/1729-4061.2019.181943 
              Digital Twin approach to Clinical DSS with Explainable AI          ,1910.13520,https://arxiv.org/abs/1910.13520,https://arxiv.org/pdf/1910.13520,"Authors:DattarajJagdishRao,ShraddhaMane","        We propose a digital twin approach to improve healthcare decision support systems with a combination of domain knowledge and data. Domain knowledge helps build decision thresholds that doctors can use to determine a risk or recommend a treatment or test based on the specific patient condition. However, these assessments tend to be highly subjective and differ from doctor to doctor and from patient to patient. We propose a system where we collate this subjective risk by compiling data from different doctors treating different patients and build a machine learning model that learns from this knowledge. Then using state-of-the-art explainability concepts we derive explanations from this model. These explanations give us a summary of different doctor domain knowledge applied in different cases to give a more generic perspective. Also these explanations are specific to a particular patient and are customized for their condition. This is a form of a digital twin for the patient that can now be used to enhance decision boundaries for earlier defined decision tables that help in diagnosis. We will show an example of running this analysis for a liver disease risk diagnosis.        △ Less","22 October, 2019","cs.AI,cs.LG",
              Model enhancement and personalization using weakly supervised learning for multi-modal mobile sensing          ,1910.13401,https://arxiv.org/abs/1910.13401,https://arxiv.org/pdf/1910.13401,"Authors:DiyanTeng,RashmiKulkarni,JustinMcGloin","        Always-on sensing of mobile device user's contextual information is critical to many intelligent use cases nowadays such as healthcare, drive assistance, voice UI. State-of-the-art approaches for predicting user context have proved the value to leverage multiple sensing modalities for better accuracy. However, those context inference algorithms that run on application processor nowadays tend to drain heavy amount of power, making them not suitable for an always-on implementation. We claim that not every sensing modality is suitable to be activated all the time and it remains challenging to build an inference engine using power friendly sensing modalities. Meanwhile, due to the diverse population, we find it challenging to learn a context inference model that generalizes well, with limited training data, especially when only using always-on low power sensors. In this work, we propose an approach to leverage the opportunistically-on counterparts in device to improve the always-on prediction model, leading to a personalized solution. We model this problem using a weakly supervised learning framework and provide both theoretical and experimental results to validate our design. The proposed framework achieves satisfying result in the IMU based activity recognition application we considered.        △ Less","29 October, 2019","stat.ML,cs.LG,eess.SP",
              A blockchain-orchestrated Federated Learning architecture for healthcare consortia          ,1910.12603,https://arxiv.org/abs/1910.12603,https://arxiv.org/pdf/1910.12603,"Authors:JonathanPasserat-Palmbach,TylerFarnan,RobertMiller,MarielleS.Gross,HeatherLeighFlannery,BillGleim","        We propose a novel architecture for federated learning within healthcare consortia. At the heart of the solution is a unique integration of privacy preserving technologies, built upon native enterprise blockchain components available in the Ethereum ecosystem. We show how the specific characteristics and challenges of healthcare consortia informed our design choices, notably the conception of a new Secure Aggregation protocol assembled with a protected hardware component and an encryption toolkit native to Ethereum. Our architecture also brings in a privacy preserving audit trail that logs events in the network without revealing identities.        △ Less","12 October, 2019","cs.CY,cs.CR,cs.LG",
              Human-AI Co-Learning for Data-Driven AI          ,1910.12544,https://arxiv.org/abs/1910.12544,https://arxiv.org/pdf/1910.12544,"Authors:Yi-ChingHuang,Yu-TingCheng,Lin-LinChen,JaneYung-jenHsu","        Human and AI are increasingly interacting and collaborating to accomplish various complex tasks in the context of diverse application domains (e.g., healthcare, transportation, and creative design). Two dynamic, learning entities (AI and human) have distinct mental model, expertise, and ability; such fundamental difference/mismatch offers opportunities for bringing new perspectives to achieve better results. However, this mismatch can cause unexpected failure and result in serious consequences. While recent research has paid much attention to enhancing interpretability or explainability to allow machine to explain how it makes a decision for supporting humans, this research argues that there is urging the need for both human and AI should develop specific, corresponding ability to interact and collaborate with each other to form a human-AI team to accomplish superior results. This research introduces a conceptual framework called ""Co-Learning,"" in which people can learn with/from and grow with AI partners over time. We characterize three key concepts of co-learning: ""mutual understanding,"" ""mutual benefits,"" and ""mutual growth"" for facilitating human-AI collaboration on complex problem solving. We will present proof-of-concepts to investigate whether and how our approach can help human-AI team to understand and benefit each other, and ultimately improve productivity and creativity on creative problem domains. The insights will contribute to the design of Human-AI collaboration.        △ Less","28 October, 2019","cs.HC,cs.AI",
              Attention-Gated Graph Convolutions for Extracting Drug Interaction Information from Drug Labels          ,1910.12419,https://arxiv.org/abs/1910.12419,https://arxiv.org/pdf/1910.12419,"Authors:TungTran,RamakanthKavuluru,HalilKilicoglu","        Preventable adverse events as a result of medical errors present a growing concern in the healthcare system. As drug-drug interactions (DDIs) may lead to preventable adverse events, being able to extract DDIs from drug labels into a machine-processable form is an important step toward effective dissemination of drug safety information. In this study, we tackle the problem of jointly extracting drugs and their interactions, including interaction outcome, from drug labels. Our deep learning approach entails composing various intermediate representations including sequence and graph based context, where the latter is derived using graph convolutions (GC) with a novel attention-based gating mechanism (holistically called GCA). These representations are then composed in meaningful ways to handle all subtasks jointly. To overcome scarcity in training data, we additionally propose transfer learning by pre-training on related DDI data. Our model is trained and evaluated on the 2018 TAC DDI corpus. Our GCA model in conjunction with transfer learning performs at 39.20% F1 and 26.09% F1 on entity recognition (ER) and relation extraction (RE) respectively on the first official test set and at 45.30% F1 and 27.87% F1 on ER and RE respectively on the second official test set corresponding to an improvement over our prior best results by up to 6 absolute F1 points. After controlling for available training data, our model exhibits state-of-the-art performance by improving over the next comparable best outcome by roughly three F1 points in ER and 1.5 F1 points in RE evaluation across two official test sets.        △ Less","4 November, 2019",cs.CL,
              Generation of digital patients for the simulation of tuberculosis with UISS-TB          ,1910.12293,https://arxiv.org/abs/1910.12293,https://arxiv.org/pdf/1910.12293,"Authors:MarzioPennisi,MiguelA.Juarez,GiuliaRusso,MarcoViceconti,FrancescoPappalardo","        EC funded STriTuVaD project aims to test, through a phase IIb clinical trial, two of the most advanced therapeutic vaccines against tuberculosis. In parallel, we have extended the Universal Immune System Simulator to include all relevant determinants of such clinical trial, to establish its predictive accuracy against the individual patients recruited in the trial, to use it to generate digital patients and predict their response to the HRT being tested, and to combine them to the observations made on physical patients using a new in silico-augmented clinical trial approach that uses a Bayesian adaptive design. This approach, where found effective could drastically reduce the cost of innovation in this critical sector of public healthcare. One of the most challenging task is to develop a methodology to reproduce biological diversity of the subjects that have to be simulated, i.e., provide an appropriate strategy for the generation of libraries of digital patients. This has been achieved through the the creation of the initial immune system repertoire in a stochastic way, and though the identification of a ""vector of features"" that combines both biological and pathophysiological parameters that personalize the digital patient to reproduce the physiology and the pathophysiology of the subject.        △ Less","27 October, 2019","q-bio.QM,cs.MA",
              Inequality in Turkey: Looking Beyond Growth          ,1910.11780,https://arxiv.org/abs/1910.11780,https://arxiv.org/pdf/1910.11780,"Authors:BayramCakir,IpekErgul","        This paper investigates the relationships between economic growth, investment in human capital and income equality in Turkey. The conclusion drawn based on the data from the OECD and the World Bank suggests that economic growth can improve income equality depending on the expenditures undertaken by the government. As opposed to the standard view that economic growth and income inequality are positively related, the findings of this paper suggest that other factors such as education and healthcare spending are also driving factors of income inequality in Turkey. The proven positive impact of investment in education and health care on income equality could aid policymakers who aim to achieve fairer income equality and economic growth, in investment decisions.        △ Less","25 October, 2019",econ.GN,
"              Substra: a framework for privacy-preserving, traceable and collaborative Machine Learning          ",1910.11567,https://arxiv.org/abs/1910.11567,https://arxiv.org/pdf/1910.11567,"Authors:MathieuNGaltier,CamilleMarini","        Machine learning is promising, but it often needs to process vast amounts of sensitive data which raises concerns about privacy. In this white-paper, we introduce Substra, a distributed framework for privacy-preserving, traceable and collaborative Machine Learning. Substra gathers data providers and algorithm designers into a network of nodes that can train models on demand but under advanced permission regimes. To guarantee data privacy, Substra implements distributed learning: the data never leave their nodes; only algorithms, predictive models and non-sensitive metadata are exchanged on the network. The computations are orchestrated by a Distributed Ledger Technology which guarantees traceability and authenticity of information without needing to trust a third party. Although originally developed for Healthcare applications, Substra is not data, algorithm or programming language specific. It supports many types of computation plans including parallel computation plan commonly used in Federated Learning. With appropriate guidelines, it can be deployed for numerous Machine Learning use-cases with data or algorithm providers where trust is limited.        △ Less","25 October, 2019","cs.CR,cs.LG",
Healthcare NER Models Using Language Model Pretraining          ,1910.11241,https://arxiv.org/abs/1910.11241,https://arxiv.org/pdf/1910.11241,"Authors:AmoghKamatTarcar,AashisTiwari,VineetNaiqueDhaimodker,PenjoRebelo,RahulDesai,DattarajRao","        In this paper, we present our approach to extracting structured information from unstructured Electronic Health Records (EHR) [2] which can be used to, for example, study adverse drug reactions in patients due to chemicals in their products. Our solution uses a combination of Natural Language Processing (NLP) techniques and a web-based annotation tool to optimize the performance of a custom Named Entity Recognition (NER) [1] model trained on a limited amount of EHR training data. This work was presented at the first Health Search and Data Mining Workshop (HSDM 2020) [26]. We showcase a combination of tools and techniques leveraging the recent advancements in NLP aimed at targeting domain shifts by applying transfer learning and language model pre-training techniques [3]. We present a comparison of our technique to the current popular approaches and show the effective increase in performance of the NER model and the reduction in time to annotate data.A key observation of the results presented is that the F1 score of model (0.734) trained with our approach with just 50% of available training data outperforms the F1 score of the blank spaCy model without language model component (0.704) trained with 100% of the available training data. We also demonstrate an annotation tool to minimize domain expert time and the manual effort required to generate such a training dataset. Further, we plan to release the annotated dataset as well as the pre-trained model to the community to further research in medical health records.        △ Less","29 January, 2020","cs.CL,cs.IR,cs.LG",
              Sensor fusion using EMG and vision for hand gesture classification in mobile applications          ,1910.11126,https://arxiv.org/abs/1910.11126,https://arxiv.org/pdf/1910.11126,"Authors:EneaCeolini,GemmaTaverni,LyesKhacef,MelikaPayvand,ElisaDonati","        The discrimination of human gestures using wearable solutions is extremely important as a supporting technique for assisted living, healthcare of the elderly and neurorehabilitation. This paper presents a mobile electromyography (EMG) analysis framework to be an auxiliary component in physiotherapy sessions or as a feedback for neuroprosthesis calibration. We implemented a framework that allows the integration of multisensors, EMG and visual information, to perform sensor fusion and to improve the accuracy of hand gesture recognition tasks. In particular, we used an event-based camera adapted to run on the limited computational resources of mobile phones. We introduced a new publicly available dataset of sensor fusion for hand gesture recognition recorded from 10 subjects and used it to train the recognition models offline. We compare the online results of the hand gesture recognition using the fusion approach with the individual sensors with an improvement in the accuracy of 13% and 11%, for EMG and vision respectively, reaching 85%.        △ Less","18 October, 2019","cs.CV,cs.LG,eess.IV,eess.SP",
              Toward estimating personal well-being using voice          ,1910.10082,https://arxiv.org/abs/1910.10082,https://arxiv.org/pdf/1910.10082,"Authors:SamuelKim,NamheeKwon,HenryO'Connell","        Estimating personal well-being draws increasing attention particularly from healthcare and pharmaceutical industries. We propose an approach to estimate personal well-being in terms of various measurements such as anxiety, sleep quality and mood using voice. With clinically validated questionnaires to score those measurements in a self-assessed way, we extract salient features from voice and train regression models with deep neural networks. Experiments with the collected database of 219 subjects show promising results in predicting the well-being related measurements; concordance correlation coefficients (CCC) between self-assessed scores and predicted scores are 0.41 for anxiety, 0.44 for sleep quality and 0.38 for mood.        △ Less","22 October, 2019","cs.CL,eess.AS",
              Towards better healthcare: What could and should be automated?          ,1910.09444,https://arxiv.org/abs/1910.09444,https://arxiv.org/pdf/1910.09444,"Authors:WolfgangFrühwirt,PaulDuckworth","        While artificial intelligence (AI) and other automation technologies might lead to enormous progress in healthcare, they may also have undesired consequences for people working in the field. In this interdisciplinary study, we capture empirical evidence of not only what healthcare work could be automated, but also what should be automated. We quantitatively investigate these research questions by utilizing probabilistic machine learning models trained on thousands of ratings, provided by both healthcare practitioners and automation experts. Based on our findings, we present an analytical tool (Automatability-Desirability Matrix) to support policymakers and organizational leaders in developing practical strategies on how to harness the positive power of automation technologies, while accompanying change and empowering stakeholders in a participatory fashion.        △ Less","21 October, 2019","stat.ML,cs.CY,cs.LG",
              A Dynamic System Model for Personalized Healthcare Delivery and Managed Individual Health Outcomes          ,1910.09104,https://arxiv.org/abs/1910.09104,https://arxiv.org/pdf/1910.09104,"Authors:InasS.Khayal,AmroM.Farid","        The current healthcare system is facing an unprecedented chronic disease burden. This paper develops a healthcare dynamic model for personalized healthcare delivery and managed individual health outcomes. It utilizes a hetero-functional graph theory rooted in Axiomatic Design for Large Flexible Engineering Systems and Petri nets. The dynamics of the model builds upon a recently developed systems architecture for healthcare delivery which bears several analogies to the architecture of mass-customized production systems. At its essence, the model consists of two synchronized Petri nets; one for the healthcare delivery system and another for individuals' health state evolution. The model is demonstrated on two clinical case studies; one acute and another chronic. Together, the case studies show that the model applies equally to the care of both acute and chronic conditions, transparently describes health outcomes and links them to the evolution of the healthcare delivery system and its associated costs.        △ Less","17 October, 2019","eess.SY,cs.CY",
              Bayesian Symbolic Regression          ,1910.08892,https://arxiv.org/abs/1910.08892,https://arxiv.org/pdf/1910.08892,"Authors:YingJin,WeilinFu,JianKang,JiadongGuo,JianGuo","        Interpretability is crucial for machine learning in many scenarios such as quantitative finance, banking, healthcare, etc. Symbolic regression (SR) is a classic interpretable machine learning method by bridging X and Y using mathematical expressions composed of some basic functions. However, the search space of all possible expressions grows exponentially with the length of the expression, making it infeasible for enumeration. Genetic programming (GP) has been traditionally and commonly used in SR to search for the optimal solution, but it suffers from several limitations, e.g. the difficulty in incorporating prior knowledge; overly-complicated output expression and reduced interpretability etc. To address these issues, we propose a new method to fit SR under a Bayesian framework. Firstly, Bayesian model can naturally incorporate prior knowledge (e.g., preference of basis functions, operators and raw features) to improve the efficiency of fitting SR. Secondly, to improve interpretability of expressions in SR, we aim to capture concise but informative signals. To this end, we assume the expected signal has an additive structure, i.e., a linear combination of several concise expressions, whose complexity is controlled by a well-designed prior distribution. In our setup, each expression is characterized by a symbolic tree, and the proposed SR model could be solved by sampling symbolic trees from the posterior distribution using an efficient Markov chain Monte Carlo (MCMC) algorithm. Finally, compared with GP, the proposed BSR(Bayesian Symbolic Regression) method saves computer memory with no need to keep an updated 'genome pool'. Numerical experiments show that, compared with GP, the solutions of BSR are closer to the ground truth and the expressions are more concise. Meanwhile we find the solution of BSR is robust to hyper-parameter specifications such as the number of trees.        △ Less","15 January, 2020",stat.ME,
              The TCGA Meta-Dataset Clinical Benchmark          ,1910.08636,https://arxiv.org/abs/1910.08636,https://arxiv.org/pdf/1910.08636,"Authors:MandanaSamiei,TobiasWürfl,TristanDeleu,MartinWeiss,FrancisDutil,ThomasFevens,GenevièveBoucher,SebastienLemieux,JosephPaulCohen","        Machine learning is bringing a paradigm shift to healthcare by changing the process of disease diagnosis and prognosis in clinics and hospitals. This development equips doctors and medical staff with tools to evaluate their hypotheses and hence make more precise decisions. Although most current research in the literature seeks to develop techniques and methods for predicting one particular clinical outcome, this approach is far from the reality of clinical decision making in which you have to consider several factors simultaneously. In addition, it is difficult to follow the recent progress concretely as there is a lack of consistency in benchmark datasets and task definitions in the field of Genomics. To address the aforementioned issues, we provide a clinical Meta-Dataset derived from the publicly available data hub called The Cancer Genome Atlas Program (TCGA) that contains 174 tasks. We believe those tasks could be good proxy tasks to develop methods which can work on a few samples of gene expression data. Also, learning to predict multiple clinical variables using gene-expression data is an important task due to the variety of phenotypes in clinical problems and lack of samples for some of the rare variables. The defined tasks cover a wide range of clinical problems including predicting tumor tissue site, white cell count, histological type, family history of cancer, gender, and many others which we explain later in the paper. Each task represents an independent dataset. We use regression and neural network baselines for all the tasks using only 150 samples and compare their performance.        △ Less","18 October, 2019","cs.LG,q-bio.QM,stat.ML",
              Indoor Information Retrieval using Lifelog Data          ,1910.07784,https://arxiv.org/abs/1910.07784,https://arxiv.org/pdf/1910.07784,Authors:DeepanwitaDatta,"        Studying human behaviour through lifelogging has seen an increase in attention from researchers over the past decade. The opportunities that lifelogging offers are based on the fact that a lifelog, as a ""black box"" of our lives, offers rich contextual information, which has been an Achilles heel of information discovery. While lifelog data has been put to use in various contexts, its application to indoor environment scenario remains unexplored. In this proposal, I plan to design a method that enables us to capture and record indoor lifelog data of a person's life in order to facilitate healthcare systems, emergency response, item tracking etc. To this end, we aim to build an Indoor Information Retrieval system that can be queried with natural language queries over lifelog data. Judicious use of the lifelog data for the indoor application may enable us to solve very fundamental but non-avoidable problems of our daily life. Analysis of lifelog data coupled with Information Retrieval is not only a promising research topic, but the possibility of its indoor application especially for healthcare, lost-item tracking would be an innovative research idea to the best of our knowledge.        △ Less","17 October, 2019","cs.IR,cs.HC",
              Bridging the Knowledge Gap: Enhancing Question Answering with World and Domain Knowledge          ,1910.07429,https://arxiv.org/abs/1910.07429,https://arxiv.org/pdf/1910.07429,"Authors:TravisR.Goodwin,DinaDemner-Fushman","        In this paper we present OSCAR (Ontology-based Semantic Composition Augmented Regularization), a method for injecting task-agnostic knowledge from an Ontology or knowledge graph into a neural network during pretraining. We evaluated the impact of including OSCAR when pretraining BERT with Wikipedia articles by measuring the performance when fine-tuning on two question answering tasks involving world knowledge and causal reasoning and one requiring domain (healthcare) knowledge and obtained 33:3%, 18:6%, and 4% improved accuracy compared to pretraining BERT without OSCAR and obtaining new state-of-the-art results on two of the tasks.        △ Less","16 October, 2019",cs.CL,
              Optimising Individual-Treatment-Effect Using Bandits          ,1910.07265,https://arxiv.org/abs/1910.07265,https://arxiv.org/pdf/1910.07265,"Authors:JeroenBerrevoets,SamVerboven,WouterVerbeke","        Applying causal inference models in areas such as economics, healthcare and marketing receives great interest from the machine learning community. In particular, estimating the individual-treatment-effect (ITE) in settings such as precision medicine and targeted advertising has peaked in application. Optimising this ITE under the strong-ignorability-assumption -- meaning all confounders expressing influence on the outcome of a treatment are registered in the data -- is often referred to as uplift modeling (UM). While these techniques have proven useful in many settings, they suffer vividly in a dynamic environment due to concept drift. Take for example the negative influence on a marketing campaign when a competitor product is released. To counter this, we propose the uplifted contextual multi-armed bandit (U-CMAB), a novel approach to optimise the ITE by drawing upon bandit literature. Experiments on real and simulated data indicate that our proposed approach compares favourably against the state-of-the-art. All our code can be found online at https://github.com/vub-dl/u-cmab.        △ Less","16 October, 2019","cs.LG,cs.AI,stat.ML",
"              Stumbling Through the Research Wilderness, Standard Methods to Shine Light on Electrically Conductive Nanocomposites for Future Health-Care Monitoring          ",1910.07249,https://arxiv.org/abs/1910.07249,https://arxiv.org/pdf/1910.07249,Authors:ConorSBoland,"        Electrically conductive nanocomposites are an exciting ever expanding area of research that has yielded many new technologies for wearable health devices. Acting as strain sensing materials, they have paved the way towards real time medical diagnostic tools that may very well lead to a golden age of healthcare. Currently, the goal in research is to create a material that simultaneously has both a large gauge factor G and sensing range. However, a weakness in the area of electromechanical research is the lack of standardisation in the reporting of the figure of merit, i.e. G, and the need for new metrics to give researchers a more complete view of the research landscape of resistive type sensors. A paradigm shift in the way in which data is reported is required, to push research in the right direction and to facilitate achieving research goals. Here, we report a standardised method for reporting strain sensing performance and the introduction of the working factor W and the Young's modulus Y of a material as two new material criteria. Using this new method, we can now for the first time define the benchmarks for an optimum sensing material, G > 7, W > 1, Y < 300 kPa, using limits set by standard commercial materials and the human body. Using extrapolated data from 200 publications normalised to this standard method, we can review what composite types meet these benchmark limits, what governs composite performances, the literary trends in composites and individual nanomaterial performance and the future prospects of research.        △ Less","16 October, 2019","physics.app-ph,cond-mat.soft",10.1021/acsnano.9b06847 
              Health Monitoring in Smart Homes Utilizing Internet of Things          ,1910.07058,https://arxiv.org/abs/1910.07058,https://arxiv.org/pdf/1910.07058,"Authors:LaurenLinkous,NasibehZohrabi,SherifAbdelwahed","        In recent years the concept of the Internet of Things (IoT) has evolved to connect commercial gadgets together with the medical field to facilitate an unprecedented range of accessibility. The development of medical devices connected to internet of things has been praised for the potential of alleviating the strain on the modern healthcare system by giving users the opportunity to reside in the home during treatment or recovery. With the IoT becoming more prevalent and available at a commercial level, there exists room for integration into emerging, intelligent environments such as smart homes. When used in tandem with conventional healthcare, the IoT offers a vast range of custom-tailored treatment options. This paper studies recent state-of-the-art research on the field of IoT for health monitoring and smart homes, examines several potential use-cases of blending the technology, and proposes integration with an existing smart home testbed for further study. Challenges of adoption and future research on the topic are also discussed.        △ Less","15 October, 2019",cs.HC,
              SCALPEL3: a scalable open-source library for healthcare claims databases          ,1910.07045,https://arxiv.org/abs/1910.07045,https://arxiv.org/pdf/1910.07045,"Authors:EmmanuelBacry,StéphaneGaïffas,FannyLeroy,MaryanMorel,DinhPhongNguyen,YoucefSebiat,DianSun","        This article introduces SCALPEL3, a scalable open-source framework for studies involving Large Observational Databases (LODs). Its design eases medical observational studies thanks to abstractions allowing concept extraction, high-level cohort manipulation, and production of data formats compatible with machine learning libraries. SCALPEL3 has successfully been used on the SNDS database (see Tuppin et al. (2017)), a huge healthcare claims database that handles the reimbursement of almost all French citizens.  SCALPEL3 focuses on scalability, easy interactive analysis and helpers for data flow analysis to accelerate studies performed on LODs. It consists of three open-source libraries based on Apache Spark. SCALPEL-Flattening allows denormalization of the LOD (only SNDS for now) by joining tables sequentially in a big table. SCALPEL-Extraction provides fast concept extraction from a big table such as the one produced by SCALPEL-Flattening. Finally, SCALPEL-Analysis allows interactive cohort manipulations, monitoring statistics of cohort flows and building datasets to be used with machine learning libraries. The first two provide a Scala API while the last one provides a Python API that can be used in an interactive environment. Our code is available on GitHub.  SCALPEL3 allowed to extract successfully complex concepts for studies such as Morel et al (2017) or studies with 14.5 million patients observed over three years (corresponding to more than 15 billion healthcare events and roughly 15 TeraBytes of data) in less than 49 minutes on a small 15 nodes HDFS cluster. SCALPEL3 provides a sharp interactive control of data processing through legible code, which helps to build studies with full reproducibility, leading to improved maintainability and audit of studies performed on LODs.        △ Less","26 August, 2020","cs.DC,cs.CY",
              Hierarchical Semantic Correspondence Learning for Post-Discharge Patient Mortality Prediction          ,1910.06492,https://arxiv.org/abs/1910.06492,https://arxiv.org/pdf/1910.06492,"Authors:ShaikaChowdhury,ChenweiZhang,PhilipS.Yu,YuanLuo","        Predicting patient mortality is an important and challenging problem in the healthcare domain, especially for intensive care unit (ICU) patients. Electronic health notes serve as a rich source for learning patient representations, that can facilitate effective risk assessment. However, a large portion of clinical notes are unstructured and also contain domain specific terminologies, from which we need to extract structured information. In this paper, we introduce an embedding framework to learn semantically-plausible distributed representations of clinical notes that exploits the semantic correspondence between the unstructured texts and their corresponding structured knowledge, known as semantic frame, in a hierarchical fashion. Our approach integrates text modeling and semantic correspondence learning into a single model that comprises 1) an unstructured embedding module that makes use of self-similarity matrix representations in order to inject structural regularities of different segments inherent in clinical texts to promote local coherence, 2) a structured embedding module to embed the semantic frames (e.g., UMLS semantic types) with deep ConvNet and 3) a hierarchical semantic correspondence module that embeds by enhancing the interactions between text-semantic frame embedding pairs at multiple levels (i.e., words, sentence, note). Evaluations on multiple embedding benchmarks on post discharge intensive care patient mortality prediction tasks demonstrate its effectiveness compared to approaches that do not exploit the semantic interactions between structured and unstructured information present in clinical notes.        △ Less","14 October, 2019","cs.CL,cs.LG",
              Mixed Pooling Multi-View Attention Autoencoder for Representation Learning in Healthcare,1910.06456,https://arxiv.org/abs/1910.06456,https://arxiv.org/pdf/1910.06456,"Authors:ShaikaChowdhury,ChenweiZhang,PhilipS.Yu,YuanLuo","        Distributed representations have been used to support downstream tasks in healthcare recently. Healthcare data (e.g., electronic health records) contain multiple modalities of data from heterogeneous sources that can provide complementary information, alongside an added dimension to learning personalized patient representations. To this end, in this paper we propose a novel unsupervised encoder-decoder model, namely Mixed Pooling Multi-View Attention Autoencoder (MPVAA), that generates patient representations encapsulating a holistic view of their medical profile. Specifically, by first learning personalized graph embeddings pertaining to each patient's heterogeneous healthcare data, it then integrates the non-linear relationships among them into a unified representation through multi-view attention mechanism. Additionally, a mixed pooling strategy is incorporated in the encoding step to learn diverse information specific to each data modality. Experiments conducted for multiple tasks demonstrate the effectiveness of the proposed model over the state-of-the-art representation learning methods in healthcare.        △ Less","14 October, 2019","cs.LG,cs.AI,stat.ML",
              SLEEPER: interpretable Sleep staging via Prototypes from Expert Rules          ,1910.06100,https://arxiv.org/abs/1910.06100,https://arxiv.org/pdf/1910.06100,"Authors:IrfanAl-Hussaini,CaoXiao,M.BrandonWestover,JimengSun","        Sleep staging is a crucial task for diagnosing sleep disorders. It is tedious and complex as it can take a trained expert several hours to annotate just one patient's polysomnogram (PSG) from a single night. Although deep learning models have demonstrated state-of-the-art performance in automating sleep staging, interpretability which defines other desiderata, has largely remained unexplored. In this study, we propose Sleep staging via Prototypes from Expert Rules (SLEEPER), which combines deep learning models with expert defined rules using a prototype learning framework to generate simple interpretable models. In particular, SLEEPER utilizes sleep scoring rules and expert defined features to derive prototypes which are embeddings of PSG data fragments via convolutional neural networks. The final models are simple interpretable models like a shallow decision tree defined over those phenotypes. We evaluated SLEEPER using two PSG datasets collected from sleep studies and demonstrated that SLEEPER could provide accurate sleep stage classification comparable to human experts and deep neural networks with about 85% ROC-AUC and .7 kappa.        △ Less","14 October, 2019","cs.LG,eess.SP,stat.ML",
              Maturity assessment and maturity models in healthcare: A multivocal literature review          ,1910.06076,https://arxiv.org/abs/1910.06076,https://arxiv.org/pdf/1910.06076,"Authors:AyçaKolukısaTarhan,VahidGarousi,OktayTuretken,MehmetSöylemez,SoniaGarossi","        Context: Maturity of practices and infrastructure in healthcare domain directly impacts the quality and efficiency of healthcare services. Therefore, various healthcare administrations (e.g., hospital management to nation-wide health authority) need to assess and improve their operational maturity.  Objective: This study aims to review and classify studies that propose/use maturity assessment or maturity models (MMs) as a vehicle to achieve operational excellence in healthcare domain.  Method: To achieve this objective, we performed a Multivocal Literature Review (MLR) that is a form of Systematic Review and includes data from the grey literature (e.g., white papers and online documents) in addition to formal, peer-reviewed literature.  Results: Based on 101 sources, 80 of which are from the peer-reviewed literature and 21 are from the grey literature, we identified 68 different MMs on, e.g., telemedicine, care pathways, and digital imaging. We reviewed them with respect to various aspects including: types of research and contribution; list of MMs proposed/used with their subject focuses; elements of maturity/capability; and application scope or scale. In the synthesis of empirical benefits of using MMs, two were found significant: (1) Identifying issues and providing guidance for improvement in healthcare contexts; (2) Improving efficiency, effectiveness, performance, and productivity.  Conclusion: This MLR provides an overview of the landscape and serves as an index to the vast body of knowledge in this area. Our review creates an opportunity to cope with the challenges in getting an overview of the state-of-the-art and practice, choosing the most suitable models, or developing new models with further specialties.        △ Less","5 October, 2019",cs.CY,
              The Outcome Range Problem in Interval Linear Programming          ,1910.05913,https://arxiv.org/abs/1910.05913,https://arxiv.org/pdf/1910.05913,"Authors:MohsenMohammadi,MonicaGentili","        Quantifying extra functions, herein referred to as outcome functions, over optimal solutions of an optimization problem can provide decision makers with additional information on a system. This bears more importance when the optimization problem is subject to uncertainty in input parameters. In this paper, we consider linear programming problems in which input parameters are described by real-valued intervals, and we address the outcome range problem which is the problem of finding the range of an outcome function over all possible optimal solutions of a linear program with interval data. We give a general definition of the problem and then focus on a special class of it where uncertainty occurs only in the right-hand side of the underlying linear program. We show that our problem is computationally hard to solve and also study some of its theoretical properties. We then develop two approximation methods to solve it: a local search algorithm and a super-set based method. We test the methods on a set of randomly generated instances. We also provide a real case study on healthcare access measurement to show the relevance of our problem for reliable decision making.        △ Less","26 September, 2020",math.OC,
              Actor Critic with Differentially Private Critic          ,1910.05876,https://arxiv.org/abs/1910.05876,https://arxiv.org/pdf/1910.05876,"Authors:JonathanLebensold,WilliamHamilton,BorjaBalle,DoinaPrecup","        Reinforcement learning algorithms are known to be sample inefficient, and often performance on one task can be substantially improved by leveraging information (e.g., via pre-training) on other related tasks. In this work, we propose a technique to achieve such knowledge transfer in cases where agent trajectories contain sensitive or private information, such as in the healthcare domain. Our approach leverages a differentially private policy evaluation algorithm to initialize an actor-critic model and improve the effectiveness of learning in downstream tasks. We empirically show this technique increases sample efficiency in resource-constrained control problems while preserving the privacy of trajectories collected in an upstream task.        △ Less","13 October, 2019","cs.LG,stat.ML",
              Facial Emotion Recognition using Convolutional Neural Networks          ,1910.05602,https://arxiv.org/abs/1910.05602,https://arxiv.org/pdf/1910.05602,"Authors:AkashSaravanan,GuruduttPerichetla,Dr.K.S.Gayathri","        Facial expression recognition is a topic of great interest in most fields from artificial intelligence and gaming to marketing and healthcare. The goal of this paper is to classify images of human faces into one of seven basic emotions. A number of different models were experimented with, including decision trees and neural networks before arriving at a final Convolutional Neural Network (CNN) model. CNNs work better for image recognition tasks since they are able to capture spacial features of the inputs due to their large number of filters. The proposed model consists of six convolutional layers, two max pooling layers and two fully connected layers. Upon tuning of the various hyperparameters, this model achieved a final accuracy of 0.60.        △ Less","12 October, 2019",cs.CV,
              High signal-to-noise ratio reconstruction of low bit-depth optical coherence tomography using deep learning          ,1910.05498,https://arxiv.org/abs/1910.05498,https://arxiv.org/pdf/1910.05498,"Authors:QiangjiangHao,KangZhou,JianlongYang,LiyangFang,ZhengjieChai,YuhuiMa,YanHu,ShenghuaGao,JiangLiu","        Reducing the bit-depth is an effective approach to lower the cost of optical coherence tomography (OCT) systems and increase the transmission efficiency in data acquisition and telemedicine. However, a low bit-depth will lead to the degeneration of the detection sensitivity thus reduce the signal-to-noise ratio (SNR) of OCT images. In this paper, we propose to use deep learning for the reconstruction of the high SNR OCT images from the low bit-depth acquisition. Its feasibility was preliminarily evaluated by applying the proposed method to the quantized 3∼83\sim8-bit data from native 12-bit interference fringes. We employed a pixel-to-pixel generative adversarial network architecture in the low to high bit-depth OCT image transition. Retinal OCT data of a healthy subject from a homemade spectral-domain OCT system was used in the study. Extensively qualitative and quantitative results show this deep-learning-based approach could significantly improve the SNR of the low bit-depth OCT images especially at the choroidal region. Superior similarity and SNR between the reconstructed images and the original 12-bit OCT images could be derived when the bit-depth ≥5\geq 5. This work demonstrates the proper integration of OCT and deep learning could benefit the development of healthcare in low-resource settings.        △ Less","11 February, 2020","eess.IV,physics.optics",
              A parameter-free population-dynamical approach to health workforce supply forecasting of EU countries          ,1910.05077,https://arxiv.org/abs/1910.05077,https://arxiv.org/pdf/1910.05077,"Authors:PeterKlimek,MichaelGyimesi,HerwigOstermann,StefanThurner","        Many countries face challenges like impending retirement waves, negative population growth, or a suboptimal distribution of resources across medical sectors and fields in supplying their healthcare systems with adequate staffing. An increasing number of countries therefore employs quantitative approaches in health workforce supply forecasting. However, these models are often of limited usability as they either require extensive individual-level data or become too simplistic to capture key demographic or epidemiological factors. We propose a novel population-dynamical and stock-flow-consistent approach to health workforce supply forecasting complex enough to address dynamically changing behaviors while requiring only publicly available timeseries data for complete calibration. We apply the model to 21 European countries to forecast the supply of generalist and specialist physicians until 2040. Compared to staffing levels required to keep the physician density constant at 2016 levels, in many countries we find a significant trend toward decreasing density for generalist physicians at the expense of increasing densities for specialists. These trends are exacerbated in many Southern and Eastern European countries by expectations of negative population growth. For the example of Austria we generalize our approach to a multi-professional, multi-regional and multi-sectoral model and find a suboptimal distribution in the supply of contracted versus non-contracted physicians. It is of the utmost importance to devise tools for decision makers to influence the allocation and supply of physicians across fields and sectors to combat imbalances.        △ Less","11 October, 2019","stat.AP,cs.CY,physics.soc-ph",
              Estimation of Bounds on Potential Outcomes For Decision Making          ,1910.04817,https://arxiv.org/abs/1910.04817,https://arxiv.org/pdf/1910.04817,"Authors:MaggieMakar,FredrikD.Johansson,JohnGuttag,DavidSontag","        Estimation of individual treatment effects is commonly used as the basis for contextual decision making in fields such as healthcare, education, and economics. However, it is often sufficient for the decision maker to have estimates of upper and lower bounds on the potential outcomes of decision alternatives to assess risks and benefits. We show that, in such cases, we can improve sample efficiency by estimating simple functions that bound these outcomes instead of estimating their conditional expectations, which may be complex and hard to estimate. Our analysis highlights a trade-off between the complexity of the learning task and the confidence with which the learned bounds hold. Guided by these findings, we develop an algorithm for learning upper and lower bounds on potential outcomes which optimize an objective function defined by the decision maker, subject to the probability that bounds are violated being small. Using a clinical dataset and a well-known causality benchmark, we demonstrate that our algorithm outperforms baselines, providing tighter, more reliable bounds.        △ Less","12 August, 2020","cs.LG,stat.ML",
              Using NIST Special Publications (SP) 800-171r2 and 800-172 to assess and evaluate the security posture of Information Systems in the Healthcare sector which support Covered Entities and/or their Business Associates          ,1910.04293,https://arxiv.org/abs/1910.04293,https://arxiv.org/pdf/1910.04293,Authors:ThomasP.Dover,"        This paper describes how NIST Special Publications (SP) 800-171r2 (Protecting Controlled but Unclassified Information in Nonfederal Systems and Organizations) and 800-172 (Enhanced Security Requirements for Protecting Controlled Unclassified Information) can be used to evaluate the security posture of information systems and supporting frameworks relative to HIPAA and HITECH. It will demonstrate that the provisions and baseline security requirements outlined in SP.800-171r2 and SP.800-172 for the protection of Controlled but Unclassified Information (CUI) can be applied to Electronic Protected Health Information (ePHI). An explanation of how both publications align with HIPAA and how this alignment suffices for evaluating IT environment security will be given along with the process and procedure for performing such evaluation. Finally, the benefits of using this approach to support formal risk assessment will be described.        △ Less","8 August, 2020","cs.CR,cs.CY",
              NGBoost: Natural Gradient Boosting for Probabilistic Prediction          ,1910.03225,https://arxiv.org/abs/1910.03225,https://arxiv.org/pdf/1910.03225,"Authors:TonyDuan,AnandAvati,DaisyYiDing,KhanhK.Thai,SanjayBasu,AndrewY.Ng,AlejandroSchuler","        We present Natural Gradient Boosting (NGBoost), an algorithm for generic probabilistic prediction via gradient boosting. Typical regression models return a point estimate, conditional on covariates, but probabilistic regression models output a full probability distribution over the outcome space, conditional on the covariates. This allows for predictive uncertainty estimation -- crucial in applications like healthcare and weather forecasting. NGBoost generalizes gradient boosting to probabilistic regression by treating the parameters of the conditional distribution as targets for a multiparameter boosting algorithm. Furthermore, we show how the Natural Gradient is required to correct the training dynamics of our multiparameter boosting approach. NGBoost can be used with any base learner, any family of distributions with continuous parameters, and any scoring rule. NGBoost matches or exceeds the performance of existing methods for probabilistic prediction while offering additional benefits in flexibility, scalability, and usability. An open-source implementation is available at github.com/stanfordmlgroup/ngboost.        △ Less","9 June, 2020","cs.LG,stat.ML",
              Sequence embeddings help to identify fraudulent cases in healthcare insurance          ,1910.03072,https://arxiv.org/abs/1910.03072,https://arxiv.org/pdf/1910.03072,"Authors:I.Fursov,A.Zaytsev,R.Khasyanov,M.Spindler,E.Burnaev","        Fraud causes substantial costs and losses for companies and clients in the finance and insurance industries. Examples are fraudulent credit card transactions or fraudulent claims. It has been estimated that roughly 1010 percent of the insurance industry's incurred losses and loss adjustment expenses each year stem from fraudulent claims. The rise and proliferation of digitization in finance and insurance have lead to big data sets, consisting in particular of text data, which can be used for fraud detection. In this paper, we propose architectures for text embeddings via deep learning, which help to improve the detection of fraudulent claims compared to other machine learning methods. We illustrate our methods using a data set from a large international health insurance company. The empirical results show that our approach outperforms other state-of-the-art methods and can help make the claims management process more efficient. As (unstructured) text data become increasingly available to economists and econometricians, our proposed methods will be valuable for many similar applications, particularly when variables have a large number of categories as is typical for example of the International Classification of Disease (ICD) codes in health economics and health services.        △ Less","7 October, 2019","cs.LG,cs.CR,stat.ML",
              Impact of Inference Accelerators on hardware selection          ,1910.03060,https://arxiv.org/abs/1910.03060,https://arxiv.org/pdf/1910.03060,"Authors:DibyajyotiPati,CarolineFavart,PurujitBahl,VivekSoni,Yun-chanTsai,MichaelPotter,JiahuiGuan,XiaomengDong,V.RatnaSaripalli","        As opportunities for AI-assisted healthcare grow steadily, model deployment faces challenges due to the specific characteristics of the industry. The configuration choice for a production device can impact model performance while influencing operational costs. Moreover, in healthcare some situations might require fast, but not real time, inference. We study different configurations and conduct a cost-performance analysis to determine the optimized hardware for the deployment of a model subject to healthcare domain constraints. We observe that a naive performance comparison may not lead to an optimal configuration selection. In fact, given realistic domain constraints, CPU execution might be preferable to GPU accelerators. Hence, defining beforehand precise expectations for model deployment is crucial.        △ Less","7 October, 2019","cs.DC,cs.LG",
              Open Set Medical Diagnosis          ,1910.02830,https://arxiv.org/abs/1910.02830,https://arxiv.org/pdf/1910.02830,"Authors:VirajPrabhu,AnithaKannan,GeoffreyJ.Tso,NamitKatariya,ManishChablani,DavidSontag,XavierAmatriain","        Machine-learned diagnosis models have shown promise as medical aides but are trained under a closed-set assumption, i.e. that models will only encounter conditions on which they have been trained. However, it is practically infeasible to obtain sufficient training data for every human condition, and once deployed such models will invariably face previously unseen conditions. We frame machine-learned diagnosis as an open-set learning problem, and study how state-of-the-art approaches compare. Further, we extend our study to a setting where training data is distributed across several healthcare sites that do not allow data pooling, and experiment with different strategies of building open-set diagnostic ensembles. Across both settings, we observe consistent gains from explicitly modeling unseen conditions, but find the optimal training strategy to vary across settings.        △ Less","7 October, 2019","cs.LG,cs.AI,stat.ML",
              Brain MRI Tumor Segmentation with Adversarial Networks          ,1910.02717,https://arxiv.org/abs/1910.02717,https://arxiv.org/pdf/1910.02717,"Authors:EdoardoGiacomello,DanieleLoiacono,LucaMainardi","        Deep Learning is a promising approach to either automate or simplify several tasks in the healthcare domain. In this work, we introduce SegAN-CAT, an approach to brain tumor segmentation in Magnetic Resonance Images (MRI), based on Adversarial Networks. In particular, we extend SegAN, successfully applied to the same task in a previous work, in two respects: (i) we used a different model input and (ii) we employed a modified loss function to train the model. We tested our approach on two large datasets, made available by the Brain Tumor Image Segmentation Benchmark (BraTS). First, we trained and tested some segmentation models assuming the availability of all the major MRI contrast modalities, i.e., T1-weighted, T1 weighted contrast-enhanced, T2-weighted, and T2-FLAIR. However, as these four modalities are not always all available for each patient, we also trained and tested four segmentation models that take as input MRIs acquired only with a single contrast modality. Finally, we proposed to apply transfer learning across different contrast modalities to improve the performance of these single-modality models. Our results are promising and show that not SegAN-CAT is able to outperform SegAN when all the four modalities are available, but also that transfer learning can actually lead to better performances when only a single modality is available.        △ Less","30 January, 2020","eess.IV,cs.LG,stat.ML",10.1109/IJCNN48605.2020.9207220 
              Differential Privacy-enabled Federated Learning for Sensitive Health Data          ,1910.02578,https://arxiv.org/abs/1910.02578,https://arxiv.org/pdf/1910.02578,"Authors:OliviaChoudhury,ArisGkoulalas-Divanis,TheodorosSalonidis,IssaSylla,YoonyoungPark,GraceHsu,AmarDas","        Leveraging real-world health data for machine learning tasks requires addressing many practical challenges, such as distributed data silos, privacy concerns with creating a centralized database from person-specific sensitive data, resource constraints for transferring and integrating data from multiple sites, and risk of a single point of failure. In this paper, we introduce a federated learning framework that can learn a global model from distributed health data held locally at different sites. The framework offers two levels of privacy protection. First, it does not move or share raw data across sites or with a centralized server during the model training process. Second, it uses a differential privacy mechanism to further protect the model from potential privacy attacks. We perform a comprehensive evaluation of our approach on two healthcare applications, using real-world electronic health data of 1 million patients. We demonstrate the feasibility and effectiveness of the federated learning framework in offering an elevated level of privacy and maintaining utility of the global model.        △ Less","27 February, 2020","cs.LG,cs.CR",
              Representation Learning of EHR Data via Graph-Based Medical Entity Embedding          ,1910.02574,https://arxiv.org/abs/1910.02574,https://arxiv.org/pdf/1910.02574,"Authors:TongWu,YunlongWang,YueWang,EmilyZhao,YilianYuan,ZhiYang","        Automatic representation learning of key entities in electronic health record (EHR) data is a critical step for healthcare informatics that turns heterogeneous medical records into structured and actionable information. Here we propose ME2Vec, an algorithmic framework for learning low-dimensional vectors of the most common entities in EHR: medical services, doctors, and patients. ME2Vec leverages diverse graph embedding techniques to cater for the unique characteristic of each medical entity. Using real-world clinical data, we demonstrate the efficacy of ME2Vec over competitive baselines on disease diagnosis prediction.        △ Less","6 October, 2019","cs.LG,cs.IR,stat.ML",
              Early Prediction of 30-day ICU Re-admissions Using Natural Language Processing and Machine Learning          ,1910.02545,https://arxiv.org/abs/1910.02545,https://arxiv.org/pdf/1910.02545,"Authors:ZhihengLi,XinyueXing,BingzhangLu,ZhixiangLi","        ICU readmission is associated with longer hospitalization, mortality and adverse outcomes. An early recognition of ICU re-admission can help prevent patients from worse situation and lower treatment cost. As the abundance of Electronics Health Records (EHR), it is popular to design clinical decision tools with machine learning technique manipulating on healthcare large scale data. We designed data-driven predictive models to estimate the risk of ICU readmission. The discharge summary of each hospital admission was carefully represented by natural language processing techniques. Unified Medical Language System (UMLS) was further used to standardize inconsistency of discharge summaries. 5 machine learning classifiers were adopted to construct predictive models. The best configuration yielded a competitive AUC of 0.748. Our work suggests that natural language processing of discharge summaries is capable to send clinicians warning of unplanned 30-day readmission upon discharge.        △ Less","6 October, 2019","cs.LG,cs.CL,stat.ML",
              Confinement and activity regulate bacterial motion in porous media          ,1910.02495,https://arxiv.org/abs/1910.02495,https://arxiv.org/pdf/1910.02495,"Authors:TapomoyBhattacharjee,SujitS.Datta","        Understanding how bacteria move in porous media is critical to applications in healthcare, agriculture, environmental remediation, and chemical sensing. Recent work has demonstrated that E. coli, which moves by run-and-tumble dynamics in a homogeneous medium, exhibits a new form of motility when confined in a disordered porous medium: hopping-and-trapping motility, in which cells perform rapid, directed hops punctuated by intervals of slow, undirected trapping. Here, we use direct visualization to shed light on how these processes depend on pore-scale confinement and cellular activity. We find that hopping is determined by pore-scale confinement, and is independent of cellular activity; by contrast, trapping is determined by the competition between pore-scale confinement and cellular activity, as predicted by an entropic trapping model. These results thus help to elucidate the factors that regulate bacterial motion in porous media, and could help aid the development of new models of motility in heterogeneous environments.        △ Less","6 October, 2019","cond-mat.soft,cond-mat.dis-nn,cond-mat.mes-hall,cond-mat.stat-mech,physics.bio-ph",
              Confederated Machine Learning on Horizontally and Vertically Separated Medical Data for Large-Scale Health System Intelligence          ,1910.02109,https://arxiv.org/abs/1910.02109,https://arxiv.org/pdf/1910.02109,"Authors:DianboLiu,TimothyAMiller,KennethD.Mandl","        Health information is generally fragmented across silos. Though it is technically feasible to unite data for analysis in a manner that underpins a rapid learning healthcare system, privacy concerns and regulatory barriers limit data centralization. Machine learning can be conducted in a federated manner on patient datasets with the same set of variables, but separated across sites of care. But federated learning cannot handle the situation where different data types for a given patient are separated vertically across different organizations and when patient ID matching across different institutions is difficult. We call methods that enable machine learning model training on data separated by two or more degrees confederated machine learning. We proposed and evaluated a confederated learning to training machine learning model to stratify the risk of several diseases among when data are horizontally separated by individual, vertically separated by data type, and separated by identity without patient ID matching.        △ Less","4 May, 2020","cs.LG,cs.AI,cs.CY",
              AI Assisted Annotator using Reinforcement Learning          ,1910.02052,https://arxiv.org/abs/1910.02052,https://arxiv.org/pdf/1910.02052,"Authors:V.RatnaSaripalli,GopalAvinash,DibyajyotiPati,MichaelPotter,CharlesW.Anderson","Healthcare data suffers from both noise and lack of ground truth. The cost of data increases as it is cleaned and annotated in healthcare. Unlike other data sets, medical data annotation, which is critical to accurate ground truth, requires medical domain expertise for a better patient outcome. In this work, we report on the use of reinforcement learning to mimic the decision making process of annotators for medical events, to automate annotation and labelling. The reinforcement agent learns to annotate alarm data based on annotations done by an expert. Our method shows promising results on medical alarm data sets. We trained DQN and A2C agents using the data from monitoring devices annotated by an expert. Initial results from these RL agents learning the expert annotation behavior are promising. The A2C agent performs better in terms of learning the sparse events in a given state, thereby choosing more right actions compared to DQN agent. To the best of our knowledge, this is the first reinforcement learning application for the automation of medical events annotation, which has far-reaching practical use.        △ Less","11 June, 2020","eess.SP,cs.AI,cs.LG",10.1007/s42979-020-00356-z 
              A Survey of Benchmarks to Evaluate Data Analytics for Smart-* Applications          ,1910.02004,https://arxiv.org/abs/1910.02004,https://arxiv.org/pdf/1910.02004,"Authors:AthanasiosKiatipis,AlvaroBrandon,RizkallahTouma,PierreMatri,MichalZasadzinski,LinhThuyNhuyen,AdrienLebre,AlexandruCostan","        The growth of ubiquitous sensor networks at an accelerating pace cuts across many areas of modern day life. They enable measuring, inferring, understanding and acting upon a wide variety of indicators, in fields ranging from agriculture to healthcare or to complex urban environments. The applications devoted to this task are designated as Smart-* Applications. They hide a staggering complexity, relying on multiple layers of data collection, transmission, aggregation, analysis and also storage, both at the network edge and on the cloud. Furthermore, Smart-* Applications raise additional specific challenges, such as the need to process and extract knowledge from diverse data, which is flowing at high velocity in near real-time or in the heavily distributed environment they rely on. How to assess the performance of such a complex stack, when faced with the specifics of \mbox{Smart-*} Applications, remains an open research question. In this article, the key specific characteristics and requirements of Smart-* Applications are initially detailed. Afterwards, for each of these requirements, there is a description of the benchmarks one can use to precisely evaluate the performance of the underlying systems and technologies. Finally, an identification of future research directions related to identified open issues for benchmarking Smart-* Applications is performed.        △ Less","4 October, 2019",cs.OH,
              The Thing With E.coli: Highlighting Opportunities and Challenges of Integrating Bacteria in IoT and HCI          ,1910.01974,https://arxiv.org/abs/1910.01974,https://arxiv.org/pdf/1910.01974,"Authors:RaphaelKim,StefanPoslad","        With advances in nano- and biotechnology, bacteria are receiving increasing attention in scientific research as a potential substrate for Internet of Bio-Nano Things (IoBNT), which involve networking and communication through nanoscale and biological entities. Harnessing the special features of bacteria, including an ability to become autonomous - helped by an embedded, natural propeller motor - the microbes show promising array of application in healthcare and environmental health. In this paper, we briefly outline significant features of bacteria that allow analogies between them and traditional computerized IoT device to be made. We argue that such comparisons are critical in terms of helping researchers to explore human-bacteria interaction in the context of IoT and HCI. Furthermore, we highlight the current lack of tangible infrastructure for researchers in IoT and HCI to access and experiment with bacteria. As a potential solution, we propose to utilize the DIY biology movement and gamification techniques to leverage user engagement and introduction to bacteria.        △ Less","17 June, 2019",cs.HC,
              Unsupervised Representation for EHR Signals and Codes as Patient Status Vector          ,1910.01803,https://arxiv.org/abs/1910.01803,https://arxiv.org/pdf/1910.01803,"Authors:SajadDarabi,MohammadKachuee,MajidSarrafzadeh","        Effective modeling of electronic health records presents many challenges as they contain large amounts of irregularity most of which are due to the varying procedures and diagnosis a patient may have. Despite the recent progress in machine learning, unsupervised learning remains largely at open, especially in the healthcare domain. In this work, we present a two-step unsupervised representation learning scheme to summarize the multi-modal clinical time series consisting of signals and medical codes into a patient status vector. First, an auto-encoder step is used to reduce sparse medical codes and clinical time series into a distributed representation. Subsequently, the concatenation of the distributed representations is further fine-tuned using a forecasting task. We evaluate the usefulness of the representation on two downstream tasks: mortality and readmission. Our proposed method shows improved generalization performance for both short duration ICU visits and long duration ICU visits.        △ Less","4 October, 2019","cs.LG,cs.AI,stat.ML",
              A Random Interaction Forest for Prioritizing Predictive Biomarkers          ,1910.01786,https://arxiv.org/abs/1910.01786,https://arxiv.org/pdf/1910.01786,"Authors:ZhenZeng,YuefengLu,JudongShen,WeiZheng,PeterShaw,MaryBethDorr","        Precision medicine is becoming a focus in medical research recently, as its implementation brings values to all stakeholders in the healthcare system. Various statistical methodologies have been developed tackling problems in different aspects of this field, e.g., assessing treatment heterogeneity, identifying patient subgroups, or building treatment decision models. However, there is a lack of new tools devoted to selecting and prioritizing predictive biomarkers. We propose a novel tree-based ensemble method, random interaction forest (RIF), to generate predictive importance scores and prioritize candidate biomarkers for constructing refined treatment decision models. RIF was evaluated by comparing with the conventional random forest and univariable regression methods and showed favorable properties under various simulation scenarios. We applied the proposed RIF method to a biomarker dataset from two phase III clinical trials of bezlotoxumab on Clostridium difficile\textit{Clostridium difficile} infection recurrence and obtained biologically meaningful results.        △ Less","3 October, 2019","q-bio.QM,cs.LG,stat.AP,stat.ML",
              Sensor Networks in Healthcare: Ensuring Confidentiality and User Anonymity in WBAN          ,1910.00991,https://arxiv.org/abs/1910.00991,https://arxiv.org/pdf/1910.00991,Authors:SayedAshrafMamun,"        Wireless body area network(WBAN) is becoming more popular in recent years. Security and privacy of this network is the major concern for the researchers. Most of the WBAN sensors currently available in the market uses Bluetooth Low Energy (BLE) protocol for connection. But the BLE protocol has an inherent security flaw due to its weak pairing protocol which can be easily be exploited by an attacker. In this paper, a different pairing protocol based on Lightweight Anonymous Authentication Protocol (LWAA) is proposed. Proposed protocol has a 3.8% power overhead over traditional BLE. But this is a very small price to pay if we consider all the benefits it brings to the system.        △ Less","2 October, 2019",cs.CR,
              Enhancing high-content imaging for studying microtubule networks at large-scale          ,1910.00662,https://arxiv.org/abs/1910.00662,https://arxiv.org/pdf/1910.00662,"Authors:Hao-ChihLee,SarahTCherng,RiccardoMiotto,JoelTDudley","        Given the crucial role of microtubules for cell survival, many researchers have found success using microtubule-targeting agents in the search for effective cancer therapeutics. Understanding microtubule responses to targeted interventions requires that the microtubule network within cells can be consistently observed across a large sample of images. However, fluorescence noise sources captured simultaneously with biological signals while using wide-field microscopes can obfuscate fine microtubule structures. Such requirements are particularly challenging for high-throughput imaging, where researchers must make decisions related to the trade-off between imaging quality and speed. Here, we propose a computational framework to enhance the quality of high-throughput imaging data to achieve fast speed and high quality simultaneously. Using CycleGAN, we learn an image model from low-throughput, high-resolution images to enhance features, such as microtubule networks in high-throughput low-resolution images. We show that CycleGAN is effective in identifying microtubules with 0.93+ AUC-ROC and that these results are robust to different kinds of image noise. We further apply CycleGAN to quantify the changes in microtubule density as a result of the application of drug compounds, and show that the quantified responses correspond well with known drug effects        △ Less","1 October, 2019","eess.IV,cs.LG,stat.ML",
              BioNLP-OST 2019 RDoC Tasks: Multi-grain Neural Relevance Ranking Using Topics and Attention Based Query-Document-Sentence Interactions          ,1910.00314,https://arxiv.org/abs/1910.00314,https://arxiv.org/pdf/1910.00314,"Authors:YatinChaudhary,PankajGupta,HinrichSchütze","        This paper presents our system details and results of participation in the RDoC Tasks of BioNLP-OST 2019. Research Domain Criteria (RDoC) construct is a multi-dimensional and broad framework to describe mental health disorders by combining knowledge from genomics to behaviour. Non-availability of RDoC labelled dataset and tedious labelling process hinders the use of RDoC framework to reach its full potential in Biomedical research community and Healthcare industry. Therefore, Task-1 aims at retrieval and ranking of PubMed abstracts relevant to a given RDoC construct and Task-2 aims at extraction of the most relevant sentence from a given PubMed abstract. We investigate (1) attention based supervised neural topic model and SVM for retrieval and ranking of PubMed abstracts and, further utilize BM25 and other relevance measures for re-ranking, (2) supervised and unsupervised sentence ranking models utilizing multi-view representations comprising of query-aware attention-based sentence representation (QAR), bag-of-words (BoW) and TF-IDF. Our best systems achieved 1st rank and scored 0.86 mean average precision (mAP) and 0.58 macro average accuracy (MAA) in Task-1 and Task-2 respectively.        △ Less","2 October, 2019","cs.LG,cs.CL,cs.IR,stat.ML",
              MonoNet: Towards Interpretable Models by Learning Monotonic Features          ,1909.13611,https://arxiv.org/abs/1909.13611,https://arxiv.org/pdf/1909.13611,"Authors:An-phiNguyen,MaríaRodríguezMartínez","        Being able to interpret, or explain, the predictions made by a machine learning model is of fundamental importance. This is especially true when there is interest in deploying data-driven models to make high-stakes decisions, e.g. in healthcare. While recent years have seen an increasing interest in interpretable machine learning research, this field is currently lacking an agreed-upon definition of interpretability, and some researchers have called for a more active conversation towards a rigorous approach to interpretability. Joining this conversation, we claim in this paper that the difficulty of interpreting a complex model stems from the existing interactions among features. We argue that by enforcing monotonicity between features and outputs, we are able to reason about the effect of a single feature on an output independently from other features, and consequently better understand the model. We show how to structurally introduce this constraint in deep learning models by adding new simple layers. We validate our model on benchmark datasets, and compare our results with previously proposed interpretable models.        △ Less","30 September, 2019","cs.LG,stat.ML",
"              ISTHMUS: Secure, Scalable, Real-time and Robust Machine Learning Platform for Healthcare",1909.13343,https://arxiv.org/abs/1909.13343,https://arxiv.org/pdf/1909.13343,"Authors:AkshayArora,ArunNethi,PriyankaKharat,VencyVerghese,GrantJenkins,SteveMiff,VikasChowdhry,XiaoWang","        In recent times, machine learning (ML) and artificial intelligence (AI) based systems have evolved and scaled across different industries such as finance, retail, insurance, energy utilities, etc. Among other things, they have been used to predict patterns of customer behavior, to generate pricing models, and to predict the return on investments. But the successes in deploying machine learning models at scale in those industries have not translated into the healthcare setting. There are multiple reasons why integrating ML models into healthcare has not been widely successful, but from a technical perspective, general-purpose commercial machine learning platforms are not a good fit for healthcare due to complexities in handling data quality issues, mandates to demonstrate clinical relevance, and a lack of ability to monitor performance in a highly regulated environment with stringent security and privacy needs. In this paper, we describe Isthmus, a turnkey, cloud-based platform which addresses the challenges above and reduces time to market for operationalizing ML/AI in healthcare. Towards the end, we describe three case studies which shed light on Isthmus capabilities. These include (1) supporting an end-to-end lifecycle of a model which predicts trauma survivability at hospital trauma centers, (2) bringing in and harmonizing data from disparate sources to create a community data platform for inferring population as well as patient level insights for Social Determinants of Health (SDoH), and (3) ingesting live-streaming data from various IoT sensors to build models, which can leverage real-time and longitudinal information to make advanced time-sensitive predictions.        △ Less","1 October, 2019","cs.LG,stat.ML",
              Service Embedding in IoT Networks          ,1909.13282,https://arxiv.org/abs/1909.13282,https://arxiv.org/pdf/1909.13282,"Authors:HaiderQaysAl-Shammari,AhmedLawey,TaisirEl-Gorashi,JaafarM.H.Elmirghani","        The Internet of Things is anticipated to participate in the execution of a variety of complex tasks in the near future. IoT objects capable of handling multiple sensing and actuating functions are the cornerstone of smart applications such as smart buildings, smart factories, home automation, and healthcare automation. These smart applications express their demands in terms of high-level requests. These requests are characterised by the different requirements of sensing actuating functions, processing and memory needs, activation zones, latency, etc. In service-oriented architecture-based IoT, application requests are translated into a business process BP workflow. In this study, we model such a BP as a virtual network containing a set of virtual nodes and links connected in a specific topology. These virtual nodes represent the requested processing and location where sensing or actuation are needed. The virtual links capture the requested communication requirements between nodes. In this paper, we introduce a framework, optimised using mixed integer linear programming MILP, that embeds the BPs from the virtual layer into a lower-level implementation at the IoT physical layer. The proposed framework results in a physical plan that optimally allocates the processing needed and provisions the sensing and actuation at the required locations requirements to an appropriate set of IoT nodes. The optimisation goal is to minimise the IoT layer total power consumption and optimise the traffic distribution in a manner that minimises the traffic latency of each IoT node. Our results show that the proposed framework enhances the network performance by reducing the power consumption and latency.        △ Less","29 September, 2019",cs.NI,
              A Lightweight Deep Learning Model for Human Activity Recognition on Edge Devices          ,1909.12917,https://arxiv.org/abs/1909.12917,https://arxiv.org/pdf/1909.12917,"Authors:PreetiAgarwal,MansafAlam","        Human Activity Recognition (HAR) using wearable and mobile sensors has gained momentum in last few years, in various fields, such as, healthcare, surveillance, education, entertainment. Nowadays, Edge Computing has emerged to reduce communication latency and network traffic.Edge devices are resource constrained devices and cannot support high computation. In literature, various models have been developed for HAR. In recent years, deep learning algorithms have shown high performance in HAR, but these algorithms require lot of computation making them inefficient to be deployed on edge devices. This paper, proposes a Lightweight Deep Learning Model for HAR requiring less computational power, making it suitable to be deployed on edge devices. The performance of proposed model is tested on the participants six daily activities data. Results show that the proposed model outperforms many of the existing machine learning and deep learning techniques.        △ Less","20 September, 2019","eess.SP,cs.CV",
              MGP-AttTCN: An Interpretable Machine Learning Model for the Prediction of Sepsis          ,1909.12637,https://arxiv.org/abs/1909.12637,https://arxiv.org/pdf/1909.12637,"Authors:MargheritaRosnati,VincentFortuin","        With a mortality rate of 5.4 million lives worldwide every year and a healthcare cost of more than 16 billion dollars in the USA alone, sepsis is one of the leading causes of hospital mortality and an increasing concern in the ageing western world. Recently, medical and technological advances have helped re-define the illness criteria of this disease, which is otherwise poorly understood by the medical society. Together with the rise of widely accessible Electronic Health Records, the advances in data mining and complex nonlinear algorithms are a promising avenue for the early detection of sepsis. This work contributes to the research effort in the field of automated sepsis detection with an open-access labelling of the medical MIMIC-III data set. Moreover, we propose MGP-AttTCN: a joint multitask Gaussian Process and attention-based deep learning model to early predict the occurrence of sepsis in an interpretable manner. We show that our model outperforms the current state-of-the-art and present evidence that different labelling heuristics lead to discrepancies in task difficulty.        △ Less","27 September, 2019","cs.LG,stat.ML",
              Tyranny to Anarchy: Regimes of Organisational Influence on Directed Hierarchical Graphs          ,1909.12603,https://arxiv.org/abs/1909.12603,https://arxiv.org/pdf/1909.12603,"Authors:CharliePilgrim,WeisiGuo,SamuelJohnson","        Social organisation is critical to coordinated human behaviour. There are a diverse range of organisational structures, which can be thought of as power structures with ""managers"" and ""subordinates"". Often a change in one part can cause cascades throughout the organisation, which can be desirable or can lead to inefficiencies. As organisations change in size, complexity and structure, we analyse how their resilience to disturbances is affected. Here, we consider majority rule dynamics on organisations modelled as hierarchical directed graphs, where the directed edges indicate influence. We utilise a topological measure called the trophic incoherence parameter, q, which effectively gauges the stratification of power structure in an organisation. We show that this measure bounds regimes of behaviour. There is fast consensus at low q (e.g. tyranny), slow consensus at mid q (e.g. democracy), and no consensus at high q (e.g. anarchy). These regimes are investigated analytically and empirically with diverse case studies in the Roman Army, US Government, and a healthcare organisation. Our work has widespread application in the design and analysis of organisations.        △ Less","23 October, 2019","nlin.AO,physics.soc-ph",
              Set Functions for Time Series          ,1909.12064,https://arxiv.org/abs/1909.12064,https://arxiv.org/pdf/1909.12064,"Authors:MaxHorn,MichaelMoor,ChristianBock,BastianRieck,KarstenBorgwardt","        Despite the eminent successes of deep neural networks, many architectures are often hard to transfer to irregularly-sampled and asynchronous time series that commonly occur in real-world datasets, especially in healthcare applications. This paper proposes a novel approach for classifying irregularly-sampled time series with unaligned measurements, focusing on high scalability and data efficiency. Our method SeFT (Set Functions for Time Series) is based on recent advances in differentiable set function learning, extremely parallelizable with a beneficial memory footprint, thus scaling well to large datasets of long time series and online monitoring scenarios. Furthermore, our approach permits quantifying per-observation contributions to the classification outcome. We extensively compare our method with existing algorithms on multiple healthcare time series datasets and demonstrate that it performs competitively whilst significantly reducing runtime.        △ Less","14 September, 2020","cs.LG,stat.ML",
              Numerical evaluation of spray position for improved nasal drug delivery          ,1909.11960,https://arxiv.org/abs/1909.11960,https://arxiv.org/pdf/1909.11960,"Authors:SaikatBasu,LandonTHolbrook,KathrynKudlaty,OluladeFasanmade,JihongWu,AlyssaBurke,BenjaminLangworthy,ZainabFarzal,MohammedMamdani,WilliamDBennett,JasonFine,BrentASenior,AdamMZanation,CharlesSEbert,Jr.,AdamJKimple,BrianDThorp,DennisOFrank-Ito,GuilhermeJMGarcia,JuliaSKimbell","        Topical intra-nasal sprays are amongst the most commonly prescribed therapeutic options for sinonasal diseases in humans. However, inconsistency and ambiguity in instructions show a lack of definitive knowledge on best spray use techniques. In this study, we have identified a new usage strategy for nasal sprays available over-the-counter, that registers an average 8-fold improvement in topical delivery of drugs at diseased sites, when compared to prevalent spray techniques. The protocol involves re-orienting the spray axis to harness inertial motion of particulates and has been developed using computational fluid dynamics simulations of respiratory airflow and droplet transport in medical imaging-based digital models. Simulated dose in representative models is validated through in vitro spray measurements in 3D-printed anatomic replicas using the gamma scintigraphy technique. This work breaks new ground in proposing an alternative user-friendly strategy that can significantly enhance topical delivery inside human nose. While these findings can eventually translate into personalized spray usage instructions and hence merit a change in nasal standard-of-care, this study also demonstrates how relatively simple engineering analysis tools can revolutionize everyday healthcare.        △ Less","20 December, 2019","physics.med-ph,physics.bio-ph,physics.flu-dyn",
              Transient Dynamics of Infection Transmission in a Simulated Intensive Care Unit          ,1909.11878,https://arxiv.org/abs/1909.11878,https://arxiv.org/pdf/1909.11878,"Authors:ChristopherT.Short,MatthewS.Mietchen,EricT.Lofgren","Healthcare-associated infections (HAIs) remain a public health problem. Previous work showed intensive care unit (ICU) population structure impacts methicillin-resistant Staphylococcus aureus (MRSA) rates. Unexplored in that work was the transient dynamics of this system. We consider the dynamics of MRSA in an ICU in three different models: 1) a Ross-McDonald model with a single healthcare staff type, 2) a Ross-McDonald model with nurses and doctors considered as separate populations and 3) a meta-population model that segments patients into smaller groups seen by a single nurse. The basic reproduction number, R0 is derived using the Next Generation Matrix method, while the importance of the position of patients within the meta-population model is assessed via stochastic simulation. The single-staff model had an R0 of 0.337, while the other two models had R0s of 0.278. The meta-population model's R0 was not sensitive to the time nurses spent with their assigned patients vs. unassigned patients. This suggests previous results showing that simulated infection rates are dependent on this parameter are the result of differences in the transient dynamics between the models, rather than differing long-term equilibria.        △ Less","26 September, 2019",q-bio.PE,
              Deep-learning-based Breast CT for Radiation Dose Reduction          ,1909.11721,https://arxiv.org/abs/1909.11721,https://arxiv.org/pdf/1909.11721,"Authors:WenxiangCong,HongmingShan,XiaohuaZhang,ShaohuaLiu,RuolaNing,GeWang","        Cone-beam breast computed tomography (CT) provides true 3D breast images with isotropic resolution and high-contrast information, detecting calcifications as small as a few hundred microns and revealing subtle tissue differences. However, breast is highly sensitive to x-ray radiation. It is critically important for healthcare to reduce radiation dose. Few-view cone-beam CT only uses a fraction of x-ray projection data acquired by standard cone-beam breast CT, enabling significant reduction of the radiation dose. However, insufficient sampling data would cause severe streak artifacts in CT images reconstructed using conventional methods. In this study, we propose a deep-learning-based method to establish a residual neural network model for the image reconstruction, which is applied for few-view breast CT to produce high quality breast CT images. We respectively evaluate the deep-learning-based image reconstruction using one third and one quarter of x-ray projection views of the standard cone-beam breast CT. Based on clinical breast imaging dataset, we perform a supervised learning to train the neural network from few-view CT images to corresponding full-view CT images. Experimental results show that the deep learning-based image reconstruction method allows few-view breast CT to achieve a radiation dose <6 mGy per cone-beam CT scan, which is a threshold set by FDA for mammographic screening.        △ Less","25 September, 2019","physics.med-ph,cs.CV,eess.IV",10.1117/12.2530234 
              Interpretable Models for Understanding Immersive Simulations          ,1909.11025,https://arxiv.org/abs/1909.11025,https://arxiv.org/pdf/1909.11025,"Authors:NicholasHoernle,KobiGal,BarbaraGrosz,LeilahLyons,AdaRen,AndeeRubin","        This paper describes methods for comparative evaluation of the interpretability of models of high dimensional time series data inferred by unsupervised machine learning algorithms. The time series data used in this investigation were logs from an immersive simulation like those commonly used in education and healthcare training. The structures learnt by the models provide representations of participants' activities in the simulation which are intended to be meaningful to people's interpretation. To choose the model that induces the best representation, we designed two interpretability tests, each of which evaluates the extent to which a model's output aligns with people's expectations or intuitions of what has occurred in the simulation. We compared the performance of the models on these interpretability tests to their performance on statistical information criteria. We show that the models that optimize interpretability quality differ from those that optimize (statistical) information theoretic criteria. Furthermore, we found that a model using a fully Bayesian approach performed well on both the statistical and human-interpretability measures. The Bayesian approach is a good candidate for fully automated model selection, i.e., when direct empirical investigations of interpretability are costly or infeasible.        △ Less","4 May, 2020",cs.AI,
              Securing Big Data from Eavesdropping Attacks in SCADA/ICS Network Data Streams through Impulsive Statistical Fingerprinting          ,1909.11021,https://arxiv.org/abs/1909.11021,https://arxiv.org/pdf/1909.11021,"Authors:JunaidChaudhry,UvaisQidwai,MahdiH.Miraz","        While data from Supervisory Control And Data Acquisition (SCADA) systems is sent upstream, it is both the length of pulses as well as their frequency present an excellent opportunity to incor-porate statistical fingerprinting. This is so, because datagrams in SCADA traffic follow a poison distribution. Although wrapping the SCADA traffic in a protective IPsec stream is an obvious choice, thin clients and unreliable communication channels make is less than ideal to use crypto-graphic solutions for security SCADA traffic. In this paper, we propose a smart alternative of data obfuscation in the form of Impulsive Statistical Fingerprinting (ISF). We provide important insights into our research in healthcare SCADA data security and the use of ISF. We substantiate the conversion of sensor data through the ISF into HL7 format and define policies of a seamless switch to a non HL7-based non-secure HIS to a secure HIS.        △ Less","2 September, 2019","cs.CR,cs.NI",10.1007/978-3-030-23943-5_6 
              Towards the Uses of Blockchain in Mobile Health Services and Applications: A Survey          ,1909.11013,https://arxiv.org/abs/1909.11013,/search/?searchtype=author&query=Santos%2C+J+A,"Authors:JoãoAmaralSantos,PedroR.M.Inácio,BrunoM.Silva","        With the advent of Bitcoin and blockchain, the growth and adaptation of cryptographic features and capabilities were quickly extended to new and underexplored areas, such as healthcare. Currently, blockchain is being implemented mainly as a mechanism to secure Electronic Health Records (EHRs). However, new studies have shown that this technology can be a powerful tool in empowering patients to control their own health data, as well for enabling a fool-proof health data history and establishing medical responsibility. With the advent of mobile health (m-Health) sustained on service-oriented architectures, the adaptation of blockchain mechanisms into m-Health applications creates the possibility for a more decentralized and available healthcare service. Hence, this paper presents a review of the current security best practices for m-Health including blockchain technologies in healthcare. Moreover, it discusses and elaborates on identified open-issues and potentialities regarding the uses of Blockchain. Finally, the paper proposes conceptual solutions for future blockchain implementations for m-Health Services and Applications.        △ Less","1 October, 2019","cs.CR,cs.DC",
              Fuzzy Knowledge-Based Architecture for Learning and Interaction in Social Robots          ,1909.11004,https://arxiv.org/abs/1909.11004,https://arxiv.org/pdf/1909.11004,"Authors:MehdiGhayoumi,MaryamPourebadi","        In this paper, we introduce an extension of our presented cognitive-based emotion model [27][28]and [30], where we enhance our knowledge-based emotion unit of the architecture by embedding a fuzzy rule-based system to it. The model utilizes the cognitive parameters dependency and their corresponding weights to regulate the robot's behavior and fuse their behavior data to achieve the final decision in their interaction with the environment. Using this fuzzy system, our previous model can simulate linguistic parameters for better controlling and generating understandable and flexible behaviors in the robots. We implement our model on an assistive healthcare robot, named Robot Nurse Assistant (RNA) and test it with human subjects. Our model records all the emotion states and essential information based on its predefined rules and learning system. Our results show that our robot interacts with patients in a reasonable, faithful way in special conditions which are defined by rules. This work has the potential to provide better on-demand service for clinical experts to monitor the patients' emotion states and help them make better decisions accordingly.        △ Less","14 September, 2019","cs.RO,cs.AI,cs.CV,cs.HC",
              Security analysis of two lightweight certificateless signature schemes          ,1909.10816,https://arxiv.org/abs/1909.10816,https://arxiv.org/pdf/1909.10816,Authors:NasrollahPakniat,"        Certificateless cryptography can be considered as an intermediate solution to overcome the issues in traditional public key infrastructure (PKI) and identity-based public key cryptography (ID-PKC). There exist a vast number of certificateless signature (CLS) schemes in the literature; however, most of them are not efficient enough to be utilized in limited resources environments such as Internet of things (IoT) or Healthcare Wireless Sensor Networks (HWSN). Recently, two lightweight CLS schemes have been proposed by Karati et al. and Kumar et al. to be employed in IoT and HWSNs, respectively. While both schemes are claimed to be existentially unforgeable, in this paper, we show that both these signatures can easily be forged. More specifically, it is shown that 1) in Karati et al.'s scheme, a type 1 adversary, considered in certificateless cryptography, can generate a valid partial private key corresponding to any user of its choice and as a consequence, it can forge any users' signature on any message of its choice, and 2) in Kumar et al.'s scheme, both types of adversaries which are considered in certificateless cryptography are able to forge any signer's signature on an arbitrary message.        △ Less","24 September, 2019",cs.CR,10.22108/jcs.2019.110889 
              HealthGuard: A Machine Learning-Based Security Framework for Smart Healthcare Systems          ,1909.10565,https://arxiv.org/abs/1909.10565,https://arxiv.org/pdf/1909.10565,"Authors:AKMIqtidarNewaz,AmitKumarSikder,MohammadAshiqurRahman,A.SelcukUluagac","        The integration of Internet-of-Things and pervasive computing in medical devices have made the modern healthcare system ""smart"". Today, the function of the healthcare system is not limited to treat the patients only. With the help of implantable medical devices and wearables, Smart Healthcare System (SHS) can continuously monitor different vital signs of a patient and automatically detect and prevent critical medical conditions. However, these increasing functionalities of SHS raise several security concerns and attackers can exploit the SHS in numerous ways: they can impede normal function of the SHS, inject false data to change vital signs, and tamper a medical device to change the outcome of a medical emergency. In this paper, we propose HealthGuard, a novel machine learning-based security framework to detect malicious activities in a SHS. HealthGuard observes the vital signs of different connected devices of a SHS and correlates the vitals to understand the changes in body functions of the patient to distinguish benign and malicious activities. HealthGuard utilizes four different machine learning-based detection techniques (Artificial Neural Network, Decision Tree, Random Forest, k-Nearest Neighbor) to detect malicious activities in a SHS. We trained HealthGuard with data collected for eight different smart medical devices for twelve benign events including seven normal user activities and five disease-affected events. Furthermore, we evaluated the performance of HealthGuard against three different malicious threats. Our extensive evaluation shows that HealthGuard is an effective security framework for SHS with an accuracy of 91% and an F-1 score of 90%.        △ Less","23 September, 2019",cs.CR,
              IoT Inspector: Crowdsourcing Labeled Network Traffic from Smart Home Devices at Scale          ,1909.09848,https://arxiv.org/abs/1909.09848,https://arxiv.org/pdf/1909.09848,"Authors:DannyYuxingHuang,NoahApthorpe,GunesAcar,FrankLi,NickFeamster","        The proliferation of smart home devices has created new opportunities for empirical research in ubiquitous computing, ranging from security and privacy to personal health. Yet, data from smart home deployments are hard to come by, and existing empirical studies of smart home devices typically involve only a small number of devices in lab settings. To contribute to data-driven smart home research, we crowdsource the largest known dataset of labeled network traffic from smart home devices from within real-world home networks. To do so, we developed and released IoT Inspector, an open-source tool that allows users to observe the traffic from smart home devices on their own home networks. Since April 2019, 4,322 users have installed IoT Inspector, allowing us to collect labeled network traffic from 44,956 smart home devices across 13 categories and 53 vendors. We demonstrate how this data enables new research into smart homes through two case studies focused on security and privacy. First, we find that many device vendors use outdated TLS versions and advertise weak ciphers. Second, we discover about 350 distinct third-party advertiser and tracking domains on smart TVs. We also highlight other research areas, such as network management and healthcare, that can take advantage of IoT Inspector's dataset. To facilitate future reproducible research in smart homes, we will release the IoT Inspector data to the public.        △ Less","21 September, 2019",cs.CR,
              Leveraging Implicit Expert Knowledge for Non-Circular Machine Learning in Sepsis Prediction          ,1909.09557,https://arxiv.org/abs/1909.09557,https://arxiv.org/pdf/1909.09557,"Authors:ShigehikoSchamoni,HolgerA.Lindner,VerenaSchneider-Lindner,ManfredThiel,StefanRiezler","        Sepsis is the leading cause of death in non-coronary intensive care units. Moreover, a delay of antibiotic treatment of patients with severe sepsis by only few hours is associated with increased mortality. This insight makes accurate models for early prediction of sepsis a key task in machine learning for healthcare. Previous approaches have achieved high AUROC by learning from electronic health records where sepsis labels were defined automatically following established clinical criteria. We argue that the practice of incorporating the clinical criteria that are used to automatically define ground truth sepsis labels as features of severity scoring models is inherently circular and compromises the validity of the proposed approaches. We propose to create an independent ground truth for sepsis research by exploiting implicit knowledge of clinical practitioners via an electronic questionnaire which records attending physicians' daily judgements of patients' sepsis status. We show that despite its small size, our dataset allows to achieve state-of-the-art AUROC scores. An inspection of learned weights for standardized features of the linear model lets us infer potentially surprising feature contributions and allows to interpret seemingly counterintuitive findings.        △ Less","20 September, 2019","q-bio.QM,cs.LG,stat.ML",10.1016/j.artmed.2019.101725 
              Representation Learning for Electronic Health Records          ,1909.09248,https://arxiv.org/abs/1909.09248,https://arxiv.org/pdf/1909.09248,"Authors:Wei-HungWeng,PeterSzolovits","        Information in electronic health records (EHR), such as clinical narratives, examination reports, lab measurements, demographics, and other patient encounter entries, can be transformed into appropriate data representations that can be used for downstream clinical machine learning tasks using representation learning. Learning better representations is critical to improve the performance of downstream tasks. Due to the advances in machine learning, we now can learn better and meaningful representations from EHR through disentangling the underlying factors inside data and distilling large amounts of information and knowledge from heterogeneous EHR sources. In this chapter, we first introduce the background of learning representations and reasons why we need good EHR representations in machine learning for medicine and healthcare in Section 1. Next, we explain the commonly-used machine learning and evaluation methods for representation learning using a deep learning approach in Section 2. Following that, we review recent related studies of learning patient state representation from EHR for clinical machine learning tasks in Section 3. Finally, in Section 4 we discuss more techniques, studies, and challenges for learning natural language representations when free texts, such as clinical notes, examination reports, or biomedical literature are used. We also discuss challenges and opportunities in these rapidly growing research fields.        △ Less","19 September, 2019","cs.LG,stat.ML",
              Sentiment-Aware Recommendation System for Healthcare using Social Media          ,1909.08686,https://arxiv.org/abs/1909.08686,https://arxiv.org/pdf/1909.08686,"Authors:AlanAipe,MukunthaNarayananSundararaman,AsifEkbal","        Over the last decade, health communities (known as forums) have evolved into platforms where more and more users share their medical experiences, thereby seeking guidance and interacting with people of the community. The shared content, though informal and unstructured in nature, contains valuable medical and/or health-related information and can be leveraged to produce structured suggestions to the common people. In this paper, at first we propose a stacked deep learning model for sentiment analysis from the medical forum data. The stacked model comprises of Convolutional Neural Network (CNN) followed by a Long Short Term Memory (LSTM) and then by another CNN. For a blog classified with positive sentiment, we retrieve the top-n similar posts. Thereafter, we develop a probabilistic model for suggesting the suitable treatments or procedures for a particular disease or health condition. We believe that integration of medical sentiment and suggestion would be beneficial to the users for finding the relevant contents regarding medications and medical conditions, without having to manually stroll through a large amount of unstructured contents.        △ Less","18 September, 2019",cs.CL,
"              Design research, eHealth, and the convergence revolution          ",1909.08398,https://arxiv.org/abs/1909.08398,https://arxiv.org/pdf/1909.08398,"Authors:ValeriaPannunzio,MaaikeKleinsmann,DirkSnelders","        The Quadruple Aim is a framework which prioritizes four aims, or dimensions of performance, for innovating in the healthcare domain, respectively: 1) enhancing the individual experience of care; 2) improving the work life of health care clinicians and staff; 3) improving the health of populations; and 4) reducing the per capita cost of care. In this contribution, recent literature providing examples of design research in the eHealth domain is reviewed to answer the research question: in which measure has design research contributed to each of the four aims of eHealth innovation in the past five years?. The results of the review are presented and employed to draw three main observations: 1) design researchers in eHealth seem to be largely focused on improving experiences of care, either patients' or health professionals; 2) design researchers' contribution on reducing per capita costs of care appears to be less pronounced, which is outlined as a point for improvement; and 3) in a considerable amount of reviewed contributions, design researchers appear to be contributing to multiple aims at once. In this sub-group of reviewed contributions, several disciplinary areas and types of stakeholders interact and integrate through design research activities. The latter observation leads to a reflection on the strategic role of design research in the contexts of the convergence revolution and of the non-communicable disease crisis. Implications of this reflection for design researchers are recognized in the opportunity and timeliness to develop eHealth-specific ways to orchestrate design integration. A direction for further research in this sense is identified in the use of sensory and self-monitored data as a boundary object for eHealth innovation. The prospective value of this direction is finally exemplified through the case of blood pressure.        △ Less","13 September, 2019",cs.HC,
              Analysis of the hospital records from AOK Plus          ,1909.08169,https://arxiv.org/abs/1909.08169,https://arxiv.org/pdf/1909.08169,"Authors:AgataLonc,MonikaJ.Piotrowska,KonradSakowski","        We present analysis of anonymised admission/discharge data from insurance provider for Saxony and Thuringia (Germany) for years 2010--2016. Study of such data are necessary to derive a structure of healthcare system transfer network, as no patients' transfer data are currently available. Hospital network can be directly used as a basis for modelling of multidrug-resistant pathogen spread allowing to study the effectiveness of disease-control strategies. In this paper, the properties of the dataset under consideration are presented and discussed.        △ Less","17 September, 2019",stat.AP,
              The Disruptions of 5G on Data-driven Technologies and Applications          ,1909.08096,https://arxiv.org/abs/1909.08096,https://arxiv.org/pdf/1909.08096,"Authors:DumitrelLoghin,ShaofengCai,GangChen,TienTuanAnhDinh,FeiyiFan,QianLin,JaniceNg,BengChinOoi,XutaoSun,Quang-TrungTa,WeiWang,XiaokuiXiao,YangYang,MeihuiZhang,ZhonghuaZhang","        With 5G on the verge of being adopted as the next mobile network, there is a need to analyze its impact on the landscape of computing and data management. In this paper, we analyze the impact of 5G on both traditional and emerging technologies and project our view on future research challenges and opportunities. With a predicted increase of 10-100x in bandwidth and 5-10x decrease in latency, 5G is expected to be the main enabler for smart cities, smart IoT and efficient healthcare, where machine learning is conducted at the edge. In this context, we investigate how 5G can help the development of federated learning. Network slicing, another key feature of 5G, allows running multiple isolated networks on the same physical infrastructure. However, security remains the main concern in the context of virtualization, multi-tenancy and high device density. Formal verification of 5G networks can be applied to detect security issues in massive virtualized environments. In summary, 5G will make the world even more densely and closely connected. What we have experienced in 4G connectivity will pale in comparison to the vast amounts of possibilities engendered by 5G.        △ Less","15 December, 2019","cs.NI,cs.DB,cs.DC",
              Machine learning in healthcare -- a system's perspective          ,1909.07370,https://arxiv.org/abs/1909.07370,https://arxiv.org/pdf/1909.07370,"Authors:AwaisAshfaq,SlawomirNowaczyk","        A consequence of the fragmented and siloed healthcare landscape is that patient care (and data) is split along multitude of different facilities and computer systems and enabling interoperability between these systems is hard. The lack interoperability not only hinders continuity of care and burdens providers, but also hinders effective application of Machine Learning (ML) algorithms. Thus, most current ML algorithms, designed to understand patient care and facilitate clinical decision-support, are trained on limited datasets. This approach is analogous to the Newtonian paradigm of Reductionism in which a system is broken down into elementary components and a description of the whole is formed by understanding those components individually. A key limitation of the reductionist approach is that it ignores the component-component interactions and dynamics within the system which are often of prime significance in understanding the overall behaviour of complex adaptive systems (CAS). Healthcare is a CAS.  Though the application of ML on health data have shown incremental improvements for clinical decision support, ML has a much a broader potential to restructure care delivery as a whole and maximize care value. However, this ML potential remains largely untapped: primarily due to functional limitations of Electronic Health Records (EHR) and the inability to see the healthcare system as a whole. This viewpoint (i) articulates the healthcare as a complex system which has a biological and an organizational perspective, (ii) motivates with examples, the need of a system's approach when addressing healthcare challenges via ML and, (iii) emphasizes to unleash EHR functionality - while duly respecting all ethical and legal concerns - to reap full benefits of ML.        △ Less","19 January, 2020","cs.CY,cs.AI,cs.LG",
              Reliability and Safety Modeling of a Digital Feed Water Control System          ,1909.07210,https://arxiv.org/abs/1909.07210,https://arxiv.org/pdf/1909.07210,"Authors:ShawkatS.Khairullah,AhmedA.Mostfa",        Much digital instrumentation and control systems embedded in the critical medical healthcare equipment aerospace devices and nuclear industry have obvious consequence of different failure modes. These failures can affect the behavior of the overall safety critical digital system and its ability to deliver its dependability attributes if any defected area that could be a hardware component or software code embedded inside the digital system is not detected and repaired appropriately. The safety and reliability analysis of safety critical systems can be accomplished with Markov modeling techniques which could express the dynamic and regenerative behavior of the digital control system. Certain states in the system represent system failure while others represent fault free behavior or correct operation in the presence of faults. This paper presents the development of a safety and reliability modeling of a digital feedwater control system using Markov based chain models. All the Markov states and the transitions between these states were assumed and calculated from the control logic for the digital control system. Finally based on the simulation results of modeling the digital feedwater control system the system does meet its reliability requirement with the probability of being in fully operational states is 0.99 over a 6 months time.        △ Less,"20 July, 2020","eess.SY,eess.SP",
              Distributed representation of patients and its use for medical cost prediction          ,1909.07157,https://arxiv.org/abs/1909.07157,https://arxiv.org/pdf/1909.07157,"Authors:XianlongZeng,SoheilMoosavinasab,En-JuDLin,SimonLin,RazvanBunescu,ChangLiu","        Efficient representation of patients is very important in the healthcare domain and can help with many tasks such as medical risk prediction. Many existing methods, such as diagnostic Cost Groups (DCG), rely on expert knowledge to build patient representation from medical data, which is resource consuming and non-scalable. Unsupervised machine learning algorithms are a good choice for automating the representation learning process. However, there is very little research focusing on onpatient-level representation learning directly from medical claims. In this paper, weproposed a novel patient vector learning architecture that learns high quality,fixed-length patient representation from claims data. We conducted several experiments to test the quality of our learned representation, and the empirical results show that our learned patient vectors are superior to vectors learned through other methods including a popular commercial model. Lastly, we provide potential clinical interpretation for using our representation on predictive tasks, as interpretability is vital in the healthcare domain        △ Less","13 September, 2019","cs.LG,cs.AI",
              Meta-Learning for Few-Shot Time Series Classification          ,1909.07155,https://arxiv.org/abs/1909.07155,https://arxiv.org/pdf/1909.07155,"Authors:JyotiNarwariya,PankajMalhotra,LovekeshVig,GautamShroff,VishnuTv","        Deep neural networks (DNNs) have achieved state-of-the-art results on time series classification (TSC) tasks. In this work, we focus on leveraging DNNs in the often-encountered practical scenario where access to labeled training data is difficult, and where DNNs would be prone to overfitting. We leverage recent advancements in gradient-based meta-learning, and propose an approach to train a residual neural network with convolutional layers as a meta-learning agent for few-shot TSC. The network is trained on a diverse set of few-shot tasks sampled from various domains (e.g. healthcare, activity recognition, etc.) such that it can solve a target task from another domain using only a small number of training samples from the target task. Most existing meta-learning approaches are limited in practice as they assume a fixed number of target classes across tasks. We overcome this limitation in order to train a common agent across domains with each domain having different number of target classes, we utilize a triplet-loss based learning procedure that does not require any constraints to be enforced on the number of classes for the few-shot TSC tasks. To the best of our knowledge, we are the first to use meta-learning based pre-training for TSC. Our approach sets a new benchmark for few-shot TSC, outperforming several strong baselines on few-shot tasks sampled from 41 datasets in UCR TSC Archive. We observe that pre-training under the meta-learning paradigm allows the network to quickly adapt to new unseen tasks with small number of labeled instances.        △ Less","25 September, 2019","cs.LG,stat.ML",
              Target-Focused Feature Selection Using a Bayesian Approach          ,1909.06772,https://arxiv.org/abs/1909.06772,https://arxiv.org/pdf/1909.06772,"Authors:OrpazGoldstein,MohammadKachuee,KimmoKarkkainen,MajidSarrafzadeh","        In many real-world scenarios where data is high dimensional, test time acquisition of features is a non-trivial task due to costs associated with feature acquisition and evaluating feature value. The need for highly confident models with an extremely frugal acquisition of features can be addressed by allowing a feature selection method to become target aware. We introduce an approach to feature selection that is based on Bayesian learning, allowing us to report target-specific levels of uncertainty, false positive, and false negative rates. In addition, measuring uncertainty lifts the restriction on feature selection being target agnostic, allowing for feature acquisition based on a single target of focus out of many. We show that acquiring features for a specific target is at least as good as common linear feature selection approaches for small non-sparse datasets, and surpasses these when faced with real-world healthcare data that is larger in scale and in sparseness.        △ Less","15 September, 2019","cs.LG,stat.ML",
              Delivering Cognitive Behavioral Therapy Using A Conversational SocialRobot          ,1909.06670,https://arxiv.org/abs/1909.06670,https://arxiv.org/pdf/1909.06670,"Authors:FrancescaDino,RoholaZandie,HojjatAbdollahi,SarahSchoeder,MohammadH.Mahoor","        Social robots are becoming an integrated part of our daily life due to their ability to provide companionship and entertainment. A subfield of robotics, Socially Assistive Robotics (SAR), is particularly suitable for expanding these benefits into the healthcare setting because of its unique ability to provide cognitive, social, and emotional support. This paper presents our recent research on developing SAR by evaluating the ability of a life-like conversational social robot, called Ryan, to administer internet-delivered cognitive behavioral therapy (iCBT) to older adults with depression. For Ryan to administer the therapy, we developed a dialogue-management system, called Program-R. Using an accredited CBT manual for the treatment of depression, we created seven hour-long iCBT dialogues and integrated them into Program-R using Artificial Intelligence Markup Language (AIML). To assess the effectiveness of Robot-based iCBT and users' likability of our approach, we conducted an HRI study with a cohort of elderly people with mild-to-moderate depression over a period of four weeks. Quantitative analyses of participant's spoken responses (e.g. word count and sentiment analysis), face-scale mood scores, and exit surveys, strongly support the notion robot-based iCBT is a viable alternative to traditional human-delivered therapy.        △ Less","14 September, 2019","cs.RO,cs.CL",
              Current Challenges in Spoken Dialogue Systems and Why They Are Critical for Those Living with Dementia          ,1909.06644,https://arxiv.org/abs/1909.06644,https://arxiv.org/pdf/1909.06644,"Authors:AngusAddlesee,ArashEshghi,IoannisKonstas","        Dialogue technologies such as Amazon's Alexa have the potential to transform the healthcare industry. However, current systems are not yet naturally interactive: they are often turn-based, have naive end-of-turn detection and completely ignore many types of verbal and visual feedback - such as backchannels, hesitation markers, filled pauses, gaze, brow furrows and disfluencies - that are crucial in guiding and managing the conversational process. This is especially important in the healthcare industry as target users of Spoken Dialogue Systems (SDSs) are likely to be frail, older, distracted or suffer from cognitive decline which impacts their ability to make effective use of current systems. In this paper, we outline some of the challenges that are in urgent need of further research, including Incremental Speech Recognition and a systematic study of the interactional patterns in conversation that are potentially diagnostic of dementia, and how these might inform research on and the design of the next generation of SDSs.        △ Less","14 September, 2019",cs.CL,
              Encoding Visual Attributes in Capsules for Explainable Medical Diagnoses          ,1909.05926,https://arxiv.org/abs/1909.05926,https://arxiv.org/pdf/1909.05926,"Authors:RodneyLaLonde,DrewTorigian,UlasBagci","        Convolutional neural network based systems have largely failed to be adopted in many high-risk application areas, including healthcare, military, security, transportation, finance, and legal, due to their highly uninterpretable ""black-box"" nature. Towards solving this deficiency, we teach a novel multi-task capsule network to improve the explainability of predictions by embodying the same high-level language used by human-experts. Our explainable capsule network, X-Caps, encodes high-level visual object attributes within the vectors of its capsules, then forms predictions based solely on these human-interpretable features. To encode attributes, X-Caps utilizes a new routing sigmoid function to independently route information from child capsules to parents. Further, to provide radiologists with an estimate of model confidence, we train our network on a distribution of expert labels, modeling inter-observer agreement and punishing over/under confidence during training, supervised by human-experts' agreement. X-Caps simultaneously learns attribute and malignancy scores from a multi-center dataset of over 1000 CT scans of lung cancer screening patients. We demonstrate a simple 2D capsule network can outperform a state-of-the-art deep dense dual-path 3D CNN at capturing visually-interpretable high-level attributes and malignancy prediction, while providing malignancy prediction scores approaching that of non-explainable 3D CNNs. To the best of our knowledge, this is the first study to investigate capsule networks for making predictions based on radiologist-level interpretable attributes and its applications to medical image diagnosis. Code is publicly available at https://github.com/lalonderodney/X-Caps .        △ Less","20 June, 2020","eess.IV,cs.CV,cs.LG,stat.ML",
              AnimalWeb: A Large-Scale Hierarchical Dataset of Annotated Animal Faces          ,1909.04951,https://arxiv.org/abs/1909.04951,https://arxiv.org/pdf/1909.04951,"Authors:MuhammadHarisKhan,JohnMcDonagh,SalmanKhan,MuhammadShahabuddin,AdityaArora,FahadShahbazKhan,LingShao,GeorgiosTzimiropoulos","        Being heavily reliant on animals, it is our ethical obligation to improve their well-being by understanding their needs. Several studies show that animal needs are often expressed through their faces. Though remarkable progress has been made towards the automatic understanding of human faces, this has regrettably not been the case with animal faces. There exists significant room and appropriate need to develop automatic systems capable of interpreting animal faces. Among many transformative impacts, such a technology will foster better and cheaper animal healthcare, and further advance animal psychology understanding.  We believe the underlying research progress is mainly obstructed by the lack of an adequately annotated dataset of animal faces, covering a wide spectrum of animal species. To this end, we introduce a large-scale, hierarchical annotated dataset of animal faces, featuring 21.9K faces from 334 diverse species and 21 animal orders across biological taxonomy. These faces are captured `in-the-wild' conditions and are consistently annotated with 9 landmarks on key facial features. The proposed dataset is structured and scalable by design; its development underwent four systematic stages involving rigorous, manual annotation effort of over 6K man-hours. We benchmark it for face alignment using the existing art under novel problem settings. Results showcase its challenging nature, unique attributes and present definite prospects for novel, adaptive, and generalized face-oriented CV algorithms. We further benchmark the dataset for face detection and fine-grained recognition tasks, to demonstrate multi-task applications and room for improvement. Experiments indicate that this dataset will push the algorithmic advancements across many related CV tasks and encourage the development of novel systems for animal facial behaviour monitoring. We will make the dataset publicly available.        △ Less","11 September, 2019",cs.CV,
              Machine learning for automatic construction of pseudo-realistic pediatric abdominal phantoms          ,1909.03723,https://arxiv.org/abs/1909.03723,https://arxiv.org/pdf/1909.03723,"Authors:MarcoVirgolin,ZiyuanWang,TanjaAlderliesten,PeterA.N.Bosman","        Machine Learning (ML) is proving extremely beneficial in many healthcare applications. In pediatric oncology, retrospective studies that investigate the relationship between treatment and late adverse effects still rely on simple heuristics. To assess the effects of radiation therapy, treatment plans are typically simulated on phantoms, i.e., virtual surrogates of patient anatomy. Currently, phantoms are built according to reasonable, yet simple, human-designed criteria. This often results in a lack of individualization. We present a novel approach that combines imaging and ML to build individualized phantoms automatically. Given the features of a patient treated historically (only 2D radiographs available), and a database of 3D Computed Tomography (CT) imaging with organ segmentations and relative patient features, our approach uses ML to predict how to assemble a patient-specific phantom automatically. Experiments on 60 abdominal CTs of pediatric patients show that our approach constructs significantly more representative phantoms than using current phantom building criteria, in terms of location and shape of the abdomen and of two considered organs, the liver and the spleen. Among several ML algorithms considered, the Gene-pool Optimal Mixing Evolutionary Algorithm for Genetic Programming (GP-GOMEA) is found to deliver the best performing models, which are, moreover, transparent and interpretable mathematical expressions.        △ Less","9 September, 2019","cs.LG,physics.med-ph,stat.AP,stat.ML",
              Krylov Subspace Method for Nonlinear Dynamical Systems with Random Noise          ,1909.03634,https://arxiv.org/abs/1909.03634,https://arxiv.org/pdf/1909.03634,"Authors:YukaHashimoto,IsaoIshikawa,MasahiroIkeda,YoichiMatsuo,YoshinobuKawahara","        Operator-theoretic analysis of nonlinear dynamical systems has attracted much attention in a variety of engineering and scientific fields, endowed with practical estimation methods using data such as dynamic mode decomposition. In this paper, we address a lifted representation of nonlinear dynamical systems with random noise based on transfer operators, and develop a novel Krylov subspace method for estimating the operators using finite data, with consideration of the unboundedness of operators. For this purpose, we first consider Perron-Frobenius operators with kernel-mean embeddings for such systems. We then extend the Arnoldi method, which is the most classical type of Kryov subspace method, so that it can be applied to the current case. Meanwhile, the Arnoldi method requires the assumption that the operator is bounded, which is not necessarily satisfied for transfer operators on nonlinear systems. We accordingly develop the shift-invert Arnoldi method for Perron-Frobenius operators to avoid this problem. Also, we describe an approach of evaluating predictive accuracy by estimated operators on the basis of the maximum mean discrepancy, which is applicable, for example, to anomaly detection in complex systems. The empirical performance of our methods is investigated using synthetic and real-world healthcare data.        △ Less","24 September, 2020","cs.LG,math.DS,math.FA,stat.ML",
              An Algorithm for Multi-Attribute Diverse Matching          ,1909.03350,https://arxiv.org/abs/1909.03350,https://arxiv.org/pdf/1909.03350,"Authors:SabaAhmadi,FaezAhmed,JohnP.Dickerson,MarkFuge,SamirKhuller","        Bipartite b-matching, where agents on one side of a market are matched to one or more agents or items on the other, is a classical model that is used in myriad application areas such as healthcare, advertising, education, and general resource allocation. Traditionally, the primary goal of such models is to maximize a linear function of the constituent matches (e.g., linear social welfare maximization) subject to some constraints. Recent work has studied a new goal of balancing whole-match diversity and economic efficiency, where the objective is instead a monotone submodular function over the matching. Basic versions of this problem are solvable in polynomial time. In this work, we prove that the problem of simultaneously maximizing diversity along several features (e.g., country of citizenship, gender, skills) is NP-hard. To address this problem, we develop the first combinatorial algorithm that constructs provably-optimal diverse b-matchings in pseudo-polynomial time. We also provide a Mixed-Integer Quadratic formulation for the same problem and show that our method guarantees optimal solutions and takes less computation time for a reviewer assignment application.        △ Less","12 February, 2020","cs.AI,cs.DS",
              Modelling Cooperation in a Dynamic Healthcare System          ,1909.03070,https://arxiv.org/abs/1909.03070,https://arxiv.org/pdf/1909.03070,"Authors:ZainabAlalawi,YifengZeng,TheAnhHan,AimanElragig","        Our research is concerned with studying behavioural changes within a dynamic system, i.e. health care, and their effects on the decision-making process. Evolutionary Game theory is applied to investigate the most probable strategy(ies) adopted by individuals in a finite population based on the interactions among them with an eye to modelling behaviour using the following metrics: cost of investment, cost of management, cost of treatment, reputation benefit for the provider(s), and the gained health benefit for the patient.        △ Less","6 September, 2019","cs.GT,cs.MA,econ.TH,math.DS",
              Big Data Intelligence Using Distributed Deep Neural Networks          ,1909.02873,https://arxiv.org/abs/1909.02873,https://arxiv.org/pdf/1909.02873,"Authors:FelixOngati,Dr.Eng.LawrenceMuchemi","        Large amount of data is often required to train and deploy useful machine learning models in industry. Smaller enterprises do not have the luxury of accessing enough data for machine learning, For privacy sensitive fields such as banking, insurance and healthcare, aggregating data to a data warehouse poses a challenge of data security and limited computational resources. These challenges are critical when developing machine learning algorithms in industry. Several attempts have been made to address the above challenges by using distributed learning techniques such as federated learning over disparate data stores in order to circumvent the need for centralised data aggregation. This paper proposes an improved algorithm to securely train deep neural networks over several data sources in a distributed way, in order to eliminate the need to centrally aggregate the data and the need to share the data thus preserving privacy. The proposed method allows training of deep neural networks using data from multiple de-linked nodes in a distributed environment and to secure the representation shared during training. Only a representation of the trained models (network architecture and weights) are shared. The algorithm was evaluated on existing healthcare patients data and the performance of this implementation was compared to that of a regular deep neural network trained on a single centralised architecture. This algorithm will pave a way for distributed training of neural networks on privacy sensitive applications where raw data may not be shared directly or centrally aggregating this data in a data warehouse is not feasible.        △ Less","4 September, 2019",cs.DC,
              Harnessing the Power of Deep Learning Methods in Healthcare: Neonatal Pain Assessment from Crying Sound          ,1909.02543,https://arxiv.org/abs/1909.02543,https://arxiv.org/pdf/1909.02543,"Authors:MdSirajusSalekin,GhadaZamzmi,RahulPaul,DmitryGoldgof,RangacharKasturi,ThaoHo,YuSun","        Neonatal pain assessment in clinical environments is challenging as it is discontinuous and biased. Facial/body occlusion can occur in such settings due to clinical condition, developmental delays, prone position, or other external factors. In such cases, crying sound can be used to effectively assess neonatal pain. In this paper, we investigate the use of a novel CNN architecture (N-CNN) along with other CNN architectures (VGG16 and ResNet50) for assessing pain from crying sounds of neonates. The experimental results demonstrate that using our novel N-CNN for assessing pain from the sounds of neonates has a strong clinical potential and provides a viable alternative to the current assessment practice.        △ Less","5 September, 2019",cs.CV,10.1109/HI-POCT45284.2019.8962827 
              Frameworks for Querying Databases Using Natural Language: A Literature Review          ,1909.01822,https://arxiv.org/abs/1909.01822,https://arxiv.org/pdf/1909.01822,"Authors:HafsaShareefDar,M.IkramullahLali,MoinUlDin,KhalidMahmoodMalik,SyedAhmadChanBukhari","        A Natural Language Interface (NLI) facilitates users to pose queries to retrieve information from a database without using any artificial language such as the Structured Query Language (SQL). Several applications in various domains including healthcare, customer support and search engines, require elaborating structured data having information on text. Moreover, many issues have been explored including configuration complexity, processing of intensive algorithms, and popularity of relational databases, due to which translating natural language to database query has become a secondary area of investigation. The emerging trend of querying systems and speech-enabled interfaces revived natural language to database queries research area., The last survey published on this topic was six years ago in 2013. To best of our knowledge, there is no recent study found which discusses the current state of the art translations frameworks for natural language for structured and non-structured query languages. In this paper, we have reviewed 47 frameworks from 2008 to 2018. Out of 47, 35 were closely relevant to our work. SQL based frameworks have been categorized as statistical, symbolic and connectionist approaches. Whereas, NoSQL based frameworks have been categorized as semantic matching and pattern matching. These frameworks are then reviewed based on their supporting language, scheme of their heuristic rule, interoperability support, dataset scope and their overall performance score. The findings stated that 70% of the work in natural language to database querying has been carried out for SQL, and NoSQL share 15%, 10% and 5% of languages like SPAROL, CYPHER and GREMLIN respectively. It has also been observed that most of the frameworks support English language only.        △ Less","3 September, 2019",cs.DB,
              Allen's Interval Algebra Makes the Difference          ,1909.01128,https://arxiv.org/abs/1909.01128,https://arxiv.org/pdf/1909.01128,"Authors:TomiJanhunen,MichaelSioutis","        Allen's Interval Algebra constitutes a framework for reasoning about temporal information in a qualitative manner. In particular, it uses intervals, i.e., pairs of endpoints, on the timeline to represent entities corresponding to actions, events, or tasks, and binary relations such as precedes and overlaps to encode the possible configurations between those entities. Allen's calculus has found its way in many academic and industrial applications that involve, most commonly, planning and scheduling, temporal databases, and healthcare. In this paper, we present a novel encoding of Interval Algebra using answer-set programming (ASP) extended by difference constraints, i.e., the fragment abbreviated as ASP(DL), and demonstrate its performance via a preliminary experimental evaluation. Although our ASP encoding is presented in the case of Allen's calculus for the sake of clarity, we suggest that analogous encodings can be devised for other point-based calculi, too.        △ Less","3 September, 2019",cs.AI,
              Multilevel latent class (MLC) modelling of healthcare provider causal effects on patient outcomes: Evaluation via simulation          ,1909.01035,https://arxiv.org/abs/1909.01035,https://arxiv.org/pdf/1909.01035,"Authors:WendyJ.Harrison,PaulD.Baxter,MarkS.Gilthorpe","        Where performance comparison of healthcare providers is of interest, characteristics of both patients and the health condition of interest must be balanced across providers for a fair comparison. This is unlikely to be feasible within observational data, as patient population characteristics may vary geographically and patient care may vary by characteristics of the health condition. We simulated data for patients and providers, based on a previously utilized real-world dataset, and separately considered both binary and continuous covariate-effects at the upper level. Multilevel latent class (MLC) modelling is proposed to partition a prediction focus at the patient level (accommodating casemix) and a causal inference focus at the provider level. The MLC model recovered a range of simulated Trust-level effects. Median recovered values were almost identical to simulated values for the binary Trust-level covariate, and we observed successful recovery of the continuous Trust-level covariate with at least 3 latent Trust classes. Credible intervals widen as the error variance increases. The MLC approach successfully partitioned modelling for prediction and for causal inference, addressing the potential conflict between these two distinct analytical strategies. This improves upon strategies which only adjust for differential selection. Patient-level variation and measurement uncertainty are accommodated within the latent classes.        △ Less","3 September, 2019","stat.ME,stat.AP",
              DeepHealth: Review and challenges of artificial intelligence in health informatics          ,1909.00384,https://arxiv.org/abs/1909.00384,https://arxiv.org/pdf/1909.00384,"Authors:GloriaHyunjungKwak,PanHui","        Artificial intelligence has provided us with an exploration of a whole new research era. As more data and better computational power become available, the approach is being implemented in various fields. The demand for it in health informatics is also increasing, and we can expect to see the potential benefits of its applications in healthcare. It can help clinicians diagnose disease, identify drug effects for each patient, understand the relationship between genotypes and phenotypes, explore new phenotypes or treatment recommendations, and predict infectious disease outbreaks with high accuracy. In contrast to traditional models, recent artificial intelligence approaches do not require domain-specific data pre-processing, and it is expected that it will ultimately change life in the future. Despite its notable advantages, there are some key challenges on data (high dimensionality, heterogeneity, time dependency, sparsity, irregularity, lack of label, bias) and model (reliability, interpretability, feasibility, security, scalability) for practical use. This article presents a comprehensive review of research applying artificial intelligence in health informatics, focusing on the last seven years in the fields of medical imaging, electronic health records, genomics, sensing, and online communication health, as well as challenges and promising directions for future research. We highlight ongoing popular approaches' research and identify several challenges in building models.        △ Less","8 August, 2020","cs.LG,cs.CV,eess.IV,stat.ML",
              Integrating Data and Image Domain Deep Learning for Limited Angle Tomography using Consensus Equilibrium          ,1909.00240,https://arxiv.org/abs/1909.00240,https://arxiv.org/pdf/1909.00240,"Authors:MuhammadUsmanGhani,W.ClemKarl","        Computed Tomography (CT) is a non-invasive imaging modality with applications ranging from healthcare to security. It reconstructs cross-sectional images of an object using a collection of projection data collected at different angles. Conventional methods, such as FBP, require that the projection data be uniformly acquired over the complete angular range. In some applications, it is not possible to acquire such data. Security is one such domain where non-rotational scanning configurations are being developed which violate the complete data assumption. Conventional methods produce images from such data that are filled with artifacts. The recent success of deep learning (DL) methods has inspired researchers to post-process these artifact laden images using deep neural networks (DNNs). This approach has seen limited success on real CT problems. Another approach has been to pre-process the incomplete data using DNNs aiming to avoid the creation of artifacts altogether. Due to imperfections in the learning process, this approach can still leave perceptible residual artifacts. In this work, we aim to combine the power of deep learning in both the data and image domains through a two-step process based on the consensus equilibrium (CE) framework. Specifically, we use conditional generative adversarial networks (cGANs) in both the data and the image domain for enhanced performance and efficient computation and combine them through a consensus process. We demonstrate the effectiveness of our approach on a real security CT dataset for a challenging 90 degree limited-angle problem. The same framework can be applied to other limited data problems arising in applications such as electron microscopy, non-destructive evaluation, and medical imaging.        △ Less","31 August, 2019","eess.IV,cs.CV,cs.LG",
              Triclustering of Gene Expression Microarray Data Using Coarse-Grained Parallel Genetic Algorithm          ,1909.00237,https://arxiv.org/abs/1909.00237,https://arxiv.org/pdf/1909.00237,"Authors:ShubhankarMohapatra,MoumitaSarkar,AnjaliMohapatra,BhawaniSankarBiswal","        Microarray data analysis is one of the major area of research in the field computational biology. Numerous techniques like clustering, biclustering are often applied to microarray data to extract meaningful outcomes which play key roles in practical healthcare affairs like disease identification, drug discovery etc. But these techniques become obsolete when time as an another factor is considered for evaluation in such data. This problem motivates to use triclustering method on gene expression 3D microarray data. In this article, a new methodology based on coarse-grained parallel genetic approach is proposed to locate meaningful triclusters in gene expression data. The outcomes are quite impressive as they are more effective as compared to traditional state of the art genetic approaches previously applied for triclustering of 3D GCT microarray data.        △ Less","31 August, 2019","cs.NE,q-bio.QM",
              Extracting information from free text through unsupervised graph-based clustering: an application to patient incident records          ,1909.00183,https://arxiv.org/abs/1909.00183,https://arxiv.org/pdf/1909.00183,"Authors:M.TarikAltuncu,EloiseSorin,JoshuaD.Symons,ErikMayer,SophiaN.Yaliraki,FrancescaToni,MauricioBarahona","        The large volume of text in electronic healthcare records often remains underused due to a lack of methodologies to extract interpretable content. Here we present an unsupervised framework for the analysis of free text that combines text-embedding with paragraph vectors and graph-theoretical multiscale community detection. We analyse text from a corpus of patient incident reports from the National Health Service in England to find content-based clusters of reports in an unsupervised manner and at different levels of resolution. Our unsupervised method extracts groups with high intrinsic textual consistency and compares well against categories hand-coded by healthcare personnel. We also show how to use our content-driven clusters to improve the supervised prediction of the degree of harm of the incident based on the text of the report. Finally, we discuss future directions to monitor reports over time, and to detect emerging trends outside pre-existing categories.        △ Less","31 August, 2019","cs.LG,cs.CL,cs.IR,math.SP,stat.ML",
              Systematic Analysis of Image Generation using GANs          ,1908.11863,https://arxiv.org/abs/1908.11863,https://arxiv.org/pdf/1908.11863,"Authors:RohanAkut,SumukhMarathe,RuchaApte,IshanJoshi,SiddhivinayakKulkarni","        Generative Adversarial Networks have been crucial in the developments made in unsupervised learning in recent times. Exemplars of image synthesis from text or other images, these networks have shown remarkable improvements over conventional methods in terms of performance. Trained on the adversarial training philosophy, these networks aim to estimate the potential distribution from the real data and then use this as input to generate the synthetic data. Based on this fundamental principle, several frameworks can be generated that are paragon implementations in several real-life applications such as art synthesis, generation of high resolution outputs and synthesis of images from human drawn sketches, to name a few. While theoretically GANs present better results and prove to be an improvement over conventional methods in many factors, the implementation of these frameworks for dedicated applications remains a challenge. This study explores and presents a taxonomy of these frameworks and their use in various image to image synthesis and text to image synthesis applications. The basic GANs, as well as a variety of different niche frameworks, are critically analyzed. The advantages of GANs for image generation over conventional methods as well their disadvantages amongst other frameworks are presented. The future applications of GANs in industries such as healthcare, art and entertainment are also discussed.        △ Less","30 August, 2019","cs.LG,cs.CV,eess.IV,stat.ML",
              Mobile Cloud Computing in Healthcare Using Dynamic Cloudlets for Energy-Aware Consumption          ,1908.11501,https://arxiv.org/abs/1908.11501,https://arxiv.org/pdf/1908.11501,"Authors:ManojMuniswamaiah,Dr.CharlesTappert","        Mobile cloud computing (MCC) has increasingly been adopted in healthcare industry by healthcare professionals (HCPs) which has resulted in the growth of medical software applications for these platforms. There are different applications which help HCPs with many important tasks. Mobile cloud computing has helped HCPs in better decision making and improved patient care. MCC enables users to acquire the benefit of cloud computing services to meet the healthcare demands. However, the restrictions posed by network bandwidth and mobile device capacity has brought challenges with respect to energy consumption and latency delays. In this paper we propose dynamic energy consumption mobile cloud computing model (DEMCCM) which addresses the energy consumption issue by healthcare mobile devices using dynamic cloudlets.        △ Less","29 August, 2019",cs.DC,10.5121/csit.2019.91006 
              A cross-sectional study of social inequities in medical crowdfunding campaigns in the United States          ,1908.11018,https://arxiv.org/abs/1908.11018,https://arxiv.org/pdf/1908.11018,"Authors:NoraKenworthy,ZhihangDong,AnneMontgomery,EmilyFuller,LaurenBerliner","        Americans are increasingly relying on crowdfunding to pay for the costs of healthcare. In medical crowdfunding, online platforms allow individuals to appeal to social networks to request donations for health and medical needs. Users are often told that success depends on how they organize and share their campaigns to increase social network engagement. However, experts have cautioned that MCF could exacerbate health and social disparities by amplifying the choices and biases of the crowd and leveraging these to determine who has access to financial support for healthcare. To date, research on potential axes of disparity in MCF, and their impacts on fundraising outcomes, has been limited. This paper presents an exploratory cross-sectional study of a randomized sample of 637 MCF campaigns on the popular platform Gofundme, for which the race, gender, age, and relationships of campaigners and campaign recipients were categorized alongside campaign characteristics and outcomes. Our analyses examine race, gender, and age disparities in MCF use, and tests how these are associated with differential campaign outcomes. The results show systemic disparities in MCF use and outcomes: non-white users are under-represented. There is significant evidence of an additional digital care labor burden on women organizers of campaigns, and marginalized race and gender groups are associated with poorer fundraising outcomes. Outcomes are only minimally associated with campaign characteristics under users' control, such as photos, videos, and updates. These results corroborate widespread concerns how technology fuels health inequities, and about how crowdfunding may be creating an unequal and biased marketplace for those seeking financial support to access healthcare. Further research and better data access are needed to explore these dynamics more deeply and inform policy for this largely unregulated industry.        △ Less","28 August, 2019",cs.SI,10.1371/journal.pone.0229760 
              A Security-Aware Access Model for Data-Driven EHR System          ,1908.10229,https://arxiv.org/abs/1908.10229,https://arxiv.org/pdf/1908.10229,"Authors:NgocHongTran,Thien-AnNguyen-Ngoc,Nhien-AnLe-Khac,M-TaharKechadi","        Digital healthcare systems are very popular lately, as they provide a variety of helpful means to monitor people's health state as well as to protect people against an unexpected health situation. These systems contain a huge amount of personal information in a form of electronic health records that are not allowed to be disclosed to unauthorized users. Hence, health data and information need to be protected against attacks and thefts. In this paper, we propose a secure distributed architecture for healthcare data storage and analysis. It uses a novel security model to rigorously control permissions of accessing sensitive data in the system, as well as to protect the transmitted data between distributed system servers and nodes. The model also satisfies the NIST security requirements. Thorough experimental results show that the model is very promising.        △ Less","27 August, 2019","cs.CR,cs.CY",
              Multi-Task Gaussian Processes and Dilated Convolutional Networks for Reconstruction of Reproductive Hormonal Dynamics          ,1908.10226,https://arxiv.org/abs/1908.10226,https://arxiv.org/pdf/1908.10226,"Authors:IñigoUrteaga,TristanBertin,TheresaM.Hardy,DavidJ.Albers,NoémieElhadad","        We present an end-to-end statistical framework for personalized, accurate, and minimally invasive modeling of female reproductive hormonal patterns. Reconstructing and forecasting the evolution of hormonal dynamics is a challenging task, but a critical one to improve general understanding of the menstrual cycle and personalized detection of potential health issues. Our goal is to infer and forecast individual hormone daily levels over time, while accommodating pragmatic and minimally invasive measurement settings. To that end, our approach combines the power of probabilistic generative models (i.e., multi-task Gaussian processes) with the flexibility of neural networks (i.e., a dilated convolutional architecture) to learn complex temporal mappings. To attain accurate hormone level reconstruction with as little data as possible, we propose a sampling mechanism for optimal reconstruction accuracy with limited sampling budget. Our results show the validity of our proposed hormonal dynamic modeling framework, as it provides accurate predictive performance across different realistic sampling budgets and outperforms baselines methods.        △ Less","27 August, 2019","cs.LG,stat.AP,stat.ML",
              A Bi-fidelity Surrogate Modeling Approach for Uncertainty Propagation in Three-Dimensional Hemodynamic Simulations          ,1908.10197,https://arxiv.org/abs/1908.10197,https://arxiv.org/pdf/1908.10197,"Authors:HanGao,XueyuZhu,Jian-XunWang","        Image-based computational fluid dynamics (CFD) modeling enables derivation of hemodynamic information, which has become a paradigm in cardiovascular research and healthcare. Nonetheless, the predictive accuracy largely depends on precisely specified boundary conditions and model parameters, which, however, are usually uncertain in most patient-specific cases. Quantifying the uncertainties in model predictions due to input randomness can provide predictive confidence and is critical to promote the transition of CFD modeling in clinical applications. In the meantime, forward propagation of input uncertainties often involves numerous expensive CFD simulations, which is computationally prohibitive in most practical scenarios. This paper presents an efficient bi-fidelity surrogate modeling framework for uncertainty quantification (UQ) in cardiovascular simulations, by leveraging the accuracy of high-fidelity models and efficiency of low-fidelity models. Contrary to most data-fit surrogate models with several scalar quantities of interest, this work aims to provide high-resolution, full-field predictions. Moreover, a novel empirical error bound estimation approach is introduced to evaluate the performance of the surrogate a priori. The proposed framework is tested on a number of vascular flows with both standardized and patient-specific vessel geometries, and different combinations of high- and low-fidelity models are investigated. The results show that the bi-fidelity approach can achieve high predictive accuracy with a significant reduction of computational cost, exhibiting its merit and effectiveness. Particularly, the uncertainties from a high-dimensional input space can be accurately propagated to clinically relevant quantities of interest in the patient-specific case using only a limited number of high-fidelity simulations, suggesting a good potential in practical clinical applications.        △ Less","23 August, 2019","physics.flu-dyn,eess.IV,physics.med-ph",10.1016/j.cma.2020.113047 
              Towards Blockchain-enabled Searchable Encryption          ,1908.09564,https://arxiv.org/abs/1908.09564,https://arxiv.org/pdf/1908.09564,Authors:QiangTang,"        Distributed Leger Technologies (DLTs), most notably Blockchain technologies, bring decentralised platforms that eliminate a single trusted third party and avoid the notorious single point of failure vulnerability. Since Nakamoto's Bitcoin cryptocurrency system, an enormous number of decentralised applications have been proposed on top of these technologies, aiming at more transparency and trustworthiness than their traditional counterparts. These applications spread over a lot of areas, e.g. financial services, healthcare, transportation, supply chain management, and cloud computing. While Blockchain brings transparency and decentralised trust intuitively due to the consensus of a (very large) group of nodes (or, miners), it introduces very subtle implications for other desirable properties such as privacy. In this work, we demonstrate these subtle implications for Blockchain-based searchable encryption solutions, which are one specific use case of cloud computing services. These solutions rely on Blockchain to achieve both the standard privacy property and the new fairness property, which requires that search operations are carried out faithfully and are rewarded accordingly. We show that directly replacing the server in an existing searchable encryption solution with a Blockchain will cause undesirable operational cost, privacy loss, and security vulnerabilities. The analysis results indicate that a dedicated server is still needed to achieve the desired privacy guarantee. To this end, we propose two frameworks which can be instantiated based on most existing searchable encryption schemes. Through analysing these two frameworks, we affirmatively show that a carefully engineered Blockchain-based solution can achieve the desired fairness property while preserving the privacy guarantee of the original searchable encryption scheme simultaneously.        △ Less","19 November, 2019",cs.CR,
              Representation Learning with Autoencoders for Electronic Health Records: A Comparative Study          ,1908.09174,https://arxiv.org/abs/1908.09174,/search/?searchtype=author&query=Sadati%2C+N,"Authors:NajibesadatSadati,MiladZafarNezhad,RatnaBabuChinnam,DongxiaoZhu","        Increasing volume of Electronic Health Records (EHR) in recent years provides great opportunities for data scientists to collaborate on different aspects of healthcare research by applying advanced analytics to these EHR clinical data. A key requirement however is obtaining meaningful insights from high dimensional, sparse and complex clinical data. Data science approaches typically address this challenge by performing feature learning in order to build more reliable and informative feature representations from clinical data followed by supervised learning. In this paper, we propose a predictive modeling approach based on deep learning based feature representations and word embedding techniques. Our method uses different deep architectures (stacked sparse autoencoders, deep belief network, adversarial autoencoders and variational autoencoders) for feature representation in higher-level abstraction to obtain effective and robust features from EHRs, and then build prediction models on top of them. Our approach is particularly useful when the unlabeled data is abundant whereas labeled data is scarce. We investigate the performance of representation learning through a supervised learning approach. Our focus is to present a comparative study to evaluate the performance of different deep architectures through supervised learning and provide insights in the choice of deep feature representation techniques. Our experiments demonstrate that for small data sets, stacked sparse autoencoder demonstrates a superior generality performance in prediction due to sparsity regularization whereas variational autoencoders outperform the competing approaches for large data sets due to its capability of learning the representation distribution        △ Less","19 September, 2019","cs.LG,cs.CL,stat.ML",
"              Integration of Blockchain and Cloud of Things: Architecture, Applications and Challenges          ",1908.09058,https://arxiv.org/abs/1908.09058,https://arxiv.org/pdf/1908.09058,"Authors:DinhCNguyen,PubuduNPathirana,MingDing,ArunaSeneviratne","        The blockchain technology is taking the world by storm. Blockchain with its decentralized, transparent and secure nature has emerged as a disruptive technology for the next generation of numerous industrial applications. One of them is Cloud of Things enabled by the combination of cloud computing and Internet of Things. In this context, blockchain provides innovative solutions to address challenges in Cloud of Things in terms of decentralization, data privacy and network security, while Cloud of Things offer elasticity and scalability functionalities to improve the efficiency of blockchain operations. Therefore, a novel paradigm of blockchain and Cloud of Things integration, called BCoT, has been widely regarded as a promising enabler for a wide range of application scenarios. In this paper, we present a state-of-the-art review on the BCoT integration to provide general readers with an overview of the BCoT in various aspects, including background knowledge, motivation, and integrated architecture. Particularly, we also provide an in-depth survey of BCoT applications in different use-case domains such as smart healthcare, smart city, smart transportation and smart industry. Then, we review the recent BCoT developments with the emerging blockchain and cloud platforms, services, and research projects. Finally, some important research challenges and future directions are highlighted to spur further research in this promising area.        △ Less","28 August, 2020",cs.CR,10.1109/COMST.2020.3020092 
              Reinforcement Learning in Healthcare: A Survey          ,1908.08796,https://arxiv.org/abs/1908.08796,https://arxiv.org/pdf/1908.08796,"Authors:ChaoYu,JimingLiu,ShamimNemati","        As a subfield of machine learning, reinforcement learning (RL) aims at empowering one's capabilities in behavioural decision making by using interaction experience with the world and an evaluative feedback. Unlike traditional supervised learning methods that usually rely on one-shot, exhaustive and supervised reward signals, RL tackles with sequential decision making problems with sampled, evaluative and delayed feedback simultaneously. Such distinctive features make RL technique a suitable candidate for developing powerful solutions in a variety of healthcare domains, where diagnosing decisions or treatment regimes are usually characterized by a prolonged and sequential procedure. This survey discusses the broad applications of RL techniques in healthcare domains, in order to provide the research community with systematic understanding of theoretical foundations, enabling methods and techniques, existing challenges, and new insights of this emerging paradigm. By first briefly examining theoretical foundations and key techniques in RL research from efficient and representational directions, we then provide an overview of RL applications in healthcare domains ranging from dynamic treatment regimes in chronic diseases and critical care, automated medical diagnosis from both unstructured and structured clinical data, as well as many other control or scheduling domains that have infiltrated many aspects of a healthcare system. Finally, we summarize the challenges and open issues in current research, and point out some potential solutions and directions for future research.        △ Less","24 April, 2020","cs.LG,cs.AI",
              Viability of machine learning to reduce workload in systematic review screenings in the health sciences: a working paper          ,1908.08610,https://arxiv.org/abs/1908.08610,https://arxiv.org/pdf/1908.08610,Authors:MuhammadMaaz,"        Systematic reviews, which summarize and synthesize all the current research in a specific topic, are a crucial component to academia. They are especially important in the biomedical and health sciences, where they synthesize the state of medical evidence and conclude the best course of action for various diseases, pathologies, and treatments. Due to the immense amount of literature that exists, as well as the output rate of research, reviewing abstracts can be a laborious process. Automation may be able to significantly reduce this workload. Of course, such classifications are not easily automated due to the peculiar nature of written language. Machine learning may be able to help. This paper explored the viability and effectiveness of using machine learning modelling to classify abstracts according to specific exclusion/inclusion criteria, as would be done in the first stage of a systematic review. The specific task was performing the classification of deciding whether an abstract is a randomized control trial (RCT) or not, a very common classification made in systematic reviews in the healthcare field. Random training/testing splits of an n=2042 dataset of labelled abstracts were repeatedly created (1000 times in total), with a model trained and tested on each of these instances. A Bayes classifier as well as an SVM classifier were used, and compared to non-machine learning, simplistic approaches to textual classification. An SVM classifier was seen to be highly effective, yielding a 90% accuracy, as well as an F1 score of 0.84, and yielded a potential workload reduction of 70%. This shows that machine learning has the potential to significantly revolutionize the abstract screening process in healthcare systematic reviews.        △ Less","22 August, 2019","cs.LG,stat.ML",
              DC3 -- A Diagnostic Case Challenge Collection for Clinical Decision Support          ,1908.08581,https://arxiv.org/abs/1908.08581,https://arxiv.org/pdf/1908.08581,"Authors:CarstenEickhoff,FloranGmehlin,AnuV.Patel,JocelynBoullier,HamishFraser","        In clinical care, obtaining a correct diagnosis is the first step towards successful treatment and, ultimately, recovery. Depending on the complexity of the case, the diagnostic phase can be lengthy and ridden with errors and delays. Such errors have a high likelihood to cause patients severe harm or even lead to their death and are estimated to cost the U.S. healthcare system several hundred billion dollars each year.  To avoid diagnostic errors, physicians increasingly rely on diagnostic decision support systems drawing from heuristics, historic cases, textbooks, clinical guidelines and scholarly biomedical literature. The evaluation of such systems, however, is often conducted in an ad-hoc fashion, using non-transparent methodology, and proprietary data.  This paper presents DC3, a collection of 31 extremely difficult diagnostic case challenges, manually compiled and solved by clinical experts. For each case, we present a number of temporally ordered physician-generated observations alongside the eventually confirmed true diagnosis. We additionally provide inferred dense relevance judgments for these cases among the PubMed collection of 27 million scholarly biomedical articles.        △ Less","22 August, 2019","cs.IR,cs.AI",10.1145/3341981.3344239 
              cSeiz: An Edge-Device for Accurate Seizure Detection and Control for Smart Healthcare,1908.08130,https://arxiv.org/abs/1908.08130,https://arxiv.org/pdf/1908.08130,"Authors:MdAbuSayeed,SarajuP.Mohanty,EliasKougianos","        Epilepsy is one of the most common neurological disorders affecting up to 1% of the world's population and approximately 2.5 million people in the United States. Seizures in more than 30% of epilepsy patients are refractory to anti-epileptic drugs. An important biomedical research effort is focused on the development of an energy efficient implantable device for the real-time control of seizures. In this paper we propose an Internet of Medical Things (IoMT) based automated seizure detection and drug delivery system (DDS) for the control of seizures. The proposed system will detect seizures and inject a fast acting anti-convulsant drug at the onset to suppress seizure progression. The drug injection is performed in two stages. Initially, the seizure detector detects the seizure from the electroencephalography (EEG) signal using a hyper-synchronous signal detection circuit and a signal rejection algorithm (SRA). In the second stage, the drug is released in the seizure onset area upon seizure detection. The design was validated using a system-level simulation and consumer electronics proof of concept. The proposed seizure detector reports a sensitivity of 96.9% and specificity of 97.5%. The use of minimal circuitry leads to a considerable reduction of power consumption compared to previous approaches. The proposed approach can be generalized to other sensor modalities and the use of both wearable and implantable solutions, or a combination of the two.        △ Less","21 August, 2019","eess.SP,eess.SY",
              Multiperspective Conformance Analysis of Central Venous Catheter Installation Procedure          ,1908.07938,https://arxiv.org/abs/1908.07938,https://arxiv.org/pdf/1908.07938,Authors:R.P.JagadeeshChandraBose,"        Training and practice play a key role in a medical students' attainment of surgical procedural skills. It is beyond doubt that good skills correlate with better clinical outcomes and improved healthcare. Timely, holistic, and effective feedback provide a significant impetus to students acquiring skills with precision. In this paper, we analyze the activities performed by students while learning the central venous catheter installation procedure. We perform a holistic analysis, using trace alignment, declarative conformance checking, data visualization, and statistical analysis techniques, at different levels of abstraction on control-flow and time perspectives and provide insights at individual student level as well as across students. These insights can help students discover what they are doing right and where they are not and take corrective steps. Instructors can uncover common patterns and mistakes that students demonstrate and think of interventions in their teaching methodology.        △ Less","14 August, 2019",physics.med-ph,
              Towards a Structural Framework for Explicit Domain Knowledge in Visual Analytics          ,1908.07752,https://arxiv.org/abs/1908.07752,https://arxiv.org/pdf/1908.07752,"Authors:AlexanderRind,MarkusWagner,WolfgangAigner","        Clinicians and other analysts working with healthcare data are in need for better support to cope with large and complex data. While an increasing number of visual analytics environments integrates explicit domain knowledge as a means to deliver a precise representation of the available data, theoretical work so far has focused on the role of knowledge in the visual analytics process. There has been little discussion about how such explicit domain knowledge can be structured in a generalized framework. This paper collects desiderata for such a structural framework, proposes how to address these desiderata based on the model of linked data, and demonstrates the applicability in a visual analytics environment for physiotherapy.        △ Less","6 January, 2020",cs.HC,10.1109/VAHC47919.2019.8945032 
              Reinforcement Learning Applications          ,1908.06973,https://arxiv.org/abs/1908.06973,https://arxiv.org/pdf/1908.06973,Authors:YuxiLi,"        We start with a brief introduction to reinforcement learning (RL), about its successful stories, basics, an example, issues, the ICML 2019 Workshop on RL for Real Life, how to use it, study material and an outlook. Then we discuss a selection of RL applications, including recommender systems, computer systems, energy, finance, healthcare, robotics, and transportation.        △ Less","19 August, 2019","cs.LG,cs.AI",
              Self-Attention Based Molecule Representation for Predicting Drug-Target Interaction          ,1908.06760,https://arxiv.org/abs/1908.06760,https://arxiv.org/pdf/1908.06760,"Authors:BonggunShin,SungsooPark,KeunsooKang,JoyceC.Ho","        Predicting drug-target interactions (DTI) is an essential part of the drug discovery process, which is an expensive process in terms of time and cost. Therefore, reducing DTI cost could lead to reduced healthcare costs for a patient. In addition, a precisely learned molecule representation in a DTI model could contribute to developing personalized medicine, which will help many patient cohorts. In this paper, we propose a new molecule representation based on the self-attention mechanism, and a new DTI model using our molecule representation. The experiments show that our DTI model outperforms the state of the art by up to 4.9% points in terms of area under the precision-recall curve. Moreover, a study using the DrugBank database proves that our model effectively lists all known drugs targeting a specific cancer biomarker in the top-30 candidate list.        △ Less","15 August, 2019","cs.LG,stat.ML",
              LabelECG: A Web-based Tool for Distributed Electrocardiogram Annotation          ,1908.06553,https://arxiv.org/abs/1908.06553,https://arxiv.org/pdf/1908.06553,"Authors:ZijianDing,ShanQiu,YutongGuo,JianpingLin,LiSun,DapengFu,ZhenYang,ChengquanLi,YangYu,LongMeng,TingtingLv,DanLi,PingZhang","        Electrocardiography plays an essential role in diagnosing and screening cardiovascular diseases in daily healthcare. Deep neural networks have shown the potentials to improve the accuracies of arrhythmia detection based on electrocardiograms (ECGs). However, more ECG records with ground truth are needed to promote the development and progression of deep learning techniques in automatic ECG analysis. Here we propose a web-based tool for ECG viewing and annotating, LabelECG. With the facilitation of unified data management, LabelECG is able to distribute large cohorts of ECGs to dozens of technicians and physicians, who can simultaneously make annotations through web-browsers on PCs, tablets and cell phones. Along with the doctors from four hospitals in China, we applied LabelECG to support the annotations of about 15,000 12-lead resting ECG records in three months. These annotated ECGs have successfully supported the First China ECG intelligent Competition. La-belECG will be freely accessible on the Internet to support similar researches, and will also be upgraded through future works.        △ Less","18 August, 2019","cs.DB,eess.SP",
              SleepGuardian: An RF-based Healthcare System Guarding Your Sleep from Afar          ,1908.06171,https://arxiv.org/abs/1908.06171,https://arxiv.org/pdf/1908.06171,"Authors:YuGu,YantongWang,ZhiLiu,JunLiu,JieLi","        The ever accelerating process of urbanization urges more and more population into the swelling cities. While city residents are enjoying an entertaining life supported by advanced informatics techniques like 5G and cloud computing, the same technologies have also gradually deprived their sleep, which is crucial for their wellness. Therefore, sleep monitoring has drawn significant attention from both research and industry communities. In this article, we first review the sleep monitoring issue and point out three essential properties of an ideal sleep healthcare system, i.e., realtime guarding, fine-grained logging, and cost-effectiveness. Based on the analysis, we present SleepGuardian, a Radio Frequence (RF) based sleep healthcare system leveraging signal processing, edge computing and machine learning.SleepGuardian offers an offline sleep logging service and an online abnormality warning service. The offline service provides a fine-grained sleep log like timing and regularity of bed time, onset of sleep and night time awakenings. The online service keeps guarding the subject for any abnormal behaviors during sleep like intensive body twitches and a sudden seizure attack. Once an abnormality happens,it will automatically warn the designated contacts like a nearby emergency room or a closeby relative.We prototype SleepGuardian with low-cost WiFi devices and evaluate it in real scenarios. Experimental results demonstrate that SleepGuardian is very effective.        △ Less","12 August, 2019","eess.SP,cs.CY,cs.HC",
              A Survey of Challenges and Opportunities in Sensing and Analytics for Cardiovascular Disorders          ,1908.06170,https://arxiv.org/abs/1908.06170,https://arxiv.org/pdf/1908.06170,"Authors:NathanC.Hurley,EricaS.Spatz,HarlanM.Krumholz,RoozbehJafari,BobakJ.Mortazavi","        Cardiovascular disorders account for nearly 1 in 3 deaths in the United States. Care for these disorders are often determined during visits to acute care facilities, such as hospitals. While the length of stay in these settings represents just a small proportion of patients' lives, they account for a disproportionately large amount of decision making. To overcome this bias towards data from acute care settings, there is a need for longitudinal monitoring in patients with cardiovascular disorders. Longitudinal monitoring can provide a more comprehensive picture of patient health, allowing for more informed decision making. This work surveys the current field of sensing technologies and machine learning analytics that exist in the field of remote monitoring for cardiovascular disorders. We highlight three primary needs in the design of new smart health technologies: 1) the need for sensing technology that can track longitudinal trends in signs and symptoms of the cardiovascular disorder despite potentially infrequent, noisy, or missing data measurements; 2) the need for new analytic techniques that model data captured in a longitudinal, continual fashion to aid in the development of new risk prediction techniques and in tracking disease progression; and 3) the need for machine learning techniques that are personalized and interpretable, allowing for advancements in shared clinical decision making. We highlight these needs based upon the current state-of-the-art in smart health technologies and analytics and discuss the ample opportunities that exist in addressing all three needs in the development of smart health technologies and analytics applied to the field of cardiovascular disorders and care.        △ Less","11 August, 2019","eess.SP,cs.CY,cs.LG",
              Two-stage Federated Phenotyping and Patient Representation Learning          ,1908.05596,https://arxiv.org/abs/1908.05596,https://arxiv.org/pdf/1908.05596,"Authors:DianboLiu,DmitriyDligach,TimothyMiller","        A large percentage of medical information is in unstructured text format in electronic medical record systems. Manual extraction of information from clinical notes is extremely time consuming. Natural language processing has been widely used in recent years for automatic information extraction from medical texts. However, algorithms trained on data from a single healthcare provider are not generalizable and error-prone due to the heterogeneity and uniqueness of medical documents. We develop a two-stage federated natural language processing method that enables utilization of clinical notes from different hospitals or clinics without moving the data, and demonstrate its performance using obesity and comorbities phenotyping as medical task. This approach not only improves the quality of a specific clinical task but also facilitates knowledge progression in the whole healthcare system, which is an essential part of learning health system. To the best of our knowledge, this is the first application of federated machine learning in clinical NLP.        △ Less","14 August, 2019","cs.IR,cs.CL,cs.LG",
              Blockchain Tree for eHealth          ,1908.04613,https://arxiv.org/abs/1908.04613,https://arxiv.org/pdf/1908.04613,"Authors:SergiiKushch,SilvioRanise,GiadaSciarretta","        The design of access control mechanisms for healthcare systems is challenging: it must strike the right balance between permissions and restrictions. In this work, we propose a novel approach that is based on the Blockchain technology for storage patient medical data and create an audit logging system able to protect health data from unauthorized modification and access. The proposed method consists of a tree structure: a main chain linked with the patient's identity and one or several Subchains which are used for storing additional critical data (e.g., medical diagnoses or access logs).        △ Less","13 August, 2019",cs.CR,10.1109/GCIoT47977.2019.9058412 
              Assessing the Impact of Blood Pressure on Cardiac Function Using Interpretable Biomarkers and Variational Autoencoders          ,1908.04538,https://arxiv.org/abs/1908.04538,https://arxiv.org/pdf/1908.04538,"Authors:EstherPuyol-Antón,BramRuijsink,JamesR.Clough,IlkayOksuz,DanielRueckert,RezaRazavi,AndrewP.King","        Maintaining good cardiac function for as long as possible is a major concern for healthcare systems worldwide and there is much interest in learning more about the impact of different risk factors on cardiac health. The aim of this study is to analyze the impact of systolic blood pressure (SBP) on cardiac function while preserving the interpretability of the model using known clinical biomarkers in a large cohort of the UK Biobank population. We propose a novel framework that combines deep learning based estimation of interpretable clinical biomarkers from cardiac cine MR data with a variational autoencoder (VAE). The VAE architecture integrates a regression loss in the latent space, which enables the progression of cardiac health with SBP to be learnt. Results on 3,600 subjects from the UK Biobank show that the proposed model allows us to gain important insight into the deterioration of cardiac function with increasing SBP, identify key interpretable factors involved in this process, and lastly exploit the model to understand patterns of positive and adverse adaptation of cardiac function.        △ Less","13 August, 2019","cs.LG,eess.SP,stat.ML",
              Regional Tree Regularization for Interpretability in Black Box Models          ,1908.04494,https://arxiv.org/abs/1908.04494,https://arxiv.org/pdf/1908.04494,"Authors:MikeWu,SonaliParbhoo,MichaelHughes,RyanKindle,LeoCeli,MaurizioZazzi,VolkerRoth,FinaleDoshi-Velez","        The lack of interpretability remains a barrier to the adoption of deep neural networks. Recently, tree regularization has been proposed to encourage deep neural networks to resemble compact, axis-aligned decision trees without significant compromises in accuracy. However, it may be unreasonable to expect that a single tree can predict well across all possible inputs. In this work, we propose regional tree regularization, which encourages a deep model to be well-approximated by several separate decision trees specific to predefined regions of the input space. Practitioners can define regions based on domain knowledge of contexts where different decision-making logic is needed. Across many datasets, our approach delivers more accurate predictions than simply training separate decision trees for each region, while producing simpler explanations than other neural net regularization schemes without sacrificing predictive power. Two healthcare case studies in critical care and HIV demonstrate how experts can improve understanding of deep models via our approach.        △ Less","16 March, 2020","cs.LG,stat.ML",
              Real-Time Elderly Healthcare Monitoring Expert System Using Wireless Sensor Network          ,1908.03518,https://arxiv.org/abs/1908.03518,https://arxiv.org/pdf/1908.03518,"Authors:IbrahimAlmarashdeh,MutasemK.Alsmadi,TamerFarag,AbdullahS.Albahussain,UsamaABadawi,NjoudAltuwaijri,HalaAlmaimoni,FatimaAsiry,ShahadAlowaid,MuneerahAlshabanah,DaniahAlrajhi,AmirahAlFraihet,GhaithJaradat","        Elderly chronic diseases are the main cause of death in the world, accounting 60% of all death. Because elderly with chronic diseases at the early stages has no observed symptoms, and then symptoms starts to appear, it is critical to observe the symptoms as early as possible to avoid any complication. This paper presents an expert system for an Elderly Health Care (EHC) at elderly home tailored for the specific needs of Elderly. The proposed EHC aims to develop an integrated and multidisciplinary method to employ communication technologies and information for covering real health needs of elderly people, mainly of people at high risk due to social and geographic isolation in addition to specific chronic diseases. The proposed EHC provides personalized intervention plans covering chronic diseases such as (body temperature (BT), blood pressure (BP), and Heart beat rate (HR)). The processes and architecture of the proposed EHC are based on the server side and three main clients, one for the elderly and another two for the nurse and the physicians whom take care of them. The proposed EHC model is discussed for proving the usefulness and effectiveness of the expert system.        △ Less","11 July, 2019","cs.HC,cs.CY,eess.SP",
              Local Differential Privacy for Deep Learning          ,1908.02997,https://arxiv.org/abs/1908.02997,https://arxiv.org/pdf/1908.02997,"Authors:M.A.P.Chamikara,P.Bertok,I.Khalil,D.Liu,S.Camtepe,M.Atiquzzaman","        The internet of things (IoT) is transforming major industries including but not limited to healthcare, agriculture, finance, energy, and transportation. IoT platforms are continually improving with innovations such as the amalgamation of software-defined networks (SDN) and network function virtualization (NFV) in the edge-cloud interplay. Deep learning (DL) is becoming popular due to its remarkable accuracy when trained with a massive amount of data, such as generated by IoT. However, DL algorithms tend to leak privacy when trained on highly sensitive crowd-sourced data such as medical data. Existing privacy-preserving DL algorithms rely on the traditional server-centric approaches requiring high processing powers. We propose a new local differentially private (LDP) algorithm named LATENT that redesigns the training process. LATENT enables a data owner to add a randomization layer before data leave the data owners' devices and reach a potentially untrusted machine learning service. This feature is achieved by splitting the architecture of a convolutional neural network (CNN) into three layers: (1) convolutional module, (2) randomization module, and (3) fully connected module. Hence, the randomization module can operate as an NFV privacy preservation service in an SDN-controlled NFV, making LATENT more practical for IoT-driven cloud-based environments compared to existing approaches. The randomization module employs a newly proposed LDP protocol named utility enhancing randomization, which allows LATENT to maintain high utility compared to existing LDP protocols. Our experimental evaluation of LATENT on convolutional deep neural networks demonstrates excellent accuracy (e.g. 91%- 96%) with high model quality even under low privacy budgets (e.g. ε=0.5\varepsilon=0.5).        △ Less","9 November, 2019","cs.LG,cs.CR",10.1109/JIOT.2019.2952146 
              Text-to-SQL Generation for Question Answering on Electronic Medical Records          ,1908.01839,https://arxiv.org/abs/1908.01839,https://arxiv.org/pdf/1908.01839,"Authors:PingWang,TianShi,ChandanK.Reddy","        Electronic medical records (EMR) contain comprehensive patient information and are typically stored in a relational database with multiple tables. Effective and efficient patient information retrieval from EMR data is a challenging task for medical experts. Question-to-SQL generation methods tackle this problem by first predicting the SQL query for a given question about a database, and then, executing the query on the database. However, most of the existing approaches have not been adapted to the healthcare domain due to a lack of healthcare Question-to-SQL dataset for learning models specific to this domain. In addition, wide use of the abbreviation of terminologies and possible typos in questions introduce additional challenges for accurately generating the corresponding SQL queries. In this paper, we tackle these challenges by developing a deep learning based TRanslate-Edit Model for Question-to-SQL (TREQS) generation, which adapts the widely used sequence-to-sequence model to directly generate the SQL query for a given question, and further performs the required edits using an attentive-copying mechanism and task-specific look-up tables. Based on the widely used publicly available electronic medical database, we create a new large-scale Question-SQL pair dataset, named MIMICSQL, in order to perform the Question-to-SQL generation task in healthcare domain. An extensive set of experiments are conducted to evaluate the performance of our proposed model on MIMICSQL. Both quantitative and qualitative experimental results indicate the flexibility and efficiency of our proposed method in predicting condition values and its robustness to random questions with abbreviations and typos.        △ Less","29 January, 2020","cs.CL,cs.AI,cs.IR,cs.LG",
              Characterising complex healthcare systems using network science: The small world of emergency surgery          ,1908.01688,https://arxiv.org/abs/1908.01688,https://arxiv.org/pdf/1908.01688,"Authors:KatharinaKohler,AriErcole","        Hospitals are complex systems and optimising their function is critical to the provision of high quality, cost effective healthcare. Nevertheless, metrics of performance have to date focused on the performance of individual elements rather than the system as a whole. Manipulation of individual elements of a complex system without an integrative understanding of its function is undesirable and may lead to counter-intuitive outcomes and a holistic metric of hospital function might help design more efficient services. We aimed to characterise the system of peri-operative care for emergency surgical admissions in our tertiary care hospital using network analysis. We used retrospective electronic health record data to construct a weighted directional network of the system. For this we selected all unplanned admissions during a 3.5 year period involving a surgical intervention during the inpatient stay and obtained a set of 16,500 individual inpatient episodes. We then constructed and analysed the structure of this network using established methods from network science such as degree distribution, betweenness centrality and small-world characteristics. The analysis showed the service to be a complex system with scale-free, small-world network properties. This finding has implications for the structure and resilience of the service as such networks, whilst being robust in general, may be vulnerable to outages at specific key nodes. We also identified such potential hubs and bottlenecks in the system based on a variety of network measures. It is hoped that such a holistic, system-wide description of a hospital service may provide better metrics for hospital strain and serve to help planners engineer systems that are as robust as possible to external shocks.        △ Less","5 August, 2019","cs.SI,physics.soc-ph,stat.AP",
              Research Opportunities in Sociotechnical Interventions for Health Disparity Reduction          ,1908.01035,https://arxiv.org/abs/1908.01035,https://arxiv.org/pdf/1908.01035,"Authors:KatieSiek,TiffanyVeinot,BethMynatt","        The implicit and explicit biases built into our computing systems are becoming increasingly clear -- they impact everything from targeting of advertisements to how we are identified as people. These biases disproportionately affect marginalized groups -- people who are excluded from mainstream social, economic, cultural, or political life -- more acutely. While these biases can affect all aspects of our lives, from leisure to criminal justice to personal finances, they are all the more critical in the context of health and healthcare due to their significant personal and societal implications. In this interdisciplinary workshop, we explored how to design and build health systems for diverse populations through the following disciplinary lenses.  The Computing Community Consortium (CCC) sponsored a two-day workshop titled Sociotechnical Interventions for Health Disparity Reduction in collaboration with the leadership of the Society for Behavioral Medicine's (SBM) 39th Annual Meeting on Monday, April 9 and Tuesday, April 10, 2018 in New Orleans, Louisiana. The workshop's goal was to bring together leading researchers in computing, health informatics, behavioral medicine, and health disparities to develop an integrative research agenda focused on sociotechnical interventions to reduce health disparities and improve the health of marginalized populations.        △ Less","7 August, 2019",cs.CY,
              Exploring Challenges and Opportunities in Cybersecurity Risk and Threat Communications Related To The Medical Internet Of Things (MIoT)          ,1908.00666,https://arxiv.org/abs/1908.00666,https://arxiv.org/pdf/1908.00666,"Authors:GeorgeW.Jackson,Jr.,ShawonRahman","        As device interconnectivity and ubiquitous computing continues to proliferate healthcare, the Medical Internet of Things (MIoT), also well known as the, Internet of Medical Things (IoMT) or the Internet of Healthcare Things (IoHT), is certain to play a major role in the health, and well-being of billions of people across the globe. When it comes to issues of cybersecurity risks and threats connected to the IoT in all of its various flavors the emphasis has been on technical challenges and technical solution. However, especially in the area of healthcare there is another substantial and potentially grave challenge. It is the challenge of thoroughly and accurately communicating the nature and extent of cybersecurity risks and threats to patients who are reliant upon these interconnected healthcare technologies to improve and even preserve their lives. This case study was conducted to assess the scope and depth of cybersecurity risk and threat communications delivered to an extremely vulnerable patient population, semi-structured interviews were held with cardiac medical device specialists across the United States. This research contributes scientific data in the field of healthcare cybersecurity and assists scholars and practitioners in advancing education and research in the field of MIoT patient communications.        △ Less","1 August, 2019","cs.CY,cs.CR",
              Classification of Cognitive Load and Expertise for Adaptive Simulation using Deep Multitask Learning          ,1908.00385,https://arxiv.org/abs/1908.00385,https://arxiv.org/pdf/1908.00385,"Authors:PritamSarkar,KyleRoss,AaronJ.Ruberto,DirkRodenburg,PaulHungler,AliEtemad","        Simulations are a pedagogical means of enabling a risk-free way for healthcare practitioners to learn, maintain, or enhance their knowledge and skills. Such simulations should provide an optimum amount of cognitive load to the learner and be tailored to their levels of expertise. However, most current simulations are a one-type-fits-all tool used to train different learners regardless of their existing skills, expertise, and ability to handle cognitive load. To address this problem, we propose an end-to-end framework for a trauma simulation that actively classifies a participant's level of cognitive load and expertise for the development of a dynamically adaptive simulation. To facilitate this solution, trauma simulations were developed for the collection of electrocardiogram (ECG) signals of both novice and expert practitioners. A multitask deep neural network was developed to utilize this data and classify high and low cognitive load, as well as expert and novice participants. A leave-one-subject-out (LOSO) validation was used to evaluate the effectiveness of our model, achieving an accuracy of 89.4% and 96.6% for classification of cognitive load and expertise, respectively.        △ Less","30 July, 2019","cs.HC,cs.LG",10.1109/ACII.2019.8925507 
              Response time optimization for drone-delivered automated external defibrillators          ,1908.00149,https://arxiv.org/abs/1908.00149,https://arxiv.org/pdf/1908.00149,"Authors:JustinJ.Boutilier,TimothyC.Y.Chan","        Out-of-hospital cardiac arrest (OHCA) claims over 400,000 lives each year in North America and is one of the most time-sensitive medical emergencies. Drone-delivered automated external defibrillators (AEDs) have the potential to be a transformative innovation in the provision of emergency care for OHCA. In this paper, we propose a simulation-optimization framework to minimize the total number of drones required to meet a pre-specified response time goal, while guaranteeing a sufficient number of drones are located at each base. To do this, we develop a location-queuing model that is based on the p-median architecture, where each base constitutes an explicit M/M/d queue, and that incorporates estimated baseline response times to the demand points. We then develop a reformulation technique that exploits the baseline response times, allowing us to solve real-world instances to optimality using an off-the-shelf solver. To test our model, we develop a two-stage machine learning approach to simulate both the locations and baseline response times for future OHCAs. We demonstrate the application of our framework using eight years of real data from an area covering 26,000 square kilometres around Toronto, Canada. A modest number of drones are required to significantly reduce response times in all regions. Furthermore, an objective function focused on improving the 90th percentile is well-suited for use in practice because the model reduces the entire response time distribution, while providing equitable coverage in both cities and rural areas. Overall, this paper provides a realistic framework that can be leveraged by healthcare providers seeking to implement a drone network.        △ Less","31 July, 2019",math.OC,
              MSNM-Sensor: An Applied Network Monitoring Tool for Anomaly Detection in Complex Networks and Systems          ,1907.13612,https://arxiv.org/abs/1907.13612,https://arxiv.org/pdf/1907.13612,"Authors:RobertoMagán-Carrión,JoséCamacho,GabrielMaciá-Fernández,ÁngelRuíz-Zafra","        Technology evolves quickly. Low-cost and ready-to-connect devices are designed to provide new services and applications. Smart grids or smart healthcare systems are some examples of these applications, all of which are in the context of smart cities. In this total-connectivity scenario, some security issues arise since the larger the number of connected devices is, the greater the surface attack dimension. In this way, new solutions for monitoring and detecting security events are needed to address new challenges brought about by this scenario, among others, the large number of devices to monitor, the large amount of data to manage and the real-time requirement to provide quick security event detection and, consequently, quick response to attacks. In this work, a practical and ready-to-use tool for monitoring and detecting security events in these environments is developed and introduced. The tool is based on the Multivariate Statistical Network Monitoring (MSNM) methodology for monitoring and anomaly detection and we call it MSNM-Sensor. Although it is in its early development stages, experimental results based on the detection of well-known attacks in hierarchical network systems prove the suitability of this tool for more complex scenarios, such as those found in smart cities or IoT ecosystems.        △ Less","16 February, 2020","cs.CR,cs.LG,cs.NI,stat.OT",
              An Efficient and Scalable Privacy Preserving Algorithm for Big Data and Data Streams          ,1907.13498,https://arxiv.org/abs/1907.13498,https://arxiv.org/pdf/1907.13498,"Authors:M.A.P.Chamikara,P.Bertok,D.Liu,S.Camtepe,I.Khalil","        A vast amount of valuable data is produced and is becoming available for analysis as a result of advancements in smart cyber-physical systems. The data comes from various sources, such as healthcare, smart homes, smart vehicles, and often includes private, potentially sensitive information that needs appropriate sanitization before being released for analysis. The incremental and fast nature of data generation in these systems necessitates scalable privacy-preserving mechanisms with high privacy and utility. However, privacy preservation often comes at the expense of data utility. We propose a new data perturbation algorithm, SEAL (Secure and Efficient data perturbation Algorithm utilizing Local differential privacy), based on Chebyshev interpolation and Laplacian noise, which provides a good balance between privacy and utility with high efficiency and scalability. Empirical comparisons with existing privacy-preserving algorithms show that SEAL excels in execution speed, scalability, accuracy, and attack resistance. SEAL provides flexibility in choosing the best possible privacy parameters, such as the amount of added noise, which can be tailored to the domain and dataset.        △ Less","31 July, 2019","cs.CR,cs.DB",10.1016/j.cose.2019.101570 
              Predicting assisted ventilation in Amyotrophic Lateral Sclerosis using a mixture of experts and conformal predictors          ,1907.13070,https://arxiv.org/abs/1907.13070,https://arxiv.org/pdf/1907.13070,"Authors:TelmaPereira,SofiaPires,MartaGromicho,SusanaPinto,MamededeCarvalho,SaraC.Madeira","        Amyotrophic Lateral Sclerosis (ALS) is a neurodegenerative disease characterized by a rapid motor decline, leading to respiratory failure and subsequently to death. In this context, researchers have sought for models to automatically predict disease progression to assisted ventilation in ALS patients. However, the clinical translation of such models is limited by the lack of insight 1) on the risk of error for predictions at patient-level, and 2) on the most adequate time to administer the non-invasive ventilation. To address these issues, we combine Conformal Prediction (a machine learning framework that complements predictions with confidence measures) and a mixture experts into a prognostic model which not only predicts whether an ALS patient will suffer from respiratory insufficiency but also the most likely time window of occurrence, at a given reliability level. Promising results were obtained, with near 80% of predictions being correctly identified.        △ Less","30 July, 2019","cs.LG,stat.ML",
              A stochastic matching model on hypergraphs          ,1907.12711,https://arxiv.org/abs/1907.12711,https://arxiv.org/pdf/1907.12711,"Authors:YoussefRahmé,PascalMoyal","        Motivated by applications to a wide range of assemble-to-order systems, operations scheduling, healthcare systems and collaborative economy applications, we introduce a stochastic matching model on hypergraphs, extending the model in [15] to the case of hypergraphical (rather than graphical) matching structures. We address a discrete-event system under a random input of single items, simply using the system as an interface to be matched by groups of two or more. We study the stability of this stochastic system, for various hypergraph geometries.        △ Less","29 July, 2019",math.PR,
              Tackling Ordinal Regression Problem for Heterogeneous Data: Sparse and Deep Multi-Task Learning Approaches          ,1907.12508,https://arxiv.org/abs/1907.12508,https://arxiv.org/pdf/1907.12508,"Authors:LuWang,DongxiaoZhu","        Many real-world datasets are labeled with natural orders, i.e., ordinal labels. Ordinal regression is a method to predict ordinal labels that finds a wide range of applications in data-rich domains, such as natural, health and social sciences. Most existing ordinal regression approaches work well for independent and identically distributed (IID) instances via formulating a single ordinal regression task. However, for heterogeneous non-IID instances with well-defined local geometric structures, e.g., subpopulation groups, multi-task learning (MTL) provides a promising framework to encode task (subgroup) relatedness, bridge data from all tasks, and simultaneously learn multiple related tasks in efforts to improve generalization performance. Even though MTL methods have been extensively studied, there is barely existing work investigating MTL for heterogeneous data with ordinal labels. We tackle this important problem via sparse and deep multi-task approaches. Specifically, we develop a regularized multi-task ordinal regression (MTOR) model for smaller datasets and a deep neural networks based MTOR model for large-scale datasets. We evaluate the performance using three real-world healthcare datasets with applications to multi-stage disease progression diagnosis. Our experiments indicate that the proposed MTOR models markedly improve the prediction performance comparing with single-task ordinal regression models.        △ Less","26 April, 2020","cs.LG,stat.ML",
              Artificial Intelligence and the Future of Psychiatry: Insights from a Global Physician Survey          ,1907.12386,https://arxiv.org/abs/1907.12386,https://arxiv.org/pdf/1907.12386,"Authors:P.MuraliDoraiswamy,CharlotteBlease,KayleeBodner","        Futurists have predicted that new technologies, embedded with artificial intelligence (AI) and machine learning (ML), will lead to substantial job loss in many sectors disrupting many aspects of healthcare. Mental health appears ripe for such disruption given the global illness burden, stigma, and shortage of care providers. Using Sermo, a global networking platform open to verified and licensed physicians, we measured the opinions of psychiatrists about the likelihood that future autonomous technology (referred to as AI/ML) would be able to fully replace the average psychiatrist in performing 10 key tasks (e.g. mental status exam, suicidality assessment, treatment planning) carried out in mental health care. Survey respondents were 791 psychiatrists from 22 countries. Only 3.8% of respondents felt that AI/ML was likely to replace a human clinician for providing empathetic care. Documenting (e.g. updating medical records) and synthesizing information to reach a diagnosis were the two tasks where a majority predicted that future AI/ML would replace human doctors. About 1 in 2 doctors believed their jobs could be changed substantially by future AI/ML. However, female and US-based doctors were more uncertain that the possible benefits of AI would outweigh potential risks, versus their male and global counterparts. To our knowledge, this is the first global survey to seek the opinions of physicians on the impact of autonomous AI/ML on the future of psychiatry. Our findings provide compelling insights into how physicians think about intelligent technologies which may better help us integrate such tools and reskill doctors, as needed, to enhance mental health care.        △ Less","29 July, 2019",cs.CY,
              A View on Edge caching Applications          ,1907.12359,https://arxiv.org/abs/1907.12359,https://arxiv.org/pdf/1907.12359,"Authors:D.Antonogiorgakis,A.Britzolakis,P.Chatziadam,A.Dimitriadis,S.Gikas,E.Michalodimitrakis,M.Oikonomakis,N.Siganos,E.Tzagkarakis,Y.Nikoloudakis,S.Panagiotakis,E.Pallis,E.K.Markakis","        Devices with the ability to connect to the internet are growing in numbers day by day thus creating the need for a new way of manag-ing the way the produced traffic travels through data networks. Smart Cities, Vehicular Content Networks, Healthcare and Virtual Reality Videos are a few examples that require high volume data while maintaining low latency. Edge caching practices are a prom-ising solution in such cases in order meet the requirements of low latency in high volume traffic. This paper is a survey on four indic-ative areas, Smart Cities, Vehicular Content Networks, Healthcare and Virtual Reality Videos that make use of edge caching.        △ Less","18 July, 2019",cs.NI,
              Optimizing Energy Efficiency of Wearable Sensors Using Fog-assisted Control          ,1907.11989,https://arxiv.org/abs/1907.11989,https://arxiv.org/pdf/1907.11989,"Authors:DelaramAmiri,ArmanAnzanpour,ImanAzimi,AmirM.Rahmani,PasiLiljeberg,NikilDutt,MarcoLevorato","        Recent advances in the Internet of Things (IoT) technologies have enabled the use of wearables for remote patient monitoring. Wearable sensors capture the patient's vital signs, and provide alerts or diagnosis based on the collected data. Unfortunately, wearables typically have limited energy and computational capacity, making their use challenging for healthcare applications where monitoring must continue uninterrupted long time, without the need to charge or change the battery. Fog computing can alleviate this problem by offloading computationally intensive tasks from the sensor layer to higher layers, thereby not only meeting the sensors' limited computational capacity but also enabling the use of local closed-loop energy optimization algorithms to increase the battery life.        △ Less","27 July, 2019","eess.SP,cs.HC",
              Remote Heart Rate Measurement from Highly Compressed Facial Videos: an End-to-end Deep Learning Solution with Video Enhancement          ,1907.11921,https://arxiv.org/abs/1907.11921,https://arxiv.org/pdf/1907.11921,"Authors:ZitongYu,WeiPeng,XiaobaiLi,XiaopengHong,GuoyingZhao","        Remote photoplethysmography (rPPG), which aims at measuring heart activities without any contact, has great potential in many applications (e.g., remote healthcare). Existing rPPG approaches rely on analyzing very fine details of facial videos, which are prone to be affected by video compression. Here we propose a two-stage, end-to-end method using hidden rPPG information enhancement and attention networks, which is the first attempt to counter video compression loss and recover rPPG signals from highly compressed videos. The method includes two parts: 1) a Spatio-Temporal Video Enhancement Network (STVEN) for video enhancement, and 2) an rPPG network (rPPGNet) for rPPG signal recovery. The rPPGNet can work on its own for robust rPPG measurement, and the STVEN network can be added and jointly trained to further boost the performance especially on highly compressed videos. Comprehensive experiments are performed on two benchmark datasets to show that, 1) the proposed method not only achieves superior performance on compressed videos with high-quality videos pair, 2) it also generalizes well on novel data with only compressed videos available, which implies the promising potential for real world applications.        △ Less","27 July, 2019","eess.IV,cs.CV",
              Evaluating the Impact of Using GRASP Framework on Clinicians and Healthcare Professionals Decisions in Selecting Clinical Predictive Tools          ,1907.11523,https://arxiv.org/abs/1907.11523,https://arxiv.org/pdf/1907.11523,"Authors:MohamedKhalifa,FarahMagrabi,BlancaGallego","        Background. When selecting predictive tools, clinicians and healthcare professionals are challenged with an overwhelming number of tools, most of which have never been evaluated for comparative effectiveness. To overcome this challenge, the authors developed and validated an evidence-based framework for grading and assessment of predictive tools (GRASP), based on the critical appraisal of published evidence. Methods. To examine GRASP impact on professionals decisions, a controlled experiment was conducted through an online survey. Randomising two groups of tools and two scenarios; participants were asked to select the best tools; most validated or implemented, with and without GRASP. A wide group of international participants were invited. Task completion time, rate of correct decisions, rate of objective vs subjective decisions, and level of decisional conflict were measured. Results. Valid responses received were 194. Compared to not using the framework, GRASP significantly increased correct decisions by 64% (T=8.53, p<0.001), increased objective decision making by 32% (T=9.24, p<0.001), and decreased subjective decision making; based on guessing and based on prior knowledge or experience by 20% (T=-5.47, p<0.001) and 8% (T=-2.99, p=0.003) respectively. GRASP significantly decreased decisional conflict; increasing confidence and satisfaction of participants with their decisions by 11% (T=4.27, p<0.001) and 13% (T=4.89, p<0.001) respectively. GRASP decreased task completion time by 52% (T=-0.87, p=0.384). The average system usability scale of GRASP was very good; 72.5%, and 88% of participants found GRASP useful. Discussion and Conclusions. Using GRASP has positively supported and significantly improved evidence-based decision making and increased accuracy and efficiency of selecting predictive tools.        △ Less","24 July, 2019",cs.CY,
              Computational Phenotype Discovery via Probabilistic Independence          ,1907.11051,https://arxiv.org/abs/1907.11051,https://arxiv.org/pdf/1907.11051,"Authors:ThomasA.Lasko,DiegoA.Mesa","        Computational Phenotype Discovery research has taken various pragmatic approaches to disentangling phenotypes from the episodic observations in Electronic Health Records. In this work, we use transformation into continuous, longitudinal curves to abstract away the sparse irregularity of the data, and we introduce probabilistic independence as a guiding principle for disentangling phenotypes into patterns that may more closely match true pathophysiologic mechanisms. We use the identification of liver disease patterns that presage development of Hepatocellular Carcinoma as a proof-of-concept demonstration.        △ Less","25 July, 2019",stat.AP,
              Self-attention based BiLSTM-CNN classifier for the prediction of ischemic and non-ischemic cardiomyopathy          ,1907.10370,https://arxiv.org/abs/1907.10370,https://arxiv.org/pdf/1907.10370,"Authors:KavitaDubey,AnantAgarwal,AstitwaSarthakLathe,RanjeetKumar,VishalSrivastava","        Heart Failure is a major component of healthcare expenditure and a leading cause of mortality worldwide. Despite higher inter-rater variability, endomyocardial biopsy (EMB) is still regarded as the standard technique, used to identify the cause (e.g. ischemic or non-ischemic cardiomyopathy, coronary artery disease, myocardial infarction etc.) of unexplained heart failure. In this paper, we focus on identifying cardiomyopathy as ischemic or non-ischemic. For this, we propose and implement a new unified architecture comprising CNN (inception-V3 model) and bidirectional LSTM (BiLSTM) with self-attention mechanism to predict the ischemic or non-ischemic to classify cardiomyopathy using histopathological images. The proposed model is based on self-attention that implicitly focuses on the information outputted from the hidden layers of BiLSTM. Through our results we demonstrate that this framework carries a high learning capacity and is able to improve the classification performance.        △ Less","29 July, 2019","cs.LG,cs.CV,eess.IV",
              Electronic health record in the era of industry 4.0: the French example          ,1907.10322,https://arxiv.org/abs/1907.10322,https://arxiv.org/pdf/1907.10322,"Authors:SarahManard,NicolasVergos,SimonTamayo,FrédéricFontane","        The recent implementation of the Electronic Health Record (EHR) in France is part of a more general process of digitizing information flows, as the world enters the fourth industrial revolution in a phenomenon known as Industry 4.0. Behind this concept lies the concern to allow Man to remain permanently in control of his destiny, despite an increasingly interconnected world (Internet of Things, cooperative robots, augmented reality, etc.). Accordingly, the implementation of EHR must guarantee the respect for the private life of each citizen. From this perspective, healthcare professionals will therefore have to constantly ensure the protection of medical confidentiality during Electronic Data Interchange (EDI). This paper summarises the current state of the use of EHR in France. Based on a survey conducted by the European Commission to assess the deployment of digitalisation in the health sector in EU countries, this article aims to highlight the opportunities and perspectives that Industry 4.0 could bring to the health sector in France. However, this study also identifies a number of limits related to the application of such a system, the first of which is cyber threat or transhumanism. To this end, a SWOT matrix identifies the strengths and weaknesses related to the implementation of the French EHR.        △ Less","25 July, 2019",cs.CY,
              Interpretable Classification of Time-Series Data using Efficient Enumerative Techniques          ,1907.10265,https://arxiv.org/abs/1907.10265,https://arxiv.org/pdf/1907.10265,"Authors:SaraMohammadinejad,JyotirmoyV.Deshmukh,AniruddhG.Puranic,MarcellVazquez-Chanlatte,AlexandreDonzé","        Cyber-physical system applications such as autonomous vehicles, wearable devices, and avionic systems generate a large volume of time-series data. Designers often look for tools to help classify and categorize the data. Traditional machine learning techniques for time-series data offer several solutions to solve these problems; however, the artifacts trained by these algorithms often lack interpretability. On the other hand, temporal logics, such as Signal Temporal Logic (STL) have been successfully used in the formal methods community as specifications of time-series behaviors. In this work, we propose a new technique to automatically learn temporal logic formulae that are able to cluster and classify real-valued time-series data. Previous work on learning STL formulas from data either assumes a formula-template to be given by the user, or assumes some special fragment of STL that enables exploring the formula structure in a systematic fashion. In our technique, we relax these assumptions, and provide a way to systematically explore the space of all STL formulas. As the space of all STL formulas is very large, and contains many semantically equivalent formulas, we suggest a technique to heuristically prune the space of formulas considered. Finally, we illustrate our technique on various case studies from the automotive, transportation and healthcare domain.        △ Less","24 July, 2019","cs.LG,stat.ML",
              Sustainable Business Models: A Review          ,1907.10052,https://arxiv.org/abs/1907.10052,https://arxiv.org/pdf/1907.10052,"Authors:SaeedNosratabadi,AmirMosavi,ShahaboddinShamshirband,EdmundasKazimierasZavadskas,AndryRakotonirainy,KwokWingChau","        The concept of the sustainable business model describes the rationale of how an organization creates, delivers, and captures value, in economic, social, cultural, or other contexts, in a sustainable way. The process of sustainable business model construction forms an innovative part of a business strategy. Different industries and businesses have utilized sustainable business models concept to satisfy their economic, environmental, and social goals simultaneously. However, the success, popularity, and progress of sustainable business models in different application domains are not clear. To explore this issue, this research provides a comprehensive review of sustainable business models literature in various application areas. Notable sustainable business models are identified and further classified in fourteen unique categories, and in every category, the progress -- either failure or success -- has been reviewed, and the research gaps are discussed. Taxonomy of the applications includes innovation, management and marketing, entrepreneurship, energy, fashion, healthcare, agri-food, supply chain management, circular economy, developing countries, engineering, construction and real estate, mobility and transportation, and hospitality. The key contribution of this study is that it provides an insight into the state of the art of sustainable business models in the various application areas and future research directions. This paper concludes that popularity and the success rate of sustainable business models in all application domains have been increased along with the increasing use of advanced technologies.        △ Less","23 July, 2019",econ.GN,10.3390/su11061663 
              Evaluation of Embeddings of Laboratory Test Codes for Patients at a Cancer Center          ,1907.09600,https://arxiv.org/abs/1907.09600,https://arxiv.org/pdf/1907.09600,"Authors:LorenzoA.Rossi,ChadShawber,JanetMunu,FinlyZachariah","        Laboratory test results are an important and generally high dimensional component of a patient's Electronic Health Record (EHR). We train embedding representations (via Word2Vec and GloVe) for LOINC codes of laboratory tests from the EHRs of about 80,000 patients at a cancer center. To include information about lab test outcomes, we also train embeddings on the concatenation of a LOINC code with a symbol indicating normality or abnormality of the result. We observe several clinically meaningful similarities among LOINC embeddings trained over our data. For the embeddings of the concatenation of LOINCs with abnormality codes, we evaluate the performance for mortality prediction tasks and the ability to preserve ordinality properties: i.e. a lab test with normal outcome should be more similar to an abnormal one than to the a very abnormal one.        △ Less","1 August, 2019","cs.LG,cs.CL,cs.CY,stat.ML",
              BEHRT: Transformer for Electronic Health Records          ,1907.09538,https://arxiv.org/abs/1907.09538,https://arxiv.org/pdf/1907.09538,"Authors:YikuanLi,ShishirRao,JoseRobertoAyalaSolares,AbdelaaliHassaine,DexterCanoy,YajieZhu,KazemRahimi,GholamrezaSalimi-Khorshidi","        Today, despite decades of developments in medicine and the growing interest in precision healthcare, vast majority of diagnoses happen once patients begin to show noticeable signs of illness. Early indication and detection of diseases, however, can provide patients and carers with the chance of early intervention, better disease management, and efficient allocation of healthcare resources. The latest developments in machine learning (more specifically, deep learning) provides a great opportunity to address this unmet need. In this study, we introduce BEHRT: A deep neural sequence transduction model for EHR (electronic health records), capable of multitask prediction and disease trajectory mapping. When trained and evaluated on the data from nearly 1.6 million individuals, BEHRT shows a striking absolute improvement of 8.0-10.8%, in terms of Average Precision Score, compared to the existing state-of-the-art deep EHR models (in terms of average precision, when predicting for the onset of 301 conditions). In addition to its superior prediction power, BEHRT provides a personalised view of disease trajectories through its attention mechanism; its flexible architecture enables it to incorporate multiple heterogeneous concepts (e.g., diagnosis, medication, measurements, and more) to improve the accuracy of its predictions; and its (pre-)training results in disease and patient representations that can help us get a step closer to interpretable predictions.        △ Less","22 July, 2019","cs.LG,stat.ML",
              FedHealth: A Federated Transfer Learning Framework for Wearable Healthcare,1907.09173,https://arxiv.org/abs/1907.09173,https://arxiv.org/pdf/1907.09173,"Authors:YiqiangChen,JindongWang,ChaohuiYu,WenGao,XinQin","        With the rapid development of computing technology, wearable devices such as smart phones and wristbands make it easy to get access to people's health information including activities, sleep, sports, etc. Smart healthcare achieves great success by training machine learning models on a large quantity of user data. However, there are two critical challenges. Firstly, user data often exists in the form of isolated islands, making it difficult to perform aggregation without compromising privacy security. Secondly, the models trained on the cloud fail on personalization. In this paper, we propose FedHealth, the first federated transfer learning framework for wearable healthcare to tackle these challenges. FedHealth performs data aggregation through federated learning, and then builds personalized models by transfer learning. It is able to achieve accurate and personalized healthcare without compromising privacy and security. Experiments demonstrate that FedHealth produces higher accuracy (5.3% improvement) for wearable activity recognition when compared to traditional methods. FedHealth is general and extensible and has the potential to be used in many healthcare applications.        △ Less","22 July, 2019","cs.LG,cs.AI,cs.NE",
              The Competitive Ratio of Threshold Policies for Online Unit-density Knapsack Problems          ,1907.08735,https://arxiv.org/abs/1907.08735,https://arxiv.org/pdf/1907.08735,"Authors:WillMa,DavidSimchi-Levi,JinglongZhao","        We study an online knapsack problem where the items arrive sequentially and must be either immediately packed into the knapsack or irrevocably discarded. Each item has a different size and the objective is to maximize the total size of items packed. We focus on the class of randomized algorithms which initially draw a threshold from some distribution, and then pack every fitting item whose size is at least that threshold. Threshold policies satisfy many desiderata including simplicity, fairness, and incentive-alignment. We derive two optimal threshold distributions, the first of which implies a competitive ratio of 0.432 relative to the optimal offline packing, and the second of which implies a competitive ratio of 0.428 relative to the optimal fractional packing. We also consider the generalization to multiple knapsacks, where an arriving item has a different size in each knapsack and must be placed in at most one. This is equivalent to the AdWords problem where item truncation is not allowed. We derive a randomized threshold algorithm for this problem which is 0.214-competitive. We also show that any randomized algorithm for this problem cannot be more than 0.461-competitive, providing the first upper bound which is strictly less than 0.5. This online knapsack problem finds applications in many areas, like supply chain ordering, online advertising, and healthcare scheduling, refugee integration, and crowdsourcing. We show how our optimal threshold distributions can be naturally implemented in the warehouses for a Latin American chain department store. We run simulations on their large-scale order data, which demonstrate the robustness of our proposed algorithms.        △ Less","3 November, 2019",cs.DS,
              Snomed2Vec: Random Walk and Poincaré Embeddings of a Clinical Knowledge Base for Healthcare Analytics          ,1907.08650,https://arxiv.org/abs/1907.08650,https://arxiv.org/pdf/1907.08650,"Authors:KhushbuAgarwal,TomeEftimov,RaghavendraAddanki,SutanayChoudhury,SuzanneTamang,RobertRallo","        Representation learning methods that transform encoded data (e.g., diagnosis and drug codes) into continuous vector spaces (i.e., vector embeddings) are critical for the application of deep learning in healthcare. Initial work in this area explored the use of variants of the word2vec algorithm to learn embeddings for medical concepts from electronic health records or medical claims datasets. We propose learning embeddings for medical concepts by using graph-based representation learning methods on SNOMED-CT, a widely popular knowledge graph in the healthcare domain with numerous operational and research applications. Current work presents an empirical analysis of various embedding methods, including the evaluation of their performance on multiple tasks of biomedical relevance (node classification, link prediction, and patient state prediction). Our results show that concept embeddings derived from the SNOMED-CT knowledge graph significantly outperform state-of-the-art embeddings, showing 5-6x improvement in ``concept similarity"" and 6-20\% improvement in patient diagnosis.        △ Less","19 July, 2019","cs.LG,cs.AI,stat.ML",
              A Wearable Medical Sensor for Provisional Healthcare,1907.08598,https://arxiv.org/abs/1907.08598,https://arxiv.org/pdf/1907.08598,"Authors:AmirJavadpour,HamidrezaMemarzadeh-Tehran","        Thispaper presents the design and realization of a context-aware wireless health monitoring system for recording the heartbeat (HR) and respiration (RR) rate based on an indirect measurement approach. The system consists of a contact-less medical sensor as well as a communication infrastructure for handling the transmission and reception of the measured results. The contact-less sensor includes a highly sensitive tri-axial accelerometer, an accurate temperature and air pressure sensor that enable one to inspect patients' health condition by continuously monitoring of two critical signs related to the cardiorespiratory system. The developed system can also be utilized in performing a number of long-term inspection on the heart and lungs while measuring the HR and RR values in addition to calculating the HR and RR ratio, which is denoted by HRR. The obtained results show the potential of the developed system for versatile monitoring applications applied to telemedicine        △ Less","18 July, 2019",cs.HC,
              An Information Extraction and Knowledge Graph Platform for Accelerating Biochemical Discoveries          ,1907.08400,https://arxiv.org/abs/1907.08400,https://arxiv.org/pdf/1907.08400,"Authors:MatteoManica,ChristophAuer,ValeryWeber,FedericoZipoli,MicheleDolfi,PeterStaar,TeodoroLaino,CostasBekas,AkihiroFujita,HirokiToda,ShuichiHirose,YasumitsuOrii","        Information extraction and data mining in biochemical literature is a daunting task that demands resource-intensive computation and appropriate means to scale knowledge ingestion. Being able to leverage this immense source of technical information helps to drastically reduce costs and time to solution in multiple application fields from food safety to pharmaceutics. We present a scalable document ingestion system that integrates data from databases and publications (in PDF format) in a biochemistry knowledge graph (BCKG). The BCKG is a comprehensive source of knowledge that can be queried to retrieve known biochemical facts and to generate novel insights. After describing the knowledge ingestion framework, we showcase an application of our system in the field of carbohydrate enzymes. The BCKG represents a way to scale knowledge ingestion and automatically exploit prior knowledge to accelerate discovery in biochemical sciences.        △ Less","19 July, 2019","cs.IR,cs.LG",
"              MIMIC-Extract: A Data Extraction, Preprocessing, and Representation Pipeline for MIMIC-III          ",1907.08322,https://arxiv.org/abs/1907.08322,https://arxiv.org/pdf/1907.08322,"Authors:ShirlyWang,MatthewB.A.McDermott,GeetickaChauhan,MichaelC.Hughes,TristanNaumann,MarzyehGhassemi","        Robust machine learning relies on access to data that can be used with standardized frameworks in important tasks and the ability to develop models whose performance can be reasonably reproduced. In machine learning for healthcare, the community faces reproducibility challenges due to a lack of publicly accessible data and a lack of standardized data processing frameworks. We present MIMIC-Extract, an open-source pipeline for transforming raw electronic health record (EHR) data for critical care patients contained in the publicly-available MIMIC-III database into dataframes that are directly usable in common machine learning pipelines. MIMIC-Extract addresses three primary challenges in making complex health records data accessible to the broader machine learning community. First, it provides standardized data processing functions, including unit conversion, outlier detection, and aggregating semantically equivalent features, thus accounting for duplication and reducing missingness. Second, it preserves the time series nature of clinical data and can be easily integrated into clinically actionable prediction tasks in machine learning for health. Finally, it is highly extensible so that other researchers with related questions can easily use the same pipeline. We demonstrate the utility of this pipeline by showcasing several benchmark tasks and baseline results.        △ Less","19 August, 2020","cs.LG,stat.ML",10.1145/3368555.3384469 
              A Systematic Approach to Detect Hierarchical Healthcare Cost Drivers and Interpretable Change Patterns          ,1907.08237,https://arxiv.org/abs/1907.08237,https://arxiv.org/pdf/1907.08237,"Authors:Ta-HsinLi,HuijingJiang,KevinTran,GigiYuen-Reed,BobKelley,ThomasHalvorson","        There is strong interest among payers to identify emerging healthcare cost drivers to support early intervention. However, many challenges arise in analyzing large, high dimensional, and noisy healthcare data. In this paper, we propose a systematic approach that utilizes hierarchical and multi-resolution search strategies using enhanced statistical process control (SPC) algorithms to surface high impact cost drivers. Our approach aims to provide interpretable, detailed, and actionable insights of detected change patterns attributing to multiple demographic and clinical factors. We also proposed an algorithm to identify comparable treatment offsets at the population level and quantify the cost impact on their utilization changes.        △ Less","18 July, 2019",stat.AP,
              Automatic Grading of Individual Knee Osteoarthritis Features in Plain Radiographs using Deep Convolutional Neural Networks          ,1907.08020,https://arxiv.org/abs/1907.08020,https://arxiv.org/pdf/1907.08020,"Authors:AlekseiTiulpin,SimoSaarakkala","        Knee osteoarthritis (OA) is the most common musculoskeletal disease in the world. In primary healthcare, knee OA is diagnosed using clinical examination and radiographic assessment. Osteoarthritis Research Society International (OARSI) atlas of OA radiographic features allows to perform independent assessment of knee osteophytes, joint space narrowing and other knee features. This provides a fine-grained OA severity assessment of the knee, compared to the gold standard and most commonly used Kellgren-Lawrence (KL) composite score. However, both OARSI and KL grading systems suffer from moderate inter-rater agreement, and therefore, the use of computer-aided methods could help to improve the reliability of the process. In this study, we developed a robust, automatic method to simultaneously predict KL and OARSI grades in knee radiographs. Our method is based on Deep Learning and leverages an ensemble of deep residual networks with 50 layers, squeeze-excitation and ResNeXt blocks. Here, we used transfer learning from ImageNet with a fine-tuning on the whole Osteoarthritis Initiative (OAI) dataset. An independent testing of our model was performed on the whole Multicenter Osteoarthritis Study (MOST) dataset. Our multi-task method yielded Cohen's kappa coefficients of 0.82 for KL-grade and 0.79, 0.84, 0.94, 0.83, 0.84, 0.90 for femoral osteophytes, tibial osteophytes and joint space narrowing for lateral and medial compartments respectively. Furthermore, our method yielded area under the ROC curve of 0.98 and average precision of 0.98 for detecting the presence of radiographic OA (KL ≥2\geq 2), which is better than the current state-of-the-art.        △ Less","18 July, 2019","eess.IV,cs.CV,cs.LG",
              A Computer Vision Application for Assessing Facial Acne Severity from Selfie Images          ,1907.07901,https://arxiv.org/abs/1907.07901,https://arxiv.org/pdf/1907.07901,"Authors:TingtingZhao,HangZhang,JacobSpoelstra","        We worked with Nestle SHIELD (Skin Health, Innovation, Education, and Longevity Development, NSH) to develop a deep learning model that is able to assess acne severity from selfie images as accurate as dermatologists. The model was deployed as a mobile application, providing patients an easy way to assess and track the progress of their acne treatment. NSH acquired 4,700 selfie images for this study and recruited 11 internal dermatologists to label them in five categories: 1-Clear, 2- Almost Clear, 3-Mild, 4-Moderate, 5-Severe. Using OpenCV to detect facial landmarks we cut specific skin patches from the selfie images in order to minimize irrelevant background. We then applied a transfer learning approach by extracting features from the patches using a ResNet 152 pre-trained model, followed by a fully connected layer trained to approximate the desired severity rating. To address the problem of spatial sensitivity of CNN models, we introduce a new image rolling data augmentation approach, effectively causing acne lesions appeared in more locations in the training images. Our results demonstrate that this approach improved the generalization of the CNN model, outperforming more than half of the panel of human dermatologists on test images. To our knowledge, this is the first deep learning-based solution for acne assessment using selfie images.        △ Less","31 July, 2019",cs.CV,
              An AI-Augmented Lesion Detection Framework For Liver Metastases With Model Interpretability          ,1907.07713,https://arxiv.org/abs/1907.07713,https://arxiv.org/pdf/1907.07713,"Authors:XinJ.Hunt,RalphAbbey,RickyTharrington,JoostHuiskens,NinaWesdorp","        Colorectal cancer (CRC) is the third most common cancer and the second leading cause of cancer-related deaths worldwide. Most CRC deaths are the result of progression of metastases. The assessment of metastases is done using the RECIST criterion, which is time consuming and subjective, as clinicians need to manually measure anatomical tumor sizes. AI has many successes in image object detection, but often suffers because the models used are not interpretable, leading to issues in trust and implementation in the clinical setting. We propose a framework for an AI-augmented system in which an interactive AI system assists clinicians in the metastasis assessment. We include model interpretability to give explanations of the reasoning of the underlying models.        △ Less","17 July, 2019","eess.IV,cs.LG,q-bio.QM,stat.ML",
              Leveraging Linguistic Characteristics for Bipolar Disorder Recognition with Gender Differences          ,1907.07366,https://arxiv.org/abs/1907.07366,https://arxiv.org/pdf/1907.07366,"Authors:Yen-HaoHuang,Yi-HsinChen,FernandoHenriqueCalderonAlvarado,Ssu-RuiLee,Shu-IWu,YuwenLai,Yi-ShinChen","        Most previous studies on automatic recognition model for bipolar disorder (BD) were based on both social media and linguistic features. The present study investigates the possibility of adopting only language-based features, namely the syntax and morpheme collocation. We also examine the effect of gender on the results considering gender has long been recognized as an important modulating factor for mental disorders, yet it received little attention in previous linguistic models. The present study collects Twitter posts 3 months prior to the self-disclosure by 349 BD users (231 female, 118 male). We construct a set of syntactic patterns in terms of the word usage based on graph pattern construction and pattern attention mechanism. The factors examined are gender differences, syntactic patterns, and bipolar recognition performance. The performance indicates our F1 scores reach over 91% and outperform several baselines, including those using TF-IDF, LIWC and pre-trained language models (ELMO and BERT). The contributions of the present study are: (1) The features are contextualized, domain-agnostic, and purely linguistic. (2) The performance of BD recognition is improved by gender-enriched linguistic pattern features, which are constructed with gender differences in language usage.        △ Less","17 July, 2019","cs.IR,cs.CL",
              End-To-End Prediction of Emotion From Heartbeat Data Collected by a Consumer Fitness Tracker          ,1907.07327,https://arxiv.org/abs/1907.07327,https://arxiv.org/pdf/1907.07327,"Authors:RossHarper,JoshuaSouthern","        Automatic detection of emotion has the potential to revolutionize mental health and wellbeing. Recent work has been successful in predicting affect from unimodal electrocardiogram (ECG) data. However, to be immediately relevant for real-world applications, physiology-based emotion detection must make use of ubiquitous photoplethysmogram (PPG) data collected by affordable consumer fitness trackers. Additionally, applications of emotion detection in healthcare settings will require some measure of uncertainty over model predictions. We present here a Bayesian deep learning model for end-to-end classification of emotional valence, using only the unimodal heartbeat time series collected by a consumer fitness tracker (Garmin Vívosmart 3). We collected a new dataset for this task, and report a peak F1 score of 0.7. This demonstrates a practical relevance of physiology-based emotion detection `in the wild' today.        △ Less","16 July, 2019","cs.HC,cs.LG,stat.ML",
              Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics          ,1907.07296,https://arxiv.org/abs/1907.07296,https://arxiv.org/pdf/1907.07296,"Authors:YuxinMa,TiankaiXie,JundongLi,RossMaciejewski","        Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies.        △ Less","3 October, 2019","cs.HC,cs.CR,cs.LG",10.1109/TVCG.2019.2934631 
              Pathways to Good Healthcare Services and Patient Satisfaction: An Evolutionary Game Theoretical Approach          ,1907.07132,https://arxiv.org/abs/1907.07132,https://arxiv.org/pdf/1907.07132,"Authors:ZainabAlalawi,TheAnhHan,YifengZeng,AimanElragig","        Spending by the UK's National Health Service (NHS) on independent healthcare treatment has been increased in recent years and is predicted to sustain its upward trend with the forecast of population growth. Some have viewed this increase as an attempt not to expand the patients' choices but to privatize public healthcare. This debate poses a social dilemma whether the NHS should stop cooperating with Private providers. This paper contributes to healthcare economic modelling by investigating the evolution of cooperation among three proposed populations: Public Healthcare Providers, Private Healthcare Providers and Patients. The Patient population is included as a main player in the decision-making process by expanding patient's choices of treatment. We develop a generic basic model that measures the cost of healthcare provision based on given parameters, such as NHS and private healthcare providers' cost of investments in both sectors, cost of treatments and gained benefits. A patient's costly punishment is introduced as a mechanism to enhance cooperation among the three populations. Our findings show that cooperation can be improved with the introduction of punishment (patient's punishment) against defecting providers. Although punishment increases cooperation, it is very costly considering the small improvement in cooperation in comparison to the basic model.        △ Less","6 July, 2019","physics.soc-ph,cs.GT,cs.MA,econ.TH,math.DS",10.13140/RG.2.2.30657.10086 
              A Scalable Framework for Multilevel Streaming Data Analytics using Deep Learning          ,1907.06690,https://arxiv.org/abs/1907.06690,https://arxiv.org/pdf/1907.06690,"Authors:ShihaoGe,HarunaIsah,FarhanaZulkernine,ShahzadKhan","        The rapid growth of data in velocity, volume, value, variety, and veracity has enabled exciting new opportunities and presented big challenges for businesses of all types. Recently, there has been considerable interest in developing systems for processing continuous data streams with the increasing need for real-time analytics for decision support in the business, healthcare, manufacturing, and security. The analytics of streaming data usually relies on the output of offline analytics on static or archived data. However, businesses and organizations like our industry partner Gnowit, strive to provide their customers with real time market information and continuously look for a unified analytics framework that can integrate both streaming and offline analytics in a seamless fashion to extract knowledge from large volumes of hybrid streaming data. We present our study on designing a multilevel streaming text data analytics framework by comparing leading edge scalable open-source, distributed, and in-memory technologies. We demonstrate the functionality of the framework for a use case of multilevel text analytics using deep learning for language understanding and sentiment analysis including data indexing and query processing. Our framework combines Spark streaming for real time text processing, the Long Short Term Memory (LSTM) deep learning model for higher level sentiment analysis, and other tools for SQL-based analytical processing to provide a scalable solution for multilevel streaming text analytics.        △ Less","15 July, 2019","eess.SY,cs.LG",10.1109/COMPSAC.2019.10205 
              Medical Concept Representation Learning from Claims Data and Application to Health Plan Payment Risk Adjustment          ,1907.06600,https://arxiv.org/abs/1907.06600,https://arxiv.org/pdf/1907.06600,"Authors:Qiu-YueZhong,AndrewH.Fairless,JasmineM.McCammon,FarbodRahmanian","        Risk adjustment has become an increasingly important tool in healthcare. It has been extensively applied to payment adjustment for health plans to reflect the expected cost of providing coverage for members. Risk adjustment models are typically estimated using linear regression, which does not fully exploit the information in claims data. Moreover, the development of such linear regression models requires substantial domain expert knowledge and computational effort for data preprocessing. In this paper, we propose a novel approach for risk adjustment that uses semantic embeddings to represent patient medical histories. Embeddings efficiently represent medical concepts learned from diagnostic, procedure, and prescription codes in patients' medical histories. This approach substantially reduces the need for feature engineering. Our results show that models using embeddings had better performance than a commercial risk adjustment model on the task of prospective risk score prediction.        △ Less","15 July, 2019","cs.LG,stat.ML",
              Counterfactual Reasoning for Fair Clinical Risk Prediction          ,1907.06260,https://arxiv.org/abs/1907.06260,https://arxiv.org/pdf/1907.06260,"Authors:StephenPfohl,TonyDuan,DaisyYiDing,NigamH.Shah","        The use of machine learning systems to support decision making in healthcare raises questions as to what extent these systems may introduce or exacerbate disparities in care for historically underrepresented and mistreated groups, due to biases implicitly embedded in observational data in electronic health records. To address this problem in the context of clinical risk prediction models, we develop an augmented counterfactual fairness criteria to extend the group fairness criteria of equalized odds to an individual level. We do so by requiring that the same prediction be made for a patient, and a counterfactual patient resulting from changing a sensitive attribute, if the factual and counterfactual outcomes do not differ. We investigate the extent to which the augmented counterfactual fairness criteria may be applied to develop fair models for prolonged inpatient length of stay and mortality with observational electronic health records data. As the fairness criteria is ill-defined without knowledge of the data generating process, we use a variational autoencoder to perform counterfactual inference in the context of an assumed causal graph. While our technique provides a means to trade off maintenance of fairness with reduction in predictive performance in the context of a learned generative model, further work is needed to assess the generality of this approach.        △ Less","14 July, 2019","cs.LG,cs.CY,stat.ML",
              Modeling the Uncertainty in Electronic Health Records: a Bayesian Deep Learning Approach          ,1907.06162,https://arxiv.org/abs/1907.06162,https://arxiv.org/pdf/1907.06162,"Authors:RiyiQiu,YugangJia,MirsadHadzikadic,MichaelDulin,XiNiu,XinWang","        Deep learning models have exhibited superior performance in predictive tasks with the explosively increasing Electronic Health Records (EHR). However, due to the lack of transparency, behaviors of deep learning models are difficult to interpret. Without trustworthiness, deep learning models will not be able to assist in the real-world decision-making process of healthcare issues. We propose a deep learning model based on Bayesian Neural Networks (BNN) to predict uncertainty induced by data noise. The uncertainty is introduced to provide model predictions with an extra level of confidence. Our experiments verify that instances with high uncertainty are harmful to model performance. Moreover, by investigating the distributions of model prediction and uncertainty, we show that it is possible to identify a group of patients for timely intervention, such that decreasing data noise will benefit more on the prediction accuracy for these patients.        △ Less","13 July, 2019","cs.LG,stat.ML",
              Aggregate-Eliminate-Predict: Detecting Adverse Drug Events from Heterogeneous Electronic Health Records          ,1907.06058,https://arxiv.org/abs/1907.06058,https://arxiv.org/pdf/1907.06058,"Authors:MariaBampa,PanagiotisPapapetrou","        We study the problem of detecting adverse drug events in electronic healthcare records. The challenge in this work is to aggregate heterogeneous data types involving diagnosis codes, drug codes, as well as lab measurements. An earlier framework proposed for the same problem demonstrated promising predictive performance for the random forest classifier by using only lab measurements as data features. We extend this framework, by additionally including diagnosis and drug prescription codes, concurrently. In addition, we employ a recursive feature selection mechanism on top, that extracts the top-k most important features. Our experimental evaluation on five medical datasets of adverse drug events and six different classifiers, suggests that the integration of these additional features provides substantial and statistically significant improvements in terms of AUC, while employing medically relevant features.        △ Less","13 July, 2019","cs.LG,cs.CY,stat.ML",
              BeSense: Leveraging WiFi Channel Data and Computational Intelligence for Behavior Analysis          ,1907.06005,https://arxiv.org/abs/1907.06005,https://arxiv.org/pdf/1907.06005,"Authors:YuGu,XiangZhang,ZhiLiu,FujiRen","        The ever evolving informatics technology has gradually bounded human and computer in a compact way. Understanding user behavior becomes a key enabler in many fields such as sedentary-related healthcare, human-computer interaction (HCI) and affective computing. Traditional sensor-based and vision-based user behavior analysis approaches are obtrusive in general, hindering their usage in realworld. Therefore, in this article, we first introduce WiFi signal as a new source instead of sensor and vision for unobtrusive user behaviors analysis. Then we design BeSense, a contactless behavior analysis system leveraging signal processing and computational intelligence over WiFi channel state information (CSI). We prototype BeSense on commodity low-cost WiFi devices and evaluate its performance in realworld environments. Experimental results have verified its effectiveness in recognizing user behaviors.        △ Less","23 March, 2020","cs.HC,cs.AI,eess.SP",10.1109/MCI.2019.2937610 
              Activity2Vec: Learning ADL Embeddings from Sensor Data with a Sequence-to-Sequence Model          ,1907.05597,https://arxiv.org/abs/1907.05597,https://arxiv.org/pdf/1907.05597,"Authors:AlirezaGhods,DianeJ.Cook","        Recognizing activities of daily living (ADLs) plays an essential role in analyzing human health and behavior. The widespread availability of sensors implanted in homes, smartphones, and smart watches have engendered collection of big datasets that reflect human behavior. To obtain a machine learning model based on these data,researchers have developed multiple feature extraction methods. In this study, we investigate a method for automatically extracting universal and meaningful features that are applicable across similar time series-based learning tasks such as activity recognition and fall detection. We propose creating a sequence-to-sequence (seq2seq) model to perform this feature learning. Beside avoiding feature engineering, the meaningful features learned by the seq2seq model can also be utilized for semi-supervised learning. We evaluate both of these benefits on datasets collected from wearable and ambient sensors.        △ Less","12 July, 2019",cs.LG,
              Organic Thermoelectric Textiles for Harvesting Thermal Energy and Powering Electronics          ,1907.04883,https://arxiv.org/abs/1907.04883,https://arxiv.org/pdf/1907.04883,"Authors:YuanyuanZheng,QihaoZhang,WenlongJin,YuanyuanJing,XinyiChen,XueHan,QinyeBao,YanpingLiu,XinhouWang,ShirenWang,YipingQiu,KunZhang,ChonganDi","        Wearable thermoelectric devices show promises to generate electricity in a ubiquitous, unintermittent and noiseless way for on-body applications. Three-dimensional thermoelectric textiles (TETs) outperform other types in smart textiles owing to their out-of-plane thermoelectric generation and good structural conformability with fabrics. Yet, there has been lack of efficient strategies in scalable manufacture of TETs for sustainably powering electronics. Here, we fabricate organic spacer fabric shaped TETs by knitting carbon nanotube yarn based segmented thermoelectric yarn in large scale. Combing finite element analysis with experimental evaluation, we elucidate that the fabric structure significantly influences the power generation. The optimally designed TET with good wearability and stability shows high output power density of 51.5 mW/m2 and high specific power of 173.3 uW/(g.K) at delta T= 47.5 K. The promising on-body applications of the TET in directly and continuously powering electronics for healthcare and environmental monitoring is fully demonstrated. This work will broaden the research vision and provide new routines for developing high-performance and large-scale TETs toward practical applications.        △ Less","10 July, 2019",physics.app-ph,
              Channel Impulse Response-based Source Localization in a Diffusion-based Molecular Communication System          ,1907.04239,https://arxiv.org/abs/1907.04239,https://arxiv.org/pdf/1907.04239,"Authors:HenryErnestBaidoo-Williams,MuhammadMahboobUrRahman,QammerHussainAbbasi","        This work localizes a molecular source in a diffusion based molecular communication (DbMC) system via a set of passive sensors and a fusion center. Molecular source localization finds its applications in future healthcare systems, including proactive diagnostics. In this paper, we propose two distinct methods which both utilize (the peak of) the channel impulse response measurements to uniquely localize the source, under assumption that the molecular source of interest lies within the open convex-hull of the sensor/anchor nodes. The first method is a one-shot, triangulation-based approach which estimates the unknown location of the molecular source using least-squares method. The corresponding Cramer-Rao bound (CRB) is also derived. The second method is an iterative approach, which utilizes gradient descent law to minimize a non-convex cost function. Simulation results reveal that the triangulation-based method performs very close to the CRB, for any given signal- to-noise ratio. Additionally, the gradient descent-based method converges to the true optima/source location uniformly (in less than hundred iterations).        △ Less","8 July, 2019","eess.SP,cs.IT",
              GP-VAE: Deep Probabilistic Time Series Imputation          ,1907.04155,https://arxiv.org/abs/1907.04155,https://arxiv.org/pdf/1907.04155,"Authors:VincentFortuin,DmitryBaranchuk,GunnarRätsch,StephanMandt","        Multivariate time series with missing values are common in areas such as healthcare and finance, and have grown in number and complexity over the years. This raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain. However, naive applications of deep learning fall short in giving reliable confidence estimates and lack interpretability. We propose a new deep sequential latent variable model for dimensionality reduction and data imputation. Our modeling assumption is simple and interpretable: the high dimensional time series has a lower-dimensional representation which evolves smoothly in time according to a Gaussian process. The non-linear dimensionality reduction in the presence of missing data is achieved using a VAE approach with a novel structured variational approximation. We demonstrate that our approach outperforms several classical and deep learning-based data imputation methods on high-dimensional data from the domains of computer vision and healthcare, while additionally improving the smoothness of the imputations and providing interpretable uncertainty estimates.        △ Less","20 February, 2020","stat.ML,cs.LG",
              Rapid Node Cardinality Estimation in Heterogeneous Machine-to-Machine Networks          ,1907.04133,https://arxiv.org/abs/1907.04133,https://arxiv.org/pdf/1907.04133,"Authors:SachinKadam,SeshaVivekY.,P.HariPrasad,RajeshKumar,GauravS.Kasbekar","        Machine-to-Machine (M2M) networks are an emerging technology with applications in various fields, including smart grids, healthcare, vehicular telematics and smart cities. Heterogeneous M2M networks contain different types of nodes, e.g., nodes that send emergency, periodic, and normal type data. An important problem is to rapidly estimate the number of active nodes of each node type in every time frame in such a network. In this paper, we design two schemes for estimating the active node cardinalities of each node type in a heterogeneous M2M network with TT types of nodes, where T≥2T \ge 2 is an arbitrary integer. Our schemes consist of two phases-- in phase 1, coarse estimates are computed, and in phase 2, these estimates are used to compute the final estimates to the required accuracy. We analytically derive a condition for one of our schemes that can be used to decide as to which of two possible approaches should be used in phase 2 to minimize its execution time. The expected number of time slots required to execute and the expected energy consumption of each active node under one of our schemes are analysed. Using simulations, we show that our proposed schemes require significantly fewer time slots to execute compared to estimation schemes designed for a heterogeneous M2M network in prior work, and also, compared to separately executing a well-known estimation protocol designed for a homogeneous network in prior work TT times to estimate the cardinalities of the TT node types, even though all these schemes obtain estimates with the same accuracy.        △ Less","9 July, 2019",eess.SP,
              Kernel Hypothesis Testing with Set-valued Data          ,1907.04081,https://arxiv.org/abs/1907.04081,https://arxiv.org/pdf/1907.04081,"Authors:AlexisBellot,MihaelavanderSchaar","        We present a general framework for hypothesis testing on distributions of sets of individual examples. Sets may represent many common data sources such as groups of observations in time series, collections of words in text or a batch of images of a given phenomenon. This observation pattern, however, differs from the common assumptions required for hypothesis testing: each set differs in size, may have differing levels of noise, and also may incorporate nuisance variability, irrelevant for the analysis of the phenomenon of interest; all features that bias test decisions if not accounted for. In this paper, we propose to interpret sets as independent samples from a collection of latent probability distributions, and introduce kernel two-sample and independence tests in this latent space of distributions. We prove the consistency of tests and observe them to outperform in a wide range of synthetic experiments. Finally, we showcase their use in practice with experiments of healthcare and climate data, where previously heuristics were needed for feature extraction and testing.        △ Less","16 October, 2020","stat.ME,stat.ML",
              Developing an Evidence-Based Framework for Grading and Assessment of Predictive Tools for Clinical Decision Support          ,1907.03706,https://arxiv.org/abs/1907.03706,https://arxiv.org/pdf/1907.03706,"Authors:MohamedKhalifa,FarahMagrabi,BlancaGallego","        Background: Clinical predictive tools quantify contributions of relevant patient characteristics to derive likelihood of diseases or predict clinical outcomes. When selecting a predictive tool, for implementation at clinical practice or for recommendation in clinical guidelines, clinicians are challenged with an overwhelming and ever growing number of tools, most of which have never been implemented or assessed for comparative effectiveness. Objective: To develop a comprehensive framework to Grade and Assess Predictive tools (GRASP), and provide clinicians with a standardised, evidence based system to support their search for and selection of effective tools. Methods: A focused review of literature was conducted to extract criteria along which tools should be evaluated. An initial framework was designed and applied to assess and grade five tools: LACE Index, Centor Score, Wells Criteria, Modified Early Warning Score, and Ottawa knee rule. After peer review, by expert clinicians and healthcare researchers, the framework was revised and the grading of the tools was updated. Results: GRASP framework grades predictive tools based on published evidence across three dimensions: 1) Phase of evaluation; 2) Level of evidence; and 3) Direction of evidence. The final grade of a tool is based on the highest phase of evaluation, supported by the highest level of positive evidence, or mixed evidence that supports positive conclusion. Discussion and Conclusion: the GRASP framework builds on well established models and widely accepted concepts to provide standardised assessment and evidence based grading of predictive tools. Unlike other methods, GRASP is based on the critical appraisal of published evidence reporting the predictive tools predictive performance before implementation, potential effect and usability during implementation, and their post implementation impact.        △ Less","18 June, 2019",cs.CY,
              Quantifying Transparency of Machine Learning Systems through Analysis of Contributions          ,1907.03483,https://arxiv.org/abs/1907.03483,https://arxiv.org/pdf/1907.03483,"Authors:IainBarclay,AlunPreece,IanTaylor,DineshVerma","        Increased adoption and deployment of machine learning (ML) models into business, healthcare and other organisational processes, will result in a growing disconnect between the engineers and researchers who developed the models and the model's users and other stakeholders, such as regulators or auditors. This disconnect is inevitable, as models begin to be used over a number of years or are shared among third parties through user communities or via commercial marketplaces, and it will become increasingly difficult for users to maintain ongoing insight into the suitability of the parties who created the model, or the data that was used to train it. This could become problematic, particularly where regulations change and once-acceptable standards become outdated, or where data sources are discredited, perhaps judged to be biased or corrupted, either deliberately or unwittingly. In this paper we present a method for arriving at a quantifiable metric capable of ranking the transparency of the process pipelines used to generate ML models and other data assets, such that users, auditors and other stakeholders can gain confidence that they will be able to validate and trust the data sources and human contributors in the systems that they rely on for their business operations. The methodology for calculating the transparency metric, and the type of criteria that could be used to make judgements on the visibility of contributions to systems are explained and illustrated through an example scenario.        △ Less","8 July, 2019","cs.LG,cs.SE",
              Exploring difference in public perceptions on HPV vaccine between gender groups from Twitter using deep learning          ,1907.03167,https://arxiv.org/abs/1907.03167,https://arxiv.org/pdf/1907.03167,"Authors:JingchengDu,ChongliangLuo,QiangWei,YongChen,CuiTao","        In this study, we proposed a convolutional neural network model for gender prediction using English Twitter text as input. Ensemble of proposed model achieved an accuracy at 0.8237 on gender prediction and compared favorably with the state-of-the-art performance in a recent author profiling task. We further leveraged the trained models to predict the gender labels from an HPV vaccine related corpus and identified gender difference in public perceptions regarding HPV vaccine. The findings are largely consistent with previous survey-based studies.        △ Less","6 July, 2019","cs.CL,cs.LG,stat.ML",
              Seeing Under the Cover: A Physics Guided Learning Approach for In-Bed Pose Estimation          ,1907.02161,https://arxiv.org/abs/1907.02161,https://arxiv.org/pdf/1907.02161,"Authors:ShuangjunLiu,SarahOstadabbas","        Human in-bed pose estimation has huge practical values in medical and healthcare applications yet still mainly relies on expensive pressure mapping (PM) solutions. In this paper, we introduce our novel physics inspired vision-based approach that addresses the challenging issues associated with the in-bed pose estimation problem including monitoring a fully covered person in complete darkness. We reformulated this problem using our proposed Under the Cover Imaging via Thermal Diffusion (UCITD) method to capture the high resolution pose information of the body even when it is fully covered by using a long wavelength IR technique. We proposed a physical hyperparameter concept through which we achieved high quality groundtruth pose labels in different modalities. A fully annotated in-bed pose dataset called Simultaneously-collected multimodal Lying Pose (SLP) is also formed/released with the same order of magnitude as most existing large-scale human pose datasets to support complex models' training and evaluation. A network trained from scratch on it and tested on two diverse settings, one in a living room and the other in a hospital room showed pose estimation performance of 99.5% and 95.7% in PCK0.2 standard, respectively. Moreover, in a multi-factor comparison with a state-of-the art in-bed pose monitoring solution based on PM, our solution showed significant superiority in all practical aspects by being 60 times cheaper, 300 times smaller, while having higher pose recognition granularity and accuracy.        △ Less","20 September, 2019",cs.CV,
              Secure Computation in Decentralized Data Markets          ,1907.01489,https://arxiv.org/abs/1907.01489,https://arxiv.org/pdf/1907.01489,"Authors:FattanehBayatbabolghani,BharathRamsundar","        Decentralized data markets gather data from many contributors to create a joint data cooperative governed by market stakeholders. The ability to perform secure computation on decentralized data markets would allow for useful insights to be gained while respecting the privacy of data contributors. In this paper, we design secure protocols for such computation by utilizing secure multi-party computation techniques including garbled circuit evaluation and homomorphic encryption. Our proposed solutions are efficient and capable of performing arbitrary computation, but we report performance on two specific applications in the healthcare domain to emphasize the applicability of our methods to sensitive datasets.        △ Less","2 July, 2019",cs.CR,
              Predicting Treatment Initiation from Clinical Time Series Data via Graph-Augmented Time-Sensitive Model          ,1907.01099,https://arxiv.org/abs/1907.01099,https://arxiv.org/pdf/1907.01099,"Authors:FanZhang,TongWu,YunlongWang,YongCai,CaoXiao,EmilyZhao,LucasGlass,JimengSun","        Many computational models were proposed to extract temporal patterns from clinical time series for each patient and among patient group for predictive healthcare. However, the common relations among patients (e.g., share the same doctor) were rarely considered. In this paper, we represent patients and clinicians relations by bipartite graphs addressing for example from whom a patient get a diagnosis. We then solve for the top eigenvectors of the graph Laplacian, and include the eigenvectors as latent representations of the similarity between patient-clinician pairs into a time-sensitive prediction model. We conducted experiments using real-world data to predict the initiation of first-line treatment for Chronic Lymphocytic Leukemia (CLL) patients. Results show that relational similarity can improve prediction over multiple baselines, for example a 5% incremental over long-short term memory baseline in terms of area under precision-recall curve.        △ Less","1 July, 2019","cs.LG,stat.ML",
              Analog Signal Compression and Multiplexing Techniques for Healthcare Internet of Things          ,1907.00322,https://arxiv.org/abs/1907.00322,https://arxiv.org/pdf/1907.00322,"Authors:XueyuanZhao,VidyasagarSadhu,DarioPompili","        Scalability is a major issue for Internet of Things (IoT) as the total amount of traffic data collected and/or the number of sensors deployed grow. In some IoT applications such as healthcare, power consumption is also a key design factor for the IoT devices. In this paper, a multi-signal compression and encoding method based on Analog Joint Source Channel Coding (AJSCC) is proposed that works fully in the analog domain without the need for power-hungry Analog-to-Digital Converters (ADCs). Compression is achieved by quantizing all the input signals but one. While saving power, this method can also reduce the number of devices by combining one or more sensing functionalities into a single device (called 'AJSCC device'). Apart from analog encoding, AJSCC devices communicate to an aggregator node (FPMM receiver) using a novel Frequency Position Modulation and Multiplexing (FPMM) technique. Such joint modulation and multiplexing technique presents three mayor advantages---it is robust to interference at particular frequency bands, it protects against eavesdropping, and it consumes low power due to a very low Signal-to-Noise Ratio (SNR) operating region at the receiver. Performance of the proposed multi-signal compression method and FPMM technique is evaluated via simulations in terms of Mean Square Error (MSE) and Miss Detection Rate (MDR), respectively.        △ Less","30 June, 2019","eess.SP,cs.NI",10.1109/MASS.2017.62 
              SLAM Endoscopy enhanced by adversarial depth prediction          ,1907.00283,https://arxiv.org/abs/1907.00283,https://arxiv.org/pdf/1907.00283,"Authors:RichardJ.Chen,TaylorL.Bobrow,ThomasAthey,FaisalMahmood,NicholasJ.Durr","        Medical endoscopy remains a challenging application for simultaneous localization and mapping (SLAM) due to the sparsity of image features and size constraints that prevent direct depth-sensing. We present a SLAM approach that incorporates depth predictions made by an adversarially-trained convolutional neural network (CNN) applied to monocular endoscopy images. The depth network is trained with synthetic images of a simple colon model, and then fine-tuned with domain-randomized, photorealistic images rendered from computed tomography measurements of human colons. Each image is paired with an error-free depth map for supervised adversarial learning. Monocular RGB images are then fused with corresponding depth predictions, enabling dense reconstruction and mosaicing as an endoscope is advanced through the gastrointestinal tract. Our preliminary results demonstrate that incorporating monocular depth estimation into a SLAM architecture can enable dense reconstruction of endoscopic scenes.        △ Less","29 June, 2019","eess.IV,cs.CV,cs.RO",
              Implementing Ethics in AI: Initial Results of an Industrial Multiple Case Study          ,1906.12307,https://arxiv.org/abs/1906.12307,https://arxiv.org/pdf/1906.12307,"Authors:VilleVakkuri,Kai-KristianKemell,PekkaAbrahamsson","        Artificial intelligence (AI) is becoming increasingly widespread in system development endeavors. As AI systems affect various stakeholders due to their unique nature, the growing influence of these systems calls for ethical considerations. Academic discussion and practical examples of autonomous system failures have highlighted the need for implementing ethics in software development. However, research on methods and tools for implementing ethics into AI system design and development in practice is still lacking. This paper begins to address this focal problem by providing elements needed for producing a baseline for ethics in AI based software development. We do so by means of an industrial multiple case study on AI systems development in the healthcare sector. Using a research model based on extant, conceptual AI ethics literature, we explore the current state of practice out on the field in the absence of formal methods and tools for ethically aligned design.        △ Less","16 June, 2020",cs.CY,10.1007/978-3-030-35333-9_24 
              Simultaneous Transformation and Rounding (STAR) Models for Integer-Valued Data          ,1906.11653,https://arxiv.org/abs/1906.11653,https://arxiv.org/pdf/1906.11653,"Authors:DanielR.Kowal,AntonioCanale","        We propose a simple yet powerful framework for modeling integer-valued data, such as counts, scores, and rounded data. The data-generating process is defined by Simultaneously Transforming and Rounding (STAR) a continuous-valued process, which produces a flexible family of integer-valued distributions capable of modeling zero-inflation, bounded or censored data, and over- or underdispersion. The transformation is modeled as unknown for greater distributional flexibility, while the rounding operation ensures a coherent integer-valued data-generating process. An efficient MCMC algorithm is developed for posterior inference and provides a mechanism for adaptation of successful Bayesian models and algorithms for continuous data to the integer-valued data setting. Using the STAR framework, we design a new Bayesian Additive Regression Tree (BART) model for integer-valued data, which demonstrates impressive predictive distribution accuracy for both synthetic data and a large healthcare utilization dataset. For interpretable regression-based inference, we develop a STAR additive model, which offers greater flexibility and scalability than existing integer-valued models. The STAR additive model is applied to study the recent decline in Amazon river dolphins.        △ Less","3 September, 2019","stat.ME,stat.AP,stat.CO,stat.ML",
              A novel music-based game with motion capture to support cognitive and motor function in the elderly          ,1906.10428,https://arxiv.org/abs/1906.10428,https://arxiv.org/pdf/1906.10428,"Authors:KatAgres,SimonLui,DorienHerremans","        This paper presents a novel game prototype that uses music and motion detection as preventive medicine for the elderly. Given the aging populations around the globe, and the limited resources and staff able to care for these populations, eHealth solutions are becoming increasingly important, if not crucial, additions to modern healthcare and preventive medicine. Furthermore, because compliance rates for performing physical exercises are often quite low in the elderly, systems able to motivate and engage this population are a necessity. Our prototype uses music not only to engage listeners, but also to leverage the efficacy of music to improve mental and physical wellness. The game is based on a memory task to stimulate cognitive function, and requires users to perform physical gestures to mimic the playing of different musical instruments. To this end, the Microsoft Kinect sensor is used together with a newly developed gesture detection module in order to process users' gestures. The resulting prototype system supports both cognitive functioning and physical strengthening in the elderly.        △ Less","25 June, 2019","cs.HC,cs.MM",
              Challenges and Opportunities of Big Data in Healthcare Mobile Applications          ,1906.10166,https://arxiv.org/abs/1906.10166,https://arxiv.org/pdf/1906.10166,"Authors:MohsenAghabozorgiNafchi,MaryamAghabozorgiNafchi","        The health and various ways to improve healthcare systems are one of the most concerns of human in history. By the growth of mobile technology, different mobile applications in the field of the healthcare system are developed. These mobile applications instantly gather and analyze the data of their users to help them in the health area. This volume of data will be a critical problem. Big data in healthcare mobile applications have its challenges and opportunities for the users and developers. Does this amount of gathered data which is increasing day by day can help the human to design new tools in healthcare systems and improve health condition? In this chapter, we will discuss meticulously the challenges and opportunities of big data in the healthcare mobile applications.        △ Less","27 June, 2019",cs.HC,
              Embedded Deep Learning for Sleep Staging          ,1906.09905,https://arxiv.org/abs/1906.09905,https://arxiv.org/pdf/1906.09905,"Authors:EnginTüretken,JérômeVanZaen,RicardDelgado-Gonzalo","        The rapidly-advancing technology of deep learning (DL) into the world of the Internet of Things (IoT) has not fully entered in the fields of m-Health yet. Among the main reasons are the high computational demands of DL algorithms and the inherent resource-limitation of wearable devices. In this paper, we present initial results for two deep learning architectures used to diagnose and analyze sleep patterns, and we compare them with a previously presented hand-crafted algorithm. The algorithms are designed to be reliable for consumer healthcare applications and to be integrated into low-power wearables with limited computational resources.        △ Less","18 June, 2019","cs.LG,eess.SP",10.1109/SDS.2019.00005 
              Detection of Myocardial Infarction Based on Novel Deep Transfer Learning Methods for Urban Healthcare in Smart Cities          ,1906.09358,https://arxiv.org/abs/1906.09358,https://arxiv.org/pdf/1906.09358,"Authors:AhmedAlghamdi,MohamedHammad,HassanUgail,AsmaaAbdel-Raheem,KhanMuhammad,HanyS.Khalifa,AhmedA.AbdEl-Latif","        . In this paper, an effective computer-aided diagnosis (CAD) system is presented to detect MI signals using the convolution neural network (CNN) for urban healthcare in smart cities. Two types of transfer learning techniques are employed to retrain the pre-trained VGG-Net (Fine-tuning and VGG-Net as fixed feature extractor) and obtained two new networks VGG-MI1 and VGG-MI2. In the VGG-MI1 model, the last layer of the VGG-Net model is replaced with a specific layer according to our requirements and various functions are optimized to reduce overfitting. In the VGG-MI2 model, one layer of the VGG-Net model is selected as a feature descriptor of the ECG images to describe it with informative features. Considering the limited availability of dataset, ECG data is augmented which has increased the classification performance. Physikalisch-technische bundesanstalt (PTB) Diagnostic ECG database is used for experimentation, which has been widely employed in MI detection studies. In case of using VGG-MI1, we achieved an accuracy, sensitivity, and specificity of 99.02%, 98.76%, and 99.17%, respectively and we achieved an accuracy of 99.22%, a sensitivity of 99.15%, and a specificity of 99.49% with VGG-MI2 model. Experimental results validate the efficiency of the proposed system in terms of accuracy sensitivity, and specificity.        △ Less","6 December, 2019","cs.LG,eess.SP,stat.ML",
              Applications of Backscatter Communications for Healthcare Networks          ,1906.09209,https://arxiv.org/abs/1906.09209,https://arxiv.org/pdf/1906.09209,"Authors:FurqanJameel,RuifengDuan,ZhengChang,AleksiLiljemark,TapaniRistaniemi,RikuJantti","        Backscatter communication is expected to help in revitalizing the domain of healthcare through its myriad applications. From on-body sensors to in-body implants and miniature embeddable devices, there are many potential use cases that can leverage the miniature and low-powered nature of backscatter devices. However, the existing literature lacks a comprehensive study that provides a distilled review of the latest studies on backscatter communications from the healthcare perspective. Thus, with the objective to promote the utility of backscatter communication in healthcare, this paper aims to identify specific applications of backscatter systems. A detailed taxonomy of recent studies and gap analysis for future research directions are provided in this work. Finally, we conduct measurements at 590 MHz in different propagation environments with the in-house designed backscatter device. The link budget results show the promise of backscatter devices to communicate over large distances for indoor environments which demonstrates its potential in the healthcare system.        △ Less","21 June, 2019","eess.SP,eess.SY",
              More Efficient Policy Learning via Optimal Retargeting          ,1906.08611,https://arxiv.org/abs/1906.08611,https://arxiv.org/pdf/1906.08611,Authors:NathanKallus,"        Policy learning can be used to extract individualized treatment regimes from observational data in healthcare, civics, e-commerce, and beyond. One big hurdle to policy learning is a commonplace lack of overlap in the data for different actions, which can lead to unwieldy policy evaluation and poorly performing learned policies. We study a solution to this problem based on retargeting, that is, changing the population on which policies are optimized. We first argue that at the population level, retargeting may induce little to no bias. We then characterize the optimal reference policy centering and retargeting weights in both binary-action and multi-action settings. We do this in terms of the asymptotic efficient estimation variance of the new learning objective. We further consider bias regularization. Extensive empirical results in a simulation study and a case study of targeted job counseling demonstrate that retargeting is a fairly easy way to significantly improve any policy learning procedure.        △ Less","20 June, 2019","stat.ML,cs.LG,math.OC",
              Cryptanalysis of Khatoon et al.'s ECC-based Authentication Protocol for Healthcare Systems          ,1906.08424,https://arxiv.org/abs/1906.08424,https://arxiv.org/pdf/1906.08424,"Authors:MahdiNikooghadam,HalehAmintoosi","        Telecare medical information systems are gaining rapid popularity in terms of providing the delivery of online health-related services such as online remote health profile access for patients and doctors. Due to being installed entirely on Internet, these systems are exposed to various security and privacy threats. Hence, establishing a secure key agreement and authentication process between the patients and the medical servers is an important challenge. Recently, Khatoon et.al proposed an ECC-based unlink-able authentication and key agreement method for healthcare related application in smart city. In this article, we provide a descriptive analysis on their proposed scheme and prove that Khatoon et al.'s scheme is vulnerable to known-session-specific temporary information attack and is not able to provide perfect forward secrecy.        △ Less","19 June, 2019",cs.CR,
              Wrist02 -- Reliable Peripheral Oxygen Saturation Readings from Wrist-Worn Pulse Oximeters          ,1906.07545,https://arxiv.org/abs/1906.07545,https://arxiv.org/pdf/1906.07545,"Authors:CalebPhillips,DaniyalLiaqat,MosheGabel,EyaldeLara","        Peripheral blood oxygen saturation Sp02 is a vital measure in healthcare. Modern off-the-shelf wrist-worn devices, such as the Apple Watch, FitBit, and Samsung Gear, have an onboard sensor called a pulse oximeter. While pulse oximeters are capable of measuring both Sp02 and heart rate, current wrist-worn devices use them only to determine heart rate, as Sp02 measurements collected from the wrist are believed to be inaccurate. Enabling oxygen saturation monitoring on wearable devices would make these devices tremendously more useful for health monitoring and open up new avenues of research. To the best of our knowledge, we present the first study of the reliability of Sp02 sensing from the wrist. Using a custom-built wrist-worn pulse oximeter, we find that existing algorithms designed for fingertip sensing are a poor match for this setting, and can lead to over 90% of readings being inaccurate and unusable. We further show that sensor placement and skin tone have a substantial effect on the measurement error, and must be considered when designing wrist-worn Sp02 sensors and measurement algorithms. Based on our findings, we propose \codename, an alternative approach for reliable Sp02 sensing. By selectively pruning data, \codename achieves an order of magnitude reduction in error compared to existing algorithms, while still providing sufficiently frequent readings for continuous health monitoring.        △ Less","16 June, 2019","cs.CY,eess.SP",
              Data-Driven Malaria Prevalence Prediction in Large Densely-Populated Urban Holoendemic sub-Saharan West Africa: Harnessing Machine Learning Approaches and 22-years of Prospectively Collected Data          ,1906.07502,https://arxiv.org/abs/1906.07502,https://arxiv.org/pdf/1906.07502,"Authors:BiobeleJ.Brown,AlexanderA.Przybylski,PetruManescu,FabioCaccioli,GbeminiyiOyinloye,MunaElmi,MichaelJ.Shaw,VijayPawar,RemyClaveau,JohnShawe-Taylor,MandayamA.Srinivasan,NathanielK.Afolabi,AdebolaE.Orimadegun,WasiuA.Ajetunmobi,FrancisAkinkunmi,OlayinkaKowobari,KikelomoOsinusi,FelixO.Akinbami,SamuelOmokhodion,WuraolaA.Shokunbi,IkeoluwaLagunju,OlugbemiroSodeinde,DelmiroFernandez-Reyes","        Plasmodium falciparum malaria still poses one of the greatest threats to human life with over 200 million cases globally leading to half-million deaths annually. Of these, 90% of cases and of the mortality occurs in sub-Saharan Africa, mostly among children. Although malaria prediction systems are central to the 2016-2030 malaria Global Technical Strategy, currently these are inadequate at capturing and estimating the burden of disease in highly endemic countries. We developed and validated a computational system that exploits the predictive power of current Machine Learning approaches on 22-years of prospective data from the high-transmission holoendemic malaria urban-densely-populated sub-Saharan West-Africa metropolis of Ibadan. Our dataset of >9x104 screened study participants attending our clinical and community services from 1996 to 2017 contains monthly prevalence, temporal, environmental and host features. Our Locality-specific Elastic-Net based Malaria Prediction System (LEMPS) achieves good generalization performance, both in magnitude and direction of the prediction, when tasked to predict monthly prevalence on previously unseen validation data (MAE<=6x10-2, MSE<=7x10-3) within a range of (+0.1 to -0.05) error-tolerance which is relevant and usable for aiding decision-support in a holoendemic setting. LEMPS is well-suited for malaria prediction, where there are multiple features which are correlated with one another, and trading-off between regularization-strength L1-norm and L2-norm allows the system to retain stability. Data-driven systems are critical for regionally-adaptable surveillance, management of control strategies and resource allocation across stretched healthcare systems.        △ Less","18 June, 2019","cs.LG,stat.AP,stat.ML",
              An IoT Based Framework For Activity Recognition Using Deep Learning Technique          ,1906.07247,https://arxiv.org/abs/1906.07247,https://arxiv.org/pdf/1906.07247,"Authors:AshwinGeetD'Sa,B.G.Prasad","        Activity recognition is the ability to identify and recognize the action or goals of the agent. The agent can be any object or entity that performs action that has end goals. The agents can be a single agent performing the action or group of agents performing the actions or having some interaction. Human activity recognition has gained popularity due to its demands in many practical applications such as entertainment, healthcare, simulations and surveillance systems. Vision based activity recognition is gaining advantage as it does not require any human intervention or physical contact with humans. Moreover, there are set of cameras that are networked with the intention to track and recognize the activities of the agent. Traditional applications that were required to track or recognize human activities made use of wearable devices. However, such applications require physical contact of the person. To overcome such challenges, vision based activity recognition system can be used, which uses a camera to record the video and a processor that performs the task of recognition. The work is implemented in two stages. In the first stage, an approach for the Implementation of Activity recognition is proposed using background subtraction of images, followed by 3D- Convolutional Neural Networks. The impact of using Background subtraction prior to 3D-Convolutional Neural Networks has been reported. In the second stage, the work is further extended and implemented on Raspberry Pi, that can be used to record a stream of video, followed by recognizing the activity that was involved in the video. Thus, a proof-of-concept for activity recognition using small, IoT based device, is provided, which can enhance the system and extend its applications in various forms like, increase in portability, networking, and other capabilities of the device.        △ Less","17 June, 2019","cs.LG,cs.CV,stat.ML",
              ASAC: Active Sensing using Actor-Critic models          ,1906.06796,https://arxiv.org/abs/1906.06796,https://arxiv.org/pdf/1906.06796,"Authors:JinsungYoon,JamesJordon,MihaelavanderSchaar","        Deciding what and when to observe is critical when making observations is costly. In a medical setting where observations can be made sequentially, making these observations (or not) should be an active choice. We refer to this as the active sensing problem. In this paper, we propose a novel deep learning framework, which we call ASAC (Active Sensing using Actor-Critic models) to address this problem. ASAC consists of two networks: a selector network and a predictor network. The selector network uses previously selected observations to determine what should be observed in the future. The predictor network uses the observations selected by the selector network to predict a label, providing feedback to the selector network (well-selected variables should be predictive of the label). The goal of the selector network is then to select variables that balance the cost of observing the selected variables with their predictive power; we wish to preserve the conditional label distribution. During training, we use the actor-critic models to allow the loss of the selector to be ""back-propagated"" through the sampling process. The selector network ""acts"" by selecting future observations to make. The predictor network acts as a ""critic"" by feeding predictive errors for the selected variables back to the selector network. In our experiments, we show that ASAC significantly outperforms state-of-the-arts in two real-world medical datasets.        △ Less","16 June, 2019","cs.LG,stat.ML",
              Optimized Blockchain Model for Internet of Things based Healthcare Applications          ,1906.06517,https://arxiv.org/abs/1906.06517,https://arxiv.org/pdf/1906.06517,"Authors:AshutoshDharDwivedi,LukasMalina,PetrDzurenda,GautamSrivastava","        There continues to be a recent push to taking the cryptocurrency based ledger system known as Blockchain and applying its techniques to non-financial applications. One of the main areas for application remains Internet of Things (IoT) as we see many areas of improvement as we move into an age of smart cities. In this paper, we examine an initial look at applying the key aspects of Blockchain to a health application network where patients health data can be used to create alerts important to authenticated healthcare providers in a secure and private manner. This paper also presents the benefits and also practical obstacles of the blockchain-based security approaches in IoT.        △ Less","15 June, 2019","cs.CR,cs.NI",
              Understanding artificial intelligence ethics and safety          ,1906.05684,https://arxiv.org/abs/1906.05684,https://arxiv.org/pdf/1906.05684,Authors:DavidLeslie,"        A remarkable time of human promise has been ushered in by the convergence of the ever-expanding availability of big data, the soaring speed and stretch of cloud computing platforms, and the advancement of increasingly sophisticated machine learning algorithms. Innovations in AI are already leaving a mark on government by improving the provision of essential social goods and services from healthcare, education, and transportation to food supply, energy, and environmental management. These bounties are likely just the start. The prospect that progress in AI will help government to confront some of its most urgent challenges is exciting, but legitimate worries abound. As with any new and rapidly evolving technology, a steep learning curve means that mistakes and miscalculations will be made and that both unanticipated and harmful impacts will occur.  This guide, written for department and delivery leads in the UK public sector and adopted by the British Government in its publication, 'Using AI in the Public Sector,' identifies the potential harms caused by AI systems and proposes concrete, operationalisable measures to counteract them. It stresses that public sector organisations can anticipate and prevent these potential harms by stewarding a culture of responsible innovation and by putting in place governance processes that support the design and implementation of ethical, fair, and safe AI systems. It also highlights the need for algorithmically supported outcomes to be interpretable by their users and made understandable to decision subjects in clear, non-technical, and accessible ways. Finally, it builds out a vision of human-centred and context-sensitive implementation that gives a central role to communication, evidence-based reasoning, situational awareness, and moral justifiability.        △ Less","11 June, 2019","cs.CY,cs.AI,cs.LG,stat.AP",10.5281/zenodo.3240529 
              HEAD-QA: A Healthcare Dataset for Complex Reasoning          ,1906.04701,https://arxiv.org/abs/1906.04701,https://arxiv.org/pdf/1906.04701,"Authors:DavidVilares,CarlosGómez-Rodríguez","        We present HEAD-QA, a multi-choice question answering testbed to encourage research on complex reasoning. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. We then consider monolingual (Spanish) and cross-lingual (to English) experiments with information retrieval and neural techniques. We show that: (i) HEAD-QA challenges current methods, and (ii) the results lag well behind human performance, demonstrating its usefulness as a benchmark for future work.        △ Less","11 June, 2019",cs.CL,
              Learning Individual Causal Effects from Networked Observational Data          ,1906.03485,https://arxiv.org/abs/1906.03485,https://arxiv.org/pdf/1906.03485,"Authors:RuochengGuo,JundongLi,HuanLiu","        Convenient access to observational data enables us to learn causal effects without randomized experiments. This research direction draws increasing attention in research areas such as economics, healthcare, and education. For example, we can study how a medicine (the treatment) causally affects the health condition (the outcome) of a patient using existing electronic health records. To validate causal effects learned from observational data, we have to control confounding bias -- the influence of variables which causally influence both the treatment and the outcome. Existing work along this line overwhelmingly relies on the unconfoundedness assumption that there do not exist unobserved confounders. However, this assumption is untestable and can even be untenable. In fact, an important fact ignored by the majority of previous work is that observational data can come with network information that can be utilized to infer hidden confounders. For example, in an observational study of the individual-level treatment effect of a medicine, instead of randomized experiments, the medicine is often assigned to each individual based on a series of factors. Some of the factors (e.g., socioeconomic status) can be challenging to measure and therefore become hidden confounders. Fortunately, the socioeconomic status of an individual can be reflected by whom she is connected in social networks. With this fact in mind, we aim to exploit the network information to recognize patterns of hidden confounders which would further allow us to learn valid individual causal effects from observational data. In this work, we propose a novel causal inference framework, the network deconfounder, which learns representations to unravel patterns of hidden confounders from the network information. Empirically, we perform extensive experiments to validate the effectiveness of the network deconfounder on various datasets.        △ Less","1 December, 2019","cs.SI,cs.LG",
              Relaxed Parameter Sharing: Effectively Modeling Time-Varying Relationships in Clinical Time-Series          ,1906.02898,https://arxiv.org/abs/1906.02898,https://arxiv.org/pdf/1906.02898,"Authors:JeehehOh,JiaxuanWang,ShengpuTang,MichaelSjoding,JennaWiens","        Recurrent neural networks (RNNs) are commonly applied to clinical time-series data with the goal of learning patient risk stratification models. Their effectiveness is due, in part, to their use of parameter sharing over time (i.e., cells are repeated hence the name recurrent). We hypothesize, however, that this trait also contributes to the increased difficulty such models have with learning relationships that change over time. Conditional shift, i.e., changes in the relationship between the input X and the output y, arises when risk factors associated with the event of interest change over the course of a patient admission. While in theory, RNNs and gated RNNs (e.g., LSTMs) in particular should be capable of learning time-varying relationships, when training data are limited, such models often fail to accurately capture these dynamics. We illustrate the advantages and disadvantages of complete parameter sharing (RNNs) by comparing an LSTM with shared parameters to a sequential architecture with time-varying parameters on prediction tasks involving three clinically-relevant outcomes: acute respiratory failure (ARF), shock, and in-hospital mortality. In experiments using synthetic data, we demonstrate how parameter sharing in LSTMs leads to worse performance in the presence of conditional shift. To improve upon the dichotomy between complete parameter sharing and no parameter sharing, we propose a novel RNN formulation based on a mixture model in which we relax parameter sharing over time. The proposed method outperforms standard LSTMs and other state-of-the-art baselines across all tasks. In settings with limited data, relaxed parameter sharing can lead to improved patient risk stratification performance.        △ Less","2 January, 2020","cs.LG,stat.ML",
              An Inverse Optimization Approach to Measuring Clinical Pathway Concordance          ,1906.02636,https://arxiv.org/abs/1906.02636,https://arxiv.org/pdf/1906.02636,"Authors:TimothyC.Y.Chan,MariaEberg,KatharinaForster,ClaireHolloway,LucianoIeraci,YusufShalaby,NasrinYousefi","        Clinical pathways outline standardized processes in the delivery of care for a specific disease. Patient journeys through the healthcare system, though, can deviate substantially from these pathways. Given the positive benefits of clinical pathways, it is important to measure the concordance of patient pathways so that variations in health system performance or bottlenecks in the delivery of care can be detected, monitored, and acted upon. This paper proposes the first data-driven inverse optimization approach to measuring pathway concordance in any problem context. Our specific application considers clinical pathway concordance for stage III colon cancer. We develop a novel concordance metric and demonstrate using real patient data from Ontario, Canada that it has a statistically significant association with survival. Our methodological approach considers a patient's journey as a walk in a directed graph, where the costs on the arcs are derived by solving an inverse shortest path problem. The inverse optimization model uses two sources of information to find the arc costs: reference pathways developed by a provincial cancer agency (primary) and data from real-world patient-related activity from patients with both positive and negative clinical outcomes (secondary). Thus, our inverse optimization framework extends existing models by including data points of both varying ""primacy"" and ""alignment"". Data primacy is addressed through a two-stage approach to imputing the cost vector, while data alignment is addressed by a hybrid objective function that aims to minimize and maximize suboptimality error for different subsets of input data.        △ Less","1 June, 2020","stat.AP,math.OC",
              SparseSense: Human Activity Recognition from Highly Sparse Sensor Data-streams Using Set-based Neural Networks          ,1906.02399,https://arxiv.org/abs/1906.02399,https://arxiv.org/pdf/1906.02399,"Authors:AlirezaAbedin,S.HamidRezatofighi,QinfengShi,DamithC.Ranasinghe","        Batteryless or so called passive wearables are providing new and innovative methods for human activity recognition (HAR), especially in healthcare applications for older people. Passive sensors are low cost, lightweight, unobtrusive and desirably disposable; attractive attributes for healthcare applications in hospitals and nursing homes. Despite the compelling propositions for sensing applications, the data streams from these sensors are characterised by high sparsity---the time intervals between sensor readings are irregular while the number of readings per unit time are often limited. In this paper, we rigorously explore the problem of learning activity recognition models from temporally sparse data. We describe how to learn directly from sparse data using a deep learning paradigm in an end-to-end manner. We demonstrate significant classification performance improvements on real-world passive sensor datasets from older people over the state-of-the-art deep learning human activity recognition models. Further, we provide insights into the model's behaviour through complementary experiments on a benchmark dataset and visualisation of the learned activity feature spaces.        △ Less","5 June, 2019","cs.LG,cs.HC,stat.ML",
              Artificial Intelligence in Clinical Health Care Applications: Viewpoint          ,1906.02090,https://arxiv.org/abs/1906.02090,https://arxiv.org/pdf/1906.02090,"Authors:MichaelvanHartskamp,SergioConsoli,WimVerhaegh,MilanPetković,AnjavandeStolpe","        The idea of Artificial Intelligence (AI) has a long history. It turned out, however, that reaching intelligence at human levels is more complicated than originally anticipated. Currently we are experiencing a renewed interest in AI, fueled by an enormous increase in computing power and an even larger increase in data, in combination with improved AI technologies like deep learning. Healthcare is considered the next domain to be revolutionized by Artificial Intelligence. While AI approaches are excellently suited to develop certain algorithms, for biomedical applications there are specific challenges. We propose recommendations to improve AI projects in the biomedical space and especially clinical healthcare.        △ Less","26 June, 2019","cs.CY,cs.AI",10.2196/12100 
              Assessing Disparate Impacts of Personalized Interventions: Identifiability and Bounds          ,1906.01552,https://arxiv.org/abs/1906.01552,https://arxiv.org/pdf/1906.01552,"Authors:NathanKallus,AngelaZhou","        Personalized interventions in social services, education, and healthcare leverage individual-level causal effect predictions in order to give the best treatment to each individual or to prioritize program interventions for the individuals most likely to benefit. While the sensitivity of these domains compels us to evaluate the fairness of such policies, we show that actually auditing their disparate impacts per standard observational metrics, such as true positive rates, is impossible since ground truths are unknown. Whether our data is experimental or observational, an individual's actual outcome under an intervention different than that received can never be known, only predicted based on features. We prove how we can nonetheless point-identify these quantities under the additional assumption of monotone treatment response, which may be reasonable in many applications. We further provide a sensitivity analysis for this assumption by means of sharp partial-identification bounds under violations of monotonicity of varying strengths. We show how to use our results to audit personalized interventions using partially-identified ROC and xROC curves and demonstrate this in a case study of a French job training dataset.        △ Less","4 June, 2019","stat.ML,cs.LG,econ.EM",
              Performance Requirements of Advanced Healthcare Services over Future Cellular Systems          ,1906.01503,https://arxiv.org/abs/1906.01503,https://arxiv.org/pdf/1906.01503,"Authors:GiuliaCisotto,EdoardoCasarin,StefanoTomasin","        The fifth generation (5G) of communication systems has ambitious targets of data rate, end-to-end latency, and connection availability, while the deployment of a new flexible network architecture will spur new applications. E-health and mobile health (m-health) solutions will meet the increasing demand of new, sustainable, and more accessible services beneficial to both practitioners and the rapidly aging population. This paper aims at defining the technical requirements of future cellular networks to support a variety of advanced healthcare services (e.g., smart hospital, telesurgery, connected ambulances, and monitoring). While 5G will be able to satisfy these requirements, it will also pave the way for future e- and m-health in the sixth-generation (6G) cellular networks.        △ Less","4 June, 2019",eess.SP,10.1109/MCOM.001.1900349 
              Constructing Energy-efficient Mixed-precision Neural Networks through Principal Component Analysis for Edge Intelligence          ,1906.01493,https://arxiv.org/abs/1906.01493,https://arxiv.org/pdf/1906.01493,"Authors:IndranilChakraborty,DeboleenaRoy,IshaGarg,AayushAnkit,KaushikRoy","        The `Internet of Things' has brought increased demand for AI-based edge computing in applications ranging from healthcare monitoring systems to autonomous vehicles. Quantization is a powerful tool to address the growing computational cost of such applications, and yields significant compression over full-precision networks. However, quantization can result in substantial loss of performance for complex image classification tasks. To address this, we propose a Principal Component Analysis (PCA) driven methodology to identify the important layers of a binary network, and design mixed-precision networks. The proposed Hybrid-Net achieves a more than 10% improvement in classification accuracy over binary networks such as XNOR-Net for ResNet and VGG architectures on CIFAR-100 and ImageNet datasets while still achieving up to 94% of the energy-efficiency of XNOR-Nets. This work furthers the feasibility of using highly compressed neural networks for energy-efficient neural computing in edge devices.        △ Less","2 December, 2019","cs.LG,cs.CV,cs.NE",10.1038/s42256-019-0134-0 
              Transfer Learning with intelligent training data selection for prediction of Alzheimer's Disease          ,1906.01160,https://arxiv.org/abs/1906.01160,https://arxiv.org/pdf/1906.01160,"Authors:NaimulMefrazKhan,MarciaHon,NabilaAbraham","        Detection of Alzheimer's Disease (AD) from neuroimaging data such as MRI through machine learning has been a subject of intense research in recent years. Recent success of deep learning in computer vision has progressed such research further. However, common limitations with such algorithms are reliance on a large number of training images, and requirement of careful optimization of the architecture of deep networks. In this paper, we attempt solving these issues with transfer learning, where the state-of-the-art VGG architecture is initialized with pre-trained weights from large benchmark datasets consisting of natural images. The network is then fine-tuned with layer-wise tuning, where only a pre-defined group of layers are trained on MRI images. To shrink the training data size, we employ image entropy to select the most informative slices. Through experimentation on the ADNI dataset, we show that with training size of 10 to 20 times smaller than the other contemporary methods, we reach state-of-the-art performance in AD vs. NC, AD vs. MCI, and MCI vs. NC classification problems, with a 4% and a 7% increase in accuracy over the state-of-the-art for AD vs. MCI and MCI vs. NC, respectively. We also provide detailed analysis of the effect of the intelligent training data selection method, changing the training size, and changing the number of layers to be fine-tuned. Finally, we provide Class Activation Maps (CAM) that demonstrate how the proposed model focuses on discriminative image regions that are neuropathologically relevant, and can help the healthcare practitioner in interpreting the model's decision making process.        △ Less","3 June, 2019","cs.CV,cs.LG",
              Pre-training of Graph Augmented Transformers for Medication Recommendation          ,1906.00346,https://arxiv.org/abs/1906.00346,https://arxiv.org/pdf/1906.00346,"Authors:JunyuanShang,TengfeiMa,CaoXiao,JimengSun","        Medication recommendation is an important healthcare application. It is commonly formulated as a temporal prediction task. Hence, most existing works only utilize longitudinal electronic health records (EHRs) from a small number of patients with multiple visits ignoring a large number of patients with a single visit (selection bias). Moreover, important hierarchical knowledge such as diagnosis hierarchy is not leveraged in the representation learning process. To address these challenges, we propose G-BERT, a new model to combine the power of Graph Neural Networks (GNNs) and BERT (Bidirectional Encoder Representations from Transformers) for medical code representation and medication recommendation. We use GNNs to represent the internal hierarchical structures of medical codes. Then we integrate the GNN representation into a transformer-based visit encoder and pre-train it on EHR data from patients only with a single visit. The pre-trained visit encoder and representation are then fine-tuned for downstream predictive tasks on longitudinal EHRs from patients with multiple visits. G-BERT is the first to bring the language model pre-training schema into the healthcare domain and it achieved state-of-the-art performance on the medication recommendation task.        △ Less","26 November, 2019","cs.AI,cs.CL,cs.LG",
              Assessing Algorithmic Fairness with Unobserved Protected Class Using Data Combination          ,1906.00285,https://arxiv.org/abs/1906.00285,https://arxiv.org/pdf/1906.00285,"Authors:NathanKallus,XiaojieMao,AngelaZhou","        The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, to construct models that predict the protected class from proxy variables, such as surname and geolocation. We show that even with such data, a variety of common disparity measures are generally unidentifiable, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the tightest-possible set of all possible true disparities that are consistent with the data (and possibly any assumptions). We further provide optimization-based algorithms for computing and visualizing these sets and statistical tools to assess sampling uncertainty. Together, these enable reliable and robust assessments of disparities -- an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.        △ Less","16 June, 2020","stat.ML,cs.LG,math.OC",
              Diversity-Inducing Policy Gradient: Using Maximum Mean Discrepancy to Find a Set of Diverse Policies          ,1906.00088,https://arxiv.org/abs/1906.00088,https://arxiv.org/pdf/1906.00088,"Authors:MuhammadA.Masood,FinaleDoshi-Velez","        Standard reinforcement learning methods aim to master one way of solving a task whereas there may exist multiple near-optimal policies. Being able to identify this collection of near-optimal policies can allow a domain expert to efficiently explore the space of reasonable solutions. Unfortunately, existing approaches that quantify uncertainty over policies are not ultimately relevant to finding policies with qualitatively distinct behaviors. In this work, we formalize the difference between policies as a difference between the distribution of trajectories induced by each policy, which encourages diversity with respect to both state visitation and action choices. We derive a gradient-based optimization technique that can be combined with existing policy gradient methods to now identify diverse collections of well-performing policies. We demonstrate our approach on benchmarks and a healthcare task.        △ Less","31 May, 2019","cs.LG,stat.ML",
              Ordinal Regression as Structured Classification          ,1905.13658,https://arxiv.org/abs/1905.13658,https://arxiv.org/pdf/1905.13658,"Authors:NiallTwomey,RafaelPoyiadzi,CallumMann,RaúlSantos-Rodríguez","        This paper extends the class of ordinal regression models with a structured interpretation of the problem by applying a novel treatment of encoded labels. The net effect of this is to transform the underlying problem from an ordinal regression task to a (structured) classification task which we solve with conditional random fields, thereby achieving a coherent and probabilistic model in which all model parameters are jointly learnt. Importantly, we show that although we have cast ordinal regression to classification, our method still fall within the class of decomposition methods in the ordinal regression ontology. This is an important link since our experience is that many applications of machine learning to healthcare ignores completely the important nature of the label ordering, and hence these approaches should considered naive in this ontology. We also show that our model is flexible both in how it adapts to data manifolds and in terms of the operations that are available for practitioner to execute. Our empirical evaluation demonstrates that the proposed approach overwhelmingly produces superior and often statistically significant results over baseline approaches on forty popular ordinal regression models, and demonstrate that the proposed model significantly out-performs baselines on synthetic and real datasets. Our implementation, together with scripts to reproduce the results of this work, will be available on a public GitHub repository.        △ Less","31 May, 2019","cs.LG,stat.ML",
              Understanding Perceptions and Attitudes in Breast Cancer Discussions on Twitter          ,1905.12469,https://arxiv.org/abs/1905.12469,https://arxiv.org/pdf/1905.12469,"Authors:FrancoisModave,YunpengZhao,JaniceKrieger,ZheHe,YiGuo,JinhaiHuo,MattiaProsperi,JiangBian","        Among American women, the rate of breast cancer is only second to lung cancer. An estimated 12.4% women will develop breast cancer over the course of their lifetime. The widespread use of social media across the socio-economic spectrum offers unparalleled ways to facilitate information sharing, in particular as it pertains to health. Social media is also used by many healthcare stakeholders, ranging from government agencies to healthcare industry, to disseminate health information and to engage patients. The purpose of this study is to investigate people's perceptions and attitudes relate to breast cancer, especially those that are related to physical activities, on Twitter. To achieve this, we first identified and collected tweets related to breast cancer; and then used topic modeling and sentiment analysis techniques to understanding discussion themes and quantify Twitter users' perceptions and emotions w.r.t breast cancer to answer 5 research questions.        △ Less","22 May, 2019","cs.CY,stat.ML",
              GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series          ,1905.12374,https://arxiv.org/abs/1905.12374,https://arxiv.org/pdf/1905.12374,"Authors:EdwardDeBrouwer,JaakSimm,AdamArany,YvesMoreau","        Modeling real-world multidimensional time series can be particularly challenging when these are sporadically observed (i.e., sampling is irregular both in time and across dimensions)-such as in the case of clinical patient data. To address these challenges, we propose (1) a continuous-time version of the Gated Recurrent Unit, building upon the recent Neural Ordinary Differential Equations (Chen et al., 2018), and (2) a Bayesian update network that processes the sporadic observations. We bring these two ideas together in our GRU-ODE-Bayes method. We then demonstrate that the proposed method encodes a continuity prior for the latent process and that it can exactly represent the Fokker-Planck dynamics of complex processes driven by a multidimensional stochastic differential equation. Additionally, empirical evaluation shows that our method outperforms the state of the art on both synthetic data and real-world data with applications in healthcare and climate forecast. What is more, the continuity prior is shown to be well suited for low number of samples settings.        △ Less","28 November, 2019","cs.LG,stat.ML",
              A Knowledge Graph-based Approach for Exploring the U.S. Opioid Epidemic          ,1905.11513,https://arxiv.org/abs/1905.11513,https://arxiv.org/pdf/1905.11513,"Authors:MaulikR.Kamdar,TymorHamamsy,SheaShelton,AyinVala,TomeEftimov,JamesZou,SuzanneTamang","        The United States is in the midst of an opioid epidemic with recent estimates indicating that more than 130 people die every day due to drug overdose. The over-prescription and addiction to opioid painkillers, heroin, and synthetic opioids, has led to a public health crisis and created a huge social and economic burden. Statistical learning methods that use data from multiple clinical centers across the US to detect opioid over-prescribing trends and predict possible opioid misuse are required. However, the semantic heterogeneity in the representation of clinical data across different centers makes the development and evaluation of such methods difficult and non-trivial. We create the Opioid Drug Knowledge Graph (ODKG) -- a network of opioid-related drugs, active ingredients, formulations, combinations, and brand names. We use the ODKG to normalize drug strings in a clinical data warehouse consisting of patient data from over 400 healthcare facilities in 42 different states. We showcase the use of ODKG to generate summary statistics of opioid prescription trends across US regions. These methods and resources can aid the development of advanced and scalable models to monitor the opioid epidemic and to detect illicit opioid misuse behavior. Our work is relevant to policymakers and pain researchers who wish to systematically assess factors that contribute to opioid over-prescribing and iatrogenic opioid addiction in the US.        △ Less","27 May, 2019","cs.CY,cs.AI",
"              Infusing domain knowledge in AI-based ""black box"" models for better explainability with application in bankruptcy prediction          ",1905.11474,https://arxiv.org/abs/1905.11474,https://arxiv.org/pdf/1905.11474,"Authors:SheikhRabiulIslam,WilliamEberle,SidBundy,SheikhKhaledGhafoor","        Although ""black box"" models such as Artificial Neural Networks, Support Vector Machines, and Ensemble Approaches continue to show superior performance in many disciplines, their adoption in the sensitive disciplines (e.g., finance, healthcare) is questionable due to the lack of interpretability and explainability of the model. In fact, future adoption of ""black box"" models is difficult because of the recent rule of ""right of explanation"" by the European Union where a user can ask for an explanation behind an algorithmic decision, and the newly proposed bill by the US government, the ""Algorithmic Accountability Act"", which would require companies to assess their machine learning systems for bias and discrimination and take corrective measures. Top Bankruptcy Prediction Models are A.I.-based and are in need of better explainability -the extent to which the internal working mechanisms of an AI system can be explained in human terms. Although explainable artificial intelligence is an emerging field of research, infusing domain knowledge for better explainability might be a possible solution. In this work, we demonstrate a way to collect and infuse domain knowledge into a ""black box"" model for bankruptcy prediction. Our understanding from the experiments reveals that infused domain knowledge makes the output from the black box model more interpretable and explainable.        △ Less","30 May, 2019","cs.AI,cs.LG",
              Evaluating Appointment Postponement in Scheduling Patients at a Diagnostic Clinic          ,1905.11201,https://arxiv.org/abs/1905.11201,https://arxiv.org/pdf/1905.11201,"Authors:MahsaKiani,BurakEksioglu,TugceIsik,AlexandriaThomas,JohnGilpin","        Diagnostic clinics are among healthcare facilities that suffer from long waiting times which can cause medical issues and lead to increases in patient no-shows. Reducing waiting times without significant capital investments is a challenging task. We tackle this challenge by proposing a new appointment scheduling policy for such clinics that does not require significant investments. The clinic in our study serves outpatients, inpatients, and emergency patients. Emergency patients must be seen on arrival, and inpatients must be given next day appointments. Outpatients, however, can be given later appointments. The proposed policy takes advantage of this by allowing the postponement of the acceptance of appointment requests from outpatients. The appointment scheduling process is modeled as a two-stage stochastic programming problem where a portion of the clinic capacity is allocated to inpatients and emergency patients in the first stage. In the second stage, outpatients are scheduled based on their priority classes. After a detailed analysis of the solutions obtained from the two-stage stochastic model, we develop a simple, non-anticipative policy for patient scheduling. We evaluate the performance of this proposed, easy-to-implement policy in a simulation study which shows significant improvements in outpatient indirect waiting times.        △ Less","16 April, 2020",math.OC,
              An Immersive Virtual Reality Serious Game to Enhance Earthquake Behavioral Responses and Post-earthquake Evacuation Preparedness in Buildings          ,1905.11082,https://arxiv.org/abs/1905.11082,https://arxiv.org/pdf/1905.11082,"Authors:ZhenanFeng,VicenteA.González,RobertAmor,MichaelSpearpoint,JaredThomas,RafaelSacks,RuggieroLovreglio,GuillermoCabrera-Guerrero","        Enhancing the earthquake behavioral responses and post-earthquake evacuation preparedness of building occupants is beneficial to increasing their chances of survival and reducing casualties after the main shock of an earthquake. Traditionally, training approaches such as seminars, posters, videos or drills are applied to enhance preparedness. However, they are not highly engaging and have limited sensory capabilities to mimic life-threatening scenarios for the purpose of training potential participants. Immersive Virtual Reality (IVR) and Serious Games (SG) as innovative digital technologies can be used to create training tools to overcome these limitations. In this study, we propose an IVR SG-based training system to improve earthquake behavioral responses and post-earthquake evacuation preparedness. Auckland City Hospital was chosen as a case study to test our IVR SG training system. A set of learning outcomes based on best evacuation practice has been identified and embedded into several training scenarios of the IVR SG. Hospital staff (healthcare and administrative professionals) and visitors were recruited as participants to be exposed to these training scenarios. Participants' preparedness has been measured along two dimensions: 1) Knowledge about best evacuation practice; 2) Self-efficacy in dealing with earthquake emergencies. Assessment results showed that there was a significant knowledge and self-efficacy increase after the training. And participants acknowledged that it was easy and engaging to learn best evacuation practice knowledge through the IVR SG training system.        △ Less","27 May, 2019",cs.CY,10.1016/j.aei.2020.101118 
              A hybrid model for predicting human physical activity status from lifelogging data          ,1905.10891,https://arxiv.org/abs/1905.10891,https://arxiv.org/pdf/1905.10891,"Authors:JiNi,BoweiChen,NigelM.Allinson,XujiongYe","        One trend in the recent healthcare transformations is people are encouraged to monitor and manage their health based on their daily diets and physical activity habits. However, much attention of the use of operational research and analytical models in healthcare has been paid to the systematic level such as country or regional policy making or organisational issues. This paper proposes a model concerned with healthcare analytics at the individual level, which can predict human physical activity status from sequential lifelogging data collected from wearable sensors. The model has a two-stage hybrid structure (in short, MOGP-HMM) -- a multi-objective genetic programming (MOGP) algorithm in the first stage to reduce the dimensions of lifelogging data and a hidden Markov model (HMM) in the second stage for activity status prediction over time. It can be used as a decision support tool to provide real-time monitoring, statistical analysis and personalized advice to individuals, encouraging positive attitudes towards healthy lifestyles. We validate the model with the real data collected from a group of participants in the UK, and compare it with other popular two-stage hybrid models. Our experimental results show that the MOGP-HMM can achieve comparable performance. To the best of our knowledge, this is the very first study that uses the MOGP in the hybrid two-stage structure for individuals' activity status prediction. It fits seamlessly with the current trend in the UK healthcare transformation of patient empowerment as well as contributing to a strategic development for more efficient and cost-effective provision of healthcare.        △ Less","12 December, 2019","cs.CY,cs.LG,cs.NE",10.1016/j.ejor.2019.05.035 
              Interpreting a Recurrent Neural Network's Predictions of ICU Mortality Risk          ,1905.09865,https://arxiv.org/abs/1905.09865,https://arxiv.org/pdf/1905.09865,"Authors:LongV.Ho,MelissaD.Aczon,DavidLedbetter,RandallWetzel","        Despite the success of deep learning models in healthcare, their lack of transparency has impeded their acceptance. The goals of this work were to highlight which features contributed to a recurrent neural network's (RNN) predictions of ICU mortality and compare this information with clinical expectations. Feature contributions to the RNN's predictions for individual patients were computed using two methods: Learned Binary Masks (LBM), a new occlusion-based method developed here; and KernelSHAP, an existing model-agnostic interpretability method. Both methods compute the contribution of each input feature to the RNN's prediction at each time, generating a matrix of the same dimensions as the patient's input data matrix. Feature contributions were extracted, analyzed, and presented here for two patients whose RNN predictions displayed similar trajectories but with different diagnoses. LBM and KernelSHAP showed that the RNN used input features that aligned with the clinical expectation of each patient's disease trajectories. In addition, feature contributions were averaged across different sub-populations to compare between any two cohorts the feature contributions to their mortality predictions. This measure, called Relative Attribution Feature (RAF), is similar to risk factors distilled from traditional clinical research. The top 10 RAFs were computed for two well-studied diseases, sepsis and brain neoplasm, and they aligned with clinical expectations of each disease. Finally, feature contributions were averaged across all patients and times to generate a form of model ""feature importance"" which describes the overall importance of each feature, analogous to analyzing the weights in logistic regressions.        △ Less","6 April, 2020","cs.LG,cs.AI,stat.ML",
              AmIE: An Ambient Intelligent Environment for Assisted Living          ,1905.08773,https://arxiv.org/abs/1905.08773,https://arxiv.org/pdf/1905.08773,"Authors:MarwaKandil,ReemAlBaghdadi,FatemahAlAttar,IssamDamaj","        In the modern world of technology Internet-of-things (IoT) systems strives to provide an extensive interconnected and automated solutions for almost every life aspect. This paper proposes an IoT context-aware system to present an Ambient Intelligence (AmI) environment; such as an apartment, house, or a building; to assist blind, visually-impaired, and elderly people. The proposed system aims at providing an easy-to-utilize voice-controlled system to locate, navigate and assist users indoors. The main purpose of the system is to provide indoor positioning, assisted navigation, outside weather information, room temperature, people availability, phone calls and emergency evacuation when needed. The system enhances the user's awareness of the surrounding environment by feeding them with relevant information through a wearable device to assist them. In addition, the system is voice-controlled in both English and Arabic languages and the information are displayed as audio messages in both languages. The system design, implementation, and evaluation consider the constraints in common types of premises in Kuwait and in challenges, such as the training needed by the users. This paper presents cost-effective implementation options by the adoption of a Raspberry Pi microcomputer, Bluetooth Low Energy devices and an Android smart watch.        △ Less","18 May, 2019","cs.CY,cs.HC,cs.NI",10.1109/ICASET.2019.8714499 
              Skin Cancer Recognition using Deep Residual Network          ,1905.08610,https://arxiv.org/abs/1905.08610,https://arxiv.org/pdf/1905.08610,"Authors:BrijRokad,Dr.SureshkumarNagarajan","        The advances in technology have enabled people to access internet from every part of the world. But to date, access to healthcare in remote areas is sparse. This proposed solution aims to bridge the gap between specialist doctors and patients. This prototype will be able to detect skin cancer from an image captured by the phone or any other camera. The network is deployed on cloud server-side processing for an even more accurate result. The Deep Residual learning model has been used for predicting the probability of cancer for server side The ResNet has three parametric layers. Each layer has Convolutional Neural Network, Batch Normalization, Maxpool and ReLU. Currently the model achieves an accuracy of 77% on the ISIC - 2017 challenge.        △ Less","14 May, 2019","cs.CV,eess.IV",
              Contrast Enhancement of Medical X-Ray Image Using Morphological Operators with Optimal Structuring Element          ,1905.08545,https://arxiv.org/abs/1905.08545,https://arxiv.org/pdf/1905.08545,"Authors:RafsanjanyKushol,Md.NishatRaihan,MdSirajusSalekin,A.B.M.AshikurRahman",        To guide surgical and medical treatment X-ray images have been used by physicians in every modern healthcare organization and hospitals. Doctor's evaluation process and disease identification in the area of skeletal system can be performed in a faster and efficient way with the help of X-ray imaging technique as they can depict bone structure painlessly. This paper presents an efficient contrast enhancement technique using morphological operators which will help to visualize important bone segments and soft tissues more clearly. Top-hat and Bottom-hat transform are utilized to enhance the image where gradient magnitude value is calculated for automatically selecting the structuring element (SE) size. Experimental evaluation on different x-ray imaging databases shows the effectiveness of our method which also produces comparatively better output against some existing image enhancement techniques.        △ Less,"21 May, 2019","cs.CV,eess.IV",
              Making ethical decisions for the immersive web          ,1905.06995,https://arxiv.org/abs/1905.06995,https://arxiv.org/pdf/1905.06995,Authors:DianeHosfelt,"        Mixed reality (MR) ethics occupies a space that intersects with web ethics, emerging tech ethics, healthcare ethics and product ethics (among others). This paper focuses on how we can build an immersive web that encourages ethical development and usage. The technology is beyond emerging (footnote: generally, the ethics of emerging technologies are focused on ethical assessments of research and innovation), but not quite entrenched. We're still in a position to intervene in the development process, instead of attempting to retrofit ethical decisions into an established design. While we have a wider range of data to analyze than most emerging technologies, we're still in a much more speculative state than entrenched technologies. This space is a challenge and an opportunity.        △ Less","14 May, 2019",cs.HC,
              Output-Constrained Bayesian Neural Networks          ,1905.06287,https://arxiv.org/abs/1905.06287,https://arxiv.org/pdf/1905.06287,"Authors:WanqianYang,LarsLorch,MoritzA.Graule,SrivatsanSrinivasan,AnirudhSuresh,JiayuYao,MelanieF.Pradier,FinaleDoshi-Velez","        Bayesian neural network (BNN) priors are defined in parameter space, making it hard to encode prior knowledge expressed in function space. We formulate a prior that incorporates functional constraints about what the output can or cannot be in regions of the input space. Output-Constrained BNNs (OC-BNN) represent an interpretable approach of enforcing a range of constraints, fully consistent with the Bayesian framework and amenable to black-box inference. We demonstrate how OC-BNNs improve model robustness and prevent the prediction of infeasible outputs in two real-world applications of healthcare and robotics.        △ Less","15 May, 2019","cs.LG,stat.ML",
              Consensus-based Interpretable Deep Neural Networks with Application to Mortality Prediction          ,1905.05849,https://arxiv.org/abs/1905.05849,https://arxiv.org/pdf/1905.05849,"Authors:ShaekeSalman,SeyedehNeelufarPayrovnaziri,XiuwenLiu,PabloRengifo-Moreno,ZheHe","        Deep neural networks have achieved remarkable success in various challenging tasks. However, the black-box nature of such networks is not acceptable to critical applications, such as healthcare. In particular, the existence of adversarial examples and their overgeneralization to irrelevant, out-of-distribution inputs with high confidence makes it difficult, if not impossible, to explain decisions by such networks. In this paper, we analyze the underlying mechanism of generalization of deep neural networks and propose an (nn, kk) consensus algorithm which is insensitive to adversarial examples and can reliably reject out-of-distribution samples. Furthermore, the consensus algorithm is able to improve classification accuracy by using multiple trained deep neural networks. To handle the complexity of deep neural networks, we cluster linear approximations of individual models and identify highly correlated clusters among different models to capture feature importance robustly, resulting in improved interpretability. Motivated by the importance of building accurate and interpretable prediction models for healthcare, our experimental results on an ICU dataset show the effectiveness of our algorithm in enhancing both the prediction accuracy and the interpretability of deep neural network models on one-year patient mortality prediction. In particular, while the proposed method maintains similar interpretability as conventional shallow models such as logistic regression, it improves the prediction accuracy significantly.        △ Less","11 September, 2019","cs.LG,cs.AI,cs.CV,cs.NE,stat.ML",
              Counterfactual Off-Policy Evaluation with Gumbel-Max Structural Causal Models          ,1905.05824,https://arxiv.org/abs/1905.05824,https://arxiv.org/pdf/1905.05824,"Authors:MichaelOberst,DavidSontag","        We introduce an off-policy evaluation procedure for highlighting episodes where applying a reinforcement learned (RL) policy is likely to have produced a substantially different outcome than the observed policy. In particular, we introduce a class of structural causal models (SCMs) for generating counterfactual trajectories in finite partially observable Markov Decision Processes (POMDPs). We see this as a useful procedure for off-policy ""debugging"" in high-risk settings (e.g., healthcare); by decomposing the expected difference in reward between the RL and observed policy into specific episodes, we can identify episodes where the counterfactual difference in reward is most dramatic. This in turn can be used to facilitate review of specific episodes by domain experts. We demonstrate the utility of this procedure with a synthetic environment of sepsis management.        △ Less","6 June, 2019","cs.LG,stat.ML",
              Modeling user context for valence prediction from narratives          ,1905.05701,https://arxiv.org/abs/1905.05701,https://arxiv.org/pdf/1905.05701,"Authors:AniruddhaTammewar,AlessandraCervone,Eva-MariaMessner,GiuseppeRiccardi","        Automated prediction of valence, one key feature of a person's emotional state, from individuals' personal narratives may provide crucial information for mental healthcare (e.g. early diagnosis of mental diseases, supervision of disease course, etc.). In the Interspeech 2018 ComParE Self-Assessed Affect challenge, the task of valence prediction was framed as a three-class classification problem using 8 seconds fragments from individuals' narratives. As such, the task did not allow for exploring contextual information of the narratives. In this work, we investigate the intrinsic information from multiple narratives recounted by the same individual in order to predict their current state-of-mind. Furthermore, with generalizability in mind, we decided to focus our experiments exclusively on textual information as the public availability of audio narratives is limited compared to text. Our hypothesis is, that context modeling might provide insights about emotion triggering concepts (e.g. events, people, places) mentioned in the narratives that are linked to an individual's state of mind. We explore multiple machine learning techniques to model narratives. We find that the models are able to capture inter-individual differences, leading to more accurate predictions of an individual's emotional state, as compared to single narratives.        △ Less","14 July, 2019","cs.CL,cs.AI,cs.LG,stat.ML",10.21437/Interspeech.2019-2489 
              Emotion recognition using a glasses-type wearable device via multi-channel facial responses          ,1905.05360,https://arxiv.org/abs/1905.05360,https://arxiv.org/pdf/1905.05360,"Authors:JanghoKwon,LaehyunKim","        We present a glasses type wearable device to detect emotions from a human face in an unobtrusive manner. The device is designed to gather multi channel responses from the user face naturally and continuously while the user is wearing it. The multi channel responses include physiological responses of the facial muscles and organs based on electrodermal activity (EDA) and photoplethysmogram. We conducted experiments to determine the optimal positions of EDA sensors on the wearable device because EDA signal quality is very sensitive to the sensing position. In addition to the physiological data, the device can capture the image region representing local facial expressions around the left eye via a built in camera. In this study, we developed and validated an algorithm to recognize emotions using multi channel responses obtained from the device. The results show that the emotion recognition algorithm using only local facial expressions has an accuracy of 78 percent at classifying emotions. Using multi channel data, this accuracy was increased by 10.1 percent. This unobtrusive wearable system with facial multi channel responses is very useful for monitoring a user emotions in daily life, which has a huge potential for use in the healthcare industry.        △ Less","13 May, 2019",cs.HC,
              Metareasoning in Modular Software Systems: On-the-Fly Configuration using Reinforcement Learning with Rich Contextual Representations          ,1905.05179,https://arxiv.org/abs/1905.05179,https://arxiv.org/pdf/1905.05179,"Authors:AdityaModi,DebadeeptaDey,AlekhAgarwal,AdithSwaminathan,BesmiraNushi,SeanAndrist,EricHorvitz","        Assemblies of modular subsystems are being pressed into service to perform sensing, reasoning, and decision making in high-stakes, time-critical tasks in such areas as transportation, healthcare, and industrial automation. We address the opportunity to maximize the utility of an overall computing system by employing reinforcement learning to guide the configuration of the set of interacting modules that comprise the system. The challenge of doing system-wide optimization is a combinatorial problem. Local attempts to boost the performance of a specific module by modifying its configuration often leads to losses in overall utility of the system's performance as the distribution of inputs to downstream modules changes drastically. We present metareasoning techniques which consider a rich representation of the input, monitor the state of the entire pipeline, and adjust the configuration of modules on-the-fly so as to maximize the utility of a system's operation. We show significant improvement in both real-world and synthetic pipelines across a variety of reinforcement learning techniques.        △ Less","12 May, 2019","cs.LG,cs.AI,cs.SE,stat.ML",
              What Clinicians Want: Contextualizing Explainable Machine Learning for Clinical End Use          ,1905.05134,https://arxiv.org/abs/1905.05134,https://arxiv.org/pdf/1905.05134,"Authors:SanaTonekaboni,ShalmaliJoshi,MelissaDMcCradden,AnnaGoldenberg","        Translating machine learning (ML) models effectively to clinical practice requires establishing clinicians' trust. Explainability, or the ability of an ML model to justify its outcomes and assist clinicians in rationalizing the model prediction, has been generally understood to be critical to establishing trust. However, the field suffers from the lack of concrete definitions for usable explanations in different settings. To identify specific aspects of explainability that may catalyze building trust in ML models, we surveyed clinicians from two distinct acute care specialties (Intenstive Care Unit and Emergency Department). We use their feedback to characterize when explainability helps to improve clinicians' trust in ML models. We further identify the classes of explanations that clinicians identified as most relevant and crucial for effective translation to clinical practice. Finally, we discern concrete metrics for rigorous evaluation of clinical explainability methods. By integrating perceptions of explainability between clinicians and ML researchers we hope to facilitate the endorsement and broader adoption and sustained use of ML systems in healthcare.        △ Less","7 August, 2019","cs.LG,stat.ML",
              Tracking Human Behavioural Consistency by Analysing Periodicity of Household Water Consumption          ,1905.05025,https://arxiv.org/abs/1905.05025,https://arxiv.org/pdf/1905.05025,"Authors:SeánQuinn,NoelMurphy,AlanF.Smeaton","        People are living longer than ever due to advances in healthcare, and this has prompted many healthcare providers to look towards remote patient care as a means to meet the needs of the future. It is now a priority to enable people to reside in their own homes rather than in overburdened facilities whenever possible. The increasing maturity of IoT technologies and the falling costs of connected sensors has made the deployment of remote healthcare at scale an increasingly attractive prospect. In this work we demonstrate that we can measure the consistency and regularity of the behaviour of a household using sensor readings generated from interaction with the home environment. We show that we can track changes in this behaviour regularity longitudinally and detect changes that may be related to significant life events or trends that may be medically significant. We achieve this using periodicity analysis on water usage readings sampled from the main household water meter every 15 minutes for over 8 months. We utilise an IoT Application Enablement Platform in conjunction with low cost LoRa-enabled sensors and a Low Power Wide Area Network in order to validate a data collection methodology that could be deployed at large scale in future. We envision the statistical methods described here being applied to data streams from the homes of elderly and at-risk groups, both as a means of early illness detection and for monitoring the well-being of those with known illnesses.        △ Less","16 October, 2019","eess.SP,cs.CY",10.1145/3365245.3365246 
              Simulation Based Formal Verification of Cyber-Physical Systems          ,1905.04780,https://arxiv.org/abs/1905.04780,https://arxiv.org/pdf/1905.04780,Authors:MassimoNazaria,"        Cyber-Physical Systems (CPSs) have become an intrinsic part of the 21st century world. Systems like Smart Grids, Transportation, and Healthcare help us run our lives and businesses smoothly, successfully and safely. Since malfunctions in these CPSs can have serious, expensive, sometimes fatal consequences, System-Level Formal Verification (SLFV) tools are vital to minimise the likelihood of errors occurring during the development process and beyond. Their applicability is supported by the increasingly widespread use of Model Based Design (MBD) tools. MBD enables the simulation of CPS models in order to check for their correct behaviour from the very initial design phase. The disadvantage is that SLFV for complex CPSs is an extremely time-consuming process, which typically requires several months of simulation. Current SLFV tools are aimed at accelerating the verification process with multiple simulators that work simultaneously. To this end, they compute all the scenarios in advance in such a way as to split and simulate them in parallel. Furthermore, they compute optimised simulation campaigns in order to simulate common prefixes of these scenarios only once, thus avoiding redundant simulation. Nevertheless, there are still limitations that prevent a more widespread adoption of SLFV tools. Firstly, current tools cannot optimise simulation campaigns from existing datasets with collected scenarios. Secondly, there are currently no methods to predict the time required to complete the SLFV process. This lack of ability to predict the length of the process makes scheduling verification activities highly problematic. In this thesis, we present how we are able to overcome these limitations with the use of a data-intensive simulation campaign optimiser and an accurate machine-independent execution time estimator.        △ Less","14 May, 2019",cs.SE,
"              Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges          ",1905.04223,https://arxiv.org/abs/1905.04223,https://arxiv.org/pdf/1905.04223,"Authors:RobAshmore,RaduCalinescu,ColinPaterson","        Machine learning has evolved into an enabling technology for a wide range of highly successful applications. The potential for this success to continue and accelerate has placed machine learning (ML) at the top of research, economic and political agendas. Such unprecedented interest is fuelled by a vision of ML applicability extending to healthcare, transportation, defence and other domains of great societal importance. Achieving this vision requires the use of ML in safety-critical applications that demand levels of assurance beyond those needed for current ML applications. Our paper provides a comprehensive survey of the state-of-the-art in the assurance of ML, i.e. in the generation of evidence that ML is sufficiently safe for its intended use. The survey covers the methods capable of providing such evidence at different stages of the machine learning lifecycle, i.e. of the complex, iterative process that starts with the collection of the data used to train an ML component for a system, and ends with the deployment of that component within the system. The paper begins with a systematic presentation of the ML lifecycle and its stages. We then define assurance desiderata for each stage, review existing methods that contribute to achieving these desiderata, and identify open challenges that require further research.        △ Less","10 May, 2019","cs.LG,cs.SE,stat.ML",
              Appointment scheduling model in healthcare using clustering algorithms          ,1905.03083,https://arxiv.org/abs/1905.03083,https://arxiv.org/pdf/1905.03083,"Authors:NooshinYousefi,FarhadHasankhani,MahsaKiani","        In this study we provided a scheduling procedure which is combination of machine learning and mathematical programming. Outpatients who request for appointment in healthcare facilities have different priorities. Determining the priority of outpatients and allocating the capacity based on the priority classes are important concepts that have to be considered in scheduling of outpatients. Two stages are defined for scheduling an incoming patient. In the first stage, We applied and compared different clustering methods such as k-mean clustering and agglomerative hierarchical clustering methods to classify outpatients into priority classes and suggested the best pattern to cluster the outpatients. In the second stage, we modeled the scheduling problem as a Markov Decision Process (MDP) problem that aims to decrease waiting time of higher priority outpatients. Due to the curse of dimensionality, we used fluid approximation method to estimate the optimal solution of the MDP. We applied our methodology on a dataset of Shaheed Rajaei Medical and Research Center in Iran, and we showed how our models work in prioritizing and scheduling of outpatients.        △ Less","31 July, 2020","eess.SP,math.OC",
              Generalized formal model of big data          ,1905.03061,https://arxiv.org/abs/1905.03061,https://arxiv.org/pdf/1905.03061,"Authors:ShakhovskaNataliya,VeresOleh,HirnyakMariia","        This article dwells on the basic characteristic features of the Big Data technologies. It is analyzed the existing definition of the ""big data"" term. The article proposes and describes the elements of the generalized formal model of big data. It is analyzed the peculiarities of the application of the proposed model components. It described the fundamental differences between Big Data technology and business analytics. Big Data is supported by the distributed file system Google File System technology, Cassandra, HBase, Lustre and ZFS, by the MapReduce and Hadoop programming constructs and many other solutions. According to the experts, such as McKinsey Institute, the manufacturing, healthcare, trade, administration and control of individual movements undergo the transformations under the influence of the Big Data.        △ Less","3 May, 2019","cs.DB,cs.DC",
              Semi-Supervised Speech Emotion Recognition with Ladder Networks          ,1905.02921,https://arxiv.org/abs/1905.02921,https://arxiv.org/pdf/1905.02921,"Authors:SrinivasParthasarathy,CarlosBusso","        Speech emotion recognition (SER) systems find applications in various fields such as healthcare, education, and security and defense. A major drawback of these systems is their lack of generalization across different conditions. This problem can be solved by training models on large amounts of labeled data from the target domain, which is expensive and time-consuming. Another approach is to increase the generalization of the models. An effective way to achieve this goal is by regularizing the models through multitask learning (MTL), where auxiliary tasks are learned along with the primary task. These methods often require the use of labeled data which is computationally expensive to collect for emotion recognition (gender, speaker identity, age or other emotional descriptors). This study proposes the use of ladder networks for emotion recognition, which utilizes an unsupervised auxiliary task. The primary task is a regression problem to predict emotional attributes. The auxiliary task is the reconstruction of intermediate feature representations using a denoising autoencoder. This auxiliary task does not require labels so it is possible to train the framework in a semi-supervised fashion with abundant unlabeled data from the target domain. This study shows that the proposed approach creates a powerful framework for SER, achieving superior performance than fully supervised single-task learning (STL) and MTL baselines. The approach is implemented with several acoustic features, showing that ladder networks generalize significantly better in cross-corpus settings. Compared to the STL baselines, the proposed approach achieves relative gains in concordance correlation coefficient (CCC) between 3.0% and 3.5% for within corpus evaluations, and between 16.1% and 74.1% for cross corpus evaluations, highlighting the power of the architecture.        △ Less","8 May, 2019",eess.AS,
              Non-invasive Blood Pressure Estimation Using Phonocardiogram          ,1905.02312,https://arxiv.org/abs/1905.02312,https://arxiv.org/pdf/1905.02312,"Authors:AmirhosseinEsmaili,MohammadKachuee,MahdiShabany","        This paper presents a novel approach based on pulse transit time (PTT) for the estimation of blood pressure (BP). In order to achieve this goal, a data acquisition hardware is designed for high-resolution sampling of phonocardiogram (PCG) and photoplethysmogram (PPG). These two signals can derive PTT values. Meanwhile, a force-sensing resistor (FSR) is placed under the cuff of the BP reference device to mark the moments of measurements accurately via recording instantaneous cuff pressure. For deriving the PTT-BP models, a calibration procedure including a supervised physical exercise is conducted for each individual. The proposed method is evaluated on 24 subjects. The final results prove that using PCG for PTT measurement alongside the proposed models, the BP can be estimated reliably. Since the use of PCG requires a minimal low-cost hardware, the proposed method enables ubiquitous BP estimation in portable healthcare devices.        △ Less","6 May, 2019",eess.SP,10.1109/ISCAS.2017.8050240 
              BlockLite: A Lightweight Emulator for Public Blockchains          ,1905.02157,https://arxiv.org/abs/1905.02157,https://arxiv.org/pdf/1905.02157,"Authors:XinyingWang,AbdullahAl-Mamun,FengYan,MohammadSadoghi,DongfangZhao","        Blockchain is an enabler of many emerging decentralized applications in areas of cryptocurrency, Internet of Things, smart healthcare, among many others. Although various open-source blockchain frameworks are available, the infrastructure is complex enough and difficult for many users to modify or test out new research ideas. To make it worse, many advantages of blockchain systems can be demonstrated only at large scales, e.g., thousands of nodes, which are not always available to researchers. This demo paper presents a lightweight single-node emulator of blockchain systems, namely \mbox{BlockLite}, designed to be executing real proof-of-work workload along with peer-to-peer network communications and hash-based immutability. BlockLite employs a preprocessing approach to avoid the per-node computation overhead at runtime and thus scales to thousands of nodes. Moreover, BlockLite offers an easy-to-use programming interface allowing for a Lego-like customization to the system, e.g. new ad-hoc consensus protocols.        △ Less","6 May, 2019",cs.DB,
              Text2Node: a Cross-Domain System for Mapping Arbitrary Phrases to a Taxonomy          ,1905.01958,https://arxiv.org/abs/1905.01958,https://arxiv.org/pdf/1905.01958,"Authors:RohollahSoltani,AlexandreTomberg","        Electronic health record (EHR) systems are used extensively throughout the healthcare domain. However, data interchangeability between EHR systems is limited due to the use of different coding standards across systems. Existing methods of mapping coding standards based on manual human experts mapping, dictionary mapping, symbolic NLP and classification are unscalable and cannot accommodate large scale EHR datasets.  In this work, we present Text2Node, a cross-domain mapping system capable of mapping medical phrases to concepts in a large taxonomy (such as SNOMED CT). The system is designed to generalize from a limited set of training samples and map phrases to elements of the taxonomy that are not covered by training data. As a result, our system is scalable, robust to wording variants between coding systems and can output highly relevant concepts when no exact concept exists in the target taxonomy. Text2Node operates in three main stages: first, the lexicon is mapped to word embeddings; second, the taxonomy is vectorized using node embeddings; and finally, the mapping function is trained to connect the two embedding spaces. We compared multiple algorithms and architectures for each stage of the training, including GloVe and FastText word embeddings, CNN and Bi-LSTM mapping functions, and node2vec for node embeddings. We confirmed the robustness and generalisation properties of Text2Node by mapping ICD-9-CM Diagnosis phrases to SNOMED CT and by zero-shot training at comparable accuracy.  This system is a novel methodological contribution to the task of normalizing and linking phrases to a taxonomy, advancing data interchangeability in healthcare. When applied, the system can use electronic health records to generate an embedding that incorporates taxonomical medical knowledge to improve clinical predictive models.        △ Less","11 April, 2019","cs.CL,cs.LG,stat.ML",
              Tight Regret Bounds for Infinite-armed Linear Contextual Bandits          ,1905.01435,https://arxiv.org/abs/1905.01435,https://arxiv.org/pdf/1905.01435,"Authors:YingkaiLi,YiningWang,YuanZhou","        Linear contextual bandit is a class of sequential decision making problems with important applications in recommendation systems, online advertising, healthcare, and other machine learning related tasks. While there is much prior research, tight regret bounds of linear contextual bandit with infinite action sets remain open. In this paper, we prove regret upper bound of O(d2TlogT−−−−−−−−√)×poly(loglogT)O(\sqrt{d^2T\log T})\times \mathrm{poly}(\log\log T) where dd is the domain dimension and TT is the time horizon. Our upper bound matches the previous lower bound of Ω(d2TlogT−−−−−−−−√)Ω(\sqrt{d^2 T\log T}) up to iterated logarithmic terms.        △ Less","4 May, 2019","stat.ML,cs.LG",
              A Federated Filtering Framework for Internet of Medical Things          ,1905.01138,https://arxiv.org/abs/1905.01138,https://arxiv.org/pdf/1905.01138,"Authors:SunnySanyal,DapengWu,BoubakrNour","        Based on the dominant paradigm, all the wearable IoT devices used in the healthcare sector also known as the internet of medical things (IoMT) are resource constrained in power and computational capabilities. The IoMT devices are continuously pushing their readings to the remote cloud servers for real-time data analytics, that causes faster drainage of the device battery. Moreover, other demerits of continuous centralizing of data include exposed privacy and high latency. This paper presents a novel Federated Filtering Framework for IoMT devices which is based on the prediction of data at the central fog server using shared models provided by the local IoMT devices. The fog server performs model averaging to predict the aggregated data matrix and also computes filter parameters for local IoMT devices. Two significant theoretical contributions of this paper are the global tolerable perturbation error (TolF{To{l_F}}) and the local filtering parameter (δδ); where the former controls the decision-making accuracy due to eigenvalue perturbation and the later balances the tradeoff between the communication overhead and perturbation error of the aggregated data matrix (predicted matrix) at the fog server. Experimental evaluation based on real healthcare data demonstrates that the proposed scheme saves upto 95\% of the communication cost while maintaining reasonable data privacy and low latency.        △ Less","16 April, 2019",cs.NI,
              HADES-IoT: A Practical Host-Based Anomaly Detection System for IoT Devices (Extended Version)          ,1905.01027,https://arxiv.org/abs/1905.01027,https://arxiv.org/pdf/1905.01027,"Authors:DominikBreitenbacher,IvanHomoliak,YanLinAung,NilsOleTippenhauer,YuvalElovici","        Internet of Things (IoT) devices have become ubiquitous and are spread across many application domains including the industry, transportation, healthcare, and households. However, the proliferation of the IoT devices has raised the concerns about their security, especially when observing that many manufacturers focus only on the core functionality of their products due to short time to market and low-cost pressures, while neglecting security aspects. Moreover, it does not exist any established or standardized method for measuring and ensuring the security of IoT devices. Consequently, vulnerabilities are left untreated, allowing attackers to exploit IoT devices for various purposes, such as compromising privacy, recruiting devices into a botnet, or misusing devices to perform cryptocurrency mining.  In this paper, we present a practical Host-based Anomaly DEtection System for IoT (HADES-IoT) that represents the last line of defense. HADES-IoT has proactive detection capabilities, provides tamper-proof resistance, and it can be deployed on a wide range of Linux-based IoT devices. The main advantage of HADES-IoT is its low performance overhead, which makes it suitable for the IoT domain, where state-of-the-art approaches cannot be applied due to their high-performance demands. We deployed HADES-IoT on seven IoT devices to evaluate its effectiveness and performance overhead. Our experiments show that HADES-IoT achieved 100% effectiveness in the detection of current IoT malware such as VPNFilter and IoTReaper; while on average, requiring only 5.5% of available memory and causing only a low CPU load.        △ Less","2 May, 2019",cs.CR,
"              A Framework for Predicting Impactability of Healthcare Interventions Using Machine Learning Methods, Administrative Claims, Sociodemographic and App Generated Data          ",1905.00751,https://arxiv.org/abs/1905.00751,https://arxiv.org/pdf/1905.00751,"Authors:HeatherMattie,PatrickReidy,PatrikBachtiger,EmilyLindemer,MohammadJouni,TrishanPanch","        It is not clear how to target patients who are most likely to benefit from digital care management programs ex-ante, a shortcoming of current risk score based approaches. This study focuses on defining impactability by identifying those patients most likely to benefit from technology enabled care management, delivered through a digital health platform, including a mobile app and clinician web dashboard. Anonymized insurance claims data were used from a commercially insured population across several U.S. states and combined with inferred sociodemographic data and data derived from the patient-held mobile application itself. Our approach involves the creation of two models and the comparative analysis of the methodologies and performances therein. We first train a cost prediction model to calculate the differences in predicted (without intervention) versus actual (with onboarding onto digital health platform) healthcare expenditure for patients (N = 1,242). This enables the classification of impactability if differences in predicted versus actual costs meet a predetermined threshold. A random forest machine learning model was then trained to accurately categorize new patients as impactable versus not impactable, reaching an overall accuracy of 71.9%. We then modify these parameters through grid search to define the parameters that deliver optimal performance. A roadmap is proposed to iteratively improve the performance of the model. As the number of newly onboarded patients and length of use continues to increase, the accuracy of predicting impactability will improve commensurately as more advanced machine learning techniques such as deep learning become relevant. This approach is generalizable to analyzing the impactability of any intervention and is a key component of realising closed loop feedback systems for continuous improvement in healthcare.        △ Less","15 May, 2019","q-bio.QM,cs.LG,stat.ML",
              Data Analytics in Operations Management: A Review          ,1905.00556,https://arxiv.org/abs/1905.00556,https://arxiv.org/pdf/1905.00556,"Authors:VeliborV.Mišić,GeorgiaPerakis","        Research in operations management has traditionally focused on models for understanding, mostly at a strategic level, how firms should operate. Spurred by the growing availability of data and recent advances in machine learning and optimization methodologies, there has been an increasing application of data analytics to problems in operations management. In this paper, we review recent applications of data analytics to operations management, in three major areas -- supply chain management, revenue management and healthcare operations -- and highlight some exciting directions for the future.        △ Less","1 May, 2019",econ.GN,
              Multi-resolution Networks For Flexible Irregular Time Series Modeling (Multi-FIT)          ,1905.00125,https://arxiv.org/abs/1905.00125,https://arxiv.org/pdf/1905.00125,"Authors:BhanuPratapSingh,ImanDeznabi,BharathNarasimhan,BryonKucharski,RheeyaUppaal,AkhilaJosyula,MadalinaFiterau","        Missing values, irregularly collected samples, and multi-resolution signals commonly occur in multivariate time series data, making predictive tasks difficult. These challenges are especially prevalent in the healthcare domain, where patients' vital signs and electronic records are collected at different frequencies and have occasionally missing information due to the imperfections in equipment or patient circumstances. Researchers have handled each of these issues differently, often handling missing data through mean value imputation and then using sequence models over the multivariate signals while ignoring the different resolution of signals. We propose a unified model named Multi-resolution Flexible Irregular Time series Network (Multi-FIT). The building block for Multi-FIT is the FIT network. The FIT network creates an informative dense representation at each time step using signal information such as last observed value, time difference since the last observed time stamp and overall mean for the signal. Vertical FIT (FIT-V) is a variant of FIT which also models the relationship between different temporal signals while creating the informative dense representations for the signal. The multi-FIT model uses multiple FIT networks for sets of signals with different resolutions, further facilitating the construction of flexible representations. Our model has three main contributions: a.) it does not impute values but rather creates informative representations to provide flexibility to the model for creating task-specific representations b.) it models the relationship between different signals in the form of support signals c.) it models different resolutions in parallel before merging them for the final prediction task. The FIT, FIT-V and Multi-FIT networks improve upon the state-of-the-art models for three predictive tasks, including the forecasting of patient survival.        △ Less","30 April, 2019","cs.LG,eess.SP,stat.ML",
              CT-To-MR Conditional Generative Adversarial Networks for Ischemic Stroke Lesion Segmentation          ,1904.13281,https://arxiv.org/abs/1904.13281,https://arxiv.org/pdf/1904.13281,"Authors:JonathanRubin,S.MazdakAbulnaga","        Infarcted brain tissue resulting from acute stroke readily shows up as hyperintense regions within diffusion-weighted magnetic resonance imaging (DWI). It has also been proposed that computed tomography perfusion (CTP) could alternatively be used to triage stroke patients, given improvements in speed and availability, as well as reduced cost. However, CTP has a lower signal to noise ratio compared to MR. In this work, we investigate whether a conditional mapping can be learned by a generative adversarial network to map CTP inputs to generated MR DWI that more clearly delineates hyperintense regions due to ischemic stroke. We detail the architectures of the generator and discriminator and describe the training process used to perform image-to-image translation from multi-modal CT perfusion maps to diffusion weighted MR outputs. We evaluate the results both qualitatively by visual comparison of generated MR to ground truth, as well as quantitatively by training fully convolutional neural networks that make use of generated MR data inputs to perform ischemic stroke lesion segmentation. Segmentation networks trained using generated CT-to-MR inputs result in at least some improvement on all metrics used for evaluation, compared with networks that only use CT perfusion input.        △ Less","30 April, 2019","eess.IV,cs.CV,cs.LG",
              A self-attention based deep learning method for lesion attribute detection from CT reports          ,1904.13018,https://arxiv.org/abs/1904.13018,https://arxiv.org/pdf/1904.13018,"Authors:YifanPeng,KeYan,VeitSandfort,RonaldM.Summers,ZhiyongLu","        In radiology, radiologists not only detect lesions from the medical image, but also describe them with various attributes such as their type, location, size, shape, and intensity. While these lesion attributes are rich and useful in many downstream clinical applications, how to extract them from the radiology reports is less studied. This paper outlines a novel deep learning method to automatically extract attributes of lesions of interest from the clinical text. Different from classical CNN models, we integrated the multi-head self-attention mechanism to handle the long-distance information in the sentence, and to jointly correlate different portions of sentence representation subspaces in parallel. Evaluation on an in-house corpus demonstrates that our method can achieve high performance with 0.848 in precision, 0.788 in recall, and 0.815 in F-score. The new method and constructed corpus will enable us to build automatic systems with a higher-level understanding of the radiological world.        △ Less","29 April, 2019",cs.CL,
              A Comparison of Online Automatic Speech Recognition Systems and the Nonverbal Responses to Unintelligible Speech          ,1904.12403,https://arxiv.org/abs/1904.12403,https://arxiv.org/pdf/1904.12403,"Authors:JoshuaY.Kim,ChunfengLiu,RafaelA.Calvo,KathrynMcCabe,SilasC.R.Taylor,BjörnW.Schuller,KaihangWu","        Automatic Speech Recognition (ASR) systems have proliferated over the recent years to the point that free platforms such as YouTube now provide speech recognition services. Given the wide selection of ASR systems, we contribute to the field of automatic speech recognition by comparing the relative performance of two sets of manual transcriptions and five sets of automatic transcriptions (Google Cloud, IBM Watson, Microsoft Azure, Trint, and YouTube) to help researchers to select accurate transcription services. In addition, we identify nonverbal behaviors that are associated with unintelligible speech, as indicated by high word error rates. We show that manual transcriptions remain superior to current automatic transcriptions. Amongst the automatic transcription services, YouTube offers the most accurate transcription service. For non-verbal behavioral involvement, we provide evidence that the variability of smile intensities from the listener is high (low) when the speaker is clear (unintelligible). These findings are derived from videoconferencing interactions between student doctors and simulated patients; therefore, we contribute towards both the ASR literature and the healthcare communication skills teaching community.        △ Less","28 April, 2019","cs.SD,eess.AS",
              X-Ray Image Compression Using Convolutional Recurrent Neural Networks          ,1904.12271,https://arxiv.org/abs/1904.12271,https://arxiv.org/pdf/1904.12271,"Authors:AsifShahriyarSushmit,ShakibUzZaman,AhmedImtiazHumayun,TaufiqHasan,MohammedImamulHassanBhuiyan","        In the advent of a digital health revolution, vast amounts of clinical data are being generated, stored and processed on a daily basis. This has made the storage and retrieval of large volumes of health-care data, especially, high-resolution medical images, particularly challenging. Effective image compression for medical images thus plays a vital role in today's healthcare information system, particularly in teleradiology. In this work, an X-ray image compression method based on a Convolutional Recurrent Neural Networks RNN-Conv is presented. The proposed architecture can provide variable compression rates during deployment while it requires each network to be trained only once for a specific dimension of X-ray images. The model uses a multi-level pooling scheme that learns contextualized features for effective compression. We perform our image compression experiments on the National Institute of Health (NIH) ChestX-ray8 dataset and compare the performance of the proposed architecture with a state-of-the-art RNN based technique and JPEG 2000. The experimental results depict improved compression performance achieved by the proposed method in terms of Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR) metrics. To the best of our knowledge, this is the first reported evaluation on using a deep convolutional RNN for medical image compression.        △ Less","9 May, 2019","cs.CV,eess.IV",
              Temporal-Clustering Invariance in Irregular Healthcare Time Series          ,1904.12206,https://arxiv.org/abs/1904.12206,https://arxiv.org/pdf/1904.12206,"Authors:MohammadTahaBahadori,ZacharyChaseLipton","        Electronic records contain sequences of events, some of which take place all at once in a single visit, and others that are dispersed over multiple visits, each with a different timestamp. We postulate that fine temporal detail, e.g., whether a series of blood tests are completed at once or in rapid succession should not alter predictions based on this data. Motivated by this intuition, we propose models for analyzing sequences of multivariate clinical time series data that are invariant to this temporal clustering. We propose an efficient data augmentation technique that exploits the postulated temporal-clustering invariance to regularize deep neural networks optimized for several clinical prediction tasks. We introduce two techniques to temporally coarsen (downsample) irregular time series: (i) grouping the data points based on regularly-spaced timestamps; and (ii) clustering them, yielding irregularly-paced timestamps. Moreover, we propose a MultiResolution Ensemble (MRE) model, improving predictive accuracy by ensembling predictions based on inputs sequences transformed by different coarsening operators. Our experiments show that MRE improves the mAP on the benchmark mortality prediction task from 51.53% to 53.92%.        △ Less","27 April, 2019","cs.LG,q-bio.QM,stat.ML",
              CoachAI: A Conversational Agent Assisted Health Coaching Platform          ,1904.11961,https://arxiv.org/abs/1904.11961,https://arxiv.org/pdf/1904.11961,"Authors:AhmedFadhil,GianlucaSchiavo,YunlongWang","        Poor lifestyle represents a health risk factor and is the leading cause of morbidity and chronic conditions. The impact of poor lifestyle can be significantly altered by individual behavior change. Although the current shift in healthcare towards a long lasting modifiable behavior, however, with increasing caregiver workload and individuals' continuous needs of care, there is a need to ease caregiver's work while ensuring continuous interaction with users. This paper describes the design and validation of CoachAI, a conversational agent assisted health coaching system to support health intervention delivery to individuals and groups. CoachAI instantiates a text based healthcare chatbot system that bridges the remote human coach and the users. This research provides three main contributions to the preventive healthcare and healthy lifestyle promotion: (1) it presents the conversational agent to aid the caregiver; (2) it aims to decrease caregiver's workload and enhance care given to users, by handling (automating) repetitive caregiver tasks; and (3) it presents a domain independent mobile health conversational agent for health intervention delivery. We will discuss our approach and analyze the results of a one month validation study on physical activity, healthy diet and stress management.        △ Less","26 April, 2019","cs.AI,cs.CL,cs.HC",
              A Game Theoretic Setting of Capitation Versus Fee-For-Service Payment Systems          ,1904.11604,https://arxiv.org/abs/1904.11604,https://arxiv.org/pdf/1904.11604,Authors:AllisonKoenecke,"        We aim to determine whether a game-theoretic model between an insurer and a healthcare practice yields a predictive equilibrium that incentivizes either player to deviate from a fee-for-service to capitation payment system. Using United States data from various primary care surveys, we find that non-extreme equilibria (i.e., shares of patients, or shares of patient visits, seen under a fee-for-service payment system) can be derived from a Stackelberg game if insurers award a non-linear bonus to practices based on performance. Overall, both insurers and practices can be incentivized to embrace capitation payments somewhat, but potentially at the expense of practice performance.        △ Less","30 September, 2019",econ.GN,10.1371/journal.pone.0223672 
              Who Gets the Job and How are They Paid? Machine Learning Application on H-1B Case Data          ,1904.10580,https://arxiv.org/abs/1904.10580,https://arxiv.org/pdf/1904.10580,"Authors:BarryKe,AngelaQiao","        In this paper, we use machine learning techniques to explore the H-1B application dataset disclosed by the Department of Labor (DOL), from 2008 to 2018, in order to provide more stylized facts of the international workers in US labor market. We train a LASSO Regression model to analyze the impact of different features on the applicant's wage, and a Logistic Regression with L1-Penalty as a classifier to study the feature's impact on the likelihood of the case being certified. Our analysis shows that working in the healthcare industry, working in California, higher job level contribute to higher salaries. In the meantime, lower job level, working in the education services industry and nationality of Philippines are negatively correlated with the salaries. In terms of application status, a Ph.D. degree, working in retail or finance, majoring in computer science will give the applicants a better chance of being certified. Applicants with no or an associate degree, working in the education services industry, or majoring in education are more likely to be rejected.        △ Less","23 April, 2019",stat.AP,
              Fine-Grained Named Entity Recognition using ELMo and Wikidata          ,1904.10503,https://arxiv.org/abs/1904.10503,https://arxiv.org/pdf/1904.10503,"Authors:CihanDogan,AimoreDutra,AdamGara,AlfredoGemma,LeiShi,MichaelSigamani,EllaWalters","        Fine-grained Named Entity Recognition is a task whereby we detect and classify entity mentions to a large set of types. These types can span diverse domains such as finance, healthcare, and politics. We observe that when the type set spans several domains the accuracy of the entity detection becomes a limitation for supervised learning models. The primary reason being the lack of datasets where entity boundaries are properly annotated, whilst covering a large spectrum of entity types. Furthermore, many named entity systems suffer when considering the categorization of fine grained entity types. Our work attempts to address these issues, in part, by combining state-of-the-art deep learning models (ELMo) with an expansive knowledge base (Wikidata). Using our framework, we cross-validate our model on the 112 fine-grained entity types based on the hierarchy given from the Wiki(gold) dataset.        △ Less","23 April, 2019","cs.IR,cs.AI,cs.CL",
              A Survey on Practical Applications of Multi-Armed and Contextual Bandits          ,1904.10040,https://arxiv.org/abs/1904.10040,https://arxiv.org/pdf/1904.10040,"Authors:DjallelBouneffouf,IrinaRish","        In recent years, multi-armed bandit (MAB) framework has attracted a lot of attention in various applications, from recommender systems and information retrieval to healthcare and finance, due to its stellar performance combined with certain attractive properties, such as learning from less feedback. The multi-armed bandit field is currently flourishing, as novel problem settings and algorithms motivated by various practical applications are being introduced, building on top of the classical bandit problem. This article aims to provide a comprehensive review of top recent developments in multiple real-life applications of the multi-armed bandit. Specifically, we introduce a taxonomy of common MAB-based applications and summarize state-of-art for each of those domains. Furthermore, we identify important current trends and provide new perspectives pertaining to the future of this exciting and fast-growing field.        △ Less","2 April, 2019","cs.LG,stat.ML",
              Health Behaviour Change Techniques in Diabetes Management Applications: A Systematic Review          ,1904.09884,https://arxiv.org/abs/1904.09884,https://arxiv.org/pdf/1904.09884,"Authors:AhmedFadhil,YunlongWang","        The rapid growth in mobile healthcare technology could significantly help control chronic diseases, such as diabetes. This paper presents a systematic review to characterise type 1 & type 2 diabetes management applications available in Apple's iTunes store. We investigated ""Health & Fitness"" and ""Medical"" apps following a two-step filtering process (Selection and Analysis phases). We firstly investigated the apps compliance to the persuasive system design (PSD) model. We then characterised the behaviour change techniques (BCTs) of top-ranked apps for diabetes management. Finally, we checked the apps regarding the stages of disease continuum. The findings revealed apps incorporation some PSD principles based on their configuration and behaviour change techniques. Most apps miss the element of BCT and focus on measuring exercise and caloric intake. Few apps consider managing specific diabetes type, which raises doubts about the effectiveness of those apps in providing sustainable diabetes management. Moreover, people may need multiple apps to initiate and maintain a healthy behaviour.        △ Less","22 April, 2019","cs.CY,cs.HC",
              You2Me: Inferring Body Pose in Egocentric Video via First and Second Person Interactions          ,1904.09882,https://arxiv.org/abs/1904.09882,https://arxiv.org/pdf/1904.09882,"Authors:EvonneNg,DonglaiXiang,HanbyulJoo,KristenGrauman","        The body pose of a person wearing a camera is of great interest for applications in augmented reality, healthcare, and robotics, yet much of the person's body is out of view for a typical wearable camera. We propose a learning-based approach to estimate the camera wearer's 3D body pose from egocentric video sequences. Our key insight is to leverage interactions with another person---whose body pose we can directly observe---as a signal inherently linked to the body pose of the first-person subject. We show that since interactions between individuals often induce a well-ordered series of back-and-forth responses, it is possible to learn a temporal model of the interlinked poses even though one party is largely out of view. We demonstrate our idea on a variety of domains with dyadic interaction and show the substantial impact on egocentric body pose estimation, which improves the state of the art. Video results are available at http://vision.cs.utexas.edu/projects/you2me/        △ Less","27 March, 2020",cs.CV,
"              Unremarkable AI: Fitting Intelligent Decision Support into Critical, Clinical Decision-Making Processes          ",1904.09612,https://arxiv.org/abs/1904.09612,https://arxiv.org/pdf/1904.09612,"Authors:QianYang,AaronSteinfeld,JohnZimmerman","        Clinical decision support tools (DST) promise improved healthcare outcomes by offering data-driven insights. While effective in lab settings, almost all DSTs have failed in practice. Empirical research diagnosed poor contextual fit as the cause. This paper describes the design and field evaluation of a radically new form of DST. It automatically generates slides for clinicians' decision meetings with subtly embedded machine prognostics. This design took inspiration from the notion of ""Unremarkable Computing"", that by augmenting the users' routines technology/AI can have significant importance for the users yet remain unobtrusive. Our field evaluation suggests clinicians are more likely to encounter and embrace such a DST. Drawing on their responses, we discuss the importance and intricacies of finding the right level of unremarkableness in DST design, and share lessons learned in prototyping critical AI systems as a situated experience.        △ Less","21 April, 2019","cs.HC,cs.AI,cs.CY,cs.LG",10.1145/3290605.3300468 
              Meta-Weighted Gaussian Process Experts for Personalized Forecasting of AD Cognitive Changes          ,1904.09370,https://arxiv.org/abs/1904.09370,https://arxiv.org/pdf/1904.09370,"Authors:OgnjenRudovic,YuriaUtsumi,RicardoGuerrero,KellyPeterson,DanielRueckert,RosalindW.Picard","        We introduce a novel personalized Gaussian Process Experts (pGPE) model for predicting per-subject ADAS-Cog13 cognitive scores -- a significant predictor of Alzheimer's Disease (AD) in the cognitive domain -- over the future 6, 12, 18, and 24 months. We start by training a population-level model using multi-modal data from previously seen subjects using a base Gaussian Process (GP) regression. Then, we personalize this model by adapting the base GP sequentially over time to a new (target) subject using domain adaptive GPs, and also by training subject-specific GP. While we show that these models achieve improved performance when selectively applied to the forecasting task (one performs better than the other on different subjects/visits), the average performance per model is suboptimal. To this end, we used the notion of meta learning in the proposed pGPE to design a regression-based weighting of these expert models, where the expert weights are optimized for each subject and his/her future visit. The results on a cohort of subjects from the ADNI dataset show that this newly introduced personalized weighting of the expert models leads to large improvements in accurately forecasting future ADAS-Cog13 scores and their fine-grained changes associated with the AD progression. This approach has potential to help identify at-risk patients early and improve the construction of clinical trials for AD.        △ Less","19 April, 2019","cs.LG,stat.ML",
              Deep Learning on Mobile Devices - A Review          ,1904.09274,https://arxiv.org/abs/1904.09274,https://arxiv.org/pdf/1904.09274,Authors:YunbinDeng,"        Recent breakthroughs in deep learning and artificial intelligence technologies have enabled numerous mobile applications. While traditional computation paradigms rely on mobile sensing and cloud computing, deep learning implemented on mobile devices provides several advantages. These advantages include low communication bandwidth, small cloud computing resource cost, quick response time, and improved data privacy. Research and development of deep learning on mobile and embedded devices has recently attracted much attention. This paper provides a timely review of this fast-paced field to give the researcher, engineer, practitioner, and graduate student a quick grasp on the recent advancements of deep learning on mobile devices. In this paper, we discuss hardware architectures for mobile deep learning, including Field Programmable Gate Arrays, Application Specific Integrated Circuit, and recent mobile Graphic Processing Units. We present Size, Weight, Area and Power considerations and their relation to algorithm optimizations, such as quantization, pruning, compression, and approximations that simplify computation while retaining performance accuracy. We cover existing systems and give a state-of-the-industry review of TensorFlow, MXNet, Mobile AI Compute Engine, and Paddle-mobile deep learning platform. We discuss resources for mobile deep learning practitioners, including tools, libraries, models, and performance benchmarks. We present applications of various mobile sensing modalities to industries, ranging from robotics, healthcare and multi-media, biometrics to autonomous drive and defense. We address the key deep learning challenges to overcome, including low quality data, and small training/adaptation data sets. In addition, the review provides numerous citations and links to existing code bases implementing various technologies.        △ Less","20 March, 2019",cs.LG,10.13140/RG.2.2.15012.12167 
              The Rise of Internet of Things (IoT) in Big Healthcare Data: Review and Open research Issues          ,1904.09270,https://arxiv.org/abs/1904.09270,https://arxiv.org/pdf/1904.09270,"Authors:ZainabAlansari,SafeeullahSoomro,MohammadRiyazBelgaum,ShahaboddinShamshirband","        Health is one of the sustainable development areas in all of the countries. Internet of Things has a variety of use in this sector which was not studied yet. The aim of this research is to prioritize IoT usage in the healthcare sector to achieve sustainable development. The study is an applied descriptive research according to data collection. As per the research methodology which is FAHP, it is a single cross sectional survey research. After data collection, the agreed paired comparison matrices, allocated to weighted criteria and the priority of IoT usage were determined. Based on the research findings, the two criteria of Economic Prosperity and Quality of Life achieved the highest priority for IoT sustainable development in the healthcare sector. Moreover, the top priorities for IoT in the area of health, according to the usage, were identified as Ultraviolet Radiation, Dental Health and Fall Detection.        △ Less","16 April, 2019",cs.OH,10.1007/978-981-10-6875-1_66 
              Explaining Deep Classification of Time-Series Data with Learned Prototypes          ,1904.08935,https://arxiv.org/abs/1904.08935,https://arxiv.org/pdf/1904.08935,"Authors:AlanH.Gee,DiegoGarcia-Olano,JoydeepGhosh,DavidPaydarfar","        The emergence of deep learning networks raises a need for explainable AI so that users and domain experts can be confident applying them to high-risk decisions. In this paper, we leverage data from the latent space induced by deep learning models to learn stereotypical representations or ""prototypes"" during training to elucidate the algorithmic decision-making process. We study how leveraging prototypes effect classification decisions of two dimensional time-series data in a few different settings: (1) electrocardiogram (ECG) waveforms to detect clinical bradycardia, a slowing of heart rate, in preterm infants, (2) respiration waveforms to detect apnea of prematurity, and (3) audio waveforms to classify spoken digits. We improve upon existing models by optimizing for increased prototype diversity and robustness, visualize how these prototypes in the latent space are used by the model to distinguish classes, and show that prototypes are capable of learning features on two dimensional time-series data to produce explainable insights during classification tasks. We show that the prototypes are capable of learning real-world features - bradycardia in ECG, apnea in respiration, and articulation in speech - as well as features within sub-classes. Our novel work leverages learned prototypical framework on two dimensional time-series data to produce explainable insights during classification tasks.        △ Less","4 September, 2019","cs.LG,cs.AI,stat.ML",
              Detection and Prediction of Cardiac Anomalies Using Wireless Body Sensors and Bayesian Belief Networks          ,1904.07976,https://arxiv.org/abs/1904.07976,https://arxiv.org/pdf/1904.07976,"Authors:AsimDarwaish,FaridNaït-Abdesselam,AshfaqKhokhar","        Intricating cardiac complexities are the primary factor associated with healthcare costs and the highest cause of death rate in the world. However, preventive measures like the early detection of cardiac anomalies can prevent severe cardiovascular arrests of varying complexities and can impose a substantial impact on healthcare cost. Encountering such scenarios usually the electrocardiogram (ECG or EKG) is the first diagnostic choice of a medical practitioner or clinical staff to measure the electrical and muscular fitness of an individual heart. This paper presents a system which is capable of reading the recorded ECG and predict the cardiac anomalies without the intervention of a human expert. The paper purpose an algorithm which read and perform analysis on electrocardiogram datasets. The proposed architecture uses the Discrete Wavelet Transform (DWT) at first place to perform preprocessing of ECG data followed by undecimated Wavelet transform (UWT) to extract nine relevant features which are of high interest to a cardiologist. The probabilistic mode named Bayesian Network Classifier is trained using the extracted nine parameters on UCL arrhythmia dataset. The proposed system classifies a recorded heartbeat into four classes using Bayesian Network classifier and Tukey's box analysis. The four classes for the prediction of a heartbeat are (a) Normal Beat, (b) Premature Ventricular Contraction (PVC) (c) Premature Atrial Contraction (PAC) and (d) Myocardial Infarction. The results of experimental setup depict that the proposed system has achieved an average accuracy of 96.6 for PAC\% 92.8\% for MI and 87\% for PVC, with an average error rate of 3.3\% for PAC, 6\% for MI and 12.5\% for PVC on real electrocardiogram datasets including Physionet and European ST-T Database (EDB).        △ Less","16 April, 2019","cs.LG,stat.ML",
              Fault Detection Effectiveness of Metamorphic Relations Developed for Testing Supervised Classifiers          ,1904.07348,https://arxiv.org/abs/1904.07348,https://arxiv.org/pdf/1904.07348,"Authors:PrashantaSaha,UpuleeKanewala","        In machine learning, supervised classifiers are used to obtain predictions for unlabeled data by inferring prediction functions using labeled data. Supervised classifiers are widely applied in domains such as computational biology, computational physics and healthcare to make critical decisions. However, it is often hard to test supervised classifiers since the expected answers are unknown. This is commonly known as the \emph{oracle problem} and metamorphic testing (MT) has been used to test such programs. In MT, metamorphic relations (MRs) are developed from intrinsic characteristics of the software under test (SUT). These MRs are used to generate test data and to verify the correctness of the test results without the presence of a test oracle. Effectiveness of MT heavily depends on the MRs used for testing. In this paper we have conducted an extensive empirical study to evaluate the fault detection effectiveness of MRs that have been used in multiple previous studies to test supervised classifiers. Our study uses a total of 709 reachable mutants generated by multiple mutation engines and uses data sets with varying characteristics to test the SUT. Our results reveal that only 14.8\% of these mutants are detected using the MRs and that the fault detection effectiveness of these MRs do not scale with the increased number of mutants when compared to what was reported in previous studies.        △ Less","15 April, 2019",cs.SE,
              Narrowband IoT for Healthcare,1904.07116,https://arxiv.org/abs/1904.07116,https://arxiv.org/pdf/1904.07116,"Authors:SudhirK.Routray,SharathAnand","        The Internet of Things (IoT) is going to have its presence in all the essential sectors of human lives. It has the ability to provide both mainstream as well as the value added services in almost all the sectors. Healthcare is an important service sector for overall development. It has far reaching implications in the quality of living. In the modern world where the quality of living has been degraded significantly IoT can certainly play a constructive role in providing better services. In healthcare, there are several occasions such as patient health monitoring, remote observation and emergency proceedings outside the hospital where sensors can play essential roles. The coordinated sensor networks can provide even better services. IoT has the ability to provide all these coordinated services. Narrowband IoT (NBIoT) is an economical and simpler version of IoT which can handle these tasks effectively. Due to the widespread requirement of healthcare, NBIoT is a preferred solution as it needs fewer amounts of resources. In this article, we provide the main issues and difficulties of NBIoT in healthcare.        △ Less","1 April, 2019",eess.SP,
"              Broadband plasmonic nanoparticles: fabrication, optical properties, and implications in liquid light chemiluminescence enhancement          ",1904.06462,https://arxiv.org/abs/1904.06462,https://arxiv.org/pdf/1904.06462,"Authors:DalerR.Dadadzhanov,TigranA.Vartanyan,PeterS.Parfenov,MikhailA.Baranov,AlinaKarabchevsky","        Chemiphores are entities, which exhibit wide-band light emission without any external light source but just due to the chemical reaction resulting in the chemiluminescence effect. Since the chemiphores usually have low quantum efficiency, chemiluminescence is a weak optical effect. We found that plasmonic nanoparticles can efficiently enhance the peculiar effect of chemiluminescence due to the acceleration of the radiative decay of the chemiphore excited state which, in turn, enlarges the chemiluminescence yield. Correspondingly, plasmonic nanoparticles are nanoparticles with sub-wavelength sizes experiencing the absorption band in specific wavelength which are characterized by unique optical properties, as well as high localization of electromagnetic radiation. However, the broadband properties of plasmonic nanoparticles and their implications in liquid light, the chemiluminescence effect, is overlooked. Therefore, they can attract attention as novel materials for photonics, sensing, and forensic science. Here, fabrication techniques of broadband plasmonic nanoparticles are reported, and their interesting optical properties together with their applications in chemiluminescence effect are discussed, as well. We fabricated the nanoparticles with laser ablation in liquids (LAL) technique and propose the physical vapor deposition (PVD) synthesis with annealing-assisted treatment for further studies. Both techniques are accessible and allow production of ensembles of nanoparticles having shape and size distributions to exhibit broad plasmonic resonance which fit the wide-band emission of a chemiphore. Our results, in particular, a specific design for plasmonic nanoparticles placed on the dielectric material, lead the way toward a new generation of chemiluminescence-based devices starting from sensing, healthcare, biomedical research and quantum systems such as pump-free laser sources.        △ Less","12 April, 2019",physics.optics,
              Estimation of group means in generalized linear mixed models          ,1904.06384,https://arxiv.org/abs/1904.06384,https://arxiv.org/pdf/1904.06384,"Authors:JiexinDuan,MichaelLevine,JunxiangLuo,YongmingQu","        In this manuscript, we investigate the concept of the mean response for a treatment group mean as well as its estimation and prediction for generalized linear models with a subject-wise random effect. Generalized linear models are commonly used to analyze categorical data. The model-based mean for a treatment group usually estimates the response at the mean covariate. However, the mean response for the treatment group for studied population is at least equally important in the context of clinical trials. New methods were proposed to estimate such a mean response in generalized linear models; however, this has only been done when there are no random effects in the model. We suggest that, in a generalized linear mixed model (GLMM), there are at least two possible definitions of a treatment group mean response that can serve as estimation/prediction targets. The estimation of these treatment group means is important for healthcare professionals to be able to understand the absolute benefit versus risk. For both of these treatment group means, we propose a new set of methods that suggests how to estimate/predict both of them in a GLMM models with a univariate subject-wise random effect. Our methods also suggest an easy way of constructing corresponding confidence and prediction intervals for both possible treatment group means. Simulations show that proposed confidence and prediction intervals provide correct empirical coverage probability under most circumstances. Proposed methods have also been applied to analyze hypoglycemia data from diabetes clinical trials.        △ Less","2 November, 2019","stat.AP,stat.ME",
              Google Street View image of a house predicts car accident risk of its resident          ,1904.05270,https://arxiv.org/abs/1904.05270,https://arxiv.org/pdf/1904.05270,"Authors:KingaKita,ŁukaszKidziński","        Road traffic injuries are a leading cause of death worldwide. Proper estimation of car accident risk is critical for appropriate allocation of resources in healthcare, insurance, civil engineering, and other industries. We show how images of houses are predictive of car accidents. We analyze 20,000 addresses of insurance company clients, collect a corresponding house image using Google Street View, and annotate house features such as age, type, and condition. We find that this information substantially improves car accident risk prediction compared to the state-of-the-art risk model of the insurance company and could be used for price discrimination. From this perspective, public availability of house images raises legal and social concerns, as they can be a proxy of ethnicity, religion and other sensitive data.        △ Less","10 April, 2019",stat.AP,
              Private Two-Party Cluster Analysis Made Formal & Scalable          ,1904.04475,https://arxiv.org/abs/1904.04475,https://arxiv.org/pdf/1904.04475,"Authors:XianruiMeng,DimitriosPapadopoulos,AlinaOprea,NikosTriandopoulos","        Machine Learning (ML) is widely used for predictive tasks in numerous important applications---most successfully, in the context of collaborative learning, where a plurality of entities contribute their own datasets to jointly deduce global ML models. Despite its efficacy, this new learning paradigm fails to encompass critical application domains, such as healthcare and security analytics, that involve learning over highly sensitive data, wherein privacy risks limit entities to individually deduce local models using solely their own datasets.  In this work, we present the first comprehensive study for privacy-preserving collaborative hierarchical clustering, overall featuring scalable cryptographic protocols that allow two parties to safely perform cluster analysis over their combined sensitive datasets. For this problem at hand, we introduce a formal security notion that achieves the required balance between intended accuracy and privacy and presents a class of two-party hierarchical clustering protocols that guarantee strong privacy protection, provable in our new security model. Crucially, our solution employs modular design and judicious use of cryptography to achieve high degrees of efficiency and extensibility. Specifically, we extend our core protocol to obtain two secure variants that significantly improve performance, an optimized variant for single-linkage clustering and a scalable approximate variant. Finally, we provide a prototype implementation of our approach and experimentally evaluate its feasibility and efficiency on synthetic and real datasets, obtaining encouraging results. For example, end-to-end execution of our secure approximate protocol, over 1M 10-dimensional records, completes in 35 sec, transferring only 896KB and achieving 97.09% accuracy.        △ Less","28 October, 2019","cs.CR,cs.AI,cs.DB,cs.DS",
"              Diabetes Mellitus Forecasting Using Population Health Data in Ontario, Canada          ",1904.04137,https://arxiv.org/abs/1904.04137,https://arxiv.org/pdf/1904.04137,"Authors:MathieuRavaut,HamedSadeghi,KinKwanLeung,MaksimsVolkovs,LauraC.Rosella","        Leveraging health administrative data (HAD) datasets for predicting the risk of chronic diseases including diabetes has gained a lot of attention in the machine learning community recently. In this paper, we use the largest health records datasets of patients in Ontario,Canada. Provided by the Institute of Clinical Evaluative Sciences (ICES), this database is age, gender and ethnicity-diverse. The datasets include demographics, lab measurements,drug benefits, healthcare system interactions, ambulatory and hospitalizations records. We perform one of the first large-scale machine learning studies with this data to study the task of predicting diabetes in a range of 1-10 years ahead, which requires no additional screening of individuals.In the best setup, we reach a test AUC of 80.3 with a single-model trained on an observation window of 5 years with a one-year buffer using all datasets. A subset of top 15 features alone (out of a total of 963) could provide a test AUC of 79.1. In this paper, we provide extensive machine learning model performance and feature contribution analysis, which enables us to narrow down to the most important features useful for diabetes forecasting. Examples include chronic conditions such as asthma and hypertension, lab results, diagnostic codes in insurance claims, age and geographical information.        △ Less","8 April, 2019","stat.AP,cs.LG",
              A Big Data Analytics Framework to Predict the Risk of Opioid Use Disorder          ,1904.03524,https://arxiv.org/abs/1904.03524,https://arxiv.org/pdf/1904.03524,"Authors:MdMahmudulHasan,Md.Noor-E-Alam,MehulRakeshkumarPatel,AliciaSasserModestino,LeonD.Sanchez,GaryYoung","        Overdose related to prescription opioids have reached an epidemic level in the US, creating an unprecedented national crisis. This has been exacerbated partly due to the lack of tools for physicians to help predict the risk of whether a patient will develop opioid use disorder. Little is known about how machine learning can be applied to a big-data platform to ensure an informed, sustained and judicious prescribing of opioids, in particular for commercially insured population. This study explores Massachusetts All Payer Claims Data, a de-identified healthcare dataset, and proposes a machine learning framework to examine how naïve users develop opioid use disorder. We perform several feature selections techniques to identify influential demographic and clinical features associated with opioid use disorder from a class imbalanced analytic sample. We then compare the predictive power of four well-known machine learning algorithms: Logistic Regression, Random Forest, Decision Tree, and Gradient Boosting to predict the risk of opioid use disorder. The study results show that the Random Forest model outperforms the other three algorithms while determining the features, some of which are consistent with prior clinical findings. Moreover, alongside the higher predictive accuracy, the proposed framework is capable of extracting some risk factors that will add significant knowledge to what is already known in the extant literature. We anticipate that this study will help healthcare practitioners improve the current prescribing practice of opioids and contribute to curb the increasing rate of opioid addiction and overdose.        △ Less","30 May, 2020","stat.AP,cs.CY,q-bio.QM",
              Data Shapley: Equitable Valuation of Data for Machine Learning          ,1904.02868,https://arxiv.org/abs/1904.02868,https://arxiv.org/pdf/1904.02868,"Authors:AmirataGhorbani,JamesZou","        As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on nn data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.        △ Less","10 June, 2019","stat.ML,cs.AI,cs.LG",
              An Approach to Identity Management in Clouds without Trusted Third Parties          ,1904.00880,https://arxiv.org/abs/1904.00880,https://arxiv.org/pdf/1904.00880,"Authors:AkramSarhan,LeszekLilien","        The management of sensitive data, including identity management (IDM), is an important problem in cloud computing, fundamental for authentication and fine-grained service access control. Our goal is creating an efficient and robust IDM solution that addresses critical issues in cloud computing. The proposed IDM scheme does not rely on trusted third parties (TTPs) or trusted dealers. The scheme is a multiparty interactive solution that combines RSA distributed key generation and attribute-based encryption. We believe that it will be a robust IDM privacy-preserving solution in cloud computing, because it has the following features: (i) protects sensitive data on untrusted hosts using active bundle; (ii) supports the minimum disclosure property; (iii) minimizes authentication overhead by providing single sign-on; (iv) supports authentication with encrypted credentials; (v) avoids using trusted third parties (TTPs_, incl. using TTPs for key management; (vi) supports revocation and delegation of access right; and (vii) supports revocation of user credentials. The scheme should also be efficient because it exploits parallelism.        △ Less","28 March, 2019",cs.CR,
              Transfer Learning for Clinical Time Series Analysis using Deep Neural Networks          ,1904.00655,https://arxiv.org/abs/1904.00655,https://arxiv.org/pdf/1904.00655,"Authors:PriyankaGupta,PankajMalhotra,JyotiNarwariya,LovekeshVig,GautamShroff","        Deep neural networks have shown promising results for various clinical prediction tasks. However, training deep networks such as those based on Recurrent Neural Networks (RNNs) requires large labeled data, significant hyper-parameter tuning effort and expertise, and high computational resources. In this work, we investigate as to what extent can transfer learning address these issues when using deep RNNs to model multivariate clinical time series. We consider two scenarios for transfer learning using RNNs: i) domain-adaptation, i.e., leveraging a deep RNN - namely, TimeNet - pre-trained for feature extraction on time series from diverse domains, and adapting it for feature extraction and subsequent target tasks in healthcare domain, ii) task-adaptation, i.e., pre-training a deep RNN - namely, HealthNet - on diverse tasks in healthcare domain, and adapting it to new target tasks in the same domain. We evaluate the above approaches on publicly available MIMIC-III benchmark dataset, and demonstrate that (a) computationally-efficient linear models trained using features extracted via pre-trained RNNs outperform or, in the worst case, perform as well as deep RNNs and statistical hand-crafted features based models trained specifically for target task; (b) models obtained by adapting pre-trained models for target tasks are significantly more robust to the size of labeled data compared to task-specific RNNs, while also being computationally efficient. We, therefore, conclude that pre-trained deep models like TimeNet and HealthNet allow leveraging the advantages of deep learning for clinical time series analysis tasks, while also minimize dependence on hand-crafted features, deal robustly with scarce labeled training data scenarios without overfitting, as well as reduce dependence on expertise and resources required to train deep networks from scratch.        △ Less","1 April, 2019","cs.LG,stat.ML",
              BlackMarks: Blackbox Multibit Watermarking for Deep Neural Networks          ,1904.00344,https://arxiv.org/abs/1904.00344,https://arxiv.org/pdf/1904.00344,"Authors:HuiliChen,BitaDarvishRouhani,FarinazKoushanfar","        Deep Neural Networks have created a paradigm shift in our ability to comprehend raw data in various important fields ranging from computer vision and natural language processing to intelligence warfare and healthcare. While DNNs are increasingly deployed either in a white-box setting where the model internal is publicly known, or a black-box setting where only the model outputs are known, a practical concern is protecting the models against Intellectual Property (IP) infringement. We propose BlackMarks, the first end-to-end multi-bit watermarking framework that is applicable in the black-box scenario. BlackMarks takes the pre-trained unmarked model and the owner's binary signature as inputs and outputs the corresponding marked model with a set of watermark keys. To do so, BlackMarks first designs a model-dependent encoding scheme that maps all possible classes in the task to bit '0' and bit '1' by clustering the output activations into two groups. Given the owner's watermark signature (a binary string), a set of key image and label pairs are designed using targeted adversarial attacks. The watermark (WM) is then embedded in the prediction behavior of the target DNN by fine-tuning the model with generated WM key set. To extract the WM, the remote model is queried by the WM key images and the owner's signature is decoded from the corresponding predictions according to the designed encoding scheme. We perform a comprehensive evaluation of BlackMarks's performance on MNIST, CIFAR10, ImageNet datasets and corroborate its effectiveness and robustness. BlackMarks preserves the functionality of the original DNN and incurs negligible WM embedding runtime overhead as low as 2.054%.        △ Less","31 March, 2019","cs.MM,cs.CR",
              Validation of a recommender system for prompting omitted foods in online dietary assessment surveys          ,1903.12264,https://arxiv.org/abs/1903.12264,https://arxiv.org/pdf/1903.12264,"Authors:TimurOsadchiy,IvanPoliakov,PatrickOlivier,MaisieRowland,EmmaFoster","        Recall assistance methods are among the key aspects that improve the accuracy of online dietary assessment surveys. These methods still mainly rely on experience of trained interviewers with nutritional background, but data driven approaches could improve cost-efficiency and scalability of automated dietary assessment. We evaluated the effectiveness of a recommender algorithm developed for an online dietary assessment system called Intake24, that automates the multiple-pass 24-hour recall method. The recommender builds a model of eating behavior from recalls collected in past surveys. Based on foods they have already selected, the model is used to remind respondents of associated foods that they may have omitted to report. The performance of prompts generated by the model was compared to that of prompts hand-coded by nutritionists in two dietary studies. The results of our studies demonstrate that the recommender system is able to capture a higher number of foods omitted by respondents of online dietary surveys than prompts hand-coded by nutritionists. However, the considerably lower precision of generated prompts indicates an opportunity for further improvement of the system.        △ Less","20 March, 2019","cs.CY,cs.HC,cs.LG,stat.ML",10.1145/3329189.3329191 
              Big Data Analytics and AI in Mental Healthcare,1903.12071,https://arxiv.org/abs/1903.12071,https://arxiv.org/pdf/1903.12071,"Authors:ArielRosenfeld,DavidBenrimoh,CaitrinArmstrong,NykanMirchi,TimotheLanglois-Therrien,ColleenRollins,MyriamTanguay-Sela,JosephMehltretter,RobertFratila,SoniaIsrael,EmilySnook,KellyPerlman,AkivaKleinerman,BecharaSaab,MarkThoburn,CherylGabbay,AmitYaniv-Rosenfeld","        Mental health conditions cause a great deal of distress or impairment; depression alone will affect 11% of the world's population. The application of Artificial Intelligence (AI) and big-data technologies to mental health has great potential for personalizing treatment selection, prognosticating, monitoring for relapse, detecting and helping to prevent mental health conditions before they reach clinical-level symptomatology, and even delivering some treatments. However, unlike similar applications in other fields of medicine, there are several unique challenges in mental health applications which currently pose barriers towards the implementation of these technologies. Specifically, there are very few widely used or validated biomarkers in mental health, leading to a heavy reliance on patient and clinician derived questionnaire data as well as interpretation of new signals such as digital phenotyping. In addition, diagnosis also lacks the same objective 'gold standard' as in other conditions such as oncology, where clinicians and researchers can often rely on pathological analysis for confirmation of diagnosis. In this chapter we discuss the major opportunities, limitations and techniques used for improving mental healthcare through AI and big-data. We explore both the computational, clinical and ethical considerations and best practices as well as lay out the major researcher directions for the near future.        △ Less","12 March, 2019","cs.CY,cs.AI,cs.LG",
              The Virtual Doctor: An Interactive Artificial Intelligence based on Deep Learning for Non-Invasive Prediction of Diabetes          ,1903.12069,https://arxiv.org/abs/1903.12069,https://arxiv.org/pdf/1903.12069,"Authors:SebastianSpänig,AgnesEmberger-Klein,Jan-PeterSowa,AliCanbay,KlausMenrad,DominikHeider","        Artificial intelligence (AI) will pave the way to a new era in medicine. However, currently available AI systems do not interact with a patient, e.g., for anamnesis, and thus are only used by the physicians for predictions in diagnosis or prognosis. However, these systems are widely used, e.g., in diabetes or cancer prediction. In the current study, we developed an AI that is able to interact with a patient (virtual doctor) by using a speech recognition and speech synthesis system and thus can autonomously interact with the patient, which is particularly important for, e.g., rural areas, where the availability of primary medical care is strongly limited by low population densities. As a proof-of-concept, the system is able to predict type 2 diabetes mellitus (T2DM) based on non-invasive sensors and deep neural networks. Moreover, the system provides an easy-to-interpret probability estimation for T2DM for a given patient. Besides the development of the AI, we further analyzed the acceptance of young people for AI in healthcare to estimate the impact of such system in the future.        △ Less","9 March, 2019","cs.CY,cs.LG,stat.ML",10.1016/j.artmed.2019.101706 
              A Survey of Protocols and Standards for Internet of Things          ,1903.11549,https://arxiv.org/abs/1903.11549,https://arxiv.org/pdf/1903.11549,"Authors:TaraSalman,RajJain","        The rapid growth in technology and internet connected devices has enabled Internet of Things (IoT) to be one of the important fields in computing. Standards, technologies and platforms targeting IoT ecosystem are being developed at a very fast pace. IoT enables things to communicate and coordinate decisions for many different types of applications including healthcare, home automation, disaster recovery, and industry automation. It is expected to expand to even more applications in the future. This paper surveys several standards by IEEE, IETF and ITU that enable technologies enabling the rapid growth of IoT. These standards include communications, routing, network and session layer protocols that are being developed to meet IoT requirements. The discussion also includes management and security protocols in addition to the current challenges in IoT which gives insights into the current research to solve such challenges.        △ Less","10 February, 2019",cs.NI,
              Machine learning approaches in Detecting the Depression from Resting-state Electroencephalogram (EEG): A Review Study          ,1903.11454,https://arxiv.org/abs/1903.11454,https://arxiv.org/pdf/1903.11454,Authors:MilenaCukicRadenkovic,"        In this paper, we aimed at reviewing several different approaches present today in the search for more accurate diagnostic and treatment management in mental healthcare. Our focus is on mood disorders, and in particular on the major depressive disorder (MDD). We are reviewing and discussing findings based on neuroimaging studies (MRI and fMRI) first to get the impression of the body of knowledge about the anatomical and functional differences in depression. Then, we are focusing on less expensive data-driven approach, applicable for everyday clinical practice, in particular, those based on electroencephalographic (EEG) recordings. Among those studies utilizing EEG, we are discussing a group of applications used for detecting of depression based on the resting state EEG (detection studies) and interventional studies (using stimulus in their protocols or aiming to predict the outcome of therapy). We conclude with a discussion and review of guidelines to improve the reliability of developed models that could serve improvement of diagnostic of depression in psychiatry.        △ Less","26 March, 2019","q-bio.NC,cs.LG,stat.AP,stat.ML",
              Automated pulmonary nodule detection using 3D deep convolutional neural networks          ,1903.09876,https://arxiv.org/abs/1903.09876,https://arxiv.org/pdf/1903.09876,"Authors:HaoTang,DanielR.Kim,XiaohuiXie","        Early detection of pulmonary nodules in computed tomography (CT) images is essential for successful outcomes among lung cancer patients. Much attention has been given to deep convolutional neural network (DCNN)-based approaches to this task, but models have relied at least partly on 2D or 2.5D components for inherently 3D data. In this paper, we introduce a novel DCNN approach, consisting of two stages, that is fully three-dimensional end-to-end and utilizes the state-of-the-art in object detection. First, nodule candidates are identified with a U-Net-inspired 3D Faster R-CNN trained using online hard negative mining. Second, false positive reduction is performed by 3D DCNN classifiers trained on difficult examples produced during candidate screening. Finally, we introduce a method to ensemble models from both stages via consensus to give the final predictions. By using this framework, we ranked first of 2887 teams in Season One of Alibaba's 2017 TianChi AI Competition for Healthcare.        △ Less","23 March, 2019","cs.CV,cs.AI,cs.LG,cs.NE",
              Optimizing the Access to Healthcare Services in Dense Refugee Hosting Urban Areas: A Case for Istanbul          ,1903.09614,https://arxiv.org/abs/1903.09614,https://arxiv.org/pdf/1903.09614,"Authors:M.TarikAltuncu,AyseSeyyideKaptaner,NurSevencan","        With over 3.5 million refugees, Turkey continues to host the world's largest refugee population. This introduced several challenges in many areas including access to healthcare system. Refugees have legal rights to free healthcare services in Turkey's public hospitals. With the aim of increasing healthcare access for refugees, we looked at where the lack of infrastructure is felt the most. Our study attempts to address these problems by assessing whether Migrant Health Centers' locations are optimal. The aim of this study is to improve refugees' access to healthcare services in Istanbul by improving the locations of health facilities available to them. We used call data records provided by Turk Telekom.        △ Less","21 February, 2019",cs.CY,10.1007/978-3-030-12554-7_20 
              Patient Clustering Improves Efficiency of Federated Machine Learning to predict mortality and hospital stay time using distributed Electronic Medical Records          ,1903.09296,https://arxiv.org/abs/1903.09296,https://arxiv.org/pdf/1903.09296,"Authors:LiHuang,DianboLiu","        Electronic medical records (EMRs) supports the development of machine learning algorithms for predicting disease incidence, patient response to treatment, and other healthcare events. But insofar most algorithms have been centralized, taking little account of the decentralized, non-identically independently distributed (non-IID), and privacy-sensitive characteristics of EMRs that can complicate data collection, sharing and learning. To address this challenge, we introduced a community-based federated machine learning (CBFL) algorithm and evaluated it on non-IID ICU EMRs. Our algorithm clustered the distributed data into clinically meaningful communities that captured similar diagnoses and geological locations, and learnt one model for each community. Throughout the learning process, the data was kept local on hospitals, while locally-computed results were aggregated on a server. Evaluation results show that CBFL outperformed the baseline FL algorithm in terms of Area Under the Receiver Operating Characteristic Curve (ROC AUC), Area Under the Precision-Recall Curve (PR AUC), and communication cost between hospitals and the server. Furthermore, communities' performance difference could be explained by how dissimilar one community was to others.        △ Less","21 March, 2019","cs.LG,stat.ML",
              Learning Hierarchical Representations of Electronic Health Records for Clinical Outcome Prediction          ,1903.08652,https://arxiv.org/abs/1903.08652,https://arxiv.org/pdf/1903.08652,"Authors:LuchenLiu,HaoranLi,ZhitingHu,HaoranShi,ZichangWang,JianTang,MingZhang","        Clinical outcome prediction based on the Electronic Health Record (EHR) plays a crucial role in improving the quality of healthcare. Conventional deep sequential models fail to capture the rich temporal patterns encoded in the longand irregular clinical event sequences. We make the observation that clinical events at a long time scale exhibit strongtemporal patterns, while events within a short time period tend to be disordered co-occurrence. We thus propose differentiated mechanisms to model clinical events at different time scales. Our model learns hierarchical representationsof event sequences, to adaptively distinguish between short-range and long-range events, and accurately capture coretemporal dependencies. Experimental results on real clinical data show that our model greatly improves over previous state-of-the-art models, achieving AUC scores of 0.94 and 0.90 for predicting death and ICU admission respectively, Our model also successfully identifies important events for different clinical outcome prediction tasks        △ Less","23 August, 2019","cs.LG,stat.ML",
              A customisable pipeline for continuously harvesting socially-minded Twitter users          ,1903.07061,https://arxiv.org/abs/1903.07061,https://arxiv.org/pdf/1903.07061,"Authors:FlavioPrimo,PaoloMissier,AlexanderRomanovsky,MickaelFigueredo,NelioCacho","        On social media platforms and Twitter in particular, specific classes of users such as influencers have been given satisfactory operational definitions in terms of network and content metrics.  Others, for instance online activists, are not less important but their characterisation still requires experimenting.  We make the hypothesis that such interesting users can be found within temporally and spatially localised contexts, i.e., small but topical fragments of the network containing interactions about social events or campaigns with a significant footprint on Twitter.  To explore this hypothesis, we have designed a continuous user profile discovery pipeline that produces an ever-growing dataset of user profiles by harvesting and analysing contexts from the Twitter stream.  The profiles dataset includes key network and content-based users metrics, enabling experimentation with user-defined score functions that characterise specific classes of online users.  The paper describes the design and implementation of the pipeline and its empirical evaluation on a case study consisting of healthcare-related campaigns in the UK, showing how it supports the operational definitions of online activism, by comparing three experimental ranking functions. The code is publicly available.        △ Less","17 March, 2019","cs.IR,cs.SI",
              Markov Chain-based Cost-Optimal Control Charts for Healthcare Data          ,1903.06675,https://arxiv.org/abs/1903.06675,https://arxiv.org/pdf/1903.06675,"Authors:BalázsDobi,AndrásZempléni","        Control charts have traditionally been used in industrial statistics, but are constantly seeing new areas of application, especially in the age of Industry 4.0. This paper introduces a new method, which is suitable for applications in the healthcare sector, especially for monitoring a health-characteristic of a patient. We adapt a Markov chain-based approach and develop a method in which not only the shift size (i.e. the degradation of the patient's health) can be random, but the effect of the repair (i.e. treatment) and time between samplings (i.e. visits) too. This means that we do not use many often-present assumptions which are usually not applicable for medical treatments. The average cost of the protocol, which is determined by the time between samplings and the control limit, can be estimated using the stationary distribution of the Markov chain.  Furthermore, we incorporate the standard deviation of the cost into the optimisation procedure, which is often very important from a process control viewpoint. The sensitivity of the optimal parameters and the resulting average cost and cost standard deviation on different parameter values is investigated. We demonstrate the usefulness of the approach for real-life data of patients treated in Hungary: namely the monitoring of cholesterol level of patients with cardiovascular event risk. The results showed that the optimal parameters from our approach can be somewhat different from the original medical parameters.        △ Less","14 February, 2019","stat.AP,cs.CY,stat.ME,stat.ML",
              A Novel Re-Targetable Application Development Platform for Healthcare Mobile Applications          ,1903.05783,https://arxiv.org/abs/1903.05783,https://arxiv.org/pdf/1903.05783,"Authors:ChaeHoCho,FatemehsadatTabei,TraNguyenPhan,YeesockKim,JoWoonChong","        The rapid enhancement of central power unit CPU performance enables the development of computationally-intensive healthcare mobile applications for smartphones and wearable devices. However, computationally intensive mobile applications require significant application development time during the application porting procedure when the number of considering target devices operating systems OSs is large. In this paper, we propose a novel retargetable application development platform for healthcare mobile applications, which reduces application development time with maintaining the performance of the algorithm. Although the number of applications target OSs increases, the amount of time required for the code conversion step in the application porting procedure remains constant in the proposed retargetable platform. Experimental results show that our proposed retargetable platform gives reduced application development time compared to the conventional platform with maintaining the performance of the mobile application.        △ Less","13 March, 2019","cs.SE,cs.CY",
              Network Medicine in the age of biomedical big data          ,1903.05449,https://arxiv.org/abs/1903.05449,https://arxiv.org/pdf/1903.05449,"Authors:AbhijeetR.Sonawane,ScottT.Weiss,KimberlyGlass,AmitabhSharma","        Network medicine is an emerging area of research dealing with molecular and genetic interactions, network biomarkers of disease, and therapeutic target discovery. Large-scale biomedical data generation offers a unique opportunity to assess the effect and impact of cellular heterogeneity and environmental perturbations on the observed phenotype. Marrying the two, network medicine with biomedical data provides a framework to build meaningful models and extract impactful results at a network level. In this review, we survey existing network types and biomedical data sources. More importantly, we delve into ways in which the network medicine approach, aided by phenotype-specific biomedical data, can be gainfully applied. We provide three paradigms, mainly dealing with three major biological network archetypes: protein-protein interaction, expression-based, and gene regulatory networks. For each of these paradigms, we discuss a broad overview of philosophies under which various network methods work. We also provide a few examples in each paradigm as a test case of its successful application. Finally, we delineate several opportunities and challenges in the field of network medicine. Taken together, the understanding gained from combining biomedical data with networks can be useful for characterizing disease etiologies and identifying therapeutic targets, which, in turn, will lead to better preventive medicine with translational impacts on personalized healthcare.        △ Less","13 March, 2019",q-bio.MN,
              Improving the quality of healthcare through Internet of Things          ,1903.05221,https://arxiv.org/abs/1903.05221,https://arxiv.org/pdf/1903.05221,"Authors:CornelTurcu,CristinaTurcu","        This paper attempts to outline how the adoption of Internet of Things (IoT) in healthcare can create real economic value and improve the patient experience. Thus, getting the maximum benefits requires understanding both the IoT paradigm and the enabling technologies, and how IoT can be applied in the field of healthcare. We will mention some open challenging issues to be addressed by the research community, and not only. Besides the real barriers in adopting the Internet of Things, there are some advantages regard collecting and processing patient data, and monitoring the daily health states of individuals, just to name a few. These aspects could revolutionize the healthcare industry.        △ Less","12 March, 2019",cs.CY,
              Analysis of the AOK Lower Saxony hospitalisation records data (years 2008 -- 2015)          ,1903.04701,https://arxiv.org/abs/1903.04701,https://arxiv.org/pdf/1903.04701,"Authors:MonikaJ.Piotrowska,KonradSakowski","        Multidrug-resistant Enterobacteriaceae (MDR-E) have become a major public health threat in many European countries. While traditional infection control strategies primarily target the containment of intra-hospital transmission, there is growing evidence highlighting the importance of inter-hospital patient traffic for the spread of MDR-E within healthcare systems.  Our aim is to propose a network model, which will reflect patient traffic in various European healthcare systems and will thus provide the framework to study systematically transmission dynamics of MDR-E and the effectiveness of infection control strategies to contain their spread within and potentially across healthcare systems. However, to do that first we need to analyse real patients data and base on that propose network model reflecting the complexity of the real hospital network connections and dynamics of patient transfers between healthcare facilities.        △ Less","11 March, 2019",stat.AP,
              Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring          ,1903.03530,https://arxiv.org/abs/1903.03530,https://arxiv.org/pdf/1903.03530,"Authors:ZhengyuanLiu,HazelLim,NurFarahAinBinteSuhaimi,ShaoChuenTong,SharonOng,AngelaNg,SheldonLee,MichaelR.Macdonald,SavithaRamasamy,PavitraKrishnaswamy,WaiLengChow,NancyF.Chen","        Data for human-human spoken dialogues for research and development are currently very limited in quantity, variety, and sources; such data are even scarcer in healthcare. In this work, we investigate fast prototyping of a dialogue comprehension system by leveraging on minimal nurse-to-patient conversations. We propose a framework inspired by nurse-initiated clinical symptom monitoring conversations to construct a simulated human-human dialogue dataset, embodying linguistic characteristics of spoken interactions like thinking aloud, self-contradiction, and topic drift. We then adopt an established bidirectional attention pointer network on this simulated dataset, achieving more than 80% F1 score on a held-out test set from real-world nurse-to-patient conversations. The ability to automatically comprehend conversations in the healthcare domain by exploiting only limited data has implications for improving clinical workflows through red flag symptom detection and triaging capabilities. We demonstrate the feasibility for efficient and effective extraction, retrieval and comprehension of symptom checking information discussed in multi-turn human-human spoken conversations.        △ Less","5 April, 2019",cs.CL,
              Novel quantitative indicators of digital ophthalmoscopy image quality          ,1903.02695,https://arxiv.org/abs/1903.02695,https://arxiv.org/pdf/1903.02695,Authors:ChrisvonCsefalvay,"        With the advent of smartphone indirect ophthalmoscopy, teleophthalmology - the use of specialist ophthalmology assets at a distance from the patient - has experienced a breakthrough, promising enormous benefits especially for healthcare in distant, inaccessible or opthalmologically underserved areas, where specialists are either unavailable or too few in number. However, accurate teleophthalmology requires high-quality ophthalmoscopic imagery. This paper considers three feature families - statistical metrics, gradient-based metrics and wavelet transform coefficient derived indicators - as possible metrics to identify unsharp or blurry images. By using standard machine learning techniques, the suitability of these features for image quality assessment is confirmed, albeit on a rather small data set. With the increased availability and decreasing cost of digital ophthalmoscopy on one hand and the increased prevalence of diabetic retinopathy worldwide on the other, creating tools that can determine whether an image is likely to be diagnostically suitable can play a significant role in accelerating and streamlining the teleophthalmology process. This paper highlights the need for more research in this area, including the compilation of a diverse database of ophthalmoscopic imagery, annotated with quality markers, to train the Point of Acquisition error detection algorithms of the future.        △ Less","6 March, 2019","cs.CV,stat.ML",
              Understanding the Artificial Intelligence Clinician and optimal treatment strategies for sepsis in intensive care          ,1903.02345,https://arxiv.org/abs/1903.02345,https://arxiv.org/pdf/1903.02345,"Authors:MatthieuKomorowski,LeoA.Celi,OmarBadawi,AnthonyC.Gordon,A.AldoFaisal","        In this document, we explore in more detail our published work (Komorowski, Celi, Badawi, Gordon, & Faisal, 2018) for the benefit of the AI in Healthcare research community. In the above paper, we developed the AI Clinician system, which demonstrated how reinforcement learning could be used to make useful recommendations towards optimal treatment decisions from intensive care data. Since publication a number of authors have reviewed our work (e.g. Abbasi, 2018; Bos, Azoulay, & Martin-Loeches, 2019; Saria, 2018). Given the difference of our framework to previous work, the fact that we are bridging two very different academic communities (intensive care and machine learning) and that our work has impact on a number of other areas with more traditional computer-based approaches (biosignal processing and control, biomedical engineering), we are providing here additional details on our recent publication.        △ Less","6 March, 2019","cs.AI,stat.AP",
              Straight to the point: reinforcement learning for user guidance in ultrasound          ,1903.00586,https://arxiv.org/abs/1903.00586,https://arxiv.org/pdf/1903.00586,"Authors:FaustoMilletari,VighneshBirodkar,MichalSofka","        Point of care ultrasound (POCUS) consists in the use of ultrasound imaging in critical or emergency situations to support clinical decisions by healthcare professionals and first responders. In this setting it is essential to be able to provide means to obtain diagnostic data to potentially inexperienced users who did not receive an extensive medical training. Interpretation and acquisition of ultrasound images is not trivial. First, the user needs to find a suitable sound window which can be used to get a clear image, and then he needs to correctly interpret it to perform a diagnosis. Although many recent approaches focus on developing smart ultrasound devices that add interpretation capabilities to existing systems, our goal in this paper is to present a reinforcement learning (RL) strategy which is capable to guide novice users to the correct sonic window and enable them to obtain clinically relevant pictures of the anatomy of interest. We apply our approach to cardiac images acquired from the parasternal long axis (PLAx) view of the left ventricle of the heart.        △ Less","1 March, 2019",cs.CV,
              IoT Virtualization: A Survey of Software Definition & Function Virtualization Techniques for Internet of Things          ,1902.10910,https://arxiv.org/abs/1902.10910,https://arxiv.org/pdf/1902.10910,"Authors:IqbalAlam,KashifSharif,FanLi,ZohaibLatif,MdMonjurulKarim,BoubakrNour,SujitBiswas,YuWang","        Internet of Things (IoT) and Network Softwarization are fast becoming core technologies of information systems and network management for next generation Internet. The deployment and applications of IoT ranges from smart cities to urban computing, and from ubiquitous healthcare to tactile Internet. For this reason the physical infrastructure of heterogeneous network systems has become more complicated, and thus requires efficient and dynamic solutions for management, configuration, and flow scheduling. Network softwarization in the form of Software Defined Networks (SDN) and Network Function Virtualization (NFV) has been extensively researched for IoT in recent past. In this article we present a systematic and comprehensive review of virtualization techniques explicitly designed for IoT networks. We have classified the literature into software defined networks designed for IoT, function virtualization for IoT networks, and software defined IoT networks. These categories are further divided into works which present architectural, security, and management solutions. In addition, the paper highlights a number of short term and long term research challenges and open issues related to adoption of software defined Internet of things.        △ Less","28 February, 2019",cs.NI,
              A Survey on Applications of Game Theory in Blockchain          ,1902.10865,https://arxiv.org/abs/1902.10865,https://arxiv.org/pdf/1902.10865,"Authors:ZiyaoLiu,NguyenCongLuong,WenboWang,DusitNiyato,PingWang,Ying-ChangLiang,DongInKim","        In the past decades, the blockchain technology has attracted tremendous attention from both academia and industry. The popularity of blockchain networks was originated from a crypto-currency to serve as a decentralized and tamperproof transaction data ledger. Nowadays, blockchain as the key framework in the decentralized public data-ledger, has been applied to a wide range of scenarios far beyond crypto-currencies, such as Internet of Things (IoT), healthcare, and insurance. This survey aims to fill the gap between the large number of studies on blockchain network, where game theory emerges as an analytical tool, and the lack of a comprehensive survey on the game theoretical approaches applied in blockchain related issues. In this paper, we review game models proposed to address common issues in the blockchain network. The issues include security issues, e.g., selfish mining, majority attack and Denial of Service (DoS) attack, issues regard mining management, e.g., computational power allocation, reward allocation, and pool selection, as well as issues regarding blockchain economic and energy trading. Additionally, we discuss advantages and disadvantages of these selected game models and solutions. Finally, we highlight important challenges and future research directions of applying game theoretical approaches to incentive mechanism design, and the combination of blockchain with other technologies.        △ Less","15 March, 2019",cs.GT,
              Efficient and Secure ECDSA Algorithm and its Applications: A Survey          ,1902.10313,https://arxiv.org/abs/1902.10313,https://arxiv.org/pdf/1902.10313,"Authors:MishallAl-Zubaidie,ZhongweiZhang,JiZhang","        Public-key cryptography algorithms, especially elliptic curve cryptography (ECC) and elliptic curve digital signature algorithm (ECDSA) have been attracting attention from many researchers in different institutions because these algorithms provide security and high performance when being used in many areas such as electronic-healthcare, electronic-banking, electronic-commerce, electronic-vehicular, and electronic-governance. These algorithms heighten security against various attacks and at the same time improve performance to obtain efficiencies (time, memory, reduced computation complexity, and energy saving) in an environment of the constrained source and large systems. This paper presents detailed and a comprehensive survey of an update of the ECDSA algorithm in terms of performance, security, and applications.        △ Less","26 February, 2019",cs.CR,
              Exploiting Population Activity Dynamics to Predict Urban Epidemiological Incidence          ,1902.10260,https://arxiv.org/abs/1902.10260,https://arxiv.org/pdf/1902.10260,"Authors:GerganaTodorova,AnastasiosNoulas","        Ambulance services worldwide are of vital importance to population health. Timely responding to incidents by dispatching an ambulance vehicle to the location a call came from can offer significant benefits to patient care across a number of medical conditions. Moreover, identifying the reasons that drive ambulance activity at an area not only can improve the operational capacity of emergency services, but can lead to better policy design in healthcare. In this work, we analyse the temporal dynamics of 5.6 million ambulance calls across a region of 7 million residents in the UK. We identify characteristic temporal patterns featuring diurnal and weekly cycles in ambulance call activity. These patterns are stable over time and across geographies. Using a dataset sourced from location intelligence platform Foursquare, we establish a link between the spatio-temporal dynamics of mobile users engaging with urban activities locally and emergency incidents. We use this information to build a novel metric that assesses the health risk of a geographic area in terms of its propensity to yield ambulance calls. Formulating then an online classification task where the goal becomes to identify which regions will need an ambulance at a given time, we demonstrate how semantic information about real world places crowdsourced through online platforms, can become a useful source of information in understanding and predicting regional epidemiological trends.        △ Less","26 February, 2019",cs.CY,
              High Dimensional Restrictive Federated Model Selection with multi-objective Bayesian Optimization over shifted distributions          ,1902.08999,https://arxiv.org/abs/1902.08999,https://arxiv.org/pdf/1902.08999,"Authors:XudongSun,AndreaBommert,FlorianPfisterer,JörgRahnenführer,MichelLang,BerndBischl","        A novel machine learning optimization process coined Restrictive Federated Model Selection (RFMS) is proposed under the scenario, for example, when data from healthcare units can not leave the site it is situated on and it is forbidden to carry out training algorithms on remote data sites due to either technical or privacy and trust concerns. To carry out a clinical research under this scenario, an analyst could train a machine learning model only on local data site, but it is still possible to execute a statistical query at a certain cost in the form of sending a machine learning model to some of the remote data sites and get the performance measures as feedback, maybe due to prediction being usually much cheaper. Compared to federated learning, which is optimizing the model parameters directly by carrying out training across all data sites, RFMS trains model parameters only on one local data site but optimizes hyper-parameters across other data sites jointly since hyper-parameters play an important role in machine learning performance. The aim is to get a Pareto optimal model with respective to both local and remote unseen prediction losses, which could generalize well across data sites. In this work, we specifically consider high dimensional data with shifted distributions over data sites. As an initial investigation, Bayesian Optimization especially multi-objective Bayesian Optimization is used to guide an adaptive hyper-parameter optimization process to select models under the RFMS scenario. Empirical results show that solely using the local data site to tune hyper-parameters generalizes poorly across data sites, compared to methods that utilize the local and remote performances. Furthermore, in terms of dominated hypervolumes, multi-objective Bayesian Optimization algorithms show increased performance across multiple data sites among other candidates.        △ Less","8 August, 2019","cs.LG,stat.ML",
              Automatic Detection of Protective Behavior in Chronic Pain Physical Rehabilitation: A Recurrent Neural Network Approach          ,1902.08990,https://arxiv.org/abs/1902.08990,https://arxiv.org/pdf/1902.08990,"Authors:ChongyangWang,TemitayoA.Olugbade,AkhilMathur,AmandaC.DeC.Williams,NicholasD.Lane,NadiaBianchi-Berthouze","        In chronic pain physical rehabilitation, physiotherapists adapt physical activity to patients' performance especially based on the expression of protective behavior, gradually exposing them to feared but harmless and essential everyday activities. As physical rehabilitation moves outside the clinic, physical rehabilitation technology needs to automatically detect such behavior as to provide similar personalized support. In this paper, we investigate the use of a Long Short-Term Memory (LSTM) network, referred to as stacked-LSTM, to detect events of protective behavior, based on wearable motion capture and electromyography data of healthy and chronic lower-back pain people engaged in five functional activities. Differently from previous works on the same dataset, we aim to continuously detect protective behavior within each activity rather than estimate the overall presence of such behavior. The stacked-LSTM reaches best mean F1 score of 0.815 with Leave-One-Subject-Out validation (LOSO), using low level features. Performances increase for some activities when modelled separately (mean F1 scores: bend-down=0.77, stand-on-one-leg=0.81, sit-to-stand=0.72, stand-to-sit=0.83, reach-forward=0.67). The performance reaches excellent level of agreement with the average experts' rating. As such, the results show clear potential for in-home technology-supported personalized physical rehabilitation.        △ Less","3 May, 2020","cs.HC,cs.AI,cs.LG",
              RAMHU: A New Robust Lightweight Scheme for Mutual Users Authentication in Healthcare Applications          ,1902.08686,https://arxiv.org/abs/1902.08686,https://arxiv.org/pdf/1902.08686,"Authors:MishallAl-Zubaidie,ZhongweiZhang,JiZhang","        Providing a mechanism to authenticate users in healthcare applications is an essential security requirement to prevent both external and internal attackers from penetrating patients' identities and revealing their health data. Many schemes have been developed to provide authentication mechanisms to ensure that only legitimate users are authorized to connect, but these schemes still suffer from vulnerable security. Various attacks expose patients' data for malicious tampering or destruction. Transferring health-related data and information between users and the health centre makes them exposed to penetration by adversaries as they may move through an insecure channel. In addition, previous mechanisms have suffered from the poor protection of users' authentication information. To ensure the protection of patients' information and data, we propose a scheme that authenticates users based on the information of both the device and the legitimate user. In this paper, we propose a Robust Authentication Model for Healthcare Users (RAMHU) that provides mutual authentication between server and clients. This model utilizes an Elliptic Curve Integrated Encryption Scheme (ECIES) and PHOTON to achieve strong security and a good overall performance. RAMHU relies on multi pseudonyms, physical address, and one-time password mechanisms to authenticate legitimate users. Moreover, extensive informal and formal security analysis with the automated validation of Internet security protocols and applications (AVISPA) tool demonstrates that our model offers a high level of security in repelling a wide variety of possible attacks.        △ Less","22 February, 2019",cs.CR,
              Inference of a Multi-Domain Machine Learning Model to Predict Mortality in Hospital Stays for Patients with Cancer upon Febrile Neutropenia Onset          ,1902.07839,https://arxiv.org/abs/1902.07839,/search/?searchtype=author&query=Du%2C+X,"Authors:XinsongDu,JaeMin,MattiaProsperi,RohitBishnoi,DominickJ.Lemas,ChintanP.Shah","        Febrile neutropenia (FN) has been associated with high mortality, especially among adults with cancer. Understanding the patient and provider level heterogeneity in FN hospital admissions has potential to inform personalized interventions focused on increasing survival of individuals with FN. We leverage machine learning techniques to disentangling the complex interactions among multi domain risk factors in a population with FN. Data from the Healthcare Cost and Utilization Project (HCUP) National Inpatient Sample and Nationwide Inpatient Sample (NIS) were used to build machine learning based models of mortality for adult cancer patients who were diagnosed with FN during a hospital admission. In particular, the importance of risk factors from different domains (including demographic, clinical, and hospital associated information) was studied. A set of more interpretable (decision tree, logistic regression) as well as more black box (random forest, gradient boosting, neural networks) models were analyzed and compared via multiple cross validation. Our results demonstrate that a linear prediction score of FN mortality among adults with cancer, based on admission information is effective in classifying high risk patients; clinical diagnoses is the domain with the highest predictive power. A number of the risk variables (e.g. sepsis, kidney failure, etc.) identified in this study are clinically actionable and may inform future studies looking at the patients prior medical history are warranted.        △ Less","27 May, 2019","q-bio.QM,cs.LG,stat.ML",
              Causal variance decompositions for institutional comparisons in healthcare,1902.07692,https://arxiv.org/abs/1902.07692,https://arxiv.org/pdf/1902.07692,"Authors:BoChen,KeithA.Lawson,AntonioFinelli,OlliSaarela","        There is increasing interest in comparing institutions delivering healthcare in terms of disease-specific quality indicators (QIs) that capture processes or outcomes showing variations in the care provided. Such comparisons can be framed in terms of causal models, where adjusting for patient case-mix is analogous to controlling for confounding, and exposure is being treated in a given hospital, for instance. Our goal here is to help identifying good QIs rather than comparing hospitals in terms of an already chosen QI, and so we focus on the presence and magnitude of overall variation in care between the hospitals rather than the pairwise differences between any two hospitals. We consider how the observed variation in care received at patient level can be decomposed into that causally explained by the hospital performance adjusting for the case-mix, the case-mix itself, and residual variation. For this purpose, we derive a three-way variance decomposition, with particular attention to its causal interpretation in terms of potential outcome variables. We propose model-based estimators for the decomposition, accommodating different link functions and either fixed or random effect models. We evaluate their performance in a simulation study and demonstrate their use in a real data application.        △ Less","4 September, 2019",stat.ME,10.1177/0962280219880571 
              An entropic feature selection method in perspective of Turing formula          ,1902.07115,https://arxiv.org/abs/1902.07115,https://arxiv.org/pdf/1902.07115,"Authors:JingyiShi,JialinZhang,YaorongGe","        Health data are generally complex in type and small in sample size. Such domain-specific challenges make it difficult to capture information reliably and contribute further to the issue of generalization. To assist the analytics of healthcare datasets, we develop a feature selection method based on the concept of Coverage Adjusted Standardized Mutual Information (CASMI). The main advantages of the proposed method are: 1) it selects features more efficiently with the help of an improved entropy estimator, particularly when the sample size is small, and 2) it automatically learns the number of features to be selected based on the information from sample data. Additionally, the proposed method handles feature redundancy from the perspective of joint-distribution. The proposed method focuses on non-ordinal data, while it works with numerical data with an appropriate binning method. A simulation study comparing the proposed method to six widely cited feature selection methods shows that the proposed method performs better when measured by the Information Recovery Ratio, particularly when the sample size is small.        △ Less","19 February, 2019","cs.LG,cs.IT",
              Cost-Sensitive Diagnosis and Learning Leveraging Public Health Data          ,1902.07102,https://arxiv.org/abs/1902.07102,https://arxiv.org/pdf/1902.07102,"Authors:MohammadKachuee,KimmoKarkkainen,OrpazGoldstein,DavinaZamanzadeh,MajidSarrafzadeh","        Traditionally, machine learning algorithms rely on the assumption that all features of a given dataset are available for free. However, there are many concerns such as monetary data collection costs, patient discomfort in medical procedures, and privacy impacts of data collection that require careful consideration in any real-world health analytics system. An efficient solution would only acquire a subset of features based on the value it provides while considering acquisition costs. Moreover, datasets that provide feature costs are very limited, especially in healthcare. In this paper, we provide a health dataset as well as a method for assigning feature costs based on the total level of inconvenience asking for each feature entails. Furthermore, based on the suggested dataset, we provide a comparison of recent and state-of-the-art approaches to cost-sensitive feature acquisition and learning. Specifically, we analyze the performance of major sensitivity-based and reinforcement learning based methods in the literature on three different problems in the health domain, including diabetes, heart disease, and hypertension classification.        △ Less","30 June, 2019","cs.LG,cs.AI,cs.CY,stat.ML",
              In Vivo Communications: Steps Toward the Next Generation of Implantable Devices          ,1902.06860,https://arxiv.org/abs/1902.06860,https://arxiv.org/pdf/1902.06860,"Authors:AliFatihDemir,Z.EsatAnkarali,QammerH.Abbasi,YangLiu,KhalidQaraqe,ErchinSerpedin,HuseyinArslan,RichardD.Gitlin","        In vivo wireless medical devices have the potential to play a vital role in future healthcare technologies by improving the quality of human life. In order to fully exploit the capabilities of such devices, it is necessary to characterize and model the in vivo wireless communication channel. Utilization of this model will have a significant role in improving the communication performance of embedded medical devices in terms of power, reliability and spectral efficiency. In this paper, the state of the art in this field is presented to provide a comprehensive understanding of current models. Such knowledge will be used to optimize the design and selection of various in vivo wireless communication methods, operational frequencies, and antenna design. Finally, open research areas are discussed for future studies.        △ Less","18 February, 2019",eess.SP,10.1109/MVT.2016.2520492 
"              BYOD, Personal Area Networks (PANs) and IOT: Threats to Patients Privacy          ",1902.06462,https://arxiv.org/abs/1902.06462,https://arxiv.org/pdf/1902.06462,Authors:SamaraAhmed,"        The passage of FISMA and HIPPA Acts have mandated various security controls that ensure the privacy of patients data. Hospitals and health-care organizations are required by law to ensure that patients data is stored and disseminated in a secure fashion. The advent of Bring Your Own Devices (BYOD), mobile devices, instant messaging (such as WhatsApp) and cloud technology however, have brought forth new challenges. The advent of Internet of Things (IOT) have complicated the matters further as organizations are not fully cognizant to the all facets of threats to data privacy. Physicians and health care practitioners need to be made aware of various new avenues of data storage and transmission that need to be secured and controlled. In this paper we look at various threats and challenges that IOT, Bring Your Own Device (BYOD) and Personal Area Networks (PANs) technologies pose to the patients privacy data. We conclude the paper by providing the results of a survey that gauge the depth of understanding of healthcare professionals regarding the emerging threats to patients privacy        △ Less","18 February, 2019","cs.CY,cs.CR",
              Anatomical Region-Specific In Vivo Wireless Communication Channel Characterization          ,1902.05990,https://arxiv.org/abs/1902.05990,https://arxiv.org/pdf/1902.05990,"Authors:AliFatihDemir,Q.H.Abbasi,Z.E.Ankarali,A.Alomainy,K.Qaraqe,E.Serpedin,H.Arslan","        In vivo wireless body area networks (WBANs) and their associated technologies are shaping the future of healthcare by providing continuous health monitoring and noninvasive surgical capabilities, in addition to remote diagnostic and treatment of diseases. To fully exploit the potential of such devices, it is necessary to characterize the communication channel which will help to build reliable and high-performance communication systems. This paper presents an in vivo wireless communication channel characterization for male torso both numerically and experimentally (on a human cadaver) considering various organs at 915 MHz and 2.4 GHz. A statistical path loss (PL) model is introduced, and the anatomical region-specific parameters are provided. It is found that the mean PL in dB scale exhibits a linear decaying characteristic rather than an exponential decaying profile inside the body, and the power decay rate is approximately twice at 2.4 GHz as compared to 915 MHz. Moreover, the variance of shadowing increases significantly as the in vivo antenna is placed deeper inside the body since the main scatterers are present in the vicinity of the antenna. Multipath propagation characteristics are also investigated to facilitate proper waveform designs in the future wireless healthcare systems, and a root-mean-square (RMS) delay spread of 2.76 ns is observed at 5 cm depth. Results show that the in vivo channel exhibit different characteristics than the classical communication channels, and location dependency is very critical for accurate, reliable, and energy-efficient link budget calculations.        △ Less","15 February, 2019",eess.SP,10.1109/JBHI.2016.2618890 
              KINN: Incorporating Expert Knowledge in Neural Networks          ,1902.05653,https://arxiv.org/abs/1902.05653,https://arxiv.org/pdf/1902.05653,"Authors:MuhammadAliChattha,ShoaibAhmedSiddiqui,MuhammadImranMalik,LudgervanElst,AndreasDengel,SherazAhmed","        The promise of ANNs to automatically discover and extract useful features/patterns from data without dwelling on domain expertise although seems highly promising but comes at the cost of high reliance on large amount of accurately labeled data, which is often hard to acquire and formulate especially in time-series domains like anomaly detection, natural disaster management, predictive maintenance and healthcare. As these networks completely rely on data and ignore a very important modality i.e. expert, they are unable to harvest any benefit from the expert knowledge, which in many cases is very useful. In this paper, we try to bridge the gap between these data driven and expert knowledge based systems by introducing a novel framework for incorporating expert knowledge into the network (KINN). Integrating expert knowledge into the network has three key advantages: (a) Reduction in the amount of data needed to train the model, (b) provision of a lower bound on the performance of the resulting classifier by obtaining the best of both worlds, and (c) improved convergence of model parameters (model converges in smaller number of epochs). Although experts are extremely good in solving different tasks, there are some trends and patterns, which are usually hidden only in the data. Therefore, KINN employs a novel residual knowledge incorporation scheme, which can automatically determine the quality of the predictions made by the expert and rectify it accordingly by learning the trends/patterns from data. Specifically, the method tries to use information contained in one modality to complement information missed by the other. We evaluated KINN on a real world traffic flow prediction problem. KINN significantly superseded performance of both the expert and as well as the base network (LSTM in this case) when evaluated in isolation, highlighting its superiority for the task.        △ Less","14 February, 2019","cs.LG,stat.ML",
              Structured Bayesian Compression for Deep models in mobile enabled devices for connected healthcare,1902.05429,https://arxiv.org/abs/1902.05429,https://arxiv.org/pdf/1902.05429,"Authors:SijiaChen,BinSong,XiaojiangDu,NadraGuizani","        Deep Models, typically Deep neural networks, have millions of parameters, analyze medical data accurately, yet in a time-consuming method. However, energy cost effectiveness and computational efficiency are important for prerequisites developing and deploying mobile-enabled devices, the mainstream trend in connected healthcare.        △ Less","13 February, 2019",cs.CV,
              QoS-Aware Buffer-Aided Relaying Implant WBAN for Healthcare IoT: Opportunities and Challenges          ,1902.04443,https://arxiv.org/abs/1902.04443,https://arxiv.org/pdf/1902.04443,"Authors:GuofaCai,YiFang,JinmingWen,GuojunHan,XiaodongYang","        Internet of Things (IoT) have motivated a paradigm shift in the development of various applications such as mobile health. Wireless body area network (WBAN) comprises many low-power devices in, on, or around the human body, which offers a desirable solution to monitor physiological signals for mobile-health applications. In the implant WBAN, an implant medical device transmits its measured biological parameters to a target hub with the help of at least one on-body device(s) to satisfy its strict requirements on size, quality of service (QoS, e.g., reliability), and power consumption. In this article, we first review the recent advances of conventional cooperative WBAN. Afterwards, to address the drawbacks of the conventional cooperative WBAN, a QoS-aware buffer-aided relaying framework is proposed for the implant WBAN. In the proposed framework, hierarchical modulations are considered to fulfill the different QoS requirements of different sensor data from an implant medical device. We further conceive some new transmission strategies for the buffer-aided signal-relay and multi-relay implant WBANs. Simulation results show that the proposed cooperative WBAN provides better performance than the conventional cooperative counterparts. Finally, some open research challenges regarding the buffer-aided multi-relay implant WBAN are pointed out to inspire more research activities.        △ Less","12 February, 2019",cs.NI,
              Prediction of Malignant & Benign Breast Cancer: A Data Mining Approach in Healthcare Applications          ,1902.03825,https://arxiv.org/abs/1902.03825,https://arxiv.org/pdf/1902.03825,"Authors:VivekKumar,BrojoKishoreMishra,ManuelMazzara,DangN.H.Thanh,AbhishekVerma","        As much as data science is playing a pivotal role everywhere, healthcare also finds it prominent application. Breast Cancer is the top rated type of cancer amongst women; which took away 627,000 lives alone. This high mortality rate due to breast cancer does need attention, for early detection so that prevention can be done in time. As a potential contributor to state-of-art technology development, data mining finds a multi-fold application in predicting Brest cancer. This work focuses on different classification techniques implementation for data mining in predicting malignant and benign breast cancer. Breast Cancer Wisconsin data set from the UCI repository has been used as experimental dataset while attribute clump thickness being used as an evaluation class. The performances of these twelve algorithms: Ada Boost M 1, Decision Table, J Rip, Lazy IBK, Logistics Regression, Multiclass Classifier, Multilayer Perceptron, Naive Bayes, Random forest and Random Tree are analyzed on this data set. Keywords- Data Mining, Classification Techniques, UCI repository, Breast Cancer, Classification Algorithms        △ Less","23 February, 2019","cs.LG,cs.CY,stat.ML",
              A Novel Secure Authentication Scheme for Heterogeneous Internet of Thing          ,1902.03562,https://arxiv.org/abs/1902.03562,https://arxiv.org/pdf/1902.03562,"Authors:JingweiLiu,AilianRen,LihuanZhang,RongSun,XiaojiangDu,MohsenGuizani","        Today, Internet of Things (IoT) technology is being increasingly popular which is applied in a wide range of industry sectors such as healthcare, transportation and some critical infrastructures. With the widespread applications of IoT technology, people's lives have changed dramatically. Due to its capabilities of sensitive data-aware, information collection, communication and processing, it raises security and privacy concerns. Moreover, a malicious attacker may impersonate a legitimate user, which may cause security threat and violation privacy. In allusion to the above problems, we propose a novel and lightweight anonymous authentication and key agreement scheme for heterogeneous IoT, which is innovatively designed to shift between the public key infrastructure (PKI) and certificateless cryptography (CLC) environment. The proposed scheme not only achieves secure communication among the legal authorized users, but also possesses more attributes with user anonymity, non-repudiation and key agreement fairness. Through the security analysis, it is proved that the proposed scheme can resist replay attacks and denial of service (DOS) attacks. Finally, the performance evaluation demonstrates that our scheme is more lightweight and innovative.        △ Less","10 February, 2019",cs.CR,
              Human Computer Interaction Design for Mobile Devices Based on a Smart Healthcare Architecture          ,1902.03541,https://arxiv.org/abs/1902.03541,https://arxiv.org/pdf/1902.03541,"Authors:PuLiu,SidneyFels,NicholasWest,MatthiasGörges","        Smart and IoT-enabled mobile devices have the potential to enhance healthcare services for both patients and healthcare providers. Human computer interaction design is key to realizing a useful and usable connection between the users and these smart healthcare technologies. Appropriate design of such devices enhances the usability, improves effective operation in an integrated healthcare system, and facilitates the collaboration and information sharing between patients, healthcare providers, and institutions. In this paper, the concept of smart healthcare is introduced, including its four-layer information architecture of sensing, communication, data integration, and application. Human Computer Interaction design principles for smart healthcare mobile devices are outlined, based on user-centered design. These include: ensuring safety, providing error-resistant displays and alarms, supporting the unique relationship between patients and healthcare providers, distinguishing end-user groups, accommodating legacy devices, guaranteeing low latency, allowing for personalization, and ensuring patient privacy. Results are synthesized in design suggestions ranging from personas, scenarios, workflow, and information architecture, to prototyping, testing and iterative development. Finally, future developments in smart healthcare and Human Computer Interaction design for mobile health devices are outlined.        △ Less","10 February, 2019","cs.HC,cs.CY",
              Measuring Patient Similarities via a Deep Architecture with Medical Concept Embedding          ,1902.03376,https://arxiv.org/abs/1902.03376,https://arxiv.org/pdf/1902.03376,"Authors:ZihaoZhu,ChangchangYin,BuyueQian,YuCheng,JishangWei,FeiWang","        Evaluating the clinical similarities between pairwise patients is a fundamental problem in healthcare informatics. A proper patient similarity measure enables various downstream applications, such as cohort study and treatment comparative effectiveness research. One major carrier for conducting patient similarity research is Electronic Health Records(EHRs), which are usually heterogeneous, longitudinal, and sparse. Though existing studies on learning patient similarity from EHRs have shown being useful in solving real clinical problems, their applicability is limited due to the lack of medical interpretations. Moreover, most previous methods assume a vector-based representation for patients, which typically requires aggregation of medical events over a certain time period. As a consequence, temporal information will be lost. In this paper, we propose a patient similarity evaluation framework based on the temporal matching of longitudinal patient EHRs. Two efficient methods are presented, unsupervised and supervised, both of which preserve the temporal properties in EHRs. The supervised scheme takes a convolutional neural network architecture and learns an optimal representation of patient clinical records with medical concept embedding. The empirical results on real-world clinical data demonstrate substantial improvement over the baselines. We make our code and sample data available for further study.        △ Less","9 February, 2019","stat.ML,cs.AI,cs.LG",
              Skin Lesion Analysis Toward Melanoma Detection 2018: A Challenge Hosted by the International Skin Imaging Collaboration (ISIC)          ,1902.03368,https://arxiv.org/abs/1902.03368,https://arxiv.org/pdf/1902.03368,"Authors:NoelCodella,VeronicaRotemberg,PhilippTschandl,M.EmreCelebi,StephenDusza,DavidGutman,BrianHelba,AadiKalloo,KonstantinosLiopyris,MichaelMarchetti,HaraldKittler,AllanHalpern","        This work summarizes the results of the largest skin image analysis challenge in the world, hosted by the International Skin Imaging Collaboration (ISIC), a global partnership that has organized the world's largest public repository of dermoscopic images of skin. The challenge was hosted in 2018 at the Medical Image Computing and Computer Assisted Intervention (MICCAI) conference in Granada, Spain. The dataset included over 12,500 images across 3 tasks. 900 users registered for data download, 115 submitted to the lesion segmentation task, 25 submitted to the lesion attribute detection task, and 159 submitted to the disease classification task. Novel evaluation protocols were established, including a new test for segmentation algorithm performance, and a test for algorithm ability to generalize. Results show that top segmentation algorithms still fail on over 10% of images on average, and algorithms with equal performance on test data can have different abilities to generalize. This is an important consideration for agencies regulating the growing set of machine learning tools in the healthcare domain, and sets a new standard for future public challenges in healthcare.        △ Less","29 March, 2019",cs.CV,
              PASTA: A Parallel Sparse Tensor Algorithm Benchmark Suite          ,1902.03317,https://arxiv.org/abs/1902.03317,https://arxiv.org/pdf/1902.03317,"Authors:JiajiaLi,YuchenMa,XiaolongWu,AngLi,KevinBarker","        Tensor methods have gained increasingly attention from various applications, including machine learning, quantum chemistry, healthcare analytics, social network analysis, data mining, and signal processing, to name a few. Sparse tensors and their algorithms become critical to further improve the performance of these methods and enhance the interpretability of their output. This work presents a sparse tensor algorithm benchmark suite (PASTA) for single- and multi-core CPUs. To the best of our knowledge, this is the first benchmark suite for sparse tensor world. PASTA targets on: 1) helping application users to evaluate different computer systems using its representative computational workloads; 2) providing insights to better utilize existed computer architecture and systems and inspiration for the future design. This benchmark suite is publicly released https://gitlab.com/tensorworld/pasta.        △ Less","8 February, 2019",cs.DC,
"              Does the ""Artificial Intelligence Clinician"" learn optimal treatment strategies for sepsis in intensive care?          ",1902.03271,https://arxiv.org/abs/1902.03271,https://arxiv.org/pdf/1902.03271,"Authors:RussellJeter,ChristopherJosef,SupreethShashikumar,ShamimNemati","        From 2017 to 2018 the number of scientific publications found via PubMed search using the keyword ""Machine Learning"" increased by 46% (4,317 to 6,307). The results of studies involving machine learning, artificial intelligence (AI), and big data have captured the attention of healthcare practitioners, healthcare managers, and the public at a time when Western medicine grapples with unmitigated cost increases and public demands for accountability. The complexity involved in healthcare applications of machine learning and the size of the associated data sets has afforded many researchers an uncontested opportunity to satisfy these demands with relatively little oversight. In a recent Nature Medicine article, ""The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care,"" Komorowski and his coauthors propose methods to train an artificial intelligence clinician to treat sepsis patients with vasopressors and IV fluids. In this post, we will closely examine the claims laid out in this paper. In particular, we will study the individual treatment profiles suggested by their AI Clinician to gain insight into how their AI Clinician intends to treat patients on an individual level.        △ Less","8 February, 2019","cs.AI,stat.AP",
              A Bayesian Deep Learning Framework for End-To-End Prediction of Emotion from Heartbeat          ,1902.03043,https://arxiv.org/abs/1902.03043,https://arxiv.org/pdf/1902.03043,"Authors:RossHarper,JoshuaSouthern","        Automatic prediction of emotion promises to revolutionise human-computer interaction. Recent trends involve fusion of multiple data modalities - audio, visual, and physiological - to classify emotional state. However, in practice, collection of physiological data `in the wild' is currently limited to heartbeat time series of the kind generated by affordable wearable heart monitors. Furthermore, real-world applications of emotion prediction often require some measure of uncertainty over model output, in order to inform downstream decision-making. We present here an end-to-end deep learning model for classifying emotional valence from unimodal heartbeat time series. We further propose a Bayesian framework for modelling uncertainty over these valence predictions, and describe a probabilistic procedure for choosing to accept or reject model output according to the intended application. We benchmarked our framework against two established datasets and achieved peak classification accuracy of 90%. These results lay the foundation for applications of affective computing in real-world domains such as healthcare, where a high premium is placed on non-invasive collection of data, and predictive certainty.        △ Less","19 April, 2020","cs.LG,cs.HC,stat.ML",10.1109/TAFFC.2020.2981610 
              Mobile Artificial Intelligence Technology for Detecting Macula Edema and Subretinal Fluid on OCT Scans: Initial Results from the DATUM alpha Study          ,1902.02905,https://arxiv.org/abs/1902.02905,https://arxiv.org/pdf/1902.02905,"Authors:StephenG.Odaibo,MikelsonMomPremier,RichardY.Hwang,SalmanJ.Yousuf,StevenL.Williams,JoshuaGrant","        Artificial Intelligence (AI) is necessary to address the large and growing deficit in retina and healthcare access globally. And mobile AI diagnostic platforms running in the Cloud may effectively and efficiently distribute such AI capability. Here we sought to evaluate the feasibility of Cloud-based mobile artificial intelligence for detection of retinal disease. And to evaluate the accuracy of a particular such system for detection of subretinal fluid (SRF) and macula edema (ME) on OCT scans. A multicenter retrospective image analysis was conducted in which board-certified ophthalmologists with fellowship training in retina evaluated OCT images of the macula. They noted the presence or absence of ME or SRF, then compared their assessment to that obtained from Fluid Intelligence, a mobile AI app that detects SRF and ME on OCT scans. Investigators consecutively selected retinal OCTs, while making effort to balance the number of scans with retinal fluid and scans without. Exclusion criteria included poor scan quality, ambiguous features, macula holes, retinoschisis, and dense epiretinal membranes. Accuracy in the form of sensitivity and specificity of the AI mobile App was determined by comparing its assessments to those of the retina specialists. At the time of this submission, five centers have completed their initial studies. This consists of a total of 283 OCT scans of which 155 had either ME or SRF (""wet"") and 128 did not (""dry""). The sensitivity ranged from 82.5% to 97% with a weighted average of 89.3%. The specificity ranged from 52% to 100% with a weighted average of 81.23%. CONCLUSION: Cloud-based Mobile AI technology is feasible for the detection retinal disease. In particular, Fluid Intelligence (alpha version), is sufficiently accurate as a screening tool for SRF and ME, especially in underserved areas. Further studies and technology development is needed.        △ Less","12 February, 2019","physics.med-ph,cs.AI,cs.CV,cs.LG,q-bio.NC",
              Achieving Data Utility-Privacy Tradeoff in Internet of Medical Things: A Machine Learning Approach          ,1902.02898,https://arxiv.org/abs/1902.02898,https://arxiv.org/pdf/1902.02898,"Authors:ZhitaoGuan,ZefangLv,XiaojiangDu,LongfeiWu,MohsenGuizani","        The emergence and rapid development of the Internet of Medical Things (IoMT), an application of the Internet of Things into the medical and healthcare systems, have brought many changes and challenges to modern medical and healthcare systems. Particularly, machine learning technology can be used to process the data involved in IoMT for medical analysis and disease diagnosis. However, in this process, the disclosure of personal privacy information must receive considerable attentions especially for sensitive medical data. Cluster analysis is an important technique for medical analysis and disease diagnosis. To enable privacy-preserving cluster analysis in IoMT, this paper proposed an Efficient Differentially Private Data Clustering scheme (EDPDCS) based on MapReduce framework. In EDPDCS, we optimize the allocation of privacy budgets and the selection of initial centroids to improve the accuracy of differentially private K-means clustering algorithm. Specifically, the number of iterations of the K-means algorithm is set to a fixed value according to the total privacy budget and the minimal privacy budget of each iteration. In addition, an improved initial centroids selection method is proposed to increase the accuracy and efficiency of the clustering algorithm. Finally, we prove that the proposed EDPDCS can improve the accuracy of the differentially private k-means algorithm by comparing the Normalized Intra-Cluster Variance (NICV) produced by our algorithm on two datasets with two other algorithms.        △ Less","7 February, 2019",cs.CR,
              Bidirectional Inference Networks: A Class of Deep Bayesian Networks for Health Profiling          ,1902.02037,https://arxiv.org/abs/1902.02037,https://arxiv.org/pdf/1902.02037,"Authors:HaoWang,ChengzhiMao,HaoHe,MingminZhao,TommiS.Jaakkola,DinaKatabi","        We consider the problem of inferring the values of an arbitrary set of variables (e.g., risk of diseases) given other observed variables (e.g., symptoms and diagnosed diseases) and high-dimensional signals (e.g., MRI images or EEG). This is a common problem in healthcare since variables of interest often differ for different patients. Existing methods including Bayesian networks and structured prediction either do not incorporate high-dimensional signals or fail to model conditional dependencies among variables. To address these issues, we propose bidirectional inference networks (BIN), which stich together multiple probabilistic neural networks, each modeling a conditional dependency. Predictions are then made via iteratively updating variables using backpropagation (BP) to maximize corresponding posterior probability. Furthermore, we extend BIN to composite BIN (CBIN), which involves the iterative prediction process in the training stage and improves both accuracy and computational efficiency by adaptively smoothing the optimization landscape. Experiments on synthetic and real-world datasets (a sleep study and a dermatology dataset) show that CBIN is a single model that can achieve state-of-the-art performance and obtain better accuracy in most inference tasks than multiple models each specifically trained for a different task.        △ Less","6 February, 2019","stat.ML,cs.AI,cs.CV,cs.LG",
              Detection of virulence factors and beta-lactamase encoding genes among the clinical isolates of Pseudomonas aeruginosa          ,1902.02014,https://arxiv.org/abs/1902.02014,https://arxiv.org/pdf/1902.02014,"Authors:FazlulMKK,NajninA,FarzanaY,RashidMA,DeepthiS,SrikumarC,SSRashid,NazmulMHM","        Background: Pseudomonas aeruginosa has emerged as a significant opportunistic bacterial pathogen that causes nosocomial infections in healthcare settings resulting in treatment failure throughout the world. This study was carried out to compare the relatedness between virulence characteristics and \b{eta}-lactamase encoding genes producing Pseudomonas aeruginosa. Methods: A total of 120 P. aeruginosa isolates were obtained from both paediatric and adult patients of Selayang Hospital, Kuala Lumpur, Malaysia. Phenotypic methods were used to detect various virulence factors (Phospholipase, Hemolysin, Gelatinase, DNAse, and Biofilm). All the isolates were evaluated for production of extended spectrum beta-lactamase (ESBL) as well as metallo \b{eta}-lactamase (MBL) by Double-disk synergy test (DDST) and E-test while AmpC \b{eta}-lactamase production was detected by disk antagonism test. Results: In this study, 120 Pseudomonas aeruginosa isolates (20 each from blood, wounds, respiratory secretions, stools, urine, and sputum samples) were studied. Among Pseudomonas aeruginosa isolates, the distribution of virulence factors was positive for hemolysin (48.33%), DNAse (43.33%), phospholipase (40.83%), gelatinase (31.66%) production and biofilm formation (34%) respectively. The prevalence of multiple \b{eta}-lactamase in P. aeruginosa showed 19.16% ESBL, 7.5% MBL and 10.83% AmpC production respectively. Conclusion: A regular surveillance is required to reduce public healt        △ Less","5 February, 2019",q-bio.OT,10.31838/ijpr/2019.11.01.031 
              Design and Implementation of Location and Activity Monitoring System Based on LoRa          ,1902.01947,https://arxiv.org/abs/1902.01947,https://arxiv.org/pdf/1902.01947,"Authors:ShengweiLin,ZiqiangYing,KanZheng","        The location and human activity are usually used as one of the important parameters to monitor the health status in healthcare devices. However, nearly all existing location and monitoring systems have the limitation of short-range communication and high power consumption. In this paper, we propose a new mechanism to collect and transmit monitoring information based on LoRa technology. The monitoring device with sensors can collect the real-time activity and location information and transmit them to the cloud server through LoRa gateway. The user can check all his history and current information through the specific designed mobile applications. Experiment was carried out to verify the communication, power consumption and monitoring performance of the entire system. Experimental results demonstrate that this system can collect monitoring and activity information accurately and provide the long rang coverage with low power consumption.        △ Less","27 January, 2019",cs.NI,
              CMS Sematrix: A Tool to Aid the Development of Clinical Quality Measures (CQMs)          ,1902.01918,https://arxiv.org/abs/1902.01918,https://arxiv.org/pdf/1902.01918,"Authors:MichaelA.Schwemmer,Po-HsuChen,MithunBalakrishna,AmyLeibrand,AaronLeonard,NancyJ.McMillan,JeffreyJ.Geppert","        As part of the effort to improve quality and to reduce national healthcare costs, the Centers for Medicare and Medicaid Services (CMS) are responsible for creating and maintaining an array of clinical quality measures (CQMs) for assessing healthcare structure, process, outcome, and patient experience across various conditions, clinical specialties, and settings. The development and maintenance of CQMs involves substantial and ongoing evaluation of the evidence on the measure's properties: importance, reliability, validity, feasibility, and usability. As such, CMS conducts monthly environmental scans of the published clinical and health service literature. Conducting time consuming, exhaustive evaluations of the ever-changing healthcare literature presents one of the largest challenges to an evidence-based approach to healthcare quality improvement. Thus, it is imperative to leverage automated techniques to aid CMS in the identification of clinical and health services literature relevant to CQMs. Additionally, the estimated labor hours and related cost savings of using CMS Sematrix compared to a traditional literature review are roughly 818 hours and 122,000 dollars for a single monthly environmental scan.        △ Less","5 February, 2019","stat.OT,cs.CL",
              Early Recognition of Sepsis with Gaussian Process Temporal Convolutional Networks and Dynamic Time Warping          ,1902.01659,https://arxiv.org/abs/1902.01659,https://arxiv.org/pdf/1902.01659,"Authors:MichaelMoor,MaxHorn,BastianRieck,DamianRoqueiro,KarstenBorgwardt","        Sepsis is a life-threatening host response to infection associated with high mortality, morbidity, and health costs. Its management is highly time-sensitive since each hour of delayed treatment increases mortality due to irreversible organ damage. Meanwhile, despite decades of clinical research, robust biomarkers for sepsis are missing. Therefore, detecting sepsis early by utilizing the affluence of high-resolution intensive care records has become a challenging machine learning problem. Recent advances in deep learning and data mining promise to deliver a powerful set of tools to efficiently address this task. This empirical study proposes two novel approaches for the early detection of sepsis: a deep learning model and a lazy learner based on time series distances. Our deep learning model employs a temporal convolutional network that is embedded in a Multi-task Gaussian Process Adapter framework, making it directly applicable to irregularly-spaced time series data. Our lazy learner, by contrast, is an ensemble approach that employs dynamic time warping. We frame the timely detection of sepsis as a supervised time series classification task. For this, we derive the most recent sepsis definition in an hourly resolution to provide the first fully accessible early sepsis detection environment. Seven hours before sepsis onset, our methods improve area under the precision--recall curve from 0.25 to 0.35/0.40 over the state of the art. This demonstrates that they are well-suited for detecting sepsis in the crucial earlier stages when management is most effective.        △ Less","15 October, 2020","cs.LG,stat.AP,stat.ML",
              Active Image Synthesis for Efficient Labeling          ,1902.01522,https://arxiv.org/abs/1902.01522,https://arxiv.org/pdf/1902.01522,"Authors:JialeiChen,YujiaXie,KanWang,ChuckZhang,ManiA.Vannan,BenWang,ZhenQian","        The great success achieved by deep neural networks attracts increasing attention from the manufacturing and healthcare communities. However, the limited availability of data and high costs of data collection are the major challenges for the applications in those fields. We propose in this work AISEL, an active image synthesis method for efficient labeling to improve the performance of the small-data learning tasks. Specifically, a complementary AISEL dataset is generated, with labels actively acquired via a physics-based method to incorporate underlining physical knowledge at hand. An important component of our AISEL method is the bidirectional generative invertible network (GIN), which can extract interpretable features from the training images and generate physically meaningful virtual images. Our AISEL method then efficiently samples virtual images not only further exploits the uncertain regions, but also explores the entire image space. We then discuss the interpretability of GIN both theoretically and experimentally, demonstrating clear visual improvements over the benchmarks. Finally, we demonstrate the effectiveness of our AISEL framework on aortic stenosis application, in which our method lower the labeling cost by 90%90\% while achieving a 15%15\% improvement in prediction accuracy.        △ Less","4 August, 2020",cs.CV,
              Improved Visualisation of Patient-Specific Heart Structure Using Three-Dimensional Printing Coupled with Image-Processing Techniques Inspired by Astrophysical Methods          ,1902.01233,https://arxiv.org/abs/1902.01233,https://arxiv.org/pdf/1902.01233,"Authors:I.Brewis,J.A.McLaughlin","        The aim of this study is to use image-processing techniques developed in the field of astrophysics as inspiration for a novel approach to the three-dimensional (3D) imaging of periprocedural medical data, with the intention of providing improved visualisation of patient-specific heart structure and thereby allowing for an improved quality of procedural planning with regards to individualized cardiovascular healthcare. Using anonymized patient DICOM data for a cardiac computed tomography (CT) angiography, two-dimensional slices of the patient's heart were processed using a series of software packages in order to produce an accurate 3D representation of the patient's heart tissue as a computer-generated stereolithography (STL) file, followed by the creation of a tactile 3D printout. We find that the models produced provide clear definition of heart structure, in particular in the left atrium, left ventricle and aorta. This level of clarity also allows for the aortic valve to be observed and 3D printed. This study provides a step-by-step blueprint of how this can be achieved using open source software, specifically Slicer 4.8.1, MeshLab and AutoDesk Netfabb. In addition, the implementation of astrophysical image-processing techniques shows an improvement in modelling of the heart based on the CT data, in particular in the case of small-scale features where echocardiography has previously been required for more reliable results.        △ Less","1 February, 2019",physics.med-ph,10.1166/jmihi.2019.2644 
              Model reduction methodology for computational simulations of endovascular repair          ,1902.01071,https://arxiv.org/abs/1902.01071,https://arxiv.org/pdf/1902.01071,"Authors:V.AcostaSantamaría,G.Daniel,D.Perrin,J.Albertini,E.Rosset,S.Avril","        Endovascular aneurysm repair (EVAR) is a current alternative treatment for thoracic and abdominal aortic aneurysms, but is still sometimes compromised by possible complications such as device migration or endoleaks. In order to assist clinicians in preventing these complications, finite element analysis (FEA) is a promising tool. However, the strong material and geometrical nonlinearities added to the complex multiple contacts result in costly finite-element models. To reduce this computational cost, we establish here an alternative and systematic methodology to simplify the computational simulations of stent-grafts (SG) based on FEA. The model reduction methodology relies on equivalent shell models with appropriate geometrical and mechanical parameters. It simplifies significantly the contact interactions but still shows very good agreement with a complete reference finite-element model. Finally, the computational time for EVAR simulations is reduced of a factor 6 to 10. An application is shown for the deployment of a SG during thoracic endovascular repair, showing that the developed methodology is both effective and accurate to determine the final position of the deployed SG inside the aneurysm.        △ Less","4 February, 2019",physics.med-ph,
              Learning Counterfactual Representations for Estimating Individual Dose-Response Curves          ,1902.00981,https://arxiv.org/abs/1902.00981,https://arxiv.org/pdf/1902.00981,"Authors:PatrickSchwab,LorenzLinhardt,StefanBauer,JoachimM.Buhmann,WalterKarlen","        Estimating what would be an individual's potential response to varying levels of exposure to a treatment is of high practical relevance for several important fields, such as healthcare, economics and public policy. However, existing methods for learning to estimate counterfactual outcomes from observational data are either focused on estimating average dose-response curves, or limited to settings with only two treatments that do not have an associated dosage parameter. Here, we present a novel machine-learning approach towards learning counterfactual representations for estimating individual dose-response curves for any number of treatments with continuous dosage parameters with neural networks. Building on the established potential outcomes framework, we introduce performance metrics, model selection criteria, model architectures, and open benchmarks for estimating individual dose-response curves. Our experiments show that the methods developed in this work set a new state-of-the-art in estimating individual dose-response.        △ Less","27 May, 2019","cs.LG,stat.ML",
"              Natural Language Processing, Sentiment Analysis and Clinical Analytics          ",1902.00679,https://arxiv.org/abs/1902.00679,https://arxiv.org/pdf/1902.00679,Authors:AdilRajput,"        Recent advances in Big Data has prompted health care practitioners to utilize the data available on social media to discern sentiment and emotions expression. Health Informatics and Clinical Analytics depend heavily on information gathered from diverse sources. Traditionally, a healthcare practitioner will ask a patient to fill out a questionnaire that will form the basis of diagnosing the medical condition. However, medical practitioners have access to many sources of data including the patients writings on various media. Natural Language Processing (NLP) allows researchers to gather such data and analyze it to glean the underlying meaning of such writings. The field of sentiment analysis (applied to many other domains) depend heavily on techniques utilized by NLP. This work will look into various prevalent theories underlying the NLP field and how they can be leveraged to gather users sentiments on social media. Such sentiments can be culled over a period of time thus minimizing the errors introduced by data input and other stressors. Furthermore, we look at some applications of sentiment analysis and application of NLP to mental health. The reader will also learn about the NLTK toolkit that implements various NLP theories and how they can make the data scavenging process a lot easier.        △ Less","2 February, 2019",cs.CL,
              Characterizing IOMT/Personal Area Networks Landscape          ,1902.00675,https://arxiv.org/abs/1902.00675,https://arxiv.org/pdf/1902.00675,"Authors:AdilRajput,TayebBrahimi","        The attention garnered by the Internet of Things (IoT) term has taken the world by a storm. IOT concepts have been applied to various domains revolutionizing the way business processes are conducted. Health Informatics is a nascent multidisciplinary field that aims at applying information engineering concepts to healthcare. The information traditionally came from a variety of sources such as healthcare IT systems but recently is being stored in variety of IOT devices. Application of IOT concepts is becoming the norm giving rise to the Internet of Medical Things (IOMT). However, IOMT introduces many challenges such as the real-time nature, security, and privacy of data along with others. Thus, understanding the underlying structure of IOMT is of paramount importance. Given the importance of the underlying networks, we explore various solutions currently employed in developing the Personal Area Networks (PANs) that are the underlying cornerstone of IOMT). PANs allow the sensors to measure the change in various stimuli and transmit the information via LANs and WANs to various stakeholders. This chapter will look at three standards that are currently used by both academia and industry along with Body Area Networks (BANs). The chapter also provides a survey of prevalent IOMT applications along with various vendors that provide such services.        △ Less","2 February, 2019",cs.CR,
              An Advanced Conceptual Diagnostic Healthcare Framework for Diabetes and Cardiovascular Disorders          ,1901.10530,https://arxiv.org/abs/1901.10530,https://arxiv.org/pdf/1901.10530,"Authors:M.Sharma,G.Singh,R.Singh","        The data mining along with emerging computing techniques have astonishingly influenced the healthcare industry. Researchers have used different Data Mining and Internet of Things (IoT) for enrooting a programmed solution for diabetes and heart patients. However, still, more advanced and united solution is needed that can offer a therapeutic opinion to individual diabetic and cardio patients. Therefore, here, a smart data mining and IoT (SMDIoT) based advanced healthcare system for proficient diabetes and cardiovascular diseases have been proposed. The hybridization of data mining and IoT with other emerging computing techniques is supposed to give an effective and economical solution to diabetes and cardio patients. SMDIoT hybridized the ideas of data mining, Internet of Things, chatbots, contextual entity search (CES), bio-sensors, semantic analysis and granular computing (GC). The bio-sensors of the proposed system assist in getting the current and precise status of the concerned patients so that in case of an emergency, the needful medical assistance can be provided. The novelty lies in the hybrid framework and the adequate support of chatbots, granular computing, context entity search and semantic analysis. The practical implementation of this system is very challenging and costly. However, it appears to be more operative and economical solution for diabetes and cardio patients.        △ Less","13 January, 2019",cs.CY,10.4108/eai.19-6-2018.154828 
              A Deep Learning Framework for Assessing Physical Rehabilitation Exercises          ,1901.10435,https://arxiv.org/abs/1901.10435,https://arxiv.org/pdf/1901.10435,"Authors:Y.Liao,A.Vakanski,M.Xian","        Computer-aided assessment of physical rehabilitation entails evaluation of patient performance in completing prescribed rehabilitation exercises, based on processing movement data captured with a sensory system. Despite the essential role of rehabilitation assessment toward improved patient outcomes and reduced healthcare costs, existing approaches lack versatility, robustness, and practical relevance. In this paper, we propose a deep learning-based framework for automated assessment of the quality of physical rehabilitation exercises. The main components of the framework are metrics for quantifying movement performance, scoring functions for mapping the performance metrics into numerical scores of movement quality, and deep neural network models for generating quality scores of input movements via supervised learning. The proposed performance metric is defined based on the log-likelihood of a Gaussian mixture model, and encodes low-dimensional data representation obtained with a deep autoencoder network. The proposed deep spatio-temporal neural network arranges data into temporal pyramids, and exploits the spatial characteristics of human movements by using sub-networks to process joint displacements of individual body parts. The presented framework is validated using a dataset of ten rehabilitation exercises. The significance of this work is that it is the first that implements deep neural networks for assessment of rehabilitation performance.        △ Less","22 January, 2020","cs.LG,stat.ML",10.1109/TNSRE.2020.2966249 
              Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples          ,1901.09035,https://arxiv.org/abs/1901.09035,https://arxiv.org/pdf/1901.09035,"Authors:YinpengDong,FanBao,HangSu,JunZhu","        Sometimes it is not enough for a DNN to produce an outcome. For example, in applications such as healthcare, users need to understand the rationale of the decisions. Therefore, it is imperative to develop algorithms to learn models with good interpretability (Doshi-Velez 2017). An important factor that leads to the lack of interpretability of DNNs is the ambiguity of neurons, where a neuron may fire for various unrelated concepts. This work aims to increase the interpretability of DNNs on the whole image space by reducing the ambiguity of neurons. In this paper, we make the following contributions:  1) We propose a metric to evaluate the consistency level of neurons in a network quantitatively.  2) We find that the learned features of neurons are ambiguous by leveraging adversarial examples.  3) We propose to improve the consistency of neurons on adversarial example subset by an adversarial training algorithm with a consistent loss.        △ Less","24 January, 2019","cs.LG,stat.ML",
              ISeeU: Visually interpretable deep learning for mortality prediction inside the ICU          ,1901.08201,https://arxiv.org/abs/1901.08201,https://arxiv.org/pdf/1901.08201,"Authors:WilliamCaicedo-Torres,JairoGutierrez","        To improve the performance of Intensive Care Units (ICUs), the field of bio-statistics has developed scores which try to predict the likelihood of negative outcomes. These help evaluate the effectiveness of treatments and clinical practice, and also help to identify patients with unexpected outcomes. However, they have been shown by several studies to offer sub-optimal performance. Alternatively, Deep Learning offers state of the art capabilities in certain prediction tasks and research suggests deep neural networks are able to outperform traditional techniques. Nevertheless, a main impediment for the adoption of Deep Learning in healthcare is its reduced interpretability, for in this field it is crucial to gain insight on the why of predictions, to assure that models are actually learning relevant features instead of spurious correlations. To address this, we propose a deep multi-scale convolutional architecture trained on the Medical Information Mart for Intensive Care III (MIMIC-III) for mortality prediction, and the use of concepts from coalitional game theory to construct visual explanations aimed to show how important these inputs are deemed by the network. Our results show our model attains state of the art performance while remaining interpretable. Supporting code can be found at https://github.com/williamcaicedo/ISeeU.        △ Less","23 January, 2019","cs.LG,stat.ML",10.1016/j.jbi.2019.103269 
              CREATE: Cohort Retrieval Enhanced by Analysis of Text from Electronic Health Records using OMOP Common Data Model          ,1901.07601,https://arxiv.org/abs/1901.07601,https://arxiv.org/pdf/1901.07601,"Authors:SijiaLiu,YanshanWang,AndrewWen,LiweiWang,NaHong,FeichenShen,StevenBedrick,WilliamHersh,HongfangLiu","        Background: Widespread adoption of electronic health records (EHRs) has enabled secondary use of EHR data for clinical research and healthcare delivery. Natural language processing (NLP) techniques have shown promise in their capability to extract the embedded information in unstructured clinical data, and information retrieval (IR) techniques provide flexible and scalable solutions that can augment the NLP systems for retrieving and ranking relevant records. Methods: In this paper, we present the implementation of Cohort Retrieval Enhanced by Analysis of Text from EHRs (CREATE), a cohort retrieval system that can execute textual cohort selection queries on both structured and unstructured EHR data. CREATE is a proof-of-concept system that leverages a combination of structured queries and IR techniques on NLP results to improve cohort retrieval performance while adopting the Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) to enhance model portability. The NLP component empowered by cTAKES is used to extract CDM concepts from textual queries. We design a hierarchical index in Elasticsearch to support CDM concept search utilizing IR techniques and frameworks. Results: Our case study on 5 cohort identification queries evaluated using the IR metric, P@5 (Precision at 5) at both the patient-level and document-level, demonstrates that CREATE achieves an average P@5 of 0.90, which outperforms systems using only structured data or only unstructured data with average P@5s of 0.54 and 0.74, respectively.        △ Less","22 January, 2019",cs.IR,
              RICERCANDO: Data Mining Toolkit for Mobile Broadband Measurements          ,1901.07287,https://arxiv.org/abs/1901.07287,https://arxiv.org/pdf/1901.07287,"Authors:VeljkoPejovic,IvanMajhen,MihaJanez,BlazZupan","        Increasing reliance on mobile broadband (MBB) networks for communication, vehicle navigation, healthcare, and other critical purposes calls for improved monitoring and troubleshooting of such networks. While recent advances in monitoring with crowdsourced as well as network infrastructure-based methods allow us to tap into a number of performance metrics from all layers of networking, huge swaths of data remain poorly or completely unexplored due to a lack of tools suitable for rapid, interactive, and rigorous MBB data analysis. In this paper we present RICERCANDO, a MBB data mining toolkit developed in a unique collaboration of networking and data mining experts. RICERCANDO consists of a preprocessing module that ensures that time-series data is stored in the most appropriate form for mining, a rapid exploration module that enables iterative analysis of time-series and geomobile data, so that anomalies are detected and singled out, and the advanced mining module that lets the analyst deduce root causes of observed anomalies. We implement and release RICERCANDO as open-source software, and validate its usability on case studies from MONROE pan-European MBB measurement testbed.        △ Less","22 January, 2019",cs.NI,
              Determining Multifunctional Genes and Diseases in Human Using Gene Ontology          ,1901.04847,https://arxiv.org/abs/1901.04847,https://arxiv.org/pdf/1901.04847,"Authors:HishamAl-Mubaid,SasikanthPotu,M.Shenify","        The study of human genes and diseases is very rewarding and can lead to improvements in healthcare, disease diagnostics and drug discovery. In this paper, we further our previous study on gene disease relationship specifically with the multifunctional genes. We investigate the multifunctional gene disease relationship based on the published molecular function annotations of genes from the Gene Ontology which is the most comprehensive source on gene functions.        △ Less","11 January, 2019","q-bio.GN,cs.AI,cs.CE",
              Online Algorithm for Unsupervised Sensor Selection          ,1901.04676,https://arxiv.org/abs/1901.04676,https://arxiv.org/pdf/1901.04676,"Authors:ArunVerma,ManjeshK.Hanawal,CsabaSzepesvári,VenkateshSaligrama","        In many security and healthcare systems, the detection and diagnosis systems use a sequence of sensors/tests. Each test outputs a prediction of the latent state and carries an inherent cost. However, the correctness of the predictions cannot be evaluated since the ground truth annotations may not be available. Our objective is to learn strategies for selecting a test that gives the best trade-off between accuracy and costs in such Unsupervised Sensor Selection (USS) problems. Clearly, learning is feasible only if ground truth can be inferred (explicitly or implicitly) from the problem structure. It is observed that this happens if the problem satisfies the 'Weak Dominance' (WD) property. We set up the USS problem as a stochastic partial monitoring problem and develop an algorithm with sub-linear regret under the WD property. We argue that our algorithm is optimal and evaluate its performance on problem instances generated from synthetic and real-world datasets.        △ Less","4 March, 2019","cs.LG,stat.ML",
              Model Checking Clinical Decision Support Systems Using SMT          ,1901.04545,https://arxiv.org/abs/1901.04545,https://arxiv.org/pdf/1901.04545,"Authors:MohammadHekmatnejad,AndrewM.Simms,GeorgiosFainekos","        Individual clinical Knowledge Artifacts (KA) are designed to be used in Clinical Decision Support (CDS) systems at the point of care for delivery of safe, evidence-based care in modern healthcare systems. For formal authoring of a KA, syntax verification and validation is guaranteed by the grammar. However, there are no methods for semantic verification. Any semantic fallacy may lead to rejection of the outcomes by care providers. As a first step toward solving this problem, we present a framework for translating the logical segments of KAs into Satisfiability Modulo Theory (SMT) models. We present the effectiveness and efficiency of our work by automatically translating the logic fragment of publicly available KAs and verifying them using Z3 SMT solver.        △ Less","4 March, 2019",cs.SE,
              High Throughput Computation of Reference Ranges of Biventricular Cardiac Function on the UK Biobank Population Cohort          ,1901.03326,https://arxiv.org/abs/1901.03326,https://arxiv.org/pdf/1901.03326,"Authors:RahmanAttar,MarcoPereanez,AliGooya,XeniaAlba,LeZhang,StefanK.Piechnik,StefanNeubauer,SteffenE.Petersen,AlejandroF.Frangi","        The exploitation of large-scale population data has the potential to improve healthcare by discovering and understanding patterns and trends within this data. To enable high throughput analysis of cardiac imaging data automatically, a pipeline should comprise quality monitoring of the input images, segmentation of the cardiac structures, assessment of the segmentation quality, and parsing of cardiac functional indexes. We present a fully automatic, high throughput image parsing workflow for the analysis of cardiac MR images, and test its performance on the UK Biobank (UKB) cardiac dataset. The proposed pipeline is capable of performing end-to-end image processing including: data organisation, image quality assessment, shape model initialisation, segmentation, segmentation quality assessment, and functional parameter computation; all without any user interaction. To the best of our knowledge,this is the first paper tackling the fully automatic 3D analysis of the UKB population study, providing reference ranges for all key cardiovascular functional indexes, from both left and right ventricles of the heart. We tested our workflow on a reference cohort of 800 healthy subjects for which manual delineations, and reference functional indexes exist. Our results show statistically significant agreement between the manually obtained reference indexes, and those automatically computed using our framework.        △ Less","10 January, 2019","eess.IV,cs.LG,stat.ML",
              Generalization Studies of Neural Network Models for Cardiac Disease Detection Using Limited Channel ECG          ,1901.03295,https://arxiv.org/abs/1901.03295,https://arxiv.org/pdf/1901.03295,"Authors:DeeptaRajan,DavidBeymer,GirishNarayan","        Acceleration of machine learning research in healthcare is challenged by lack of large annotated and balanced datasets. Furthermore, dealing with measurement inaccuracies and exploiting unsupervised data are considered to be central to improving existing solutions. In particular, a primary objective in predictive modeling is to generalize well to both unseen variations within the observed classes, and unseen classes. In this work, we consider such a challenging problem in machine learning driven diagnosis: detecting a gamut of cardiovascular conditions (e.g. infarction, dysrhythmia etc.) from limited channel ECG measurements. Though deep neural networks have achieved unprecedented success in predictive modeling, they rely solely on discriminative models that can generalize poorly to unseen classes. We argue that unsupervised learning can be utilized to construct effective latent spaces that facilitate better generalization. This work extensively compares the generalization of our proposed approach against a state-of-the-art deep learning solution. Our results show significant improvements in F1-scores.        △ Less","4 January, 2019","eess.SP,cs.LG,stat.ML",
              Variable Importance Clouds: A Way to Explore Variable Importance for the Set of Good Models          ,1901.03209,https://arxiv.org/abs/1901.03209,https://arxiv.org/pdf/1901.03209,"Authors:JiayunDong,CynthiaRudin","        Variable importance is central to scientific studies, including the social sciences and causal inference, healthcare, and other domains. However, current notions of variable importance are often tied to a specific predictive model. This is problematic: what if there were multiple well-performing predictive models, and a specific variable is important to some of them and not to others? In that case, we may not be able to tell from a single well-performing model whether a variable is always important in predicting the outcome. Rather than depending on variable importance for a single predictive model, we would like to explore variable importance for all approximately-equally-accurate predictive models. This work introduces the concept of a variable importance cloud, which maps every variable to its importance for every good predictive model. We show properties of the variable importance cloud and draw connections to other areas of statistics. We introduce variable importance diagrams as a projection of the variable importance cloud into two dimensions for visualization purposes. Experiments with criminal justice, marketing data, and image classification tasks illustrate how variables can change dramatically in importance for approximately-equally-accurate predictive models        △ Less","9 February, 2020","stat.ML,cs.LG",
              Adaptive Feature Processing for Robust Human Activity Recognition on a Novel Multi-Modal Dataset          ,1901.02858,https://arxiv.org/abs/1901.02858,https://arxiv.org/pdf/1901.02858,"Authors:MircoMoencks,VarunaDeSilva,JamieRoche,AhmetKondoz","        Human Activity Recognition (HAR) is a key building block of many emerging applications such as intelligent mobility, sports analytics, ambient-assisted living and human-robot interaction. With robust HAR, systems will become more human-aware, leading towards much safer and empathetic autonomous systems. While human pose detection has made significant progress with the dawn of deep convolutional neural networks (CNNs), the state-of-the-art research has almost exclusively focused on a single sensing modality, especially video. However, in safety critical applications it is imperative to utilize multiple sensor modalities for robust operation. To exploit the benefits of state-of-the-art machine learning techniques for HAR, it is extremely important to have multimodal datasets. In this paper, we present a novel, multi-modal sensor dataset that encompasses nine indoor activities, performed by 16 participants, and captured by four types of sensors that are commonly used in indoor applications and autonomous vehicles. This multimodal dataset is the first of its kind to be made openly available and can be exploited for many applications that require HAR, including sports analytics, healthcare assistance and indoor intelligent mobility. We propose a novel data preprocessing algorithm to enable adaptive feature extraction from the dataset to be utilized by different machine learning algorithms. Through rigorous experimental evaluations, this paper reviews the performance of machine learning approaches to posture recognition, and analyses the robustness of the algorithms. When performing HAR with the RGB-Depth data from our new dataset, machine learning algorithms such as a deep neural network reached a mean accuracy of up to 96.8% for classification across all stationary and dynamic activities        △ Less","9 January, 2019",cs.CV,
              Hourly Forecasting of Emergency Department Arrivals : Time Series Analysis          ,1901.02714,https://arxiv.org/abs/1901.02714,https://arxiv.org/pdf/1901.02714,Authors:AvishekChoudhury,"        Background: The stochastic behavior of patient arrival at an emergency department (ED) complicates the management of an ED. More than 50% of hospitals ED capacity tends to operate beyond its normal capacity and eventually fails to deliver high-quality care. To address the concern of stochastics ED arrivals, many types of research has been done using yearly, monthly and weekly time series forecasting. Aim: Our research team believes that hourly time-series forecasting of the load can improve ED management by predicting the arrivals of future patients, and thus, can support strategic decisions in terms of quality enhancement. Methods: Our research does not involve any human subject, only ED admission data from January 2014 to August 2017 retrieved from the UnityPoint Health database. Autoregressive integrated moving average (ARIMA), Holt Winters, TBATS, and neural network methods were implemented to forecast hourly ED patient arrival. Findings: ARIMA (3,0,0) (2,1,0) was selected as the best fit model with minimum Akaike information criterion and Schwartz Bayesian criterion. The model was stationary and qualified the Box Ljung correlation test and the Jarque Bera test for normality. The mean error (ME) and root mean square error (RMSE) were selected as performance measures. An ME of 1.001 and an RMSE of 1.55 was obtained. Conclusions: ARIMA can be used to provide hourly forecasts for ED arrivals and can be utilized as a decision support system in the healthcare industry. Application: This technique can be implemented in hospitals worldwide to predict ED patient arrival.        △ Less","5 January, 2019","cs.CY,stat.OT",
              Adaptive Activity Monitoring with Uncertainty Quantification in Switching Gaussian Process Models          ,1901.02427,https://arxiv.org/abs/1901.02427,https://arxiv.org/pdf/1901.02427,"Authors:RandyArdywibowo,GuangZhao,ZhangyangWang,BobakMortazavi,ShuaiHuang,XiaoningQian","        Emerging wearable sensors have enabled the unprecedented ability to continuously monitor human activities for healthcare purposes. However, with so many ambient sensors collecting different measurements, it becomes important not only to maintain good monitoring accuracy, but also low power consumption to ensure sustainable monitoring. This power-efficient sensing scheme can be achieved by deciding which group of sensors to use at a given time, requiring an accurate characterization of the trade-off between sensor energy usage and the uncertainty in ignoring certain sensor signals while monitoring. To address this challenge in the context of activity monitoring, we have designed an adaptive activity monitoring framework. We first propose a switching Gaussian process to model the observed sensor signals emitting from the underlying activity states. To efficiently compute the Gaussian process model likelihood and quantify the context prediction uncertainty, we propose a block circulant embedding technique and use Fast Fourier Transforms (FFT) for inference. By computing the Bayesian loss function tailored to switching Gaussian processes, an adaptive monitoring procedure is developed to select features from available sensors that optimize the trade-off between sensor power consumption and the prediction performance quantified by state prediction entropy. We demonstrate the effectiveness of our framework on the popular benchmark of UCI Human Activity Recognition using Smartphones.        △ Less","8 January, 2019","cs.LG,stat.ML",
              Designing Data Protection for GDPR Compliance into IoT Healthcare Systems          ,1901.02426,https://arxiv.org/abs/1901.02426,https://arxiv.org/pdf/1901.02426,"Authors:FlorianKammüller,OladapoO.Ogunyanwo,ChristianW.Probst","        In this paper, we investigate the implications of the General Data Privacy Regulation (GDPR) on the design of an IoT healthcare system. On 25th May 2018, the GDPR has become mandatory within the European Union and hence also for all suppliers of IT products. Infringements on the regulation are now fined with penalties of up 20 Million EUR or 4\% of the annual turnover of a company whichever is higher. This is a clear motivation for system designers to guarantee compliance to the GDPR. We propose a data labeling model to support access control for privacy-critical patient data together with the Fusion/UML process to design GDPR compliant system. We illustrate this design process on the case study of IoT based monitoring of Alzheimer's patients that we work on in the CHIST-ERA project SUCCESS.        △ Less","8 January, 2019",cs.SE,
              Low-Cost Device Prototype for Automatic Medical Diagnosis Using Deep Learning Methods          ,1901.00751,https://arxiv.org/abs/1901.00751,https://arxiv.org/pdf/1901.00751,Authors:NeilDeshmukh,"        This paper introduces a novel low-cost device prototype for the automatic diagnosis of diseases, utilizing inputted symptoms and personal background. The engineering goal is to solve the problem of limited healthcare access with a single device. Diagnosing diseases automatically is an immense challenge, owing to their variable properties and symptoms. On the other hand, Neural Networks have developed into a powerful tool in the field of machine learning, one that is showing to be extremely promising at computing diagnosis even with inconsistent variables.  In this research, a cheap device was created to allow for straightforward diagnosis and treatment of human diseases. By utilizing Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs), outfitted on a Raspberry Pi Zero processor ($5), the device is able to detect up to 1537 different diseases and conditions and utilize a CNN for on-device visual diagnostics. The user can input the symptoms using the buttons on the device and can take pictures using the same mechanism. The algorithm processes inputted symptoms, providing diagnosis and possible treatment options for common conditions. The purpose of this work was to be able to diagnose diseases through an affordable processor with high accuracy, as it is currently achieving an accuracy of 90% for Top-5 symptom-based diagnoses, and 91% for visual skin diseases. The NNs achieve performance far above any other tested system, and its efficiency and ease of use will prove it to be a helpful tool for people around the world. This device could potentially provide low-cost universal access to vital diagnostics and treatment options.        △ Less","3 January, 2019","cs.CY,cs.CV,cs.LG,stat.ML",
              Multi-task Prediction of Patient Workload          ,1901.00746,https://arxiv.org/abs/1901.00746,https://arxiv.org/pdf/1901.00746,"Authors:MohammadHessamOlya,DongxiaoZhu,KaiYang","        Developing reliable workload predictive models can affect many aspects of clinical decision making procedure. The primary challenge in healthcare systems is handling the demand uncertainty over the time. This issue becomes more critical for the healthcare facilities that provide service for chronic disease treatment because of the need for continuous treatments over the time. Although some researchers focused on exploring the methods for workload prediction recently, few types of research mainly focused on forecasting a quantitative measure for the workload of healthcare providers. Also, among the mentioned studies most of them just focused on workload prediction within one facility. The drawback of the previous studies is the problem is not investigated for multiple facilities where the quality of provided service, the equipment, and resources used for provided service as well as the diagnosis and treatment procedures may differ even for patients with similar conditions. To tackle the mentioned issue, this paper suggests a framework for patient workload prediction by using patients data from VA facilities across the US. To capture the information of patients with similar attributes and make the prediction more accurate, a heuristic cluster based algorithm for single task learning as well as a multi task learning approach are developed in this research.        △ Less","27 December, 2018","cs.CY,cs.LG,stat.ML",
              Efficient augmentation and relaxation learning for individualized treatment rules using observational data          ,1901.00663,https://arxiv.org/abs/1901.00663,https://arxiv.org/pdf/1901.00663,"Authors:Ying-QiZhao,EricB.Laber,YangNing,SumonaSaha,BruceSands","        Individualized treatment rules aim to identify if, when, which, and to whom treatment should be applied. A globally aging population, rising healthcare costs, and increased access to patient-level data have created an urgent need for high-quality estimators of individualized treatment rules that can be applied to observational data. A recent and promising line of research for estimating individualized treatment rules recasts the problem of estimating an optimal treatment rule as a weighted classification problem. We consider a class of estimators for optimal treatment rules that are analogous to convex large-margin classifiers. The proposed class applies to observational data and is doubly-robust in the sense that correct specification of either a propensity or outcome model leads to consistent estimation of the optimal individualized treatment rule. Using techniques from semiparametric efficiency theory, we derive rates of convergence for the proposed estimators and use these rates to characterize the bias-variance trade-off for estimating individualized treatment rules with classification-based methods. Simulation experiments informed by these results demonstrate that it is possible to construct new estimators within the proposed framework that significantly outperform existing ones. We illustrate the proposed methods using data from a labor training program and a study of inflammatory bowel syndrome.        △ Less","3 January, 2019",stat.ME,
"              Consumption, Investment, and Healthcare with Aging          ",1901.00424,https://arxiv.org/abs/1901.00424,https://arxiv.org/pdf/1901.00424,"Authors:PaoloGuasoni,Yu-JuiHuang","        This paper solves the problem of optimal dynamic consumption, investment, and healthcare spending with isoelastic utility, when natural mortality grows exponentially to reflect Gompertz' law and investment opportunities are constant. Healthcare slows the natural growth of mortality, indirectly increasing utility from consumption through longer lifetimes. Optimal consumption and healthcare imply an endogenous mortality law that is asymptotically exponential in the old-age limit, with lower growth rate than natural mortality. Healthcare spending steadily increases with age, both in absolute terms and relative to total spending. The optimal stochastic control problem reduces to a nonlinear ordinary differential equation with a unique solution, which has an explicit expression in the old-age limit. The main results are obtained through a novel version of Perron's method.        △ Less","7 January, 2019","q-fin.MF,math.OC",
              Predicting with Proxies: Transfer Learning in High Dimension          ,1812.11097,https://arxiv.org/abs/1812.11097,https://arxiv.org/pdf/1812.11097,Authors:HamsaBastani,"        Predictive analytics is increasingly used to guide decision-making in many applications. However, in practice, we often have limited data on the true predictive task of interest, and must instead rely on more abundant data on a closely-related proxy predictive task. For example, e-commerce platforms use abundant customer click data (proxy) to make product recommendations rather than the relatively sparse customer purchase data (true outcome of interest); alternatively, hospitals often rely on medical risk scores trained on a different patient population (proxy) rather than their own patient population (true cohort of interest) to assign interventions. Yet, not accounting for the bias in the proxy can lead to sub-optimal decisions. Using real datasets, we find that this bias can often be captured by a sparse function of the features. Thus, we propose a novel two-step estimator that uses techniques from high-dimensional statistics to efficiently combine a large amount of proxy data and a small amount of true data. We prove upper bounds on the error of our proposed estimator and lower bounds on several heuristics used by data scientists; in particular, our proposed estimator can achieve the same accuracy with exponentially less true data (in the number of features). Our proof relies on a new LASSO tail inequality for approximately sparse vectors. Finally, we demonstrate the effectiveness of our approach on e-commerce and healthcare datasets; in both cases, we achieve significantly better predictive accuracy as well as managerial insights into the nature of the bias in the proxy data.        △ Less","5 May, 2020","stat.ML,cs.LG",
              Evaluating Patient Readmission Risk: A Predictive Analytics Approach          ,1812.11028,https://arxiv.org/abs/1812.11028,https://arxiv.org/pdf/1812.11028,"Authors:AvishekChoudhury,ChristopherMGreene","        With the emergence of the Hospital Readmission Reduction Program of the Center for Medicare and Medicaid Services on October 1, 2012, forecasting unplanned patient readmission risk became crucial to the healthcare domain. There are tangible works in the literature emphasizing on developing readmission risk prediction models; However, the models are not accurate enough to be deployed in an actual clinical setting. Our study considers patient readmission risk as the objective for optimization and develops a useful risk prediction model to address unplanned readmissions. Furthermore, Genetic Algorithm and Greedy Ensemble is used to optimize the developed model constraints.        △ Less","20 August, 2019","cs.CY,cs.LG,stat.ML",10.3844/ajeassp.2018.1320.1331 
              Decision Support System for Renal Transplantation          ,1812.10381,https://arxiv.org/abs/1812.10381,https://arxiv.org/pdf/1812.10381,"Authors:EhsanKhan,AvishekChoudhury,AmyLFriedman,DaehanWon","        The burgeoning need for kidney transplantation mandates immediate attention. Mismatch of deceased donor-recipient kidney leads to post-transplant death. To ensure ideal kidney donor-recipient match and minimize post-transplant deaths, the paper develops a prediction model that identifies factors that determine the probability of success of renal transplantation, that is, if the kidney procured from the deceased donor can be transplanted or discarded. The paper conducts a study enveloping data for 584 imported kidneys collected from 12 transplant centers associated with an organ procurement organization located in New York City, NY. The predicting model yielding best performance measures can be beneficial to the healthcare industry. Transplant centers and organ procurement organizations can take advantage of the prediction model to efficiently predict the outcome of kidney transplantation. Consequently, it will reduce the mortality rate caused by mismatching of donor-recipient kidney transplantation during the surgery. Keywords        △ Less","11 December, 2018","cs.CY,cs.LG,q-bio.QM,stat.ML",10.13140/RG.2.2.18890.00965 
              Artificial neural networks condensation: A strategy to facilitate adaption of machine learning in medical settings by reducing computational burden          ,1812.09659,https://arxiv.org/abs/1812.09659,https://arxiv.org/pdf/1812.09659,"Authors:DianboLiu,NestorSepulveda,MingZheng","        Machine Learning (ML) applications on healthcare can have a great impact on people's lives helping deliver better and timely treatment to those in need. At the same time, medical data is usually big and sparse requiring important computational resources. Although it might not be a problem for wide-adoption of ML tools in developed nations, availability of computational resource can very well be limited in third-world nations. This can prevent the less favored people from benefiting of the advancement in ML applications for healthcare. In this project we explored methods to increase computational efficiency of ML algorithms, in particular Artificial Neural Nets (NN), while not compromising the accuracy of the predicted results. We used in-hospital mortality prediction as our case analysis based on the MIMIC III publicly available dataset. We explored three methods on two different NN architectures. We reduced the size of recurrent neural net (RNN) and dense neural net (DNN) by applying pruning of ""unused"" neurons. Additionally, we modified the RNN structure by adding a hidden-layer to the LSTM cell allowing to use less recurrent layers for the model. Finally, we implemented quantization on DNN forcing the weights to be 8-bits instead of 32-bits. We found that all our methods increased computational efficiency without compromising accuracy and some of them even achieved higher accuracy than the pre-condensed baseline models.        △ Less","23 December, 2018","cs.LG,cs.AI,stat.ML",
              Neural networks versus Logistic regression for 30 days all-cause readmission prediction          ,1812.09549,https://arxiv.org/abs/1812.09549,https://arxiv.org/pdf/1812.09549,"Authors:AhmedAllam,MateNagy,GeorgeThoma,MichaelKrauthammer","        Heart failure (HF) is one of the leading causes of hospital admissions in the US. Readmission within 30 days after a HF hospitalization is both a recognized indicator for disease progression and a source of considerable financial burden to the healthcare system. Consequently, the identification of patients at risk for readmission is a key step in improving disease management and patient outcome. In this work, we used a large administrative claims dataset to (1)explore the systematic application of neural network-based models versus logistic regression for predicting 30 days all-cause readmission after discharge from a HF admission, and (2)to examine the additive value of patients' hospitalization timelines on prediction performance. Based on data from 272,778 (49% female) patients with a mean (SD) age of 73 years (14) and 343,328 HF admissions (67% of total admissions), we trained and tested our predictive readmission models following a stratified 5-fold cross-validation scheme. Among the deep learning approaches, a recurrent neural network (RNN) combined with conditional random fields (CRF) model (RNNCRF) achieved the best performance in readmission prediction with 0.642 AUC (95% CI, 0.640-0.645). Other models, such as those based on RNN, convolutional neural networks and CRF alone had lower performance, with a non-timeline based model (MLP) performing worst. A competitive model based on logistic regression with LASSO achieved a performance of 0.643 AUC (95%CI, 0.640-0.646). We conclude that data from patient timelines improve 30 day readmission prediction for neural network-based models, that a logistic regression with LASSO has equal performance to the best neural network model and that the use of administrative data result in competitive performance compared to published approaches based on richer clinical datasets.        △ Less","22 December, 2018","cs.LG,q-bio.QM,stat.ML",
              Computational Anatomy for Multi-Organ Analysis in Medical Imaging: A Review          ,1812.08577,https://arxiv.org/abs/1812.08577,https://arxiv.org/pdf/1812.08577,"Authors:JuanJ.Cerrolaza,MirellaLopez-Picazo,LudovicHumbert,YoshinobuSato,DanielRueckert,MiguelAngelGonzalezBallester,MariusGeorgeLinguraru","        The medical image analysis field has traditionally been focused on the development of organ-, and disease-specific methods. Recently, the interest in the development of more 20 comprehensive computational anatomical models has grown, leading to the creation of multi-organ models. Multi-organ approaches, unlike traditional organ-specific strategies, incorporate inter-organ relations into the model, thus leading to a more accurate representation of the complex human anatomy. Inter-organ relations are not only spatial, but also functional and physiological. Over the years, the strategies 25 proposed to efficiently model multi-organ structures have evolved from the simple global modeling, to more sophisticated approaches such as sequential, hierarchical, or machine learning-based models. In this paper, we present a review of the state of the art on multi-organ analysis and associated computation anatomy methodology. The manuscript follows a methodology-based classification of the different techniques 30 available for the analysis of multi-organs and multi-anatomical structures, from techniques using point distribution models to the most recent deep learning-based approaches. With more than 300 papers included in this review, we reflect on the trends and challenges of the field of computational anatomy, the particularities of each anatomical region, and the potential of multi-organ analysis to increase the impact of 35 medical imaging applications on the future of healthcare.        △ Less","20 December, 2018",cs.CV,
              A Tour of Unsupervised Deep Learning for Medical Image Analysis          ,1812.07715,https://arxiv.org/abs/1812.07715,https://arxiv.org/pdf/1812.07715,"Authors:KhalidRaza,NripendraKumarSingh","        Interpretation of medical images for diagnosis and treatment of complex disease from high-dimensional and heterogeneous data remains a key challenge in transforming healthcare. In the last few years, both supervised and unsupervised deep learning achieved promising results in the area of medical imaging and image analysis. Unlike supervised learning which is biased towards how it is being supervised and manual efforts to create class label for the algorithm, unsupervised learning derive insights directly from the data itself, group the data and help to make data driven decisions without any external bias. This review systematically presents various unsupervised models applied to medical image analysis, including autoencoders and its several variants, Restricted Boltzmann machines, Deep belief networks, Deep Boltzmann machine and Generative adversarial network. Future research opportunities and challenges of unsupervised techniques for medical image analysis have also been discussed.        △ Less","19 December, 2018","eess.IV,cs.CV,cs.LG",
              Interpretable Optimal Stopping          ,1812.07211,https://arxiv.org/abs/1812.07211,https://arxiv.org/pdf/1812.07211,"Authors:DragosFlorinCiocan,VeliborV.Mišić","        Optimal stopping is the problem of deciding when to stop a stochastic system to obtain the greatest reward, arising in numerous application areas such as finance, healthcare and marketing. State-of-the-art methods for high-dimensional optimal stopping involve approximating the value function or the continuation value, and then using that approximation within a greedy policy. Although such policies can perform very well, they are generally not guaranteed to be interpretable; that is, a decision maker may not be able to easily see the link between the current system state and the policy's action. In this paper, we propose a new approach to optimal stopping, wherein the policy is represented as a binary tree, in the spirit of naturally interpretable tree models commonly used in machine learning. We show that the class of tree policies is rich enough to approximate the optimal policy. We formulate the problem of learning such policies from observed trajectories of the stochastic system as a sample average approximation (SAA) problem. We prove that the SAA problem converges under mild conditions as the sample size increases, but that computationally even immediate simplifications of the SAA problem are theoretically intractable. We thus propose a tractable heuristic for approximately solving the SAA problem, by greedily constructing the tree from the top down. We demonstrate the value of our approach by applying it to the canonical problem of option pricing, using both synthetic instances and instances using real S&P-500 data. Our method obtains policies that (1) outperform state-of-the-art non-interpretable methods, based on simulation-regression and martingale duality, and (2) possess a remarkably simple and intuitive structure.        △ Less","30 December, 2019","math.OC,cs.AI,cs.LG",
              Humanoid Robot-Application and Influence          ,1812.06090,https://arxiv.org/abs/1812.06090,https://arxiv.org/pdf/1812.06090,"Authors:AvishekChoudhury,HuiyangLi,ChristopherGreene,SunandaPerumalla","        Application of humanoid robots has been common in the field of healthcare and education. It has been recurrently used to improve social behavior and mollify distress level among children with autism, cancer and cerebral palsy. This article discusses the same from a human factors perspective. It shows how people of different age and gender have a different opinion towards the application and acceptance of humanoid robots. Additionally, this article highlights the influence of cerebral condition and social interaction on a user behavior and attitude towards humanoid robots. Our study performed a literature review and found that (a) children and elderly individuals prefer humanoid robots due to inactive social interaction, (b) The deterministic behavior of humanoid robots can be acknowledged to improve social behavior of autistic children, (c) Trust on humanoid robots is highly driven by its application and a user age, gender, and social life.        △ Less","14 December, 2018","cs.RO,cs.HC",10.26502/acbr.50170059 
              Patient-Centric Cellular Networks Optimization using Big Data Analytics          ,1812.04712,https://arxiv.org/abs/1812.04712,https://arxiv.org/pdf/1812.04712,"Authors:MohammedS.Hadi,AhmedQ.Lawey,TaisirE.H.El-Gorashi,J.M.HElmirghani","        Big data analytics is one of the state-of-the-art tools to optimize networks and transform them from merely being a blind tube that conveys data, into a cognitive, conscious, and self-optimizing entity that can intelligently adapt according to the needs of its users. This, in fact, can be regarded as one of the highest forthcoming priorities of future networks. In this paper, we propose a system for Out-Patient (OP) centric Long Term Evolution-Advanced (LTE-A) network optimization. Big data harvested from the OPs' medical records, along with current readings from their body sensors are processed and analyzed to predict the likelihood of a life-threatening medical condition, for instance, an imminent stroke. This prediction is used to ensure that the OP is assigned an optimal LTE-A Physical Resource Blocks (PRBs) to transmit their critical data to their healthcare provider with minimal delay. To the best of our knowledge, this is the first time big data analytics are utilized to optimize a cellular network in an OP-conscious manner. The PRBs assignment is optimized using Mixed Integer Linear Programming (MILP) and a real-time heuristic. Two approaches are proposed, the Weighted Sum Rate Maximization (WSRMax) approach and the Proportional Fairness (PF) approach. The approaches increased the OPs' average SINR by 26.6% and 40.5%, respectively. The WSRMax approach increased the system's total SINR to a level higher than that of the PF approach, however, the PF approach reported higher SINRs for the OPs, better fairness and a lower margin of error.        △ Less","9 November, 2018","cs.NI,cs.CY",
              Privacy-preserving data aggregation in resource-constrained sensor nodes in Internet of Things: A review          ,1812.04216,https://arxiv.org/abs/1812.04216,https://arxiv.org/pdf/1812.04216,"Authors:InayatAli,SoniaSabir,ErajKhan","        Privacy problems are lethal and getting more attention than any other issue with the notion of the Internet of Things (IoT). Since IoT has many application areas including smart home, smart grids, smart healthcare system, smart and intelligent transportation and many more. Most of these applications are fueled by the resource-constrained sensor network, such as Smart healthcare system is powered by Wireless Body Area Network (WBAN) and Smart home and weather monitoring systems are fueled by Wireless Sensor Networks (WSN). In the mentioned application areas sensor node life is a very important aspect of these technologies as it explicitly effects the network life and performance. Data aggregation techniques are used to increase sensor node life by decreasing communication overhead. However, when the data is aggregated at intermediate nodes to reduce communication overhead, data privacy problems becomes more vulnerable. Different Privacy-Preserving Data Aggregation (PPDA) techniques have been proposed to ensure data privacy during data aggregation in resource-constrained sensor nodes. We provide a review and comparative analysis of the state of the art PPDA techniques in this paper. The comparative analysis is based on Computation Cost, Communication overhead, Privacy Level, resistance against malicious aggregator, sensor node life and energy consumption by the sensor node. We have studied the most recent techniques and provide in-depth analysis of the minute steps involved in these techniques. To the best of our knowledge, this survey is the most recent and comprehensive study of PPDA techniques.        △ Less","10 December, 2018",cs.CR,
              Rapid Prototyping Model for Healthcare Alternative Payment Models: Replicating the Federally Qualified Health Center Advanced Primary Care Practice Demonstration          ,1812.03940,https://arxiv.org/abs/1812.03940,https://arxiv.org/pdf/1812.03940,"Authors:JarrodOlson,AmirRahimi,PoHsuAllenChen,J.ElizabethJackson,TylerCoy,AdrienneCocci,NancyMcMillan,JeffGeppert","        Innovation in healthcare payment and service delivery utilizes high cost, high risk pilots paired with traditional program evaluations. Decision-makers are unable to reliably forecast the impacts of pilot interventions in this complex system, complicating the feasibility assessment of proposed healthcare models. We developed and validated a Discrete Event Simulation (DES) model of primary care for patients with Diabetes to allow rapid prototyping and assessment of models before pilot implementation. We replicated four outcomes from the Centers for Medicare and Medicaid Services Federally Qualified Health Center Advanced Primary Care Practice pilot. The DES model simulates a synthetic population's healthcare experience, including symptom onset, appointment scheduling, screening, and treatment, as well as the impact of physician training. A network of detailed event modules was developed from peer-reviewed literature. Synthetic patients' attributes modify the probability distributions for event outputs and direct them through an episode of care; attributes are in turn modified by patients' experiences. Our model replicates the direction of the effect of physician training on the selected outcomes, and the strength of the effect increases with the number of trainings. The simulated effect strength replicates the pilot results for eye exams and nephropathy screening, but over-estimates results for HbA1c and LDL screening. Our model will improve decision-makers' abilities to assess the feasibility of pilot success, with reproducible, literature-based systems models. Our model identifies intervention and healthcare system components to which outcomes are sensitive, so these aspects can be monitored and controlled during pilot implementation. More work is needed to improve replication of HbA1c and LDL screening, and to elaborate sub-models related to intervention components.        △ Less","10 December, 2018",stat.OT,
              Fast Node Cardinality Estimation and Cognitive MAC Protocol Design for Heterogeneous Machine-to-Machine Networks          ,1812.03902,https://arxiv.org/abs/1812.03902,https://arxiv.org/pdf/1812.03902,"Authors:SachinKadam,ChaitanyaS.Raut,AmandeepMeena,GauravS.Kasbekar","        Machine-to-Machine (M2M) networks are an emerging technology with applications in numerous areas including smart grids, smart cities, vehicular telematics, and healthcare. In this paper, we design two estimation protocols for rapidly obtaining separate estimates of the number of active nodes of each traffic type in a heterogeneous M2M network with TT types of M2M nodes (e.g., those that send emergency, periodic, normal type data etc), where T≥2T \geq 2 is an arbitrary integer. One of these protocols, Method I, is a simple scheme, and the other, Method II, is more sophisticated and performs better than Method I. Also, we design a medium access control (MAC) protocol that supports multi-channel operation for a heterogeneous M2M network with an arbitrary number of types of M2M nodes, operating as a secondary network using Cognitive Radio technology. Our Cognitive MAC protocol uses the proposed node cardinality estimation protocols to rapidly estimate the number of active nodes of each type in every time frame; these estimates are used to find the optimal contention probabilities to be used in the MAC protocol. We compute a closed form expression for the expected number of time slots required by Method I to execute as well as a simple upper bound on it. Also, we mathematically analyze the performance of the Cognitive MAC protocol and obtain expressions for the expected number of successful contentions per frame and the expected amount of energy consumed. Finally, we evaluate the performances of our proposed estimation protocols and Cognitive MAC protocol using simulations.        △ Less","10 December, 2018",eess.SP,
              Automatically Explaining Machine Learning Prediction Results: A Demonstration on Type 2 Diabetes Risk Prediction          ,1812.02852,https://arxiv.org/abs/1812.02852,https://arxiv.org/pdf/1812.02852,Authors:GangLuo,"        Background: Predictive modeling is a key component of solutions to many healthcare problems. Among all predictive modeling approaches, machine learning methods often achieve the highest prediction accuracy, but suffer from a long-standing open problem precluding their widespread use in healthcare. Most machine learning models give no explanation for their prediction results, whereas interpretability is essential for a predictive model to be adopted in typical healthcare settings. Methods: This paper presents the first complete method for automatically explaining results for any machine learning predictive model without degrading accuracy. We did a computer coding implementation of the method. Using the electronic medical record data set from the Practice Fusion diabetes classification competition containing patient records from all 50 states in the United States, we demonstrated the method on predicting type 2 diabetes diagnosis within the next year. Results: For the champion machine learning model of the competition, our method explained prediction results for 87.4% of patients who were correctly predicted by the model to have type 2 diabetes diagnosis within the next year. Conclusions: Our demonstration showed the feasibility of automatically explaining results for any machine learning predictive model without degrading accuracy.        △ Less","6 December, 2018","cs.LG,stat.ML",10.1186/s13755-016-0015-4 
              Applications of Blockchain in Healthcare: Current Landscape & Challenges          ,1812.02776,https://arxiv.org/abs/1812.02776,https://arxiv.org/pdf/1812.02776,"Authors:GajendraJ.Katuwal,SandipPandey,MarkHennessey,BishalLamichhane","        Several problems in healthcare stem from the complex network of intermediaries and the lack of traceability of transactions. To mention a few: healthcare data is fragmented across several silos negatively affecting research and services, about half of the clinical trials are never reported, the cost of drug discovery is ever increasing, and substandard and fake medicines are still a huge problem. Blockchain has the potential to solve these problems as it provides trust without any intermediaries, has traceability as a default feature, and promises new business models by enabling novel incentive structures. Due to its potential, blockchain has gathered significant interest in the healthcare industry. In this paper, we review major use cases of blockchain in healthcare: patient data management, pharmaceutical research, supply chain management of medical goods, prescription management, billing claims management, analytics, and telemedicine alongside the related projects. We found that most of the blockchain projects are limited as white-papers, proof of concepts, and products with a limited user base. However, we observed that the quantity, quality, and maturity of the projects are increasing. We also discuss technical, regulatory, and business challenges to the adoption of blockchain in the healthcare industry        △ Less","6 December, 2018",cs.CY,
              Relevant Word Order Vectorization for Improved Natural Language Processing in Electronic Healthcare Records          ,1812.02627,https://arxiv.org/abs/1812.02627,https://arxiv.org/pdf/1812.02627,"Authors:JeffreyThompson,JinxiangHu,DineshPalMudaranthakam,DavidStreeter,LisaNeums,MichelePark,DevinC.Koestler,ByronGajewski,MatthewS.Mayo","        Objective: Electronic health records (EHR) represent a rich resource for conducting observational studies, supporting clinical trials, and more. However, much of the relevant information is stored in an unstructured format that makes it difficult to use. Natural language processing approaches that attempt to automatically classify the data depend on vectorization algorithms that impose structure on the text, but these algorithms were not designed for the unique characteristics of EHR. Here, we propose a new algorithm for structuring so-called free-text that may help researchers make better use of EHR. We call this method Relevant Word Order Vectorization (RWOV).  Materials and Methods: As a proof-of-concept, we attempted to classify the hormone receptor status of breast cancer patients treated at the University of Kansas Medical Center during a recent year, from the unstructured text of pathology reports. Our approach attempts to account for the semi-structured way that healthcare providers often enter information. We compared this approach to the ngrams and word2vec methods.  Results: Our approach resulted in the most consistently high accuracy, as measured by F1 score and area under the receiver operating characteristic curve (AUC).  Discussion: Our results suggest that methods of structuring free text that take into account its context may show better performance, and that our approach is promising.  Conclusion: By using a method that accounts for the fact that healthcare providers tend to use certain key words repetitively and that the order of these key words is important, we showed improved performance over methods that do not.        △ Less","6 December, 2018","cs.CL,cs.CY",
              Multi-materials beam hardening artifacts correction for computed tomography (CT) based on X-ray spectrum estimation          ,1812.02365,https://arxiv.org/abs/1812.02365,https://arxiv.org/pdf/1812.02365,"Authors:WeiZhao,DengwangLi,KaiNiu,WenjianQin,HaoPeng,TianyeNiu","        Due to the energy-dependent nature of the attenuation coefficient and the polychromaticity of the X-ray source, beam hardening effect occurs when X-ray photons penetrate through an object, causing a nonlinear projection data. When a linear reconstruction algorithm, such as filtered backprojection, is applied to reconstruct the projection data, beam hardening artifacts which show as cupping and streaks are present in the CT image. The aim of this study was to develop a fast and accurate beam hardening correction method which can deal with beam hardening artifacts induced by multi-materials objects. Based on spectrum estimation, the nonlinear attenuation process of the X-ray projection was modeled by reprojecting a template image with the estimated polychromatic spectrum. The template images were obtained by segmenting the uncorrected into different components using a simple segmentation algorithm. Numerical simulations, experimental phantom data and animal data which were acquired on a modern diagnostic CT scanner (Discovery CT750 HD, GE Healthcare, WI, USA) and a modern C-Arm CT scanner (Artis Zee, Siemens Healthcare, Forchheim, Germany), respectively, were used to evaluate the proposed method. The results show the proposed method significantly reduced both cupping and streak artifacts, and successfully recovered the Hounsfield Units (HU) accuracy.        △ Less","6 December, 2018",physics.med-ph,
              Differential Privacy Techniques for Cyber Physical Systems: A Survey          ,1812.02282,https://arxiv.org/abs/1812.02282,https://arxiv.org/pdf/1812.02282,"Authors:MuneebUlHassan,MubashirHusainRehmani,JinjunChen","        Modern cyber physical systems (CPSs) has widely being used in our daily lives because of development of information and communication technologies (ICT).With the provision of CPSs, the security and privacy threats associated to these systems are also increasing. Passive attacks are being used by intruders to get access to private information of CPSs. In order to make CPSs data more secure, certain privacy preservation strategies such as encryption, and k-anonymity have been presented in the past. However, with the advances in CPSs architecture, these techniques also needs certain modifications. Meanwhile, differential privacy emerged as an efficient technique to protect CPSs data privacy. In this paper, we present a comprehensive survey of differential privacy techniques for CPSs. In particular, we survey the application and implementation of differential privacy in four major applications of CPSs named as energy systems, transportation systems, healthcare and medical systems, and industrial Internet of things (IIoT). Furthermore, we present open issues, challenges, and future research direction for differential privacy techniques for CPSs. This survey can serve as basis for the development of modern differential privacy techniques to address various problems and data privacy scenarios of CPSs.        △ Less","27 September, 2019",cs.CR,
              An Empirical Study towards Understanding How Deep Convolutional Nets Recognize Falls          ,1812.01923,https://arxiv.org/abs/1812.01923,https://arxiv.org/pdf/1812.01923,"Authors:YanZhang,HeikoNeumann","        Detecting unintended falls is essential for ambient intelligence and healthcare of elderly people living alone. In recent years, deep convolutional nets are widely used in human action analysis, based on which a number of fall detection methods have been proposed. Despite their highly effective performances, the behaviors of how the convolutional nets recognize falls are still not clear. In this paper, instead of proposing a novel approach, we perform a systematical empirical study, attempting to investigate the underlying fall recognition process. We propose four tasks to investigate, which involve five types of input modalities, seven net instances and different training samples. The obtained quantitative and qualitative results reveal the patterns that the nets tend to learn, and several factors that can heavily influence the performances on fall recognition. We expect that our conclusions are favorable to proposing better deep learning solutions to fall detection systems.        △ Less","5 December, 2018",cs.CV,
              A System for Efficient Communication between Patients and Pharmacies          ,1812.01499,https://arxiv.org/abs/1812.01499,https://arxiv.org/pdf/1812.01499,"Authors:RuiPortocarreroSarmento,AndréTarrinho,PedroCâmara,VeraCosta","        When studying human-technology interaction systems, researchers thrive to achieve intuitiveness and facilitate the people's life through a thoughtful and in-depth study of several components of the application system that supports some particular business communication with customers. Particularly in the healthcare field, some requirements such as clarity, transparency, efficiency, and speed in transmitting information to patients and or healthcare professionals might mean an important increase in the well-being of the patient and productivity of the healthcare professional. In this work, the authors study the difficulties patients frequently have when communicating with pharmacists. In addition to a statistical study of a survey conducted with more than two hundred frequent pharmacy customers, we propose an IT solution for better communication between patients and pharmacists.        △ Less","4 December, 2018","cs.HC,stat.AP",
              Sturm: Sparse Tubal-Regularized Multilinear Regression for fMRI          ,1812.01496,https://arxiv.org/abs/1812.01496,https://arxiv.org/pdf/1812.01496,"Authors:WenwenLi,JianLou,ShuoZhou,HaipingLu","        While functional magnetic resonance imaging (fMRI) is important for healthcare/neuroscience applications, it is challenging to classify or interpret due to its multi-dimensional structure, high dimensionality, and small number of samples available. Recent sparse multilinear regression methods based on tensor are emerging as promising solutions for fMRI, yet existing works rely on unfolding/folding operations and a tensor rank relaxation with limited tightness. The newly proposed tensor singular value decomposition (t-SVD) sheds light on new directions. In this work, we study t-SVD for sparse multilinear regression and propose a Sparse tubal-regularized multilinear regression (Sturm) method for fMRI. Specifically, the Sturm model performs multilinear regression with two regularization terms: a tubal tensor nuclear norm based on t-SVD and a standard L1 norm. We further derive the algorithm under the alternating direction method of multipliers framework. We perform experiments on four classification problems, including both resting-state fMRI for disease diagnosis and task-based fMRI for neural decoding. The results show the superior performance of Sturm in classifying fMRI using just a small number of voxels.        △ Less","4 December, 2018",cs.CV,
              Towards Continuous Domain adaptation for Healthcare,1812.01281,https://arxiv.org/abs/1812.01281,https://arxiv.org/pdf/1812.01281,"Authors:RahulVenkataramani,HariharanRavishankar,SaihareeshAnamandra","        Deep learning algorithms have demonstrated tremendous success on challenging medical imaging problems. However, post-deployment, these algorithms are susceptible to data distribution variations owing to \emph{limited data issues} and \emph{diversity} in medical images. In this paper, we propose \emph{ContextNets}, a generic memory-augmented neural network framework for semantic segmentation to achieve continuous domain adaptation without the necessity of retraining. Unlike existing methods which require access to entire source and target domain images, our algorithm can adapt to a target domain with a few similar images. We condition the inference on any new input with features computed on its support set of images (and masks, if available) through contextual embeddings to achieve site-specific adaptation. We demonstrate state-of-the-art domain adaptation performance on the X-ray lung segmentation problem from three independent cohorts that differ in disease type, gender, contrast and intensity variations.        △ Less","4 December, 2018","cs.CV,cs.LG",
              A Hybrid Instance-based Transfer Learning Method          ,1812.01063,https://arxiv.org/abs/1812.01063,https://arxiv.org/pdf/1812.01063,"Authors:AzinAsgarian,ParinazSobhani,JiChaoZhang,MadalinMihailescu,ArielSibilia,AhmedBilalAshraf,BabakTaati","        In recent years, supervised machine learning models have demonstrated tremendous success in a variety of application domains. Despite the promising results, these successful models are data hungry and their performance relies heavily on the size of training data. However, in many healthcare applications it is difficult to collect sufficiently large training datasets. Transfer learning can help overcome this issue by transferring the knowledge from readily available datasets (source) to a new dataset (target). In this work, we propose a hybrid instance-based transfer learning method that outperforms a set of baselines including state-of-the-art instance-based transfer learning approaches. Our method uses a probabilistic weighting strategy to fuse information from the source domain to the model learned in the target domain. Our method is generic, applicable to multiple source domains, and robust with respect to negative transfer. We demonstrate the effectiveness of our approach through extensive experiments for two different applications.        △ Less","3 December, 2018","cs.LG,cs.AI,cs.CV,stat.ML",
              Microscope 2.0: An Augmented Reality Microscope with Real-time Artificial Intelligence Integration          ,1812.00825,https://arxiv.org/abs/1812.00825,https://arxiv.org/pdf/1812.00825,"Authors:Po-HsuanCameronChen,KrishnaGadepalli,RobertMacDonald,YunLiu,KunalNagpal,TimoKohlberger,JeffreyDean,GregS.Corrado,JasonD.Hipp,MartinC.Stumpe","        The brightfield microscope is instrumental in the visual examination of both biological and physical samples at sub-millimeter scales. One key clinical application has been in cancer histopathology, where the microscopic assessment of the tissue samples is used for the diagnosis and staging of cancer and thus guides clinical therapy. However, the interpretation of these samples is inherently subjective, resulting in significant diagnostic variability. Moreover, in many regions of the world, access to pathologists is severely limited due to lack of trained personnel. In this regard, Artificial Intelligence (AI) based tools promise to improve the access and quality of healthcare. However, despite significant advances in AI research, integration of these tools into real-world cancer diagnosis workflows remains challenging because of the costs of image digitization and difficulties in deploying AI solutions. Here we propose a cost-effective solution to the integration of AI: the Augmented Reality Microscope (ARM). The ARM overlays AI-based information onto the current view of the sample through the optical pathway in real-time, enabling seamless integration of AI into the regular microscopy workflow. We demonstrate the utility of ARM in the detection of lymph node metastases in breast cancer and the identification of prostate cancer with a latency that supports real-time workflows. We anticipate that ARM will remove barriers towards the use of AI in microscopic analysis and thus improve the accuracy and efficiency of cancer diagnosis. This approach is applicable to other microscopy tasks and AI algorithms in the life sciences and beyond.        △ Less","4 December, 2018","cs.CV,cs.AI,cs.LG",10.1038/s41591-019-0539-7 
              Care2Vec: A Deep learning approach for the classification of self-care problems in physically disabled children          ,1812.00715,https://arxiv.org/abs/1812.00715,https://arxiv.org/pdf/1812.00715,Authors:SayanPutatunda,"        Accurate classification of self-care problems in children who suffer from physical and motor affliction is an important problem in the healthcare industry. This is a difficult and a time consumming process and it needs the expertise of occupational therapists. In recent years, healthcare professionals have opened up to the idea of using expert systems and artificial intelligence in the diagnosis and classification of self care problems. In this study, we propose a new deep learning based approach named Care2Vec for solving these kind of problems and use a real world self care activities dataset that is based on a conceptual framework designed by the World Health Organization (WHO). Care2Vec is a mix of unsupervised and supervised learning where we use Autoencoders and Deep neural networks as a two step modeling process. We found that Care2Vec has a better prediction accuracy than some of the traditional methods reported in the literature for solving the self care classification problem viz. Decision trees and Artificial neural networks.        △ Less","23 December, 2019","cs.LG,stat.ML",10.1007/s00521-020-04943-2 
              Deep Learning Approach for Predicting 30 Day Readmissions after Coronary Artery Bypass Graft Surgery          ,1812.00596,https://arxiv.org/abs/1812.00596,https://arxiv.org/pdf/1812.00596,"Authors:RameshB.Manyam,YanqingZhang,WilliamB.Keeling,JoseBinongo,MichaelKayatta,SethCarter","        Hospital Readmissions within 30 days after discharge following Coronary Artery Bypass Graft (CABG) Surgery are substantial contributors to healthcare costs. Many predictive models were developed to identify risk factors for readmissions. However, majority of the existing models use statistical analysis techniques with data available at discharge. We propose an ensembled model to predict CABG readmissions using pre-discharge perioperative data and machine learning survival analysis techniques. Firstly, we applied fifty one potential readmission risk variables to Cox Proportional Hazard (CPH) survival regression univariate analysis. Fourteen of them turned out to be significant (with p value < 0.05), contributing to readmissions. Subsequently, we applied these 14 predictors to multivariate CPH model and Deep Learning Neural Network (NN) representation of the CPH model, DeepSurv. We validated this new ensembled model with 453 isolated adult CABG cases. Nine of the fourteen perioperative risk variables were identified as the most significant with Hazard Ratios (HR) of greater than 1.0. The concordance index metrics for CPH, DeepSurv, and ensembled models were then evaluated with training and validation datasets. Our ensembled model yielded promising results in terms of c-statistics, as we raised the the number of iterations and data set sizes. 30 day all-cause readmissions among isolated CABG patients can be predicted more effectively with perioperative pre-discharge data, using machine learning survival analysis techniques. Prediction accuracy levels could be improved further with deep learning algorithms.        △ Less","3 December, 2018","cs.LG,q-bio.QM,stat.ML",
              Interpretable Clustering via Optimal Trees          ,1812.00539,https://arxiv.org/abs/1812.00539,https://arxiv.org/pdf/1812.00539,"Authors:DimitrisBertsimas,AgniOrfanoudaki,HollyWiberg","        State-of-the-art clustering algorithms use heuristics to partition the feature space and provide little insight into the rationale for cluster membership, limiting their interpretability. In healthcare applications, the latter poses a barrier to the adoption of these methods since medical researchers are required to provide detailed explanations of their decisions in order to gain patient trust and limit liability. We present a new unsupervised learning algorithm that leverages Mixed Integer Optimization techniques to generate interpretable tree-based clustering models. Utilizing the flexible framework of Optimal Trees, our method approximates the globally optimal solution leading to high quality partitions of the feature space. Our algorithm, can incorporate various internal validation metrics, naturally determines the optimal number of clusters, and is able to account for mixed numeric and categorical data. It achieves comparable or superior performance on both synthetic and real world datasets when compared to K-Means while offering significantly higher interpretability.        △ Less","2 December, 2018","stat.ML,cs.LG",
              Modeling disease progression in longitudinal EHR data using continuous-time hidden Markov models          ,1812.00528,https://arxiv.org/abs/1812.00528,https://arxiv.org/pdf/1812.00528,"Authors:AmanVerma,GuidoPowell,YuLuo,DavidStephens,DavidL.Buckeridge","        Modeling disease progression in healthcare administrative databases is complicated by the fact that patients are observed only at irregular intervals when they seek healthcare services. In a longitudinal cohort of 76,888 patients with chronic obstructive pulmonary disease (COPD), we used a continuous-time hidden Markov model with a generalized linear model to model healthcare utilization events. We found that the fitted model provides interpretable results suitable for summarization and hypothesis generation.        △ Less","2 December, 2018","cs.LG,q-bio.PE,stat.ML",
              Multiple Instance Learning for ECG Risk Stratification          ,1812.00475,https://arxiv.org/abs/1812.00475,https://arxiv.org/pdf/1812.00475,"Authors:DivyaShanmugam,DavisBlalock,JohnGuttag","        Patients who suffer an acute coronary syndrome are at elevated risk for adverse cardiovascular events such as myocardial infarction and cardiovascular death. Accurate assessment of this risk is crucial to their course of care. We focus on estimating a patient's risk of cardiovascular death after an acute coronary syndrome based on a patient's raw electrocardiogram (ECG) signal. Learning from this signal is challenging for two reasons: 1) positive examples signifying a downstream cardiovascular event are scarce, causing drastic class imbalance, and 2) each patient's ECG signal consists of thousands of heartbeats, accompanied by a single label for the downstream outcome. Machine learning has been previously applied to this task, but most approaches rely on hand-crafted features and domain knowledge. We propose a method that learns a representation from the raw ECG signal by using a multiple instance learning framework. We present a learned risk score for cardiovascular death that outperforms existing risk metrics in predicting cardiovascular death within 30, 60, 90, and 365 days on a dataset of 5000 patients.        △ Less","25 March, 2020","cs.LG,stat.ML",
              Imputation of Clinical Covariates in Time Series          ,1812.00418,https://arxiv.org/abs/1812.00418,https://arxiv.org/pdf/1812.00418,"Authors:DimitrisBertsimas,AgniOrfanoudaki,ColinPawlowski","        Missing data is a common problem in real-world settings and particularly relevant in healthcare applications where researchers use Electronic Health Records (EHR) and results of observational studies to apply analytics methods. This issue becomes even more prominent for longitudinal data sets, where multiple instances of the same individual correspond to different observations in time. Standard imputation methods do not take into account patient specific information incorporated in multivariate panel data. We introduce the novel imputation algorithm MedImpute that addresses this problem, extending the flexible framework of OptImpute suggested by Bertsimas et al. (2018). Our algorithm provides imputations for data sets with missing continuous and categorical features, and we present the formulation and implement scalable first-order methods for a KK-NN model. We test the performance of our algorithm on longitudinal data from the Framingham Heart Study when data are missing completely at random (MCAR). We demonstrate that MedImpute leads to significant improvements in both imputation accuracy and downstream model AUC compared to state-of-the-art methods.        △ Less","2 December, 2018","stat.ML,cs.LG",
              Feature Selection Based on Unique Relevant Information for Health Data          ,1812.00415,https://arxiv.org/abs/1812.00415,https://arxiv.org/pdf/1812.00415,"Authors:ShiyuLiu,MehulMotani","        Feature selection, which searches for the most representative features in observed data, is critical for health data analysis. Unlike feature extraction, such as PCA and autoencoder based methods, feature selection preserves interpretability, meaning that the selected features provide direct information about certain health conditions (i.e., the label). Thus, feature selection allows domain experts, such as clinicians, to understand the predictions made by machine learning based systems, as well as improve their own diagnostic skills. Mutual information is often used as a basis for feature selection since it measures dependencies between features and labels. In this paper, we introduce a novel mutual information based feature selection (MIBFS) method called SURI, which boosts features with high unique relevant information. We compare SURI to existing MIBFS methods using 3 different classifiers on 6 publicly available healthcare data sets. The results indicate that, in addition to preserving interpretability, SURI selects more relevant feature subsets which lead to higher classification performance. More importantly, we explore the dynamics of mutual information on a public low-dimensional health data set via exhaustive search. The results suggest the important role of unique relevant information in feature selection and verify the principles behind SURI.        △ Less","2 December, 2018","cs.LG,stat.ML",
              Vision-Based Gait Analysis for Senior Care          ,1812.00169,https://arxiv.org/abs/1812.00169,https://arxiv.org/pdf/1812.00169,"Authors:DavidXue,AninSayana,EvanDarke,KellyShen,Jun-TingHsieh,ZelunLuo,Li-JiaLi,N.LanceDowning,ArnoldMilstein,LiFei-Fei","        As the senior population rapidly increases, it is challenging yet crucial to provide effective long-term care for seniors who live at home or in senior care facilities. Smart senior homes, which have gained widespread interest in the healthcare community, have been proposed to improve the well-being of seniors living independently. In particular, non-intrusive, cost-effective sensors placed in these senior homes enable gait characterization, which can provide clinically relevant information including mobility level and early neurodegenerative disease risk. In this paper, we present a method to perform gait analysis from a single camera placed within the home. We show that we can accurately calculate various gait parameters, demonstrating the potential for our system to monitor the long-term gait of seniors and thus aid clinicians in understanding a patient's medical profile.        △ Less","1 December, 2018",cs.CV,
              A Longitudinal Study of Identifying and Paying Down Architectural Debt          ,1811.12904,https://arxiv.org/abs/1811.12904,https://arxiv.org/pdf/1811.12904,"Authors:MaleknazNayebi,YuanfangCai,RickKazman,GuentherRuhe,QiongFeng,ChrisCarlson,FrancisChew","        Architectural debt is a form of technical debt that derives from the gap between the architectural design of the system as it ""should be"" compared to ""as it is"". We measured architecture debt in two ways: 1) in terms of system-wide coupling measures, and 2) in terms of the number and severity of architectural flaws. In recent work it was shown that the amount of architectural debt has a huge impact on software maintainability and evolution. Consequently, detecting and reducing the debt is expected to make software more amenable to change. This paper reports on a longitudinal study of a healthcare communications product created by Brightsquid Secure Communications Corp. This start-up company is facing the typical trade-off problem of desiring responsiveness to change requests, but wanting to avoid the ever-increasing effort that the accumulation of quick-and-dirty changes eventually incurs. In the first stage of the study, we analyzed the status of the ""before"" system, which indicated the impacts of change requests. This initial study motivated a more in-depth analysis of architectural debt. The results of this analysis were used to motivate a comprehensive refactoring of the software system. The third phase of the study was a follow-on architectural debt analysis which quantified the improvements made. Using this quantitative evidence, augmented by qualitative evidence gathered from in-depth interviews with Brightsquid's architects, we present lessons learned about the costs and benefits of paying down architecture debt in practice.        △ Less","30 November, 2018",cs.SE,
              Time Aggregation and Model Interpretation for Deep Multivariate Longitudinal Patient Outcome Forecasting Systems in Chronic Ambulatory Care          ,1811.12589,https://arxiv.org/abs/1811.12589,https://arxiv.org/pdf/1811.12589,"Authors:BeauNorgeot,DmytroLituiev,BenjaminS.Glicksberg,AtulJ.Butte","        Clinical data for ambulatory care, which accounts for 90% of the nations healthcare spending, is characterized by relatively small sample sizes of longitudinal data, unequal spacing between visits for each patient, with unequal numbers of data points collected across patients. While deep learning has become state-of-the-art for sequence modeling, it is unknown which methods of time aggregation may be best suited for these challenging temporal use cases. Additionally, deep models are often considered uninterpretable by physicians which may prevent the clinical adoption, even of well performing models. We show that time-distributed-dense layers combined with GRUs produce the most generalizable models. Furthermore, we provide a framework for the clinical interpretation of the models.        △ Less","29 November, 2018","cs.LG,cs.AI,stat.ML",
              Rethinking clinical prediction: Why machine learning must consider year of care and feature aggregation          ,1811.12583,https://arxiv.org/abs/1811.12583,https://arxiv.org/pdf/1811.12583,"Authors:BretNestor,MatthewB.A.McDermott,GeetickaChauhan,TristanNaumann,MichaelC.Hughes,AnnaGoldenberg,MarzyehGhassemi","        Machine learning for healthcare often trains models on de-identified datasets with randomly-shifted calendar dates, ignoring the fact that data were generated under hospital operation practices that change over time. These changing practices induce definitive changes in observed data which confound evaluations which do not account for dates and limit the generalisability of date-agnostic models. In this work, we establish the magnitude of this problem on MIMIC, a public hospital dataset, and showcase a simple solution. We augment MIMIC with the year in which care was provided and show that a model trained using standard feature representations will significantly degrade in quality over time. We find a deterioration of 0.3 AUC when evaluating mortality prediction on data from 10 years later. We find a similar deterioration of 0.15 AUC for length-of-stay. In contrast, we demonstrate that clinically-oriented aggregates of raw features significantly mitigate future deterioration. Our suggested aggregated representations, when retrained yearly, have prediction quality comparable to year-agnostic models.        △ Less","29 November, 2018","cs.LG,stat.ML",
              An Introduction to Deep Reinforcement Learning          ,1811.12560,https://arxiv.org/abs/1811.12560,https://arxiv.org/pdf/1811.12560,"Authors:VincentFrancois-Lavet,PeterHenderson,RiashatIslam,MarcG.Bellemare,JoellePineau","        Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.        △ Less","3 December, 2018","cs.LG,cs.AI,stat.ML",10.1561/2200000071 
              Leveraging Clinical Time-Series Data for Prediction: A Cautionary Tale          ,1811.12520,https://arxiv.org/abs/1811.12520,https://arxiv.org/pdf/1811.12520,"Authors:EliSherman,HitinderGurm,UlyssesBalis,ScottOwens,JennaWiens","        In healthcare, patient risk stratification models are often learned using time-series data extracted from electronic health records. When extracting data for a clinical prediction task, several formulations exist, depending on how one chooses the time of prediction and the prediction horizon. In this paper, we show how the formulation can greatly impact both model performance and clinical utility. Leveraging a publicly available ICU dataset, we consider two clinical prediction tasks: in-hospital mortality, and hypokalemia. Through these case studies, we demonstrate the necessity of evaluating models using an outcome-independent reference point, since choosing the time of prediction relative to the event can result in unrealistic performance. Further, an outcome-independent scheme outperforms an outcome-dependent scheme on both tasks (In-Hospital Mortality AUROC .882 vs. .831; Serum Potassium: AUROC .829 vs. .740) when evaluated on test sets that mimic real-world use.        △ Less","29 November, 2018","cs.LG,stat.AP,stat.ML",
              Regression by clustering using Metropolis-Hastings          ,1811.12295,https://arxiv.org/abs/1811.12295,https://arxiv.org/pdf/1811.12295,"Authors:AdolfoQuiroz,SimónRamírez-Amaya,ÁlvaroRiascos",        High quality risk adjustment in health insurance markets weakens insurer incentives to engage in inefficient behavior to attract lower-cost enrollees. We propose a novel methodology based on Markov Chain Monte Carlo methods to improve risk adjustment by clustering diagnostic codes into risk groups optimal for health expenditure prediction. We test the performance of our methodology against common alternatives using panel data from 500 thousand enrollees of the Colombian Healthcare System. Results show that our methodology outperforms common alternatives and suggest that it has potential to improve access to quality healthcare for the chronically ill.        △ Less,"13 September, 2019","stat.ML,cs.LG,stat.AP,stat.ME",
              Machine Learning on Electronic Health Records: Models and Features Usages to predict Medication Non-Adherence          ,1811.12234,https://arxiv.org/abs/1811.12234,https://arxiv.org/pdf/1811.12234,"Authors:ThomasJanssoone,ClémenceBic,DorraKanoun,PierreHornus,PierreRinder","        Adherence can be defined as ""the extent to which patients take their medications as prescribed by their healthcare providers""[Osterberg and Blaschke, 2005]. World Health Organization's reports point out that, in developed countries, only about 50% of patients with chronic diseases correctly follow their treatments. This severely compromises the efficiency of long-term therapy and increases the cost of health services. We propose in this paper different models of patient drug consumption in breast cancer treatments. The aim of these different approaches is to predict medication non-adherence while giving insights to doctors of the underlying reasons of these illegitimate drop-outs. Working with oncologists, we show the interest of Machine- Learning algorithms fined tune by the feedback of experts to estimate a risk score of a patient's non-adherence and thus improve support throughout their care path.        △ Less","29 November, 2018","cs.LG,cs.AI,stat.ML",
"              Effective, Fast, and Memory-Efficient Compressed Multi-function Convolutional Neural Networks for More Accurate Medical Image Classification          ",1811.11996,https://arxiv.org/abs/1811.11996,https://arxiv.org/pdf/1811.11996,Authors:LunaM.Zhang,"        Convolutional Neural Networks (CNNs) usually use the same activation function, such as RELU, for all convolutional layers. There are performance limitations of just using RELU. In order to achieve better classification performance, reduce training and testing times, and reduce power consumption and memory usage, a new ""Compressed Multi-function CNN"" is developed. Google's Inception-V4, for example, is a very deep CNN that consists of 4 Inception-A blocks, 7 Inception-B blocks, and 3 Inception-C blocks. RELU is used for all convolutional layers. A new ""Compressed Multi-function Inception-V4"" (CMI) that can use different activation functions is created with k Inception-A blocks, m Inception-B blocks, and n Inception-C blocks where k in {1, 2, 3, 4}, m in {1, 2, 3, 4, 5, 6, 7}, n in {1, 2, 3}, and (k+m+n)<14. For performance analysis, a dataset for classifying brain MRI images into one of the four stages of Alzheimer's disease is used to compare three CMI architectures with Inception-V4 in terms of F1-score, training and testing times (related to power consumption), and memory usage (model size). Overall, simulations show that the new CMI models can outperform both the commonly used Inception-V4 and Inception-V4 using different activation functions. In the future, other ""Compressed Multi-function CNNs"", such as ""Compressed Multi-function ResNets and DenseNets"" that have a reduced number of convolutional blocks using different activation functions, will be developed to further increase classification accuracy, reduce training and testing times, reduce computational power, and reduce memory usage (model size) for building more effective healthcare systems, such as implementing accurate and convenient disease diagnosis systems on mobile devices that have limited battery power and memory.        △ Less","29 November, 2018",cs.CV,
              FADL:Federated-Autonomous Deep Learning for Distributed Electronic Health Record          ,1811.11400,https://arxiv.org/abs/1811.11400,https://arxiv.org/pdf/1811.11400,"Authors:DianboLiu,TimothyMiller,RaheelSayeed,KennethD.Mandl","        Electronic health record (EHR) data is collected by individual institutions and often stored across locations in silos. Getting access to these data is difficult and slow due to security, privacy, regulatory, and operational issues. We show, using ICU data from 58 different hospitals, that machine learning models to predict patient mortality can be trained efficiently without moving health data out of their silos using a distributed machine learning strategy. We propose a new method, called Federated-Autonomous Deep Learning (FADL) that trains part of the model using all data sources in a distributed manner and other parts using data from specific data sources. We observed that FADL outperforms traditional federated learning strategy and conclude that balance between global and local training is an important factor to consider when design distributed machine learning methods , especially in healthcare.        △ Less","2 December, 2018","cs.CY,cs.LG",
              Predicting the Flu from Instagram          ,1811.10949,https://arxiv.org/abs/1811.10949,https://arxiv.org/pdf/1811.10949,"Authors:OguzhanGencoglu,MiikkaErmes","        Conventional surveillance systems for monitoring infectious diseases, such as influenza, face challenges due to shortage of skilled healthcare professionals, remoteness of communities and absence of communication infrastructures. Internet-based approaches for surveillance are appealing logistically as well as economically. Search engine queries and Twitter have been the primarily used data sources in such approaches. The aim of this study is to assess the predictive power of an alternative data source, Instagram. By using 317 weeks of publicly available data from Instagram, we trained several machine learning algorithms to both nowcast and forecast the number of official influenza-like illness incidents in Finland where population-wide official statistics about the weekly incidents are available. In addition to date and hashtag count features of online posts, we were able to utilize also the visual content of the posted images with the help of deep convolutional neural networks. Our best nowcasting model reached a mean absolute error of 11.33 incidents per week and a correlation coefficient of 0.963 on the test data. Forecasting models for predicting 1 week and 2 weeks ahead showed statistical significance as well by reaching correlation coefficients of 0.903 and 0.862, respectively. This study demonstrates how social media and in particular, digital photographs shared in them, can be a valuable source of information for the field of infodemiology.        △ Less","27 November, 2018","cs.LG,stat.ML",
              Robustness against the channel effect in pathological voice detection          ,1811.10376,https://arxiv.org/abs/1811.10376,https://arxiv.org/pdf/1811.10376,"Authors:Yi-TeHsu,ZiningZhu,Chi-TeWang,Shih-HauFang,FrankRudzicz,YuTsao","        Many people are suffering from voice disorders, which can adversely affect the quality of their lives. In response, some researchers have proposed algorithms for automatic assessment of these disorders, based on voice signals. However, these signals can be sensitive to the recording devices. Indeed, the channel effect is a pervasive problem in machine learning for healthcare. In this study, we propose a detection system for pathological voice, which is robust against the channel effect. This system is based on a bidirectional LSTM network. To increase the performance robustness against channel mismatch, we integrate domain adversarial training (DAT) to eliminate the differences between the devices. When we train on data recorded on a high-quality microphone and evaluate on smartphone data without labels, our robust detection system increases the PR-AUC from 0.8448 to 0.9455 (and 0.9522 with target sample labels). To the best of our knowledge, this is the first study applying unsupervised domain adaptation to pathological voice detection. Notably, our system does not need target device sample labels, which allows for generalization to many new devices.        △ Less","2 December, 2018","cs.LG,cs.SD,eess.AS,stat.ML",
              Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead          ,1811.10154,https://arxiv.org/abs/1811.10154,https://arxiv.org/pdf/1811.10154,Authors:CynthiaRudin,"        Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to \textit{explain} black box models, rather than creating models that are \textit{interpretable} in the first place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward -- it is to design models that are inherently interpretable. This manuscript clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.        △ Less","21 September, 2019","stat.ML,cs.LG",
              An overview of deep learning in medical imaging focusing on MRI          ,1811.10052,https://arxiv.org/abs/1811.10052,https://arxiv.org/pdf/1811.10052,"Authors:AlexanderSelvikvågLundervold,ArvidLundervold","        What has happened in machine learning lately, and what does it mean for the future of medical image analysis? Machine learning has witnessed a tremendous amount of attention over the last few years. The current boom started around 2009 when so-called deep artificial neural networks began outperforming other established models on a number of important benchmarks. Deep neural networks are now the state-of-the-art machine learning models across a variety of areas, from image analysis to natural language processing, and widely deployed in academia and industry. These developments have a huge potential for medical imaging technology, medical data analysis, medical diagnostics and healthcare in general, slowly being realized. We provide a short overview of recent advances and some associated challenges in machine learning applied to medical image processing and image analysis. As this has become a very broad and fast expanding field we will not survey the entire landscape of applications, but put particular focus on deep learning in MRI.  Our aim is threefold: (i) give a brief introduction to deep learning with pointers to core references; (ii) indicate how deep learning has been applied to the entire MRI processing chain, from acquisition to image retrieval, from segmentation to disease prediction; (iii) provide a starting point for people interested in experimenting and perhaps contributing to the field of machine learning for medical imaging by pointing out good educational resources, state-of-the-art open-source code, and interesting sources of data and problems related medical imaging.        △ Less","16 December, 2018","cs.CV,cs.LG,stat.ML",10.1016/j.zemedi.2018.11.002 
              Privacy-Preserving Action Recognition for Smart Hospitals using Low-Resolution Depth Images          ,1811.09950,https://arxiv.org/abs/1811.09950,https://arxiv.org/pdf/1811.09950,"Authors:EdwardChou,MatthewTan,CherryZou,MichelleGuo,AlbertHaque,ArnoldMilstein,LiFei-Fei","        Computer-vision hospital systems can greatly assist healthcare workers and improve medical facility treatment, but often face patient resistance due to the perceived intrusiveness and violation of privacy associated with visual surveillance. We downsample video frames to extremely low resolutions to degrade private information from surveillance videos. We measure the amount of activity-recognition information retained in low resolution depth images, and also apply a privately-trained DCSCN super-resolution model to enhance the utility of our images. We implement our techniques with two actual healthcare-surveillance scenarios, hand-hygiene compliance and ICU activity-logging, and show that our privacy-preserving techniques preserve enough information for realistic healthcare tasks.        △ Less","25 November, 2018",cs.CV,
              Predicting Diabetes Disease Evolution Using Financial Records and Recurrent Neural Networks          ,1811.09350,https://arxiv.org/abs/1811.09350,https://arxiv.org/pdf/1811.09350,"Authors:RafaelT.Sousa,LucasA.Pereira,AndersonS.Soares","        Managing patients with chronic diseases is a major and growing healthcare challenge in several countries. A chronic condition, such as diabetes, is an illness that lasts a long time and does not go away, and often leads to the patient's health gradually getting worse. While recent works involve raw electronic health record (EHR) from hospitals, this work uses only financial records from health plan providers (medical claims) to predict diabetes disease evolution with a self-attentive recurrent neural network. The use of financial data is due to the possibility of being an interface to international standards, as the records standard encodes medical procedures. The main goal was to assess high risk diabetics, so we predict records related to diabetes acute complications such as amputations and debridements, revascularization and hemodialysis. Our work succeeds to anticipate complications between 60 to 240 days with an area under ROC curve ranging from 0.81 to 0.94. In this paper we describe the first half of a work-in-progress developed within a health plan provider with ROC curve ranging from 0.81 to 0.83. This assessment will give healthcare providers the chance to intervene earlier and head off hospitalizations. We are aiming to deliver personalized predictions and personalized recommendations to individual patients, with the goal of improving outcomes and reducing costs        △ Less","20 March, 2020","cs.LG,stat.ML",
              Fog Computing Architecture: Survey and Challenges          ,1811.09047,https://arxiv.org/abs/1811.09047,https://arxiv.org/pdf/1811.09047,"Authors:RaneshKumarNaha,SaurabhGarg,AndrewChan","        Emerging technologies that generate a huge amount of data such as the Internet of Things (IoT) services need latency aware computing platforms to support time-critical applications. Due to the on-demand services and scalability features of cloud computing, Big Data application processing is done in the cloud infrastructure. Managing Big Data applications exclusively in the cloud is not an efficient solution for latency-sensitive applications related to smart transportation systems, healthcare solutions, emergency response systems and content delivery applications. Thus, the Fog computing paradigm that allows applications to perform computing operations in-between the cloud and the end devices has emerged. In Fog architecture, IoT devices and sensors are connected to the Fog devices which are located in close proximity to the users and it is also responsible for intermediate computation and storage. Most computations will be done on the edge by eliminating full dependencies on the cloud resources. In this chapter, we investigate and survey Fog computing architectures which have been proposed over the past few years. Moreover, we study the requirements of IoT applications and platforms, and the limitations faced by cloud systems when executing IoT applications. Finally, we review current research works that particularly focus on Big Data application execution on Fog and address several open challenges as well as future research directions.        △ Less","22 November, 2018",cs.DC,
              Robust Active Learning for Electrocardiographic Signal Classification          ,1811.08919,https://arxiv.org/abs/1811.08919,https://arxiv.org/pdf/1811.08919,"Authors:XuChen,SaratenduSethi","        The classification of electrocardiographic (ECG) signals is a challenging problem for healthcare industry. Traditional supervised learning methods require a large number of labeled data which is usually expensive and difficult to obtain for ECG signals. Active learning is well-suited for ECG signal classification as it aims at selecting the best set of labeled data in order to maximize the classification performance. Motivated by the fact that ECG data are usually heavily unbalanced among different classes and the class labels are noisy as they are manually labeled, this paper proposes a novel solution based on robust active learning for addressing these challenges. The key idea is to first apply the clustering of the data in a low dimensional embedded space and then select the most information instances within local clusters. By selecting the most informative instances relying on local average minimal distances, the algorithm tends to select the data for labelling in a more diversified way. Finally, the robustness of the model is further enhanced by adding a novel noisy label reduction scheme after the selection of the labeled data. Experiments on the ECG signal classification from the MIT-BIH arrhythmia database demonstrate the effectiveness of the proposed algorithm.        △ Less","21 November, 2018","eess.SP,cs.LG,stat.ML",
              Generating Classes of 3D Virtual Mandibles for AR-Based Medical Simulation          ,1811.08053,https://arxiv.org/abs/1811.08053,https://arxiv.org/pdf/1811.08053,"Authors:NehaR.Hippalgaonkar,AlexaD.Sider,FelixG.Hamza-Lup,AnandP.Santhanam,BalaJaganathan,CelinaImielinska,JannickP.Rolland","        Simulation and modeling represent promising tools for several application domains from engineering to forensic science and medicine. Advances in 3D imaging technology convey paradigms such as augmented reality (AR) and mixed reality inside promising simulation tools for the training industry. Motivated by the requirement for superimposing anatomically correct 3D models on a Human Patient Simulator (HPS) and visualizing them in an AR environment, the purpose of this research effort is to derive method for scaling a source human mandible to a target human mandible. Results show that, given a distance between two same landmarks on two different mandibles, a relative scaling factor may be computed. Using this scaling factor, results show that a 3D virtual mandible model can be made morphometrically equivalent to a real target-specific mandible within a 1.30 millimeter average error bound. The virtual mandible may be further used as a reference target for registering other anatomical models, such as the lungs, on the HPS. Such registration will be made possible by physical constraints among the mandible and the spinal column in the horizontal normal rest position.        △ Less","19 November, 2018",cs.GR,10.1097/SIH.0b013e31816b5d54 
              ApproxCS: Near-Sensor Approximate Compressed Sensing for IoT-Healthcare Systems          ,1811.07330,https://arxiv.org/abs/1811.07330,https://arxiv.org/pdf/1811.07330,"Authors:AyeshaSiddique,OsmanHasan,FaiqKhalid,MuhammadShafique","        Internet of Things (IoTs) is an emerging trend that has enabled an upgrade in the design of wearable healthcare monitoring systems through the (integrated) edge, fog, and cloud computing paradigm. Energy efficiency is one of the most important design metrics in such IoT-healthcare systems especially, for the edge and fog nodes. Due to the sensing noise and inherent redundancy in the input data, even the most safety-critical biomedical applications can sometimes afford a slight degradation in the output quality. Hence, such inherent error tolerance in the bio-signals can be exploited to achieve high energy savings through the emerging trends like, the Approximate Computing which is applicable at both software and hardware levels. In this paper, we propose to leverage the approximate computing in digital Compressed Sensing (CS), through low-power approximate adders (LPAA) in an accurate Bernoulli sensing-based CS acquisition (BCS). We demonstrate that approximations can indeed be safely employed in IoT healthcare without affecting the detection of critical events in the biomedical signals. Towards this, we explored the trade-of between energy efficiency and output quality using the state-of-the-art lp2d RLS reconstruction algorithm. The proposed framework is validated with the MIT-BIH Arrhythmia database. Our results demonstrated approximately 59% energy savings as compared to the accurate design.        △ Less","18 November, 2018",eess.SP,
              Contextual Care Protocol using Neural Networks and Decision Trees          ,1811.06437,https://arxiv.org/abs/1811.06437,https://arxiv.org/pdf/1811.06437,"Authors:YashPratyushSinha,PranshuMalviya,MinervaPanda,SyedMohdAli","        A contextual care protocol is used by a medical practitioner for patient healthcare, given the context or situation that the specified patient is in. This paper proposes a method to build an automated self-adapting protocol which can help make relevant, early decisions for effective healthcare delivery. The hybrid model leverages neural networks and decision trees. The neural network estimates the chances of each disease and each tree in the decision trees represents care protocol for a disease. These trees are subject to change in case of aberrations found by the diagnosticians. These corrections or prediction errors are clustered into similar groups for scalability and review by the experts. The corrections as suggested by the experts are incorporated into the model.        △ Less","15 November, 2018","cs.LG,cs.CY,stat.ML",10.1109/ICAECC.2018.8479433 
              QUENN: QUantization Engine for low-power Neural Networks          ,1811.05896,https://arxiv.org/abs/1811.05896,https://arxiv.org/pdf/1811.05896,"Authors:MigueldePrado,MaurizioDenna,LucaBenini,NuriaPazos","        Deep Learning is moving to edge devices, ushering in a new age of distributed Artificial Intelligence (AI). The high demand of computational resources required by deep neural networks may be alleviated by approximate computing techniques, and most notably reduced-precision arithmetic with coarsely quantized numerical representations. In this context, Bonseyes comes in as an initiative to enable stakeholders to bring AI to low-power and autonomous environments such as: Automotive, Medical Healthcare and Consumer Electronics. To achieve this, we introduce LPDNN, a framework for optimized deployment of Deep Neural Networks on heterogeneous embedded devices. In this work, we detail the quantization engine that is integrated in LPDNN. The engine depends on a fine-grained workflow which enables a Neural Network Design Exploration and a sensitivity analysis of each layer for quantization. We demonstrate the engine with a case study on Alexnet and VGG16 for three different techniques for direct quantization: standard fixed-point, dynamic fixed-point and k-means clustering, and demonstrate the potential of the latter. We argue that using a Gaussian quantizer with k-means clustering can achieve better performance than linear quantizers. Without retraining, we achieve over 55.64\% saving for weights' storage and 69.17\% for run-time memory accesses with less than 1\% drop in top5 accuracy in Imagenet.        △ Less","14 November, 2018","cs.NE,cs.LG",10.1145/3203217.3203282 
              From Free Text to Clusters of Content in Health Records: An Unsupervised Graph Partitioning Approach          ,1811.05711,https://arxiv.org/abs/1811.05711,https://arxiv.org/pdf/1811.05711,"Authors:M.TarikAltuncu,ErikMayer,SophiaN.Yaliraki,MauricioBarahona","        Electronic Healthcare records contain large volumes of unstructured data in different forms. Free text constitutes a large portion of such data, yet this source of richly detailed information often remains under-used in practice because of a lack of suitable methodologies to extract interpretable content in a timely manner. Here we apply network-theoretical tools to the analysis of free text in Hospital Patient Incident reports in the English National Health Service, to find clusters of reports in an unsupervised manner and at different levels of resolution based directly on the free text descriptions contained within them. To do so, we combine recently developed deep neural network text-embedding methodologies based on paragraph vectors with multi-scale Markov Stability community detection applied to a similarity graph of documents obtained from sparsified text vector similarities. We showcase the approach with the analysis of incident reports submitted in Imperial College Healthcare NHS Trust, London. The multiscale community structure reveals levels of meaning with different resolution in the topics of the dataset, as shown by relevant descriptive terms extracted from the groups of records, as well as by comparing a posteriori against hand-coded categories assigned by healthcare personnel. Our content communities exhibit good correspondence with well-defined hand-coded categories, yet our results also provide further medical detail in certain areas as well as revealing complementary descriptors of incidents beyond the external classification. We also discuss how the method can be used to monitor reports over time and across different healthcare providers, and to detect emerging trends that fall outside of pre-existing categories.        △ Less","14 November, 2018","cs.CL,cs.IR,cs.LG,cs.SI,math.SP",10.1007/s41109-018-0109-9 
"              Health Care Expenditures, Financial Stability, and Participation in the Supplemental Nutrition Assistance Program (SNAP)          ",1811.05421,https://arxiv.org/abs/1811.05421,https://arxiv.org/pdf/1811.05421,"Authors:YunheeChang,JinheeKim,SwarnChatterjee","        This paper examines the association between household healthcare expenses and participation in the Supplemental Nutrition Assistance Program (SNAP) when moderated by factors associated with financial stability of households. Using a large longitudinal panel encompassing eight years, this study finds that an inter-temporal increase in out-of-pocket medical expenses increased the likelihood of household SNAP participation in the current period. Financially stable households with precautionary financial assets to cover at least 6 months worth of household expenses were significantly less likely to participate in SNAP. The low income households who recently experienced an increase in out of pocket medical expenses but had adequate precautionary savings were less likely than similar households who did not have precautionary savings to participate in SNAP. Implications for economists, policy makers, and household finance professionals are discussed.        △ Less","13 November, 2018",econ.GN,
              The need for an integrative thinking to fight against emerging infectious diseases          ,1811.05205,https://arxiv.org/abs/1811.05205,https://arxiv.org/pdf/1811.05205,"Authors:C.Burdet,J.Guegan,X.Duval,M.LeTyrant,H.Bergeron,C.Manuguerra,J.Raude,C.Leport,P.Zylberman","        We present here the proceedings of the 5th seminar on emerging infectious diseases (EIDs), held in Paris on March 22nd, 2016, with seven  priority proposals that can be outlined as follows:∙\bulletEncourage research on the prediction, screening and early detection of new risks of infection∙\bulletDevelop research and surveillance concerning transmission of pathogens between animals and humans, with their reinforcement in particular in intertropical areas (`hot-spots') thanks to public support∙\bulletPursue aid development and support in these areas of prevention and training for local health personnel, and to foster risk awareness in the population∙\bulletEnsure adapted patient care in order to promote adherence to treatment and to epidemic propagation reduction measures∙\bulletDevelop greater sensitization and training among politicians and healthcare providers, in order to better prepare them to respond to new types of crises∙\bulletModify the logic of governance, drawing from all available modes of communication and incorporating new information-sharing tools∙\bulletDevelop economic research on the fight against EIDs, taking into account specific driving factors in order to create a balance between preventive and treatment approaches.        △ Less","13 November, 2018",q-bio.OT,10.1016/j.respe.2017.08.001 
              Learning Temporal Point Processes via Reinforcement Learning          ,1811.05016,https://arxiv.org/abs/1811.05016,https://arxiv.org/pdf/1811.05016,"Authors:ShuangLi,ShuaiXiao,ShixiangZhu,NanDu,YaoXie,LeSong","        Social goods, such as healthcare, smart city, and information networks, often produce ordered event data in continuous time. The generative processes of these event data can be very complex, requiring flexible models to capture their dynamics. Temporal point processes offer an elegant framework for modeling event data without discretizing the time. However, the existing maximum-likelihood-estimation (MLE) learning paradigm requires hand-crafting the intensity function beforehand and cannot directly monitor the goodness-of-fit of the estimated model in the process of training. To alleviate the risk of model-misspecification in MLE, we propose to generate samples from the generative model and monitor the quality of the samples in the process of training until the samples and the real data are indistinguishable. We take inspiration from reinforcement learning (RL) and treat the generation of each event as the action taken by a stochastic policy. We parameterize the policy as a flexible recurrent neural network and gradually improve the policy to mimic the observed event distribution. Since the reward function is unknown in this setting, we uncover an analytic and nonparametric form of the reward function using an inverse reinforcement learning formulation. This new RL framework allows us to derive an efficient policy gradient algorithm for learning flexible point process models, and we show that it performs well in both synthetic and real data.        △ Less","12 November, 2018","cs.LG,stat.ML",
              Efficient Reduced-Order Models for Soft Actuators          ,1811.04764,https://arxiv.org/abs/1811.04764,https://arxiv.org/pdf/1811.04764,"Authors:YueChen,KevinC.Galloway,IsuruS.Godage","        Soft robotics have gained increased attention from the robotic community due to their unique features such as compliance and human safety. Impressive amount of soft robotic prototypes have shown their superior performance over their rigid counter parts in healthcare, rehabilitation, and search and rescue applications. However, soft robots are yet to capitalize on their potential outside laboratories and this could be attributed to lack of advanced sensing capabilities and real-time dynamic models. In this pilot study, we explore the use of high-accuracy, high-bandwidth deformation sensing via fiber optic strain sensing (FOSS) in soft bending actuators (SBA). Based on the high density sensor feedback, we introduce a reduced order kinematic model. Together with cubic spline interpolation, this model is able to reconstruct the continuous deformation of SBAs. The kinematic model is extended to derive an efficient real-time equation of motion and validated against the experimental data.        △ Less","12 November, 2018",cs.RO,
              Deep Ensemble Bayesian Active Learning : Addressing the Mode Collapse issue in Monte Carlo dropout via Ensembles          ,1811.03897,https://arxiv.org/abs/1811.03897,https://arxiv.org/pdf/1811.03897,"Authors:RemusPop,PatricFulop","        In image classification tasks, the ability of deep CNNs to deal with complex image data has proven to be unrivalled. However, they require large amounts of labeled training data to reach their full potential. In specialised domains such as healthcare, labeled data can be difficult and expensive to obtain. Active Learning aims to alleviate this problem, by reducing the amount of labelled data needed for a specific task while delivering satisfactory performance. We propose DEBAL, a new active learning strategy designed for deep neural networks. This method improves upon the current state-of-the-art deep Bayesian active learning method, which suffers from the mode collapse problem. We correct for this deficiency by making use of the expressive power and statistical properties of model ensembles. Our proposed method manages to capture superior data uncertainty, which translates into improved classification performance. We demonstrate empirically that our ensemble method yields faster convergence of CNNs trained on the MNIST and CIFAR-10 datasets.        △ Less","9 November, 2018","cs.LG,stat.ML",
              Deep Learning Predicts Hip Fracture using Confounding Patient and Healthcare Variables          ,1811.03695,https://arxiv.org/abs/1811.03695,https://arxiv.org/pdf/1811.03695,"Authors:MarcusA.Badgeley,JohnR.Zech,LukeOakden-Rayner,BenjaminS.Glicksberg,ManwayLiu,WilliamGale,MichaelV.McConnell,BethPercha,ThomasM.Snyder,JoelT.Dudley","        Hip fractures are a leading cause of death and disability among older adults. Hip fractures are also the most commonly missed diagnosis on pelvic radiographs. Computer-Aided Diagnosis (CAD) algorithms have shown promise for helping radiologists detect fractures, but the image features underpinning their predictions are notoriously difficult to understand. In this study, we trained deep learning models on 17,587 radiographs to classify fracture, five patient traits, and 14 hospital process variables. All 20 variables could be predicted from a radiograph (p < 0.05), with the best performances on scanner model (AUC=1.00), scanner brand (AUC=0.98), and whether the order was marked ""priority"" (AUC=0.79). Fracture was predicted moderately well from the image (AUC=0.78) and better when combining image features with patient data (AUC=0.86, p=2e-9) or patient data plus hospital process features (AUC=0.91, p=1e-21). The model performance on a test set with matched patient variables was significantly lower than a random test set (AUC=0.67, p=0.003); and when the test set was matched on patient and image acquisition variables, the model performed randomly (AUC=0.52, 95% CI 0.46-0.58), indicating that these variables were the main source of the model's predictive ability overall. We also used Naive Bayes to combine evidence from image models with patient and hospital data and found their inclusion improved performance, but that this approach was nevertheless inferior to directly modeling all variables. If CAD algorithms are inexplicably leveraging patient and process variables in their predictions, it is unclear how radiologists should interpret their predictions in the context of other known patient data. Further research is needed to illuminate deep learning decision processes so that computers and clinicians can effectively cooperate.        △ Less","8 November, 2018",cs.CV,
              Phenotyping Endometriosis through Mixed Membership Models of Self-Tracking Data          ,1811.03431,https://arxiv.org/abs/1811.03431,https://arxiv.org/pdf/1811.03431,"Authors:IñigoUrteaga,MollieMcKillop,SharonLipsky-Gorman,NoémieElhadad","        We investigate the use of self-tracking data and unsupervised mixed-membership models to phenotype endometriosis. Endometriosis is a systemic, chronic condition of women in reproductive age and, at the same time, a highly enigmatic condition with no known biomarkers to monitor its progression and no established staging. We leverage data collected through a self-tracking app in an observational research study of over 2,800 women with endometriosis tracking their condition over a year and a half (456,900 observations overall). We extend a classical mixed-membership model to accommodate the idiosyncrasies of the data at hand (i.e., the multimodality of the tracked variables). Our experiments show that our approach identifies potential subtypes that are robust in terms of biases of self-tracked data (e.g., wide variations in tracking frequency amongst participants), as well as to variations in hyperparameters of the model. Jointly modeling a wide range of observations about participants (symptoms, quality of life, treatments) yields clinically meaningful subtypes that both validate what is already known about endometriosis and suggest new findings.        △ Less","6 November, 2018","cs.CY,cs.IR,cs.LG,stat.ML",
              Automated Remote Patient Monitoring: Data Sharing and Privacy Using Blockchain          ,1811.03417,https://arxiv.org/abs/1811.03417,https://arxiv.org/pdf/1811.03417,"Authors:GautamSrivastava,AshutoshDharDwivedi,RajaniSingh","        The revolution of Internet of Things (IoT) devices and wearable technology has opened up great possibilities in remote patient monitoring. To streamline the diagnosis and treatment process, healthcare professionals are now adopting the wearable technology. However, these technologies also pose grave privacy risks and security concerns about the transfer and the logging of data transactions. One solution to protect privacy in healthcare is the use of blockchain technology. However, one of the primary problems with blockchain is its highly limited scalability. In this work here, we propose the utilization of a blockchain based protocol to provide secure management and analysis of data. In this paper we use recently introduced PoW based protocol GHOSTDAG, that generalizes Satoshi's blockchain to a direct acyclic graph of blocks (blockDAG) and provides high throughput while also avoiding the security-scalability problem. We use two blockchains based on the original GHOSTDAG protocol, one that is private and one that is public. Using a private blockchain, we create a system where we use smart contracts to analyze patient health data. If the smart contract for any reason issues an alert for an abnormal reading then the system makes the record of that event to the public blockchain. This would resolve the privacy and security vulnerabilities associated with remote patient monitoring and also the limited scalability problem of Satoshi's original blockchain.        △ Less","30 October, 2018","cs.CY,cs.CR",
              BPDS: A Blockchain based Privacy-Preserving Data Sharing for Electronic Medical Records          ,1811.03223,https://arxiv.org/abs/1811.03223,https://arxiv.org/pdf/1811.03223,"Authors:JingweiLiu,XiaoluLi,LinYe,HongliZhang,XiaojiangDu,MohsenGuizani","        Electronic medical record (EMR) is a crucial form of healthcare data, currently drawing a lot of attention. Sharing health data is considered to be a critical approach to improve the quality of healthcare service and reduce medical costs. However, EMRs are fragmented across decentralized hospitals, which hinders data sharing and puts patients' privacy at risks. To address these issues, we propose a blockchain based privacy-preserving data sharing for EMRs, called BPDS. In BPDS, the original EMRs are stored securely in the cloud and the indexes are reserved in a tamper-proof consortium blockchain. By this means, the risk of the medical data leakage could be greatly reduced, and at the same time, the indexes in blockchain ensure that the EMRs can not be modified arbitrarily. Secure data sharing can be accomplished automatically according to the predefined access permissions of patients through the smart contracts of blockchain. Besides, the joint-design of the CP-ABE-based access control mechanism and the content extraction signature scheme provides strong privacy preservation in data sharing. Security analysis shows that BPDS is a secure and effective way to realize data sharing for EMRs.        △ Less","7 November, 2018",cs.CR,
              Policy Certificates: Towards Accountable Reinforcement Learning          ,1811.03056,https://arxiv.org/abs/1811.03056,https://arxiv.org/pdf/1811.03056,"Authors:ChristophDann,LihongLi,WeiWei,EmmaBrunskill","        The performance of a reinforcement learning algorithm can vary drastically during learning because of exploration. Existing algorithms provide little information about the quality of their current policy before executing it, and thus have limited use in high-stakes applications like healthcare. We address this lack of accountability by proposing that algorithms output policy certificates. These certificates bound the sub-optimality and return of the policy in the next episode, allowing humans to intervene when the certified quality is not satisfactory. We further introduce two new algorithms with certificates and present a new framework for theoretical analysis that guarantees the quality of their policies and certificates. For tabular MDPs, we show that computing certificates can even improve the sample-efficiency of optimism-based exploration. As a result, one of our algorithms is the first to achieve minimax-optimal PAC bounds up to lower-order terms, and this algorithm also matches (and in some settings slightly improves upon) existing minimax regret bounds.        △ Less","27 May, 2019","cs.LG,cs.AI,stat.ML",
              Mobile Edge Cloud: Opportunities and Challenges          ,1811.01929,https://arxiv.org/abs/1811.01929,https://arxiv.org/pdf/1811.01929,Authors:SayedChhattanShah,"        Mobile edge cloud is emerging as a promising technology to the internet of things and cyber-physical system applications such as smart home and intelligent video surveillance. In a smart home, various sensors are deployed to monitor the home environment and physiological health of individuals. The data collected by sensors are sent to an application, where numerous algorithms for emotion and sentiment detection, activity recognition and situation management are applied to provide healthcare- and emergency-related services and to manage resources at the home. The executions of these algorithms require a vast amount of computing and storage resources. To address the issue, the conventional approach is to send the collected data to an application on an internet cloud. This approach has several problems such as high communication latency, communication energy consumption and unnecessary data traffic to the core network. To overcome the drawbacks of the conventional cloud-based approach, a new system called mobile edge cloud is proposed. In mobile edge cloud, multiple mobiles and stationary devices interconnected through wireless local area networks are combined to create a small cloud infrastructure at a local physical area such as a home. Compared to traditional mobile distributed computing systems, mobile edge cloud introduces several complex challenges due to the heterogeneous computing environment, heterogeneous and dynamic network environment, node mobility, and limited battery power. The real-time requirements associated with the internet of things and cyber-physical system applications make the problem even more challenging. In this paper, we describe the applications and challenges associated with the design and development of mobile edge cloud system and propose an architecture based on a cross layer design approach for effective decision making.        △ Less","3 November, 2018",cs.DC,
              Effective Feature Representation for Clinical Text Concept Extraction          ,1811.00070,https://arxiv.org/abs/1811.00070,https://arxiv.org/pdf/1811.00070,"Authors:YifengTao,BrunoGodefroy,GuillaumeGenthial,ChristopherPotts","        Crucial information about the practice of healthcare is recorded only in free-form text, which creates an enormous opportunity for high-impact NLP. However, annotated healthcare datasets tend to be small and expensive to obtain, which raises the question of how to make maximally efficient uses of the available data. To this end, we develop an LSTM-CRF model for combining unsupervised word representations and hand-built feature representations derived from publicly available healthcare ontologies. We show that this combined model yields superior performance on five datasets of diverse kinds of healthcare text (clinical, social, scientific, commercial). Each involves the labeling of complex, multi-word spans that pick out different healthcare concepts. We also introduce a new labeled dataset for identifying the treatment relations between drugs and diseases.        △ Less","5 April, 2019",cs.CL,
              Multimodal Machine Learning for Automated ICD Coding          ,1810.13348,https://arxiv.org/abs/1810.13348,https://arxiv.org/pdf/1810.13348,"Authors:KeyangXu,MikeLam,JingzhiPang,XinGao,CharlotteBand,PiyushMathurMD,FrankPapayMD,AshishK.KhannaMD,JacekB.CywinskiMD,KamalMaheshwariMD,PengtaoXie,EricXing","        This study presents a multimodal machine learning model to predict ICD-10 diagnostic codes. We developed separate machine learning models that can handle data from different modalities, including unstructured text, semi-structured text and structured tabular data. We further employed an ensemble method to integrate all modality-specific models to generate ICD-10 codes. Key evidence was also extracted to make our prediction more convincing and explainable. We used the Medical Information Mart for Intensive Care III (MIMIC -III) dataset to validate our approach. For ICD code prediction, our best-performing model (micro-F1 = 0.7633, micro-AUC = 0.9541) significantly outperforms other baseline models including TF-IDF (micro-F1 = 0.6721, micro-AUC = 0.7879) and Text-CNN model (micro-F1 = 0.6569, micro-AUC = 0.9235). For interpretability, our approach achieves a Jaccard Similarity Coefficient (JSC) of 0.1806 on text data and 0.3105 on tabular data, where well-trained physicians achieve 0.2780 and 0.5002 respectively.        △ Less","6 August, 2019","cs.LG,stat.ML",
"              Prior-preconditioned conjugate gradient method for accelerated Gibbs sampling in ""large nn & large pp"" sparse Bayesian regression          ",1810.12437,https://arxiv.org/abs/1810.12437,https://arxiv.org/pdf/1810.12437,"Authors:AkihikoNishimura,MarcA.Suchard","        In a modern observational study based on healthcare databases, the number of observations and of predictors typically range in the order of 10510^5 ~ 10610^6 and of 10410^4 ~ 10510^5. Despite the large sample size, data rarely provide sufficient information to reliably estimate such a large number of parameters. Sparse regression techniques provide potential solutions, one notable approach being the Bayesian methods based on shrinkage priors. In the ""large nn & large pp"" setting, however, posterior computation encounters a major bottleneck at repeated sampling from a high-dimensional Gaussian distribution, whose precision matrix ΦΦ is expensive to compute and factorize. In this article, we present a novel algorithm to speed up this bottleneck based on the following observation: we can cheaply generate a random vector bb such that the solution to the linear system Φβ=bΦβ= b has the desired Gaussian distribution. We can then solve the linear system by the conjugate gradient (CG) algorithm through matrix-vector multiplications by ΦΦ, without ever explicitly inverting ΦΦ. Rapid convergence of CG in this specific context is achieved by the theory of prior-preconditioning we develop. We apply our algorithm to a clinically relevant large-scale observational study with nn = 72,489 patients and pp = 22,175 clinical covariates, designed to assess the relative risk of adverse events from two alternative blood anti-coagulants. Our algorithm demonstrates an order of magnitude speed-up in the posterior computation.        △ Less","17 January, 2020","stat.CO,stat.ML",
              Investigating IoT Middleware Platforms for Smart Application Development          ,1810.12292,https://arxiv.org/abs/1810.12292,https://arxiv.org/pdf/1810.12292,"Authors:PreetiAgarwal,MansafAlam","        With the growing number of Internet of Things (IoT) devices, the data generated through these devices is also increasing. By 2030, it is been predicted that the number of IoT devices will exceed the number of human beings on earth. This gives rise to the requirement of middleware platform that can manage IoT devices, intelligently store and process gigantic data generated for building smart applications such as Smart Cities, Smart Healthcare, Smart Industry, and others. At present, market is overwhelming with the number of IoT middleware platforms with specific features. This raises one of the most serious and least discussed challenge for application developer to choose suitable platform for their application development. Across the literature, very little attempt is done in classifying or comparing IoT middleware platforms for the applications. This paper categorizes IoT platforms into four categories namely-publicly traded, open source, developer friendly and end-to-end connectivity. Some of the popular middleware platforms in each category are investigated based on general IoT architecture. Comparison of IoT middleware platforms in each category, based on basic, sensing, communication and application development features is presented. This study can be useful for IoT application developers to select the most appropriate platform according to their application requirement.        △ Less","9 February, 2019",cs.DC,
              MCA-based Rule Mining Enables Interpretable Inference in Clinical Psychiatry          ,1810.11558,https://arxiv.org/abs/1810.11558,https://arxiv.org/pdf/1810.11558,"Authors:QingzhuGao,HumbertoGonzalez,ParvezAhammad","        Development of interpretable machine learning models for clinical healthcare applications has the potential of changing the way we understand, treat, and ultimately cure, diseases and disorders in many areas of medicine. These models can serve not only as sources of predictions and estimates, but also as discovery tools for clinicians and researchers to reveal new knowledge from the data. High dimensionality of patient information (e.g., phenotype, genotype, and medical history), lack of objective measurements, and the heterogeneity in patient populations often create significant challenges in developing interpretable machine learning models for clinical psychiatry in practice. In this paper we take a step towards the development of such interpretable models. First, by developing a novel categorical rule mining method based on Multivariate Correspondence Analysis (MCA) capable of handling datasets with large numbers of features, and second, by applying this method to build transdiagnostic Bayesian Rule List models to screen for psychiatric disorders using the Consortium for Neuropsychiatric Phenomics dataset. We show that our method is not only at least 100 times faster than state-of-the-art rule mining techniques for datasets with 50 features, but also provides interpretability and comparable prediction accuracy across several benchmark datasets.        △ Less","16 December, 2018","cs.LG,stat.ML",10.1007/978-3-030-24409-5_3 
              Intermediated Implementation          ,1810.11475,https://arxiv.org/abs/1810.11475,https://arxiv.org/pdf/1810.11475,"Authors:AnqiLi,YiqingXing","        We examine problems of ``intermediated implementation,'' in which a single principal can only regulate limited aspects of the consumption bundles traded between intermediaries and agents with hidden characteristics. An example is sales, in which retailers offer menus of consumption bundles to customers with hidden tastes, whereas a manufacturer with a potentially different goal from retailers' is limited to regulating sold consumption goods but not retail prices by legal barriers. We study how the principal can implement through intermediaries any social choice rule that is incentive compatible and individually rational for agents. We demonstrate the effectiveness of per-unit fee schedules and distribution regulations, which hinges on whether intermediaries have private or interdependent values. We give further applications to healthcare regulation and income redistribution.        △ Less","20 January, 2020","econ.TH,econ.GN",
              Building Reality Checks into the Translational Pathway for Diagnostic and Prognostic Models          ,1810.10936,https://arxiv.org/abs/1810.10936,https://arxiv.org/pdf/1810.10936,"Authors:DennisWLendrem,BClareLendrem,ArthurGPratt,JessicaRTarn,AndrewSkelton,KathrynJames,PeterMcMeekin,MattLinsley,ColinGillespie,HeatherCordell,Wan-FaiNg,JohnDIsaacs","        There has been a significant increase in the number of diagnostic and prognostic models published in the last decade. Testing such models in an independent, external validation cohort gives some assurance the model will transfer to a naturalistic, healthcare setting. Of 2,147 published models in the PubMed database, we found just 120 included some kind of separate external validation cohort. Of these studies not all were sufficiently well documented to allow a judgement about whether that model was likely to transfer to other centres, with other patients, treated by other clinicians, using data scored or analysed by other laboratories. We offer a solution to better characterizing the validation cohort and identify the key steps on the translational pathway for diagnostic and prognostic models.        △ Less","25 October, 2018",stat.AP,
              Urban Healthcare Big Data System Based on Crowdsourced and Cloud-Based Air Quality Indicators          ,1810.10723,https://arxiv.org/abs/1810.10723,https://arxiv.org/pdf/1810.10723,"Authors:MinChen,JunYang,LongHu,M.ShamimHossain,GhulamMuhammad","        The ever-accelerating process of globalization enables more than half the population to live in cities. Thus, the air quality in cities exerts critical influence on the health status of more and more urban residents. In this article, based on urban air quality data collected through meteorological sites, mobile crowdsourcing, and IoT sensing, along with users' body signals, we propose an urban healthcare big data system named UH-BigDataSys. In this article, we first introduce a method of integrating multi-source air quality data for the data preparation of artificial-intelligence-based smart urban services. Then a testbed of UH-BigDataSys is set up with the deployment of air-quality-aware healthcare applications. Finally, we provide health guidance for urban residents in aspects of respiratory diseases, outdoor travel, sleep quality, and so on. The ultimate goal of UH-BigDataSys is for urban residents to lead healthier lives.        △ Less","25 October, 2018",cs.CY,
              Label Propagation for Learning with Label Proportions          ,1810.10328,https://arxiv.org/abs/1810.10328,https://arxiv.org/pdf/1810.10328,"Authors:RafaelPoyiadzi,RaulSantos-Rodriguez,NiallTwomey","        Learning with Label Proportions (LLP) is the problem of recovering the underlying true labels given a dataset when the data is presented in the form of bags. This paradigm is particularly suitable in contexts where providing individual labels is expensive and label aggregates are more easily obtained. In the healthcare domain, it is a burden for a patient to keep a detailed diary of their daily routines, but often they will be amenable to provide higher level summaries of daily behavior. We present a novel and efficient graph-based algorithm that encourages local smoothness and exploits the global structure of the data, while preserving the `mass' of each bag.        △ Less","24 October, 2018","cs.LG,stat.ML",
              Mechanism Design for Social Good          ,1810.09832,https://arxiv.org/abs/1810.09832,https://arxiv.org/pdf/1810.09832,"Authors:RedietAbebe,KiraGoldner","        Across various domains--such as health, education, and housing--improving societal welfare involves allocating resources, setting policies, targeting interventions, and regulating activities. These solutions have an immense impact on the day-to-day lives of individuals, whether in the form of access to quality healthcare, labor market outcomes, or how votes are accounted for in a democratic society. Problems that can have an out-sized impact on individuals whose opportunities have historically been limited often pose conceptual and technical challenges, requiring insights from many disciplines. Conversely, the lack of interdisciplinary approach can leave these urgent needs unaddressed and can even exacerbate underlying socioeconomic inequalities. To realize the opportunities in these domains, we need to correctly set objectives and reason about human behavior and actions. Doing so requires a deep grounding in the field of interest and collaboration with domain experts who understand the societal implications and feasibility of proposed solutions. These insights can play an instrumental role in proposing algorithmically-informed policies.  In this article, we describe the Mechanism Design for Social Good (MD4SG) research agenda, which involves using insights from algorithms, optimization, and mechanism design to improve access to opportunity. The MD4SG research community takes an interdisciplinary, multi-stakeholder approach to improve societal welfare. We discuss three exciting research avenues within MD4SG related to improving access to opportunity in the developing world, labor markets and discrimination, and housing. For each of these, we showcase ongoing work, underline new directions, and discuss potential for implementing existing work in practice.        △ Less","21 October, 2018","cs.GT,cs.AI,cs.CY,cs.DS",10.1145/3284751.328476 
              MiME: Multilevel Medical Embedding of Electronic Health Records for Predictive Healthcare,1810.09593,https://arxiv.org/abs/1810.09593,https://arxiv.org/pdf/1810.09593,"Authors:EdwardChoi,CaoXiao,WalterF.Stewart,JimengSun","        Deep learning models exhibit state-of-the-art performance for many predictive healthcare tasks using electronic health records (EHR) data, but these models typically require training data volume that exceeds the capacity of most healthcare systems. External resources such as medical ontologies are used to bridge the data volume constraint, but this approach is often not directly applicable or useful because of inconsistencies with terminology. To solve the data insufficiency challenge, we leverage the inherent multilevel structure of EHR data and, in particular, the encoded relationships among medical codes. We propose Multilevel Medical Embedding (MiME) which learns the multilevel embedding of EHR data while jointly performing auxiliary prediction tasks that rely on this inherent EHR structure without the need for external labels. We conducted two prediction tasks, heart failure prediction and sequential disease prediction, where MiME outperformed baseline methods in diverse evaluation settings. In particular, MiME consistently outperformed all baselines when predicting heart failure on datasets of different volumes, especially demonstrating the greatest performance improvement (15% relative gain in PR-AUC over the best baseline) on the smallest dataset, demonstrating its ability to effectively model the multilevel structure of EHR data.        △ Less","22 October, 2018","cs.LG,cs.CL,stat.ML",
              BioSentVec: creating sentence embeddings for biomedical texts          ,1810.09302,https://arxiv.org/abs/1810.09302,https://arxiv.org/pdf/1810.09302,"Authors:QingyuChen,YifanPeng,ZhiyongLu","        Sentence embeddings have become an essential part of today's natural language processing (NLP) systems, especially together advanced deep learning methods. Although pre-trained sentence encoders are available in the general domain, none exists for biomedical texts to date. In this work, we introduce BioSentVec: the first open set of sentence embeddings trained with over 30 million documents from both scholarly articles in PubMed and clinical notes in the MIMIC-III Clinical Database. We evaluate BioSentVec embeddings in two sentence pair similarity tasks in different text genres. Our benchmarking results demonstrate that the BioSentVec embeddings can better capture sentence semantics compared to the other competitive alternatives and achieve state-of-the-art performance in both tasks. We expect BioSentVec to facilitate the research and development in biomedical text mining and to complement the existing resources in biomedical word embeddings. BioSentVec is publicly available at https://github.com/ncbi-nlp/BioSentVec        △ Less","24 January, 2020","cs.CL,cs.AI,cs.LG",10.1109/ICHI.2019.8904728 
              Deep Diabetologist: Learning to Prescribe Hyperglycemia Medications with Hierarchical Recurrent Neural Networks          ,1810.07692,https://arxiv.org/abs/1810.07692,https://arxiv.org/pdf/1810.07692,"Authors:JingMei,ShiwanZhao,FengJin,EryuXia,HaifengLiu,XiangLi","        In healthcare, applying deep learning models to electronic health records (EHRs) has drawn considerable attention. EHR data consist of a sequence of medical visits, i.e. a multivariate time series of diagnosis, medications, physical examinations, lab tests, etc. This sequential nature makes EHR well matching the power of Recurrent Neural Network (RNN). In this paper, we propose ""Deep Diabetologist"" - using RNNs for EHR sequential data modelling, to provide the personalized hyperglycemia medication prediction for diabetic patients. Particularly, we develop a hierarchical RNN to capture the heterogeneous sequential information in the EHR data. Our experimental results demonstrate the improved performance, compared with a baseline classifier using logistic regression. Moreover, hierarchical RNN models outperform basic ones, providing deeper data insights for clinical decision support.        △ Less","16 October, 2018",cs.LG,
"              Cyber-Physical Systems, a new formal paradigm to model redundancy and resiliency          ",1810.06911,https://arxiv.org/abs/1810.06911,https://arxiv.org/pdf/1810.06911,"Authors:MarioLezoche,HervéPanetto","        Cyber-Physical Systems (CPS) are systems composed by a physical component that is controlled or monitored by a cyber-component, a computer-based algorithm. Advances in CPS technologies and science are enabling capability, adaptability, scalability, resiliency, safety, security, and usability that will far exceed the simple embedded systems of today. CPS technologies are transforming the way people interact with engineered systems. New smart CPS are driving innovation in various sectors such as agriculture, energy, transportation, healthcare, and manufacturing. They are leading the 4-th Industrial Revolution (Industry 4.0) that is having benefits thanks to the high flexibility of production. The Industry 4.0 production paradigm is characterized by high intercommunicating properties of its production elements in all the manufacturing processes. This is the reason it is a core concept how the systems should be structurally optimized to have the adequate level of redundancy to be satisfactorily resilient. This goal can benefit from formal methods well known in various scientific domains such as artificial intelligence. So, the current research concerns the proposal of a CPS meta-model and its instantiation. In this way it lists all kind of relationships that may occur between the CPSs themselves and between their (cyber-and physical-) components. Using the CPS meta-model formalization, with an adaptation of the Formal Concept Analysis (FCA) formal approach, this paper presents a way to optimize the modelling of CPS systems emphasizing their redundancy and their resiliency.        △ Less","16 October, 2018","cs.LO,cs.DB,cs.IT",
              Unsupervised Ensemble Learning via Ising Model Approximation with Application to Phenotyping Prediction          ,1810.06376,https://arxiv.org/abs/1810.06376,https://arxiv.org/pdf/1810.06376,"Authors:LuwanZhang,TianrunCai","        Unsupervised ensemble learning has long been an interesting yet challenging problem that comes to prominence in recent years with the increasing demand of crowdsourcing in various applications. In this paper, we propose a novel method-- unsupervised ensemble learning via Ising model approximation (unElisa) that combines a pruning step with a predicting step. We focus on the binary case and use an Ising model to characterize interactions between the ensemble and the underlying true classifier. The presence of an edge between an observed classifier and the true classifier indicates a direct dependence whereas the absence indicates the corresponding one provides no additional information and shall be eliminated. This observation leads to the pruning step where the key is to recover the neighborhood of the true classifier. We show that it can be recovered successfully with exponentially decaying error in the high-dimensional setting by performing nodewise ℓ1\ell_1-regularized logistic regression. The pruned ensemble allows us to get a consistent estimate of the Bayes classifier for predicting. We also propose an augmented version of majority voting by reversing all labels given by a subgroup of the pruned ensemble. We demonstrate the efficacy of our method through extensive numerical experiments and through the application to EHR-based phenotyping prediction on Rheumatoid Arthritis (RA) using data from Partners Healthcare System.        △ Less","15 October, 2018","stat.ML,cs.LG",
              Deep Reinforcement Learning          ,1810.06339,https://arxiv.org/abs/1810.06339,https://arxiv.org/pdf/1810.06339,Authors:YuxiLi,"        We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.        △ Less","15 October, 2018","cs.LG,stat.ML",
              End-to-End Service Level Agreement Specification for IoT Applications          ,1810.05937,https://arxiv.org/abs/1810.05937,https://arxiv.org/pdf/1810.05937,"Authors:AwatifAlqahtani,YinhaoLi,PankeshPatel,EllisSolaiman,RajivRanjan","        The Internet of Things (IoT) promises to help solve a wide range of issues that relate to our wellbeing within application domains that include smart cities, healthcare monitoring, and environmental monitoring. IoT is bringing new wireless sensor use cases by taking advantage of the computing power and flexibility provided by Edge and Cloud Computing. However, the software and hardware resources used within such applications must perform correctly and optimally. Especially in applications where a failure of resources can be critical. Service Level Agreements (SLA) where the performance requirements of such applications are defined, need to be specified in a standard way that reflects the end-to-end nature of IoT application domains, accounting for the Quality of Service (QoS) metrics within every layer including the Edge, Network Gateways, and Cloud. In this paper, we propose a conceptual model that captures the key entities of an SLA and their relationships, as a prior step for end-to-end SLA specification and composition. Service level objective (SLO) terms are also considered to express the QoS constraints. Moreover, we propose a new SLA grammar which considers workflow activities and the multi-layered nature of IoT applications. Accordingly, we develop a tool for SLA specification and composition that can be used as a template to generate SLAs in a machine-readable format. We demonstrate the effectiveness of the proposed specification language through a literature survey that includes an SLA language comparison analysis, and via reflecting the user satisfaction results of a usability study.        △ Less","13 October, 2018",cs.DC,
              ClinicalVis: Supporting Clinical Task-Focused Design Evaluation          ,1810.05798,https://arxiv.org/abs/1810.05798,https://arxiv.org/pdf/1810.05798,"Authors:MarzyehGhassemi,MahimaPushkarna,JamesWexler,JesseJohnson,PaulVarghese","        Making decisions about what clinical tasks to prepare for is multi-factored, and especially challenging in intensive care environments where resources must be balanced with patient needs. Electronic health records (EHRs) are a rich data source, but are task-agnostic and can be difficult to use as summarizations of patient needs for a specific task, such as ""could this patient need a ventilator tomorrow?"" In this paper, we introduce ClinicalVis, an open-source EHR visualization-based prototype system for task-focused design evaluation of interactions between healthcare providers (HCPs) and EHRs. We situate ClinicalVis in a task-focused proof-of-concept design study targeting these interactions with real patient data. We conduct an empirical study of 14 HCPs, and discuss our findings on usability, accuracy, preference, and confidence in treatment decisions. We also present design implications that our findings suggest for future EHR interfaces, the presentation of clinical data for task-based planning, and evaluating task-focused HCP/EHR interactions in practice.        △ Less","13 October, 2018",cs.HC,
              Interactive Cognitive Assessment Tools: A Case Study on Digital Pens for the Clinical Assessment of Dementia          ,1810.04943,https://arxiv.org/abs/1810.04943,https://arxiv.org/pdf/1810.04943,Authors:DanielSonntag,"        Interactive cognitive assessment tools may be valuable for doctors and therapists to reduce costs and improve quality in healthcare systems. Use cases and scenarios include the assessment of dementia. In this paper, we present our approach to the semi-automatic assessment of dementia. We describe a case study with digital pens for the patients including background, problem description and possible solutions. We conclude with lessons learned when implementing digital tests, and a generalisation for use outside the cognitive impairments field.        △ Less","11 October, 2018","cs.HC,cs.AI",
              Generating Shared Latent Variables for Robots to Imitate Human Movements and Understand their Physical Limitations          ,1810.04879,https://arxiv.org/abs/1810.04879,https://arxiv.org/pdf/1810.04879,"Authors:MaximeDevanne,SaoMaiNguyen","        Assistive robotics and particularly robot coaches may be very helpful for rehabilitation healthcare. In this context, we propose a method based on Gaussian Process Latent Variable Model (GP-LVM) to transfer knowledge between a physiotherapist, a robot coach and a patient. Our model is able to map visual human body features to robot data in order to facilitate the robot learning and imitation. In addition , we propose to extend the model to adapt robots' understanding to patient's physical limitations during the assessment of rehabilitation exercises. Experimental evaluation demonstrates promising results for both robot imitation and model adaptation according to the patients' limitations.        △ Less","14 February, 2019","cs.RO,cs.AI,cs.CV,cs.HC,cs.LG",10.1007/978-3-030-11012-3_15 
              Rethinking multiscale cardiac electrophysiology with machine learning and predictive modelling          ,1810.04227,https://arxiv.org/abs/1810.04227,https://arxiv.org/pdf/1810.04227,"Authors:ChrisD.Cantwell,YumnahMohamied,KonstantinosN.Tzortzis,StefGarasto,CharlesHouston,RashedaA.Chowdhury,FuSiongNg,AnilA.Bharath,NicholasS.Peters","        We review some of the latest approaches to analysing cardiac electrophysiology data using machine learning and predictive modelling. Cardiac arrhythmias, particularly atrial fibrillation, are a major global healthcare challenge. Treatment is often through catheter ablation, which involves the targeted localized destruction of regions of the myocardium responsible for initiating or perpetuating the arrhythmia. Ablation targets are either anatomically defined, or identified based on their functional properties as determined through the analysis of contact intracardiac electrograms acquired with increasing spatial density by modern electroanatomic mapping systems. While numerous quantitative approaches have been investigated over the past decades for identifying these critical curative sites, few have provided a reliable and reproducible advance in success rates. Machine learning techniques, including recent deep-learning approaches, offer a potential route to gaining new insight from this wealth of highly complex spatio-temporal information that existing methods struggle to analyse. Coupled with predictive modelling, these techniques offer exciting opportunities to advance the field and produce more accurate diagnoses and robust personalised treatment. We outline some of these methods and illustrate their use in making predictions from the contact electrogram and augmenting predictive modelling tools, both by more rapidly predicting future states of the system and by inferring the parameters of these models from experimental observations.        △ Less","9 October, 2018","cs.LG,math.DS,q-bio.TO,stat.ML",10.1016/j.compbiomed.2018.10.015 
              Using learning to control artificial avatars in human motor coordination tasks          ,1810.04191,https://arxiv.org/abs/1810.04191,https://arxiv.org/pdf/1810.04191,"Authors:MariaLombardi,DavideLiuzza,MariodiBernardo","        Designing artificial cyber-agents able to interact with human safely, smartly and in a natural way is a current open problem in control. Solving such an issue will allow the design of cyber-agents capable of co-operatively interacting with people in order to fulfil common joint tasks in a multitude of different applications. This is particularly relevant in the context of healthcare applications. Indeed, the use has been proposed of artificial agents interacting and coordinating their movements with those of a patient suffering from social or motor disorders. Specifically, it has been shown that an artificial agent exhibiting certain kinematic properties could provide innovative and efficient rehabilitation strategies for these patients. Moreover, it has also been shown that the level of motor coordination is enhanced if these kinematic properties are similar to those of the individual it is interacting with. In this paper we discuss, first, a new method based on Markov Chains to confer ""human motor characteristics"" on a virtual agent, so as that it can coordinate its motion with that of a target individual while exhibiting specific kinematic properties. Then, we embed such synthetic model in a control architecture based on reinforcement learning to synthesize a cyber-agent able to mimic the behaviour of a specific human performing a joint motor task with one or more individuals.        △ Less","17 June, 2020","eess.SY,q-bio.NC",
              Artificial Intelligence for Diabetes Case Management: The Intersection of Physical and Mental Health          ,1810.03044,https://arxiv.org/abs/1810.03044,https://doi.org/10.1016/j.imu.2019.100191,Authors:CaseyC.Bennett,"        Diabetes is a major public health problem in the United States, affecting roughly 30 million people. Diabetes complications, along with the mental health comorbidities that often co-occur with them, are major drivers of high healthcare costs, poor outcomes, and reduced treatment adherence in diabetes. Here, we evaluate in a large state-wide population whether we can use artificial intelligence (AI) techniques to identify clusters of patient trajectories within the broader diabetes population in order to create cost-effective, narrowly-focused case management intervention strategies to reduce development of complications. This approach combined data from: 1) claims, 2) case management notes, and 3) social determinants of health from ~300,000 real patients between 2014 and 2016. We categorized complications as five types: Cardiovascular, Neuropathy, Opthalmic, Renal, and Other. Modeling was performed combining a variety of machine learning algorithms, including supervised classification, unsupervised clustering, natural language processing of unstructured care notes, and feature engineering. The results showed that we can predict development of diabetes complications roughly 83.5% of the time using claims data or social determinants of health data. They also showed we can reveal meaningful clusters in the patient population related to complications and mental health that can be used to cost-effective screening program, reducing the number of patients to be screened down by 85%. This study outlines creation of an AI framework to develop protocols to better address mental health comorbidities that lead to complications development in the diabetes population. Future work is described that outlines potential lines of research and the need for better addressing the 'people side' of the equation.        △ Less","10 May, 2019","q-bio.QM,cs.AI,cs.LG,stat.ML",10.1016/j.imu.2019.100191 
              Demonstration Abstract: A Toolkit for Specifying Service Level Agreements for IoT applications          ,1810.02749,https://arxiv.org/abs/1810.02749,https://arxiv.org/pdf/1810.02749,"Authors:AwatifAlqahtani,PankeshPatel,EllisSolaiman,RajivRanjan","        Today we see the use of the Internet of Things (IoT) in various application domains such as healthcare, smart homes, smart cars, and smart-x applications in smart cities. The number of applications based on IoT and cloud computing is projected to increase rapidly over the next few years. IoT-based services must meet the guaranteed levels of quality of service (QoS) to match users' expectations. Ensuring QoS through specifying the QoS constraints using Service Level Agreements (SLAs) is crucial. Therefore, as a first step toward SLA management, it is essential to provide an SLA specification in a machine-readable format. In this paper, we demonstrate a toolkit for creating SLA specifications for IoT applications. The toolkit is used to simplify the process of capturing the requirements of IoT applications. We present a demonstration of the toolkit using a Remote Health Monitoring Service (RHMS) usecase. The toolkit supports the following: (1) specifying the Service-Level Objectives (SLO) of an IoT application at the application level; (2) specifying the workflow activities of the IoT application; (3) mapping each activity to the required software and hardware resources and specifying the constraints of SLOs and other configuration- related metrics of the required hardware and software; and (4) creating the composed SLA in JSON format.        △ Less","5 October, 2018",cs.DC,
              Diffusive Molecular Communication in a Biological Spherical Environment with Partially Absorbing Boundary          ,1810.02657,https://arxiv.org/abs/1810.02657,https://arxiv.org/pdf/1810.02657,"Authors:HamidrezaArjmandi,MohammadZoofaghari,AdamNoel","        Diffusive molecular communication (DMC) is envisioned as a promising approach to help realize healthcare applications within bounded biological environments. In this paper, a DMC system within a biological spherical environment is considered, inspired by bounded biological sphere-like structures throughout the body. As a biological environment, it is assumed that the inner surface of the sphere's boundary is fully covered by biological receptors that may irreversibly react with hitting molecules. Moreover, information molecules diffusing in the sphere may undergo a degradation reaction and be transformed to another molecule type. Concentration Green's function (CGF) of diffusion inside this environment is analytically obtained in terms of a convergent infinite series. By employing the obtained CGF, the information channel between transmitter and transparent receiver of DMC in this environment is characterized. Interestingly, it is revealed that the information channel is reciprocal, i.e., interchanging the position of receiver and transmitter does not change the information channel. Our results indicate that the conventional simplifying assumption that the environment is unbounded may lead to an inaccurate characterization in such biological environments.        △ Less","12 November, 2018",eess.SP,
              Perfect Match: A Simple Method for Learning Representations For Counterfactual Inference With Neural Networks          ,1810.00656,https://arxiv.org/abs/1810.00656,https://arxiv.org/pdf/1810.00656,"Authors:PatrickSchwab,LorenzLinhardt,WalterKarlen","        Learning representations for counterfactual inference from observational data is of high practical relevance for many domains, such as healthcare, public policy and economics. Counterfactual inference enables one to answer ""What if...?"" questions, such as ""What would be the outcome if we gave this patient treatment t1t_1?"". However, current methods for training neural networks for counterfactual inference on observational data are either overly complex, limited to settings with only two available treatments, or both. Here, we present Perfect Match (PM), a method for training neural networks for counterfactual inference that is easy to implement, compatible with any architecture, does not add computational complexity or hyperparameters, and extends to any number of treatments. PM is based on the idea of augmenting samples within a minibatch with their propensity-matched nearest neighbours. Our experiments demonstrate that PM outperforms a number of more complex state-of-the-art methods in inferring counterfactual outcomes across several benchmarks, particularly in settings with many treatments.        △ Less","27 May, 2019","cs.LG,stat.ML",
"              Wearable, Epidermal, and Implantable Sensors for Medical Applications          ",1810.00321,https://arxiv.org/abs/1810.00321,https://arxiv.org/pdf/1810.00321,"Authors:NadeenRishani,HadeelElayan,RaedShubair,AsiminaKiourti","        Continuous health monitoring using wireless body area networks (WBANs) of wearable, epidermal and implantable medical devices is envisioned as a transformative approach to healthcare. Rapid advances in biomedical sensors, low-power electronics, and wireless communications have brought this vision to the verge of reality. However, key challenges still remain to be addressed. This paper surveys the current state-of-the-art in the area of wireless sensors for medical applications. Specifically, it focuses on presenting the recent advancements in wearable, epidermal and implantable technologies, and discusses reported ways of powering up such sensors. Furthermore, this paper addresses the challenges that exist in the various Open Systems Interconnection (OSI) layers and illustrates future research areas concerning the utilization of wireless sensors in healthcare applications.        △ Less","30 September, 2018",eess.SP,
              Design of Intra-body Nano-communication Network for Future Nano-medicine          ,1810.00186,https://arxiv.org/abs/1810.00186,https://arxiv.org/pdf/1810.00186,"Authors:MonaYoussef,FatimaGhanim,NoorImad,AyeshaAlqasim,RaedShubair",        Intra-body communication is a method that utilizes the human body as a broadcast biological medium for electromagnetic signals to inter-connect wireless body sensors. Study of the collaboration between electromagnetic waves and human cells has gained importance in recent years leading towards developing and establishing new novel concept which is the idea of nano-communications using nano-networks to form in-vivo communication that are aimed to offer wireless communication between interior nano-sensors. The emergent of this advanced unprecedented prospective approach of deploying the in vivo communication concept in the health sector is considered as a key potential technology that enhances healthcare delivery and enables the progress of future applications and services.        △ Less,"29 September, 2018","eess.SP,physics.med-ph",
              A SwarmESB Based Architecture for an European Healthcare Insurance System in Compliance with GDPR          ,1809.10911,https://arxiv.org/abs/1809.10911,https://arxiv.org/pdf/1809.10911,"Authors:CristinaGeorgianaCalancea,LenutaAlboaie,AndreiPanu","        With the everlasting development of technology and society, data privacy has proven to grow into a pressing issue. The bureaucratic state system seems to expand the number of personal documents required for any kind of request. Therefore, it becomes obvious that the number of people having access to information that should be private is on the rise as well. This paper offers an alternative cloud integration solution centered on user data privacy, its main purpose being to help software services providers and public institutions to comply with the General Data Protection Regulation. Throughout this proposal we describe how data confidentiality can be achieved by transitioning complex human procedures into a coordinated and decoupled swarm system, whose core lies within the ""Privacy by Design"" principles.        △ Less","28 September, 2018","cs.CY,cs.DC",
              Estimation of Personalized Effects Associated With Causal Pathways          ,1809.10791,https://arxiv.org/abs/1809.10791,https://arxiv.org/pdf/1809.10791,"Authors:RaziehNabi,PhyllisKanki,IlyaShpitser","        The goal of personalized decision making is to map a unit's characteristics to an action tailored to maximize the expected outcome for that unit. Obtaining high-quality mappings of this type is the goal of the dynamic regime literature. In healthcare settings, optimizing policies with respect to a particular causal pathway may be of interest as well. For example, we may wish to maximize the chemical effect of a drug given data from an observational study where the chemical effect of the drug on the outcome is entangled with the indirect effect mediated by differential adherence. In such cases, we may wish to optimize the direct effect of a drug, while keeping the indirect effect to that of some reference treatment. [16] shows how to combine mediation analysis and dynamic treatment regime ideas to defines policies associated with causal pathways and counterfactual responses to these policies. In this paper, we derive a variety of methods for learning high quality policies of this type from data, in a causal model corresponding to a longitudinal setting of practical importance. We illustrate our methods via a dataset of HIV patients undergoing therapy, gathered in the Nigerian PEPFAR program.        △ Less","27 September, 2018","cs.LG,cs.AI,stat.ML",
              MedTruth: A Semi-supervised Approach to Discovering Knowledge Condition Information from Multi-Source Medical Data          ,1809.10404,https://arxiv.org/abs/1809.10404,https://arxiv.org/pdf/1809.10404,"Authors:YangDeng,YaliangLi,YingShen,NanDu,WeiFan,MinYang,KaiLei","        Knowledge Graph (KG) contains entities and the relations between entities. Due to its representation ability, KG has been successfully applied to support many medical/healthcare tasks. However, in the medical domain, knowledge holds under certain conditions. For example, symptom \emph{runny nose} highly indicates the existence of disease \emph{whooping cough} when the patient is a baby rather than the people at other ages. Such conditions for medical knowledge are crucial for decision-making in various medical applications, which is missing in existing medical KGs. In this paper, we aim to discovery medical knowledge conditions from texts to enrich KGs.  Electronic Medical Records (EMRs) are systematized collection of clinical data and contain detailed information about patients, thus EMRs can be a good resource to discover medical knowledge conditions. Unfortunately, the amount of available EMRs is limited due to reasons such as regularization. Meanwhile, a large amount of medical question answering (QA) data is available, which can greatly help the studied task. However, the quality of medical QA data is quite diverse, which may degrade the quality of the discovered medical knowledge conditions. In the light of these challenges, we propose a new truth discovery method, MedTruth, for medical knowledge condition discovery, which incorporates prior source quality information into the source reliability estimation procedure, and also utilizes the knowledge triple information for trustworthy information computation. We conduct series of experiments on real-world medical datasets to demonstrate that the proposed method can discover meaningful and accurate conditions for medical knowledge by leveraging both EMR and QA data. Further, the proposed method is tested on synthetic datasets to validate its effectiveness under various scenarios.        △ Less","18 August, 2019",cs.DB,
              Enhanced Session Initiation Protocols for Emergency Healthcare Applications          ,1809.09526,https://arxiv.org/abs/1809.09526,https://arxiv.org/pdf/1809.09526,"Authors:SahaSourav,VangaOdelu,RajendraPrasath","        In medical emergencies, an instant and secure messaging is an important service to provide quality healthcare services. A session initiation protocol (SIP) is an IP-based multimedia and telephony communication protocol used to provide instant messaging services. Thus, design of secure and efficient SIP for quality medical services is an emerging problem. In this paper, we first explore the security limitations of the existing SIPs proposed by Sureshkumar et al. and Zhang et al. in the literature. Our analysis shows that most of the existing schemes fail to protect the user credentials when unexpectedly the session-specific ephemeral secrets revealed to an adversary by the session exposure attacks. We then present a possible improvement over Sureshkumar et al.'s scheme without increasing the computational cost. We compare the proposed improvement for computational overheads and security features with the various related existing schemes in the literature.        △ Less","25 September, 2018",cs.CR,
              Learning to Address Health Inequality in the United States with a Bayesian Decision Network          ,1809.09215,https://arxiv.org/abs/1809.09215,https://arxiv.org/pdf/1809.09215,"Authors:TavpriteshSethi,AnantMittal,ShubhamMaheshwari,SamarthChugh","        Life-expectancy is a complex outcome driven by genetic, socio-demographic, environmental and geographic factors. Increasing socio-economic and health disparities in the United States are propagating the longevity-gap, making it a cause for concern. Earlier studies have probed individual factors but an integrated picture to reveal quantifiable actions has been missing. There is a growing concern about a further widening of healthcare inequality caused by Artificial Intelligence (AI) due to differential access to AI-driven services. Hence, it is imperative to explore and exploit the potential of AI for illuminating biases and enabling transparent policy decisions for positive social and health impact. In this work, we reveal actionable interventions for decreasing the longevity-gap in the United States by analyzing a County-level data resource containing healthcare, socio-economic, behavioral, education and demographic features. We learn an ensemble-averaged structure, draw inferences using the joint probability distribution and extend it to a Bayesian Decision Network for identifying policy actions. We draw quantitative estimates for the impact of diversity, preventive-care quality and stable-families within the unified framework of our decision network. Finally, we make this analysis and dashboard available as an interactive web-application for enabling users and policy-makers to validate our reported findings and to explore the impact of ones beyond reported in this work.        △ Less","16 November, 2018","stat.AP,cs.LG,stat.ML",
              Trusted Multi-Party Computation and Verifiable Simulations: A Scalable Blockchain Approach          ,1809.08438,https://arxiv.org/abs/1809.08438,https://arxiv.org/pdf/1809.08438,"Authors:RaviKiranRaman,RomanVaculin,MichaelHind,SekouL.Remy,EleftheriaK.Pissadaki,NelsonKibichiiBore,RoozbehDaneshvar,BiplavSrivastava,KushR.Varshney","        Large-scale computational experiments, often running over weeks and over large datasets, are used extensively in fields such as epidemiology, meteorology, computational biology, and healthcare to understand phenomena, and design high-stakes policies affecting everyday health and economy. For instance, the OpenMalaria framework is a computationally-intensive simulation used by various non-governmental and governmental agencies to understand malarial disease spread and effectiveness of intervention strategies, and subsequently design healthcare policies. Given that such shared results form the basis of inferences drawn, technological solutions designed, and day-to-day policies drafted, it is essential that the computations are validated and trusted. In particular, in a multi-agent environment involving several independent computing agents, a notion of trust in results generated by peers is critical in facilitating transparency, accountability, and collaboration. Using a novel combination of distributed validation of atomic computation blocks and a blockchain-based immutable audits mechanism, this work proposes a universal framework for distributed trust in computations. In particular we address the scalaibility problem by reducing the storage and communication costs using a lossy compression scheme. This framework guarantees not only verifiability of final results, but also the validity of local computations, and its cost-benefit tradeoffs are studied using a synthetic example of training a neural network.        △ Less","22 September, 2018","cs.DC,cs.IT,eess.SY,stat.ML",
              Arianna+: Scalable Human Activity Recognition by Reasoning with a Network of Ontologies          ,1809.08208,https://arxiv.org/abs/1809.08208,https://arxiv.org/pdf/1809.08208,"Authors:SyedYushaKareem,LucaBuoncompagni,FulvioMastrogiovanni","        Aging population ratios are rising significantly. Meanwhile, smart home based health monitoring services are evolving rapidly to become a viable alternative to traditional healthcare solutions. Such services can augment qualitative analyses done by gerontologists with quantitative data. Hence, the recognition of Activities of Daily Living (ADL) has become an active domain of research in recent times. For a system to perform human activity recognition in a real-world environment, multiple requirements exist, such as scalability, robustness, ability to deal with uncertainty (e.g., missing sensor data), to operate with multi-occupants and to take into account their privacy and security. This paper attempts to address the requirements of scalability and robustness, by describing a reasoning mechanism based on modular spatial and/or temporal context models as a network of ontologies. The reasoning mechanism has been implemented in a smart home system referred to as Arianna+. The paper presents and discusses a use case, and experiments are performed on a simulated dataset, to showcase Arianna+'s modularity feature, internal working, and computational performance. Results indicate scalability and robustness for human activity recognition processes.        △ Less","21 September, 2018",cs.AI,
              Composer: Visual Cohort Analysis of Patient Outcomes          ,1809.08177,https://arxiv.org/abs/1809.08177,https://arxiv.org/pdf/1809.08177,"Authors:JenniferRogers,NicholasSpina,AshleyNeese,RachelHess,DarrelBrodke,AlexanderLex","        Objective: Visual cohort analysis utilizing electronic health record data has become an important tool in clinical assessment of patient outcomes. In this paper, we introduce Composer, a visual analysis tool for orthopedic surgeons to compare changes in physical functions of a patient cohort following various spinal procedures. The goal of our project is to help researchers analyze outcomes of procedures and facilitate informed decision-making about treatment options between patient and clinician. Methods: In collaboration with Orthopedic surgeons and researchers, we defined domain-specific user requirements to inform the design. We developed the tool in an iterative process with our collaborators to develop and refine functionality. With Composer, analysts can dynamically define a patient cohort using demographic information, clinical parameters, and events in patient medical histories and then analyze patient-reported outcome scores for the cohort over time, as well as compare it to other cohorts. Using Composer's current iteration, we provide a usage scenario for use of the tool in a clinical setting. Conclusion: We have developed a prototype cohort analysis tool to help clinicians assess patient treatment options by analyzing prior cases with similar characteristics. Though Composer was designed using patient data specific to Orthopedic research, we believe the tool is generalizable to other healthcare domains. A long term goal for Composer is to develop the application into a shared decision-making tool that allows translation of comparison and analysis from a clinician facing interface into visual representations to communicate treatment options to patients.        △ Less","18 March, 2019",cs.HC,
              Understanding Behavior of Clinical Models under Domain Shifts          ,1809.07806,https://arxiv.org/abs/1809.07806,https://arxiv.org/pdf/1809.07806,"Authors:JayaramanJ.Thiagarajan,DeeptaRajan,PrasannaSattigeri","        The hypothesis that computational models can be reliable enough to be adopted in prognosis and patient care is revolutionizing healthcare. Deep learning, in particular, has been a game changer in building predictive models, thus leading to community-wide data curation efforts. However, due to inherent variabilities in population characteristics and biological systems, these models are often biased to the training datasets. This can be limiting when models are deployed in new environments, when there are systematic domain shifts not known a priori. In this paper, we propose to emulate a large class of domain shifts, that can occur in clinical settings, with a given dataset, and argue that evaluating the behavior of predictive models in light of those shifts is an effective way to quantify their reliability. More specifically, we develop an approach for building realistic scenarios, based on analysis of \textit{disease landscapes} in multi-label classification. Using the openly available MIMIC-III EHR dataset for phenotyping, for the first time, our work sheds light into data regimes where deep clinical models can fail to generalize. This work emphasizes the need for novel validation mechanisms driven by real-world domain shifts in AI for healthcare.        △ Less","13 June, 2019","stat.ML,cs.AI,cs.LG",
              Deep Generative Classifiers for Thoracic Disease Diagnosis with Chest X-ray Images          ,1809.07436,https://arxiv.org/abs/1809.07436,https://arxiv.org/pdf/1809.07436,"Authors:ChengshengMao,YihengPan,ZexianZeng,LiangYao,YuanLuo","        Thoracic diseases are very serious health problems that plague a large number of people. Chest X-ray is currently one of the most popular methods to diagnose thoracic diseases, playing an important role in the healthcare workflow. However, reading the chest X-ray images and giving an accurate diagnosis remain challenging tasks for expert radiologists. With the success of deep learning in computer vision, a growing number of deep neural network architectures were applied to chest X-ray image classification. However, most of the previous deep neural network classifiers were based on deterministic architectures which are usually very noise-sensitive and are likely to aggravate the overfitting issue. In this paper, to make a deep architecture more robust to noise and to reduce overfitting, we propose using deep generative classifiers to automatically diagnose thorax diseases from the chest X-ray images. Unlike the traditional deterministic classifier, a deep generative classifier has a distribution middle layer in the deep neural network. A sampling layer then draws a random sample from the distribution layer and input it to the following layer for classification. The classifier is generative because the class label is generated from samples of a related distribution. Through training the model with a certain amount of randomness, the deep generative classifiers are expected to be robust to noise and can reduce overfitting and then achieve good performances. We implemented our deep generative classifiers based on a number of well-known deterministic neural network architectures, and tested our models on the chest X-ray14 dataset. The results demonstrated the superiority of deep generative classifiers compared with the corresponding deep deterministic classifiers.        △ Less","8 November, 2018","cs.CV,cs.AI,cs.LG",
              Towards Dialogue-based Navigation with Multivariate Adaptation driven by Intention and Politeness for Social Robots          ,1809.07269,https://arxiv.org/abs/1809.07269,https://arxiv.org/pdf/1809.07269,"Authors:ChandrakantBothe,FernandoGarcia,ArturoCruzMaya,AmitKumarPandey,StefanWermter","        Service robots need to show appropriate social behaviour in order to be deployed in social environments such as healthcare, education, retail, etc. Some of the main capabilities that robots should have are navigation and conversational skills. If the person is impatient, the person might want a robot to navigate faster and vice versa. Linguistic features that indicate politeness can provide social cues about a person's patient and impatient behaviour. The novelty presented in this paper is to dynamically incorporate politeness in robotic dialogue systems for navigation. Understanding the politeness in users' speech can be used to modulate the robot behaviour and responses. Therefore, we developed a dialogue system to navigate in an indoor environment, which produces different robot behaviours and responses based on users' intention and degree of politeness. We deploy and test our system with the Pepper robot that adapts to the changes in user's politeness.        △ Less","14 November, 2018","cs.RO,cs.AI,cs.CL",10.1007/978-3-030-05204-1_23 
              Wearable proximity sensors for monitoring a mass casualty incident exercise: a feasibility study          ,1809.06887,https://arxiv.org/abs/1809.06887,https://arxiv.org/pdf/1809.06887,"Authors:LauraOzella,LaetitiaGauvin,LucaCarenzo,MarcoQuaggiotto,PierLuigiIngrassia,MicheleTizzoni,AndréPanisson,DavideColombo,AnnaSapienza,KyriakiKalimeri,FrancescoDellaCorte,CiroCattuto","        Over the past several decades, naturally occurring and man-made mass casualty incidents (MCI) have increased in frequency and number, worldwide. To test the impact of such event on medical resources, simulations can provide a safe, controlled setting while replicating the chaotic environment typical of an actual disaster. A standardised method to collect and analyse data from mass casualty exercises is needed, in order to assess preparedness and performance of the healthcare staff involved. We report on the use of wearable proximity sensors to measure proximity events during a MCI simulation. We investigated the interactions between medical staff and patients, to evaluate the time dedicated by the medical staff with respect to the severity of the injury of the victims depending on the roles. We estimated the presence of the patients in the different spaces of the field hospital, in order to study the patients' flow. Data were obtained and collected through the deployment of wearable proximity sensors during a mass casualty incident functional exercise. The scenario included two areas: the accident site and the Advanced Medical Post (AMP), and the exercise lasted 3 hours. A total of 238 participants simulating medical staff and victims were involved. Each participant wore a proximity sensor and 30 fixed devices were placed in the field hospital. The contact networks show a heterogeneous distribution of the cumulative time spent in proximity by participants. We obtained contact matrices based on cumulative time spent in proximity between victims and the rescuers. Our results showed that the time spent in proximity by the healthcare teams with the victims is related to the severity of the patient's injury. The analysis of patients' flow showed that the presence of patients in the rooms of the hospital is consistent with triage code and diagnosis, and no obvious bottlenecks were found.        △ Less","18 September, 2018",physics.soc-ph,
              Lung Cancer Concept Annotation from Spanish Clinical Narratives          ,1809.06639,https://arxiv.org/abs/1809.06639,https://arxiv.org/pdf/1809.06639,"Authors:MarjanNajafabadipour,JuanManuelTuñas,AlejandroRodríguez-González,ErnestinaMenasalvas","        Recent rapid increase in the generation of clinical data and rapid development of computational science make us able to extract new insights from massive datasets in healthcare industry. Oncological clinical notes are creating rich databases for documenting patients history and they potentially contain lots of patterns that could help in better management of the disease. However, these patterns are locked within free text (unstructured) portions of clinical documents and consequence in limiting health professionals to extract useful information from them and to finally perform Query and Answering (QA) process in an accurate way. The Information Extraction (IE) process requires Natural Language Processing (NLP) techniques to assign semantics to these patterns. Therefore, in this paper, we analyze the design of annotators for specific lung cancer concepts that can be integrated over Apache Unstructured Information Management Architecture (UIMA) framework. In addition, we explain the details of generation and storage of annotation outcomes.        △ Less","18 September, 2018","cs.AI,cs.CL",
              A Conceptual Approach to Complex Model Management with Generalized Modelling Patterns and Evolutionary Identification          ,1809.04656,https://arxiv.org/abs/1809.04656,https://arxiv.org/pdf/1809.04656,"Authors:SergeyV.Kovalchuk,OlegG.Metsker,AnastasiaA.Funkner,IliaO.Kisliakovskii,NikolayO.Nikitin,AnnaV.Kalyuzhnaya,DanilaA.Vaganov,KlavdiyaO.Bochenina","        Complex systems' modeling and simulation are powerful ways to investigate a multitude of natural phenomena providing extended knowledge on their structure and behavior. However, enhanced modeling and simulation require integration of various data and knowledge sources, models of various kinds (data-driven models, numerical models, simulation models, etc.), intelligent components in one composite solution. Growing complexity of such composite model leads to the need of specific approaches for management of such model. This need extends where the model itself becomes a complex system. One of the important aspects of complex model management is dealing with the uncertainty of various kinds (context, parametric, structural, input/output) to control the model. In the situation where a system being modeled, or modeling requirements change over time, specific methods and tools are needed to make modeling and application procedures (meta-modeling operations) in an automatic manner. To support automatic building and management of complex models we propose a general evolutionary computation approach which enables managing of complexity and uncertainty of various kinds. The approach is based on an evolutionary investigation of model phase space to identify the best model's structure and parameters. Examples of different areas (healthcare, hydrometeorology, social network analysis) were elaborated with the proposed approach and solutions.        △ Less","12 September, 2018",cs.OH,10.1155/2018/5870987 
              Efficient and Privacy-preserving Voice-based Search over mHealth Data          ,1809.04583,https://arxiv.org/abs/1809.04583,https://arxiv.org/pdf/1809.04583,"Authors:MohammadHadian,ThamerAltuwaiyan,XiaohuiLiang,WeiLi","        In-home IoT devices play a major role in healthcare systems as smart personal assistants. They usually come with a voice-enabled feature to add an extra level of usability and convenience to elderly, disabled people, and patients. In this paper, we propose an efficient and privacy-preserving voice-based search scheme to enhance the efficiency and the privacy of in-home healthcare applications. We consider an application scenario where patients use the devices to record and upload their voice to servers and the caregivers search the interested voices of their patient's based on the voice content, mood, tone and background sound. Our scheme preserves the richness and privacy of voice data and enables accurate and efficient voice-based search, while in current systems that use speech recognition the richness and privacy of voice data are compromised. Specifically, our scheme achieves the privacy by employing a homomorphic encryption; only encrypted voice data is uploaded to the server who is unable to access the original voice data. In addition, our scheme enables the server to selectively and accurately respond to caregiver's queries on the voice data based on voice's feature similarity. We evaluate our scheme through real experiments and show that our scheme even with privacy preservation can successfully match similar voice data at an average accuracy of 80.8%.        △ Less","12 September, 2018",cs.CR,
              Privacy-preserving mHealth Data Release with Pattern Consistency          ,1809.04579,https://arxiv.org/abs/1809.04579,https://arxiv.org/pdf/1809.04579,"Authors:MohammadHadian,XiaohuiLiang,ThamerAltuwaiyan,MohamedMEAMahmoud","        Mobile healthcare system integrating wearable sensing and wireless communication technologies continuously monitors the users' health status. However, the mHealth system raises a severe privacy concern as the data it collects are private information, such as heart rate and blood pressure. In this paper, we propose an efficient and privacy-preserving mHealth data release approach for the statistic data with the objectives to preserve the unique patterns in the original data bins. The proposed approach adopts the bucket partition algorithm and the differential privacy algorithm for privacy preservation. A customized bucket partition algorithm is proposed to combine the database value bins into buckets according to certain conditions and parameters such that the patterns are preserved. The differential privacy algorithm is then applied to the buckets to prevent an attacker from being able to identify the small changes at the original data. We prove that the proposed approach achieves differential privacy. We also show the accuracy of the proposed approach through extensive simulations on real data. Real experiments show that our partitioning algorithm outperforms the state-of-the-art in preserving the patterns of the original data by a factor of 1.75.        △ Less","12 September, 2018",cs.CR,
              Camouflaged with Size: A Case Study of Espionage using Acquirable Single-Board Computers          ,1809.04112,https://arxiv.org/abs/1809.04112,https://arxiv.org/pdf/1809.04112,"Authors:KiavashSatvat,MahshidHosseini,MalihehShirvanian","        Single-Board Computers (SBC) refer to pocket-sized computers built on a single circuit board. A number of studies have explored the use of these highly popular devices in a variety of domains, including military, agriculture, healthcare, and more. However, no attempt was made to signify possible security risks that misuse of these devices may bring to organizations. In this study, we perform a series of experiments to validate the possibility of using SBCs as an espionage gadget. We show how an attacker can turn a Raspberry Pi device to an attacking gadget and benefit from short-term physical access to attach the gadget to the network in order to access unauthorized data or perform other malicious activities. We then provide experimental results of placing such tools in two real-world networks. Given the small size of SBCs, traditional physical security measures deployed in organizations may not be sufficient to detect and restrict the entrance of SBCs to their premises. Therefore, we reiterate possible directions for network administrators to deploy defensive mechanisms for detecting and preventing such attacks.        △ Less","11 September, 2018",cs.CR,
              Bayesian Patchworks: An Approach to Case-Based Reasoning          ,1809.03541,https://arxiv.org/abs/1809.03541,https://arxiv.org/pdf/1809.03541,"Authors:RaminMoghaddass,CynthiaRudin","        Doctors often rely on their past experience in order to diagnose patients. For a doctor with enough experience, almost every patient would have similarities to key cases seen in the past, and each new patient could be viewed as a mixture of these key past cases. Because doctors often tend to reason this way, an efficient computationally aided diagnostic tool that thinks in the same way might be helpful in locating key past cases of interest that could assist with diagnosis. This article develops a novel mathematical model to mimic the type of logical thinking that physicians use when considering past cases. The proposed model can also provide physicians with explanations that would be similar to the way they would naturally reason about cases. The proposed method is designed to yield predictive accuracy, computational efficiency, and insight into medical data; the key element is the insight into medical data, in some sense we are automating a complicated process that physicians might perform manually. We finally implemented the result of this work on two publicly available healthcare datasets, for heart disease prediction and breast cancer prediction.        △ Less","10 September, 2018","cs.AI,cs.LG,stat.ML",
              A collection of database industrial techniques and optimization approaches of database operations          ,1809.03445,https://arxiv.org/abs/1809.03445,https://arxiv.org/pdf/1809.03445,Authors:JasperKyleCatapang,"        Databases play an essential role in our society today. Databases are embedded in sectors like corporations, institutions, and government organizations, among others. These databases are used for our video and audio streaming platforms, social gaming, finances, cloud storage, e-commerce, healthcare, economy, etc. It is therefore imperative that we learn how to properly execute database operations and efficiently implement methodologies so that we may optimize the performance of databases.        △ Less","10 September, 2018",cs.DB,10.5281/zenodo.1439511 
              Monitoring data quality for telehealth systems in the presence of missing data          ,1809.03127,https://arxiv.org/abs/1809.03127,https://arxiv.org/pdf/1809.03127,"Authors:TahirMahmood,PhilippWittenberg,InezMariaZwetsloot,HailiangWang,KwokLeungTsui","        Background: All-in-one station-based health monitoring devices are implemented in elder homes in Hong Kong to support the monitoring of vital signs of the elderly. During a pilot study, it was discovered that the systolic blood pressure was incorrectly measured during multiple weeks. A real-time solution was needed to identify future data quality issues as soon as possible.  Methods: Control charts are an effective tool for real-time monitoring and signaling issues (changes) in data. In this study, as in other healthcare applications, many observations are missing. Few methods are available for monitoring data with missing observations. A data quality monitoring method is developed to signal issues with the accuracy of the collected data quickly. This method has the ability to deal with missing observations. A Hotelling's T-squared control chart is selected as the basis for our proposed method.  Findings: The proposed method is retrospectively validated on a case study with a known measurement error in the systolic blood pressure measurements. The method is able to adequately detect this data quality problem. The proposed method was integrated into a personalized telehealth monitoring system and prospectively implemented in a second case study. It was found that the proposed scheme supports the control of data quality.  Conclusions: Data quality is an important issue and control charts are useful for real-time monitoring of data quality. However, these charts must be adjusted to account for missing data that often occur in healthcare context.        △ Less","11 June, 2019","stat.AP,stat.ME",10.1016/j.ijmedinf.2019.03.011 
              A Block Coordinate Ascent Algorithm for Mean-Variance Optimization          ,1809.02292,https://arxiv.org/abs/1809.02292,https://arxiv.org/pdf/1809.02292,"Authors:BoLiu,TengyangXie,YangyangXu,MohammadGhavamzadeh,YinlamChow,DaomingLyu,DaesubYoon","        Risk management in dynamic decision problems is a primary concern in many fields, including financial investment, autonomous driving, and healthcare. The mean-variance function is one of the most widely used objective functions in risk management due to its simplicity and interpretability. Existing algorithms for mean-variance optimization are based on multi-time-scale stochastic approximation, whose learning rate schedules are often hard to tune, and have only asymptotic convergence proof. In this paper, we develop a model-free policy search framework for mean-variance optimization with finite-sample error bound analysis (to local optima). Our starting point is a reformulation of the original mean-variance function with its Fenchel dual, from which we propose a stochastic block coordinate ascent policy search algorithm. Both the asymptotic convergence guarantee of the last iteration's solution and the convergence rate of the randomly picked solution are provided, and their applicability is demonstrated on several benchmark domains.        △ Less","1 November, 2018","cs.LG,stat.ML",
              Sample Design for Medicaid and Healthcare Audits          ,1809.02023,https://arxiv.org/abs/1809.02023,https://arxiv.org/pdf/1809.02023,Authors:MichelleNorris,"        We develop several tools for the determination of sample size and design for Medicaid and healthcare audits. The goal of these audits is to examine a population of claims submitted by a healthcare provider for reimbursement by a third party payer to determine the total amount of money which is erroneously claimed. For large audit populations, conclusions about the total amount of reimbursement claimed erroneously are often based on sample data. Often, sample size determination must be made in the absence of pilot study data and existing methods for doing so typically rely on restrictive assumptions. This includes the `all-or-nothing errors' assumption which assumes the error in a claim is either the entire claim amount or none of it. Under the all-or-nothing errors assumption, Roberts (1978) has derived estimates of the variances needed for sample size calculations under simple expansion and ratio estimation. Some audit populations, however, will contain claims which are partially in error. We broaden existing methodology to handle this scenario by proposing an error model which allows for partial errors by modeling the line-item error mechanism. We use this model to derive estimates of the variances needed for sample size determination under simple expansion and ratio estimation in the presence of partial errors. In the absence of certain error-rate parameter estimates needed to implement our method, we show that conservative sample sizes can be determined using the claim data alone. We further show that, under all-or-nothing errors, ratio estimation will tend to outperform simple expansion and that optimal stratification is independent of the population error rate under ratio estimation. The proposed sample design methods are illustrated on three simulated audit populations.        △ Less","4 September, 2018",stat.ME,
              GAMENet: Graph Augmented MEmory Networks for Recommending Medication Combination          ,1809.01852,https://arxiv.org/abs/1809.01852,https://arxiv.org/pdf/1809.01852,"Authors:JunyuanShang,CaoXiao,TengfeiMa,HongyanLi,JimengSun","        Recent progress in deep learning is revolutionizing the healthcare domain including providing solutions to medication recommendations, especially recommending medication combination for patients with complex health conditions. Existing approaches either do not customize based on patient health history, or ignore existing knowledge on drug-drug interactions (DDI) that might lead to adverse outcomes. To fill this gap, we propose the Graph Augmented Memory Networks (GAMENet), which integrates the drug-drug interactions knowledge graph by a memory module implemented as a graph convolutional networks, and models longitudinal patient records as the query. It is trained end-to-end to provide safe and personalized recommendation of medication combination. We demonstrate the effectiveness and safety of GAMENet by comparing with several state-of-the-art methods on real EHR data. GAMENet outperformed all baselines in all effectiveness measures, and also achieved 3.60% DDI rate reduction from existing EHR data.        △ Less","6 March, 2019","cs.AI,cs.LG,stat.ML",
              Predicting Smoking Events with a Time-Varying Semi-Parametric Hawkes Process Model          ,1809.01740,https://arxiv.org/abs/1809.01740,https://arxiv.org/pdf/1809.01740,"Authors:MatthewEngelhard,HongtengXu,LawrenceCarin,JasonAOliver,MatthewHallyburton,FJosephMcClernon","        Health risks from cigarette smoking -- the leading cause of preventable death in the United States -- can be substantially reduced by quitting. Although most smokers are motivated to quit, the majority of quit attempts fail. A number of studies have explored the role of self-reported symptoms, physiologic measurements, and environmental context on smoking risk, but less work has focused on the temporal dynamics of smoking events, including daily patterns and related nicotine effects. In this work, we examine these dynamics and improve risk prediction by modeling smoking as a self-triggering process, in which previous smoking events modify current risk. Specifically, we fit smoking events self-reported by 42 smokers to a time-varying semi-parametric Hawkes process (TV-SPHP) developed for this purpose. Results show that the TV-SPHP achieves superior prediction performance compared to related and existing models, with the incorporation of time-varying predictors having greatest benefit over longer prediction windows. Moreover, the impact function illustrates previously unknown temporal dynamics of smoking, with possible connections to nicotine metabolism to be explored in future work through a randomized study design. By more effectively predicting smoking events and exploring a self-triggering component of smoking risk, this work supports development of novel or improved cessation interventions that aim to reduce death from smoking.        △ Less","5 September, 2018","stat.ML,cs.LG,stat.AP",
              Measures of Cluster Informativeness for Medical Evidence Aggregation and Dissemination          ,1809.01678,https://arxiv.org/abs/1809.01678,https://arxiv.org/pdf/1809.01678,"Authors:MichaelSegundoOrtiz,SamBubnovich,MengqianWang,KazuhiroSekiPh.D.,JavedMostafaPh.D","        The largest collection of medical evidence in the world is PubMed. However, the significant barrier in accessing and extracting information is information organization. A factor that contributes towards this barrier is managing medical controlled vocabularies that allow us to systematically and consistently organize, index, and search biomedical literature. Additionally, from users' perspective, to ultimately improve access, visualization is likely to play a powerful role. There is a strong link between information organization and information visualization, as many powerful visualizations depend on clustering methods. To improve visualization, therefore, one has to develop concrete and scalable measures for vocabularies used in indexing and their impact on document clustering. The focus of this study is on the development and evaluation of clustering methods. The paper concludes with demonstration of downstream network visualizations and their impact on discovering potentially valuable and latent genetic and molecular associations.        △ Less","5 September, 2018",cs.IR,
              Location and Capacity Planning of Facilities with General Service-Time Distributions Using Conic Optimization          ,1809.00080,https://arxiv.org/abs/1809.00080,https://arxiv.org/pdf/1809.00080,"Authors:AmirAhmadi-Javid,OdedBerman,PooyaHoseinpour","        This paper studies a stochastic congested location problem in the network of a service system that consists of facilities to be established in a finite number of candidate locations. Population zones allocated to each open service facility together creates a stream of demand that follows a Poisson process and may cause congestion at the facility. The service time at each facility is stochastic and depends on the service capacity and follows a general distribution that can differ for each facility. The service capacity is selected from a given (bounded or unbounded) interval. The objective of our problem is to optimize a balanced performance measure that compromises between facility-induced and customer-related costs. Service times are represented by a flexible location-scale stochastic model. The problem is formulated using quadratic conic optimization. Valid inequalities and a cut-generation procedure are developed to increase computational efficiency. A comprehensive numerical study is carried out to show the efficiency and effectiveness of the solution procedure. Moreover, our numerical experiments using real data of a preventive healthcare system in Toronto show that the optimal service network configuration is highly sensitive to the service-time distribution. Our method for convexifying the waiting-time formulas of M/G/1 queues is general and extends the existing convexity results in queueing theory such that they can be used in optimization problems where the service rates are continuous.        △ Less","31 August, 2018","math.OC,cs.DM,math.PR",
              Reasoning with graded information: the case of diagnostic rating scales in healthcare,1808.10330,https://arxiv.org/abs/1808.10330,https://arxiv.org/pdf/1808.10330,"Authors:ThomasVetterlein,AnnaZamansky","        In medicine one frequently deals with vague information. As a tool for reasoning in this area, fuzzy logic suggests itself. In this paper we explore the applicability of the basic ideas of fuzzy set theory in the context of medical assessment questionnaires, which are commonly used, for instance, to support the diagnosis of psychological disorders.  The items of a questionnaire are answered in a graded form; patients are asked to choose an element on a linear scale. The derived diagnostic hypotheses are graded as well. This leads to the question whether there is a logical formalism that is suitable to capture the score calculation of medical assessment questionnaires and thereby provides a mathematical justification of the way in which the calculation is typically done.  We elaborate two alternative approaches to this problem. First, we follow the lines of mathematical fuzzy logic. For the proposed logic, which can deal with the formation of mean values, we present a Hilbert-style deduction system. In addition, we consider a variant of the prototype approach to vagueness. In this case we are led to a framework for which to obtain a logical calculus turns out to be difficult, yet our gain is a model that is conceptually comparably well-justifiable.        △ Less","30 August, 2018",math.LO,
              An integrated rolling horizon approach to increase operating theatre efficiency          ,1808.10139,https://arxiv.org/abs/1808.10139,https://arxiv.org/pdf/1808.10139,"Authors:BelindaSpratt,ErhanKozan","        Demand for healthcare is increasing rapidly. To meet demand, we must improve the efficiency of our public health services. We present a mixed integer programming (MIP) formulation that simultaneously tackles the integrated Master Surgical Schedule (MSS) and Surgical Case Assignment (SCA) problems. We consider volatile surgical durations and non-elective arrivals whilst applying a rolling horizon approach to adjust the schedule after cancellations, equipment failure, or new arrivals on the waiting list. A case study of an Australian public hospital with a large surgical department is the basis for the model. The formulation includes significant detail and provides practitioners with a globally implementable model. We produce good feasible solutions in short amounts of computational time with a constructive heuristic and two hyper metaheuristics. Using a rolling horizon schedule increases patient throughput and can help reduce waiting lists.        △ Less","14 January, 2019","math.OC,cs.AI",
              Using Apple Machine Learning Algorithms to Detect and Subclassify Non-Small Cell Lung Cancer          ,1808.08230,https://arxiv.org/abs/1808.08230,https://arxiv.org/pdf/1808.08230,"Authors:AndrewA.Borkowski,CatherineP.Wilson,StevenA.Borkowski,LaurenA.Deland,StephenM.Mastorides","        Lung cancer continues to be a major healthcare challenge with high morbidity and mortality rates among both men and women worldwide. The majority of lung cancer cases are of non-small cell lung cancer type. With the advent of targeted cancer therapy, it is imperative not only to properly diagnose but also sub-classify non-small cell lung cancer. In our study, we evaluated the utility of using Apple Create ML module to detect and sub-classify non-small cell carcinomas based on histopathological images. After module optimization, the program detected 100% of non-small cell lung cancer images and successfully subclassified the majority of the images. Trained modules, such as ours, can be utilized in diagnostic smartphone-based applications, augmenting diagnostic services in understaffed areas of the world.        △ Less","18 January, 2019","q-bio.QM,cs.LG,stat.ML",
              Effect of active case finding on dengue control: Implications from a mathematical model          ,1808.08123,https://arxiv.org/abs/1808.08123,https://arxiv.org/pdf/1808.08123,"Authors:IndrajitGhosh,PankajKumarTiwari,JoydevChattopadhyay","        Dengue control in India is a challenging task due to complex healthcare settings. In yesteryears, an amplification of dengue infections in India posed the need for introspection of existing dengue control policies. Prior understanding of the impacts of control interventions is necessary for their future implementation. In this paper, we propose and analyze a compartmental model of dengue to assess the impact of active case finding (ACF) on dengue disease transmission. Currently, primary prevention of dengue is possible only with vector control and personal protection from the bites of infected mosquitoes. Although a few experimental studies are performed to assess ACF in dengue disease, but this is the first attempt to represent and study the dynamics of disease using ACF as a control strategy. Local and global dynamics of the system are studied. We use sensitivity analysis to see the effects of controllable parameters of the model on the basic reproduction number and total number of infective population. We find that decrease in the biting rate of mosquitoes, and increase in the rate of hospitalization and/or notification, death rate of mosquitoes and ACF for asymptomatic and symptomatic individuals play crucial role for the reduction of disease prevalence. We calibrate our model to the yearly dengue cases in eight dengue endemic states of India. The results of our study show that ACF of symptomatic individuals will have significant effect on dengue case reduction but ACF of asymptomatic individuals cannot be ignored. Our findings indicate that the healthcare organizations must focus on ACF of symptomatic as well as asymptomatic individuals along with personal protection and mosquitoes control to achieve rapid reduction of dengue cases in India.        △ Less","23 August, 2018","q-bio.PE,math.DS",
              A Century Long Commitment to Assessing Artificial Intelligence and its Impact on Society          ,1808.07899,https://arxiv.org/abs/1808.07899,https://arxiv.org/pdf/1808.07899,"Authors:BarbaraJ.Grosz,PeterStone","        In September 2016, Stanford's ""One Hundred Year Study on Artificial Intelligence"" project (AI100) issued the first report of its planned long-term periodic assessment of artificial intelligence (AI) and its impact on society. The report, entitled ""Artificial Intelligence and Life in 2030,"" examines eight domains of typical urban settings on which AI is likely to have impact over the coming years: transportation, home and service robots, healthcare, education, public safety and security, low-resource communities, employment and workplace, and entertainment. It aims to provide the general public with a scientifically and technologically accurate portrayal of the current state of AI and its potential and to help guide decisions in industry and governments, as well as to inform research and development in the field. This article by the chair of the 2016 Study Panel and the inaugural chair of the AI100 Standing Committee describes the origins of this ambitious longitudinal study, discusses the framing of the inaugural report, and presents the report's main findings. It concludes with a brief description of the AI100 project's ongoing efforts and planned next steps.        △ Less","23 August, 2018","cs.AI,cs.CY",
              Privacy Mining from IoT-based Smart Homes          ,1808.07379,https://arxiv.org/abs/1808.07379,https://arxiv.org/pdf/1808.07379,"Authors:Ming-ChangLee,Jia-ChunLin,OlafOwe","        Recently, a wide range of smart devices are deployed in a variety of environments to improve the quality of human life. One of the important IoT-based applications is smart homes for healthcare, especially for elders. IoT-based smart homes enable elders' health to be properly monitored and taken care of. However, elders' privacy might be disclosed from smart homes due to non-fully protected network communication or other reasons. To demonstrate how serious this issue is, we introduce in this paper a Privacy Mining Approach (PMA) to mine privacy from smart homes by conducting a series of deductions and analyses on sensor datasets generated by smart homes. The experimental results demonstrate that PMA is able to deduce a global sensor topology for a smart home and disclose elders' privacy in terms of their house layouts.        △ Less","11 September, 2018","cs.CY,cs.LG,stat.ML",
              HAC Analysis to Explore Clusters within Chronic Comorbid Inpatient Visits          ,1808.07358,https://arxiv.org/abs/1808.07358,https://arxiv.org/pdf/1808.07358,Authors:RasikaKarkare,"        Multimorbidities are associated with significant burden on the healthcare system and the lack of accurate and pertinent statistical exploratory techniques have often limited their analysis. Here we employ exploratory hierarchal agglomerative clustering (HAC) of multimorbidities in the inpatient population in the state of Ohio. The examination exposed the presence of ten discrete, clinically pertinent groups of multimorbidities within the Ohio inpatient population. This method offers an assessable empirical exploration of the multimorbidities present in a specific geographic populace.        △ Less","20 August, 2018",q-bio.TO,
              Learning to Exploit Invariances in Clinical Time-Series Data using Sequence Transformer Networks          ,1808.06725,https://arxiv.org/abs/1808.06725,https://arxiv.org/pdf/1808.06725,"Authors:JeehehOh,JiaxuanWang,JennaWiens","        Recently, researchers have started applying convolutional neural networks (CNNs) with one-dimensional convolutions to clinical tasks involving time-series data. This is due, in part, to their computational efficiency, relative to recurrent neural networks and their ability to efficiently exploit certain temporal invariances, (e.g., phase invariance). However, it is well-established that clinical data may exhibit many other types of invariances (e.g., scaling). While preprocessing techniques, (e.g., dynamic time warping) may successfully transform and align inputs, their use often requires one to identify the types of invariances in advance. In contrast, we propose the use of Sequence Transformer Networks, an end-to-end trainable architecture that learns to identify and account for invariances in clinical time-series data. Applied to the task of predicting in-hospital mortality, our proposed approach achieves an improvement in the area under the receiver operating characteristic curve (AUROC) relative to a baseline CNN (AUROC=0.851 vs. AUROC=0.838). Our results suggest that a variety of valuable invariances can be learned directly from the data.        △ Less","20 August, 2018","cs.LG,stat.ML",
              SmartEAR: Smartwatch-based Unsupervised Learning for Multi-modal Signal Analysis in Opportunistic Sensing Framework          ,1808.06473,https://arxiv.org/abs/1808.06473,https://arxiv.org/pdf/1808.06473,"Authors:DebanjanBorthakur,AndrewPeltier,HarishchandraDubey,JoshuaGyllinsky,KunalMankodiya","        Wrist-bands such as smartwatches have become an unobtrusive interface for collecting physiological and contextual data from users. Smartwatches are being used for smart healthcare, telecare, and wellness monitoring. In this paper, we used data collected from the AnEAR framework leveraging smartwatches to gather and store physiological data from patients in naturalistic settings. This data included temperature, galvanic skin response (GSR), acceleration, and heart rate (HR). In particular, we focused on HR and acceleration, as these two modalities are often correlated. Since the data was unlabeled we relied on unsupervised learning for multi-modal signal analysis. We propose using k-means clustering, GMM clustering, and Self-Organizing maps based on Neural Networks for group the multi-modal data into homogeneous clusters. This strategy helped in discovering latent structures in our data.        △ Less","11 August, 2018",cs.CY,
              Synthetic Patient Generation: A Deep Learning Approach Using Variational Autoencoders          ,1808.06444,https://arxiv.org/abs/1808.06444,https://arxiv.org/pdf/1808.06444,Authors:AllySalimJr,"        Artificial Intelligence in healthcare is a new and exciting frontier and the possibilities are endless. With deep learning approaches beating human performances in many areas, the logical next step is to attempt their application in the health space. For these and other Machine Learning approaches to produce good results and have their potential realized, the need for, and importance of, large amounts of accurate data is second to none. This is a challenge faced by many industries and more so in the healthcare space. We present an approach of using Variational Autoencoders (VAE's) as an approach to generating more data for training deeper networks, as well as uncovering underlying patterns in diagnoses and the patients suffering from them. By training a VAE, on available data, it was able to learn the latent distribution of the patient features given the diagnosis. It is then possible, after training, to sample from the learnt latent distribution to generate new accurate patient records given the patient diagnosis.        △ Less","20 August, 2018","cs.LG,stat.ML",
              Robust training of recurrent neural networks to handle missing data for disease progression modeling          ,1808.05500,https://arxiv.org/abs/1808.05500,https://arxiv.org/pdf/1808.05500,"Authors:MostafaMehdipourGhazi,MadsNielsen,AkshayPai,M.JorgeCardoso,MarcModat,SebastienOurselin,LaugeSørensen","        Disease progression modeling (DPM) using longitudinal data is a challenging task in machine learning for healthcare that can provide clinicians with better tools for diagnosis and monitoring of disease. Existing DPM algorithms neglect temporal dependencies among measurements and make parametric assumptions about biomarker trajectories. In addition, they do not model multiple biomarkers jointly and need to align subjects' trajectories. In this paper, recurrent neural networks (RNNs) are utilized to address these issues. However, in many cases, longitudinal cohorts contain incomplete data, which hinders the application of standard RNNs and requires a pre-processing step such as imputation of the missing values. We, therefore, propose a generalized training rule for the most widely used RNN architecture, long short-term memory (LSTM) networks, that can handle missing values in both target and predictor variables. This algorithm is applied for modeling the progression of Alzheimer's disease (AD) using magnetic resonance imaging (MRI) biomarkers. The results show that the proposed LSTM algorithm achieves a lower mean absolute error for prediction of measurements across all considered MRI biomarkers compared to using standard LSTM networks with data imputation or using a regression-based DPM method. Moreover, applying linear discriminant analysis to the biomarkers' values predicted by the proposed algorithm results in a larger area under the receiver operating characteristic curve (AUC) for clinical diagnosis of AD compared to the same alternatives, and the AUC is comparable to state-of-the-art AUCs from a recent cross-sectional medical image classification challenge. This paper shows that built-in handling of missing values in LSTM network training paves the way for application of RNNs in disease progression modeling.        △ Less","16 August, 2018","cs.CV,cs.LG",
              A Scalable Data Science Platform for Healthcare and Precision Medicine Research          ,1808.04849,https://arxiv.org/abs/1808.04849,https://arxiv.org/pdf/1808.04849,"Authors:JacobMcPadden,ThomasJSDurant,DustinRBunch,AndreasCoppi,NathanPrice,KrisRodgerson,CharlesJTorreJr,WilliamByron,HPatrickYoung,AllenLHsiao,HarlanMKrumholz,WadeLSchulz","        Objective: To (1) demonstrate the implementation of a data science platform built on open-source technology within a large, academic healthcare system and (2) describe two computational healthcare applications built on such a platform. Materials and Methods: A data science platform based on several open source technologies was deployed to support real-time, big data workloads. Data acquisition workflows for Apache Storm and NiFi were developed in Java and Python to capture patient monitoring and laboratory data for downstream analytics. Results: The use of emerging data management approaches along with open-source technologies such as Hadoop can be used to create integrated data lakes to store large, real-time data sets. This infrastructure also provides a robust analytics platform where healthcare and biomedical research data can be analyzed in near real-time for precision medicine and computational healthcare use cases. Discussion: The implementation and use of integrated data science platforms offer organizations the opportunity to combine traditional data sets, including data from the electronic health record, with emerging big data sources, such as continuous patient monitoring and real-time laboratory results. These platforms can enable cost-effective and scalable analytics for the information that will be key to the delivery of precision medicine initiatives. Conclusion: Organizations that can take advantage of the technical advances found in data science platforms will have the opportunity to provide comprehensive access to healthcare data for computational healthcare and precision medicine research.        △ Less","14 August, 2018",cs.DC,
              Murmur Detection Using Parallel Recurrent & Convolutional Neural Networks          ,1808.04411,https://arxiv.org/abs/1808.04411,https://arxiv.org/pdf/1808.04411,"Authors:ShahnawazAlam,RohanBanerjee,SomaBandyopadhyay","        In this article, we propose a novel technique for classification of the Murmurs in heart sound. We introduce a novel deep neural network architecture using parallel combination of the Recurrent Neural Network (RNN) based Bidirectional Long Short-Term Memory (BiLSTM) & Convolutional Neural Network (CNN) to learn visual and time-dependent characteristics of Murmur in PCG waveform. Set of acoustic features are presented to our proposed deep neural network to discriminate between Normal and Murmur class. The proposed method was evaluated on a large dataset using 5-fold cross-validation, resulting in a sensitivity and specificity of 96 +- 0.6 % , 100 +- 0 % respectively and F1 Score of 98 +- 0.3 %.        △ Less","13 August, 2018","cs.SD,cs.LG,eess.AS,stat.ML",
              A Domain Guided CNN Architecture for Predicting Age from Structural Brain Images          ,1808.04362,https://arxiv.org/abs/1808.04362,https://arxiv.org/pdf/1808.04362,"Authors:PascalSturmfels,SaigeRutherford,MikeAngstadt,MarkPeterson,ChandraSripada,JennaWiens","        Given the wide success of convolutional neural networks (CNNs) applied to natural images, researchers have begun to apply them to neuroimaging data. To date, however, exploration of novel CNN architectures tailored to neuroimaging data has been limited. Several recent works fail to leverage the 3D structure of the brain, instead treating the brain as a set of independent 2D slices. Approaches that do utilize 3D convolutions rely on architectures developed for object recognition tasks in natural 2D images. Such architectures make assumptions about the input that may not hold for neuroimaging. For example, existing architectures assume that patterns in the brain exhibit translation invariance. However, a pattern in the brain may have different meaning depending on where in the brain it is located. There is a need to explore novel architectures that are tailored to brain images. We present two simple modifications to existing CNN architectures based on brain image structure. Applied to the task of brain age prediction, our network achieves a mean absolute error (MAE) of 1.4 years and trains 30% faster than a CNN baseline that achieves a MAE of 1.6 years. Our results suggest that lessons learned from developing models on natural images may not directly transfer to neuroimaging tasks. Instead, there remains a large space of unexplored questions regarding model development in this area, whose answers may differ from conventional wisdom.        △ Less","11 August, 2018","cs.CV,cs.LG,stat.ML",
              Compound Poisson Noise Sources in Diffusion-based Molecular Communication          ,1808.03876,https://arxiv.org/abs/1808.03876,https://arxiv.org/pdf/1808.03876,"Authors:AliEtemadi,PaeizAzmi,HamidrezaArjmandi,NaderMokari","        Diffusion-based molecular communication (DMC) is one of the most promising approaches for realizing nano-scale communications for healthcare applications. The DMC systems in in-vivo environments may encounter biological entities that release molecules identical to the molecules used for signaling as part of their functionality. Such entities in the environment act as external noise sources from the DMC system's perspective. In this paper, the release of molecules by biological external noise sources is particularly modeled as a compound Poisson process. The impact of compound Poisson noise sources (CPNSs) on the performance of a point-to-point DMC system is investigated. To this end, the noise from the CPNS observed at the receiver is characterized. Considering a simple on-off keying (OOK) modulation and formulating symbol-by-symbol maximum likelihood (ML) detector, the performance of DMC system in the presence of the CPNS is analyzed. For special case of CPNS in high-rate regime, the noise received from the CPNS is approximated as a Poisson process whose rate is normally distributed. In this case, it is proved that a simple single-threshold detector (STD) is an optimal ML detector. Our results reveal that in general, adopting the conventional simple homogeneous Poisson noise model may lead to overly optimistic performance predictions, if a CPNS is present.        △ Less","21 November, 2018",cs.IT,
              Racial Disparities and Mistrust in End-of-Life Care          ,1808.03827,https://arxiv.org/abs/1808.03827,https://arxiv.org/pdf/1808.03827,"Authors:WillieBoag,HariniSuresh,LeoAnthonyCeli,PeterSzolovits,MarzyehGhassemi","        There are established racial disparities in healthcare, including during end-of-life care, when poor communication and trust can lead to suboptimal outcomes for patients and their families. In this work, we find that racial disparities which have been reported in existing literature are also present in the MIMIC-III database. We hypothesize that one underlying cause of this disparity is due to mistrust between patient and caregivers, and we develop multiple possible trust metric proxies (using coded interpersonal variables and clinical notes) to measure this phenomenon more directly. These metrics show even stronger disparities in end-of-life care than race does, and they also tend to demonstrate statistically significant higher levels of mistrust for black patients than white ones. Finally, we demonstrate that these metrics improve performance on three clinical tasks: in-hospital mortality, discharge against medical advice (AMA) and modified care status (e.g., DNR, DNI, etc.).        △ Less","15 August, 2018",stat.AP,
              Making effective use of healthcare data using data-to-text technology          ,1808.03507,https://arxiv.org/abs/1808.03507,https://arxiv.org/pdf/1808.03507,"Authors:SteffenPauws,AlbertGatt,EmielKrahmer,EhudReiter","Healthcare organizations are in a continuous effort to improve health outcomes, reduce costs and enhance patient experience of care. Data is essential to measure and help achieving these improvements in healthcare delivery. Consequently, a data influx from various clinical, financial and operational sources is now overtaking healthcare organizations and their patients. The effective use of this data, however, is a major challenge. Clearly, text is an important medium to make data accessible. Financial reports are produced to assess healthcare organizations on some key performance indicators to steer their healthcare delivery. Similarly, at a clinical level, data on patient status is conveyed by means of textual descriptions to facilitate patient review, shift handover and care transitions. Likewise, patients are informed about data on their health status and treatments via text, in the form of reports or via ehealth platforms by their doctors. Unfortunately, such text is the outcome of a highly labour-intensive process if it is done by healthcare professionals. It is also prone to incompleteness, subjectivity and hard to scale up to different domains, wider audiences and varying communication purposes. Data-to-text is a recent breakthrough technology in artificial intelligence which automatically generates natural language in the form of text or speech from data. This chapter provides a survey of data-to-text technology, with a focus on how it can be deployed in a healthcare setting. It will (1) give an up-to-date synthesis of data-to-text approaches, (2) give a categorized overview of use cases in healthcare, (3) seek to make a strong case for evaluating and implementing data-to-text in a healthcare setting, and (4) highlight recent research challenges.        △ Less","10 August, 2018","cs.CL,cs.CY",
              A Hybrid Recommender System for Patient-Doctor Matchmaking in Primary Care          ,1808.03265,https://arxiv.org/abs/1808.03265,https://arxiv.org/pdf/1808.03265,"Authors:QiweiHan,MengxinJi,InigoMartinezdeRituertodeTroya,ManasGaur,LeidZejnilovic","        We partner with a leading European healthcare provider and design a mechanism to match patients with family doctors in primary care. We define the matchmaking process for several distinct use cases given different levels of available information about patients. Then, we adopt a hybrid recommender system to present each patient a list of family doctor recommendations. In particular, we model patient trust of family doctors using a large-scale dataset of consultation histories, while accounting for the temporal dynamics of their relationships. Our proposed approach shows higher predictive accuracy than both a heuristic baseline and a collaborative filtering approach, and the proposed trust measure further improves model performance.        △ Less","18 March, 2019","cs.IR,cs.LG,stat.ML",
              Ultra-Conformable Free-Standing Capacitors Based on Ultrathin Polyvinyl Formal Films          ,1808.03259,https://arxiv.org/abs/1808.03259,https://arxiv.org/pdf/1808.03259,"Authors:JonathanBarsotti,IkueHirata,FrancescaPignatelli,MarioCaironi,FrancescoGreco,VirgilioMattoli","        Conformable Electronics refers to a class of electronic devices that have the ability to conformally adhere onto non-planar surfaces and materials, resulting particularly appealing for skin applications, such as the case of skin-worn unobtrusive (bio)sensors for healthcare monitoring. Conformability can be addressed by integrating basic electronic components on ultrathin polymeric film substrates. Among other basic electronic components, capacitors are fundamental ones for energy storage, sensing, frequency tuning, impedance adaptation and signal processing. In this work we present a novel approach for conformable capacitors based on a free-standing, ultrathin and ultra-conformable nanosheets of poly (vinyl formal) (PVF), which serve both as structural and dielectric component of the capacitor. A novel fabrication approach is proposed and applied to fully free-standing ultrathin capacitors fabrication, having an overall thickness as low as 200 nm; that represents, to the best of our knowledge, the thinnest free-standing capacitors ever reported. Thanks to the ultra-low thickness, the proposed capacitors are able to sustain flexure to extremely small curvature radii (as low as 1.5 μm) and to conform to complex surfaces, such as a nylon mesh with micrometric texture without compromising their operation.        △ Less","9 August, 2018",physics.app-ph,10.1002/aelm.201800215 
              PHI Scrubber: A Deep Learning Approach          ,1808.01128,https://arxiv.org/abs/1808.01128,https://arxiv.org/pdf/1808.01128,"Authors:AbhaiKollaraDilip,KamalRajK,MalaikannanSankarasubbu","        Confidentiality of patient information is an essential part of Electronic Health Record System. Patient information, if exposed, can cause a serious damage to the privacy of individuals receiving healthcare. Hence it is important to remove such details from physician notes. A system is proposed which consists of a deep learning model where a de-convolutional neural network and bi-directional LSTM-CNN is used along with regular expressions to recognize and eliminate the individually identifiable information. This information is then removed from a medical practitioner's data which further allows the fair usage of such information among researchers and in clinical trials.        △ Less","3 August, 2018","cs.LG,stat.ML",
              Mixed effects models for healthcare longitudinal data with an informative visiting process: a Monte Carlo simulation study          ,1808.00419,https://arxiv.org/abs/1808.00419,https://arxiv.org/pdf/1808.00419,"Authors:AlessandroGasparini,KeithR.Abrams,JessicaK.Barrett,RupertW.Major,MichaelJ.Sweeting,NigelJ.Brunskill,MichaelJ.Crowther","        Electronic health records are being increasingly used in medical research to answer more relevant and detailed clinical questions; however, they pose new and significant methodological challenges. For instance, observation times are likely correlated with the underlying disease severity: patients with worse conditions utilise health care more and may have worse biomarker values recorded. Traditional methods for analysing longitudinal data assume independence between observation times and disease severity; yet, with healthcare data such assumptions unlikely holds. Through Monte Carlo simulation, we compare different analytical approaches proposed to account for an informative visiting process to assess whether they lead to unbiased results. Furthermore, we formalise a joint model for the observation process and the longitudinal outcome within an extended joint modelling framework. We illustrate our results using data from a pragmatic trial on enhanced care for individuals with chronic kidney disease, and we introduce user-friendly software that can be used to fit the joint model for the observation process and a longitudinal outcome.        △ Less","25 July, 2019",stat.ME,10.1111/stan.12188 
              Mobile Technology in Healthcare Environment: Security Vulnerabilities and Countermeasures          ,1807.11086,https://arxiv.org/abs/1807.11086,https://arxiv.org/pdf/1807.11086,"Authors:SajedulTalukder,ShalishaWitherspoon,KanishkSrivastava,RyanThompson","        Mobile devices and technologies offer a tremendous amount of benefits to users, although it is also understood that it introduces a set of challenges when it comes to security, compliance, and risks. More and more healthcare organizations have been seeking to update their outdated technology, and have considered the adoption of mobile devices to meet these needs. However, introducing mobile devices and technology also introduces new risks and threats to the organization. As a test case, we examine Epic Rover, a mobile application that has been identified as a viable solution to manage the electronic medical system. In this paper, we study the insights that the security team needs to investigate, before the adoption of this mobile technology, as well as provide a thorough examination of the vulnerabilities and threats that the use of mobile devices in the healthcare environment brings, and introduce countermeasures and mitigations to reduce the risk while maintaining regulatory compliance.        △ Less","29 July, 2018","cs.CR,cs.CY",
              Leveraging Machine Learning Techniques for Windows Ransomware Network Traffic Detection          ,1807.10440,https://arxiv.org/abs/1807.10440,https://arxiv.org/pdf/1807.10440,"Authors:OmarM.K.Alhawi,JamesBaldwin,AliDehghantanha","        Ransomware has become a significant global threat with the ransomware-as-a-service model enabling easy availability and deployment, and the potential for high revenues creating a viable criminal business model. Individuals, private companies or public service providers e.g. healthcare or utilities companies can all become victims of ransomware attacks and consequently suffer severe disruption and financial loss. Although machine learning algorithms are already being used to detect ransomware, variants are being developed to specifically evade detection when using dynamic machine learning techniques. In this paper, we introduce NetConverse, a machine learning analysis of Windows ransomware network traffic to achieve a high, consistent detection rate. Using a dataset created from conversation-based network traffic features we achieved a true positive detection rate of 97.1% using the Decision Tree (J48) classifier.        △ Less","27 July, 2018",cs.CR,10.1007/978-3-319-73951-9_5 
"              DeepSPINE: Automated Lumbar Vertebral Segmentation, Disc-level Designation, and Spinal Stenosis Grading Using Deep Learning          ",1807.10215,https://arxiv.org/abs/1807.10215,https://arxiv.org/pdf/1807.10215,"Authors:Jen-TangLu,StefanoPedemonte,BernardoBizzo,SeanDoyle,KatherineP.Andriole,MarkH.Michalski,R.GilbertoGonzalez,StuartR.Pomerantz","        The high prevalence of spinal stenosis results in a large volume of MRI imaging, yet interpretation can be time-consuming with high inter-reader variability even among the most specialized radiologists. In this paper, we develop an efficient methodology to leverage the subject-matter-expertise stored in large-scale archival reporting and image data for a deep-learning approach to fully-automated lumbar spinal stenosis grading. Specifically, we introduce three major contributions: (1) a natural-language-processing scheme to extract level-by-level ground-truth labels from free-text radiology reports for the various types and grades of spinal stenosis (2) accurate vertebral segmentation and disc-level localization using a U-Net architecture combined with a spine-curve fitting method, and (3) a multi-input, multi-task, and multi-class convolutional neural network to perform central canal and foraminal stenosis grading on both axial and sagittal imaging series inputs with the extracted report-derived labels applied to corresponding imaging level segments. This study uses a large dataset of 22796 disc-levels extracted from 4075 patients. We achieve state-of-the-art performance on lumbar spinal stenosis classification and expect the technique will increase both radiology workflow efficiency and the perceived value of radiology reports for referring clinicians and patients.        △ Less","26 July, 2018","cs.CV,cs.LG",
              Bivariate network meta-analysis for surrogate endpoint evaluation          ,1807.08928,https://arxiv.org/abs/1807.08928,https://arxiv.org/pdf/1807.08928,"Authors:SylwiaBujkiewicz,DanJackson,JohnRThompson,RebeccaTurner,KeithRAbrams,IanRWhite","        Surrogate endpoints are very important in regulatory decision-making in healthcare, in particular if they can be measured early compared to the long-term final clinical outcome and act as good predictors of clinical benefit. Bivariate meta-analysis methods can be used to evaluate surrogate endpoints and to predict the treatment effect on the final outcome from the treatment effect measured on a surrogate endpoint. However, candidate surrogate endpoints are often imperfect, and the level of association between the treatment effects on the surrogate and final outcomes may vary between treatments. This imposes a limitation on the pairwise methods which do not differentiate between the treatments. We develop bivariate network meta-analysis (bvNMA) methods which combine data on treatment effects on the surrogate and final outcomes, from trials investigating heterogeneous treatment contrasts. The bvNMA methods estimate the effects on both outcomes for all treatment contrasts individually in a single analysis. At the same time, they allow us to model the surrogacy patterns across multiple trials (different populations) within a treatment contrast and across treatment contrasts, thus enabling predictions of the treatment effect on the final outcome for a new study in a new population or investigating a new treatment. Modelling assumptions about the between-studies heterogeneity and the network consistency, and their impact on predictions, are investigated using simulated data and an illustrative example in advanced colorectal cancer. When the strength of the surrogate relationships varies across treatment contrasts, bvNMA has the advantage of identifying treatments for which surrogacy holds, thus leading to better predictions.        △ Less","24 July, 2018",stat.ME,
              Visual Affordance and Function Understanding: A Survey          ,1807.06775,https://arxiv.org/abs/1807.06775,https://arxiv.org/pdf/1807.06775,"Authors:MohammedHassanin,SalmanKhan,MuratTahtali","        Nowadays, robots are dominating the manufacturing, entertainment and healthcare industries. Robot vision aims to equip robots with the ability to discover information, understand it and interact with the environment. These capabilities require an agent to effectively understand object affordances and functionalities in complex visual domains. In this literature survey, we first focus on Visual affordances and summarize the state of the art as well as open problems and research gaps. Specifically, we discuss sub-problems such as affordance detection, categorization, segmentation and high-level reasoning. Furthermore, we cover functional scene understanding and the prevalent functional descriptors used in the literature. The survey also provides necessary background to the problem, sheds light on its significance and highlights the existing challenges for affordance and functionality learning.        △ Less","18 July, 2018","cs.CV,cs.RO",
              Efficient Deep Learning on Multi-Source Private Data          ,1807.06689,https://arxiv.org/abs/1807.06689,https://arxiv.org/pdf/1807.06689,"Authors:NickHynes,RaymondCheng,DawnSong","        Machine learning models benefit from large and diverse datasets. Using such datasets, however, often requires trusting a centralized data aggregator. For sensitive applications like healthcare and finance this is undesirable as it could compromise patient privacy or divulge trade secrets. Recent advances in secure and privacy-preserving computation, including trusted hardware enclaves and differential privacy, offer a way for mutually distrusting parties to efficiently train a machine learning model without revealing the training data. In this work, we introduce Myelin, a deep learning framework which combines these privacy-preservation primitives, and use it to establish a baseline level of performance for fully private machine learning.        △ Less","17 July, 2018","cs.LG,stat.ML",
              Siamese Survival Analysis with Competing Risks          ,1807.05935,https://arxiv.org/abs/1807.05935,https://arxiv.org/pdf/1807.05935,"Authors:AntonNemchenko,TrentKyono,MihaelaVanDerSchaar","        Survival analysis in the presence of multiple possible adverse events, i.e., competing risks, is a pervasive problem in many industries (healthcare, finance, etc.). Since only one event is typically observed, the incidence of an event of interest is often obscured by other related competing events. This nonidentifiability, or inability to estimate true cause-specific survival curves from empirical data, further complicates competing risk survival analysis. We introduce Siamese Survival Prognosis Network (SSPN), a novel deep learning architecture for estimating personalized risk scores in the presence of competing risks. SSPN circumvents the nonidentifiability problem by avoiding the estimation of cause-specific survival curves and instead determines pairwise concordant time-dependent risks, where longer event times are assigned lower risks. Furthermore, SSPN is able to directly optimize an approximation to the C-discrimination index, rather than relying on well-known metrics which are unable to capture the unique requirements of survival analysis with competing risks.        △ Less","16 August, 2018","cs.LG,stat.ML",
              A latent factor approach for prediction from multiple assays          ,1807.05675,https://arxiv.org/abs/1807.05675,https://arxiv.org/pdf/1807.05675,"Authors:J.KennethTay,RobertTibshirani","        In many domains such as healthcare or finance, data often come in different assays or measurement modalities, with features in each assay having a common theme. Simply concatenating these assays together and performing prediction can be effective but ignores this structure. In this setting, we propose a model which contains latent factors specific to each assay, as well as a common latent factor across assays. We frame our model-fitting procedure, which we call the ""Sparse Factor Method"" (SFM), as an optimization problem and present an iterative algorithm to solve it.        △ Less","16 July, 2018",stat.ME,
              Improving Photoplethysmographic Measurements under Motion Artifacts using Artificial Neural Network for Personal Healthcare,1807.05331,https://arxiv.org/abs/1807.05331,https://arxiv.org/pdf/1807.05331,"Authors:MonalisaSinghaRoy,RajarshiGupta,JayantaK.Chandra,KaushikDasSharma,ArunansuTalukdar","        Photoplethysmographic (PPG) measurements are susceptible to motion artifacts (MA) due to movement of the peripheral body parts. In this paper, we present a new approach to identify the MA corrupted PPG beats and then rectify the beat morphology using artificial neural network (ANN). Initially, beat quality assessment was done to identify the clean PPG beats by a pre-trained feedback ANN to generate a reference beat template for each person. The PPG data was decomposed using principal component analysis (PCA) and reconstructed using fixed energy retention. A weight coefficient was assigned for each PPG samples in such a way that when they are multiplied , the modified beat morphology matches the reference template. A particle swarm optimization (PSO) based technique was utilized to select the best weight weight vector coefficients to tune another feedback ANN, fed with a set of significant features generated by an auto encoder from PCA reconstructed data. For real time implementation, this pre-trained ANN was operated in feed-forward mode to directly generate the weight vectors for any subsequent measurements of PPG. The method was validated with PPG data collected from 55 human subjects. An average RMSE of 0.28 and SNR improvement of 14.54 dB was obtained, with an average improvement of 36% and 47% measurement accuracy on crest time and systolic to diastolic peak height ratio respectively. With IEEE Signal Processing Cup 2015 Challenge database, Pearson's correlation coefficient between PPG estimated and ECG derived heart rate was 0.990. The proposed method can be useful for personal health monitoring applications.        △ Less","14 July, 2018",eess.SP,10.1109/TIM.2018.2829488 
              Online Heart Rate Prediction using Acceleration from a Wrist Worn Wearable          ,1807.04667,https://arxiv.org/abs/1807.04667,https://arxiv.org/pdf/1807.04667,"Authors:RyanMcConville,GarethArcher,IanCraddock,HermanterHorst,RobertPiechocki,JamesPope,RaulSantos-Rodriguez","        In this paper we study the prediction of heart rate from acceleration using a wrist worn wearable. Although existing photoplethysmography (PPG) heart rate sensors provide reliable measurements, they use considerably more energy than accelerometers and have a major impact on battery life of wearable devices. By using energy-efficient accelerometers to predict heart rate, significant energy savings can be made. Further, we are interested in understanding patient recovery after a heart rate intervention, where we expect a variation in heart rate over time. Therefore, we propose an online approach to tackle the concept as time passes. We evaluate the methods on approximately 4 weeks of free living data from three patients over a number of months. We show that our approach can achieve good predictive performance (e.g., 2.89 Mean Absolute Error) while using the PPG heart rate sensor infrequently (e.g., 20.25% of the samples).        △ Less","25 June, 2018","stat.AP,cs.LG",
              Recognising Cardiac Abnormalities in Wearable Device Photoplethysmography (PPG) with Deep Learning          ,1807.04077,https://arxiv.org/abs/1807.04077,https://arxiv.org/pdf/1807.04077,"Authors:StewartWhiting,SamuelMoreland,JasonCostello,GlenColopy,ChristopherMcCann","        Cardiac abnormalities affecting heart rate and rhythm are commonly observed in both healthy and acutely unwell people. Although many of these are benign, they can sometimes indicate a serious health risk. ECG monitors are typically used to detect these events in electrical heart activity, however they are impractical for continuous long-term use. In contrast, current-generation wearables with optical photoplethysmography (PPG) have gained popularity with their low-cost, lack of wires and tiny size. Many cardiac abnormalities such as ectopic beats and AF can manifest as both obvious and subtle anomalies in a PPG waveform as they disrupt blood flow. We propose an automatic method for recognising these anomalies in PPG signal alone, without the need for ECG. We train an LSTM deep neural network on 400,000 clean PPG samples to learn typical PPG morphology and rhythm, and flag PPG signal diverging from this as cardiac abnormalities. We compare the cardiac abnormalities our approach recognises with the ectopic beats recorded by a bedside ECG monitor for 29 patients over 47.6 hours of gold standard observations. Our proposed cardiac abnormality recognition approach recognises 60%+ of ECG-detected PVCs in PPG signal, with a false positive rate of 23% - demonstrating the compelling power and value of this novel approach. Finally we examine how cardiac abnormalities manifest in PPG signal for in- and out-of-hospital patient populations using a wearable device during standard care.        △ Less","11 July, 2018",eess.SP,
              Causal Discovery in the Presence of Missing Data          ,1807.04010,https://arxiv.org/abs/1807.04010,https://arxiv.org/pdf/1807.04010,"Authors:RuiboTu,KunZhang,PaulAckermann,BoChristerBertilson,ClarkGlymour,HedvigKjellström,ChengZhang","        Missing data are ubiquitous in many domains including healthcare. When these data entries are not missing completely at random, the (conditional) independence relations in the observed data may be different from those in the complete data generated by the underlying causal process. Consequently, simply applying existing causal discovery methods to the observed data may lead to wrong conclusions. In this paper, we aim at developing a causal discovery method to recover the underlying causal structure from observed data that follow different missingness mechanisms, including missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). With missingness mechanisms represented by missingness graphs, we analyse conditions under which additional correction is needed to derive conditional independence/dependence relations in the complete data. Based on our analysis, we propose the Missing Value PC (MVPC) algorithm for both continuous and binary variables, which extends the PC algorithm to incorporate additional corrections. Our proposed MVPC is shown in theory to give asymptotically correct results even on data that are MAR or MNAR. Experimental results on synthetic data show that the proposed algorithm is able to find correct causal relations even in the general case of MNAR. Moreover, we create a neuropathic pain diagnostic simulator for evaluating causal discovery methods. Evaluated on such simulated neuropathic pain diagnosis records and the other two real world applications, MVPC outperforms the other benchmark methods.        △ Less","12 July, 2020","cs.LG,stat.ML",
              FHIRChain: Applying Blockchain to Securely and Scalably Share Clinical Data          ,1807.03227,https://arxiv.org/abs/1807.03227,https://arxiv.org/pdf/1807.03227,"Authors:PengZhang,JulesWhite,DouglasC.Schmidt,GuntherLenz,S.TrentRosenbloom","        Secure and scalable data sharing is essential for collaborative clinical decision making. Conventional clinical data efforts are often siloed, however, which creates barriers to efficient information exchange and impedes effective treatment decision made for patients. This paper provides four contributions to the study of applying blockchain technology to clinical data sharing in the context of technical requirements defined in the ""Shared Nationwide Interoperability Roadmap"" from the Office of the National Coordinator for Health Information Technology (ONC). First, we analyze the ONC requirements and their implications for blockchain-based systems. Second, we present FHIRChain, which is a blockchain-based architecture designed to meet ONC requirements by encapsulating the HL7 Fast Healthcare Interoperability Resources (FHIR) standard for shared clinical data. Third, we demonstrate a FHIRChain-based decentralized app using digital health identities to authenticate participants in a case study of collaborative decision making for remote cancer care. Fourth, we highlight key lessons learned from our case study.        △ Less","9 July, 2018","cs.CY,cs.CR",
              YouTube for Patient Education: A Deep Learning Approach for Understanding Medical Knowledge from User-Generated Videos          ,1807.03179,https://arxiv.org/abs/1807.03179,https://arxiv.org/pdf/1807.03179,"Authors:XiaoLiu,BinZhang,AnjanaSusarla,RemaPadman","        YouTube presents an unprecedented opportunity to explore how machine learning methods can improve healthcare information dissemination. We propose an interdisciplinary lens that synthesizes machine learning methods with healthcare informatics themes to address the critical issue of developing a scalable algorithmic solution to evaluate videos from a health literacy and patient education perspective. We develop a deep learning method to understand the level of medical knowledge encoded in YouTube videos. Preliminary results suggest that we can extract medical knowledge from YouTube videos and classify videos according to the embedded knowledge with satisfying performance. Deep learning methods show great promise in knowledge extraction, natural language understanding, and image classification, especially in an era of patient-centric care and precision medicine.        △ Less","6 July, 2018","cs.CV,cs.LG,stat.ML",
              ChestNet: A Deep Neural Network for Classification of Thoracic Diseases on Chest Radiography          ,1807.03058,https://arxiv.org/abs/1807.03058,https://arxiv.org/pdf/1807.03058,"Authors:HongyuWang,YongXia","        Computer-aided techniques may lead to more accurate and more acces-sible diagnosis of thorax diseases on chest radiography. Despite the success of deep learning-based solutions, this task remains a major challenge in smart healthcare, since it is intrinsically a weakly supervised learning problem. In this paper, we incorporate the attention mechanism into a deep convolutional neural network, and thus propose the ChestNet model to address effective diagnosis of thorax diseases on chest radiography. This model consists of two branches: a classification branch serves as a uniform feature extraction-classification network to free users from troublesome handcrafted feature extraction, and an attention branch exploits the correlation between class labels and the locations of patholog-ical abnormalities and allows the model to concentrate adaptively on the patholog-ically abnormal regions. We evaluated our model against three state-of-the-art deep learning models on the Chest X-ray 14 dataset using the official patient-wise split. The results indicate that our model outperforms other methods, which use no extra training data, in diagnosing 14 thorax diseases on chest radiography.        △ Less","9 July, 2018",cs.CV,
              Convolutional Recurrent Neural Networks for Glucose Prediction          ,1807.03043,https://arxiv.org/abs/1807.03043,https://arxiv.org/pdf/1807.03043,"Authors:KezhiLi,JohnDaniels,ChengyuanLiu,PauHerrero,PantelisGeorgiou","        Control of blood glucose is essential for diabetes management. Current digital therapeutic approaches for subjects with Type 1 diabetes mellitus (T1DM) such as the artificial pancreas and insulin bolus calculators leverage machine learning techniques for predicting subcutaneous glucose for improved control. Deep learning has recently been applied in healthcare and medical research to achieve state-of-the-art results in a range of tasks including disease diagnosis, and patient state prediction among others. In this work, we present a deep learning model that is capable of forecasting glucose levels with leading accuracy for simulated patient cases (RMSE = 9.38±\pm0.71 [mg/dL] over a 30-minute horizon, RMSE = 18.87±\pm2.25 [mg/dL] over a 60-minute horizon) and real patient cases (RMSE = 21.07±\pm2.35 [mg/dL] for 30-minute, RMSE = 33.27±\pm4.79\% for 60-minute). In addition, the model provides competitive performance in providing effective prediction horizon (PHeffPH_{eff}) with minimal time lag both in a simulated patient dataset (PHeffPH_{eff} = 29.0±\pm0.7 for 30-min and PHeffPH_{eff} = 49.8±\pm2.9 for 60-min) and in a real patient dataset (PHeffPH_{eff} = 19.3±\pm3.1 for 30-min and PHeffPH_{eff} = 29.3±\pm9.4 for 60-min). This approach is evaluated on a dataset of 10 simulated cases generated from the UVa/Padova simulator and a clinical dataset of 10 real cases each containing glucose readings, insulin bolus, and meal (carbohydrate) data. Performance of the recurrent convolutional neural network is benchmarked against four algorithms. The proposed algorithm is implemented on an Android mobile phone, with an execution time of 66ms on a phone compared to an execution time of 780780ms on a laptop.        △ Less","25 February, 2019",cs.CV,
              Abnormality Detection inside Blood Vessels with Mobile Nanomachines          ,1807.02728,https://arxiv.org/abs/1807.02728,https://arxiv.org/pdf/1807.02728,"Authors:NeerajVarshney,AdarshPatel,YanshaDeng,WernerHaselmayr,PramodK.Varshney,ArumugamNallanathan","        Motivated by the numerous healthcare applications of molecular communication within Internet of Bio-Nano Things (IoBNT), this work addresses the problem of abnormality detection in a blood vessel using multiple biological embedded computing devices called cooperative biological nanomachines (CNs), and a common receiver called the fusion center (FC). Due to blood flow inside a vessel, each CN and the FC are assumed to be mobile. In this work, each of the CNs perform abnormality detection with certain probabilities of detection and false alarm by counting the number of molecules received from a source, e.g., infected tissue. These CNs subsequently report their local decisions to a FC over a diffusion-advection blood flow channel using different types of molecules in the presence of inter-symbol interference, multi-source interference, and counting errors. Due to limited computational capability at the FC, OR and AND logic based fusion rules are employed to make the final decision after obtaining each local decision based on the optimal likelihood ratio test. For the aforementioned system, probabilities of detection and false alarm at the FC are derived for OR and AND fusion rules. Finally, simulation results are presented to validate the derived analytical results, which provide important insights.        △ Less","7 July, 2018",cs.IT,
              Diffusive Molecular Communication in Biological Cylindrical Environment          ,1807.02683,https://arxiv.org/abs/1807.02683,https://arxiv.org/pdf/1807.02683,"Authors:MohammadZoofaghari,HamidrezaArjmandi","        Diffusive molecular communication (DMC) is one of the most promising approaches for realizing nano-scale communications in biological environments for healthcare applications. In this paper, a DMC system in biological cylindrical environment is considered, inspired by blood vessel structures in the body. The internal surface of the cylinder boundary is assumed to be covered by the biological receptors which may irreversibly react with hitting molecules. Also, information molecules diffusing in the fluid medium are subject to a degradation reaction and flow. The concentration Green's function of diffusion in this environment is analytically derived which takes into account asymmetry in all radial, axial and azimuthal coordinates. Employing obtained Green's function, information channel between transmitter and transparent receiver of DMC is characterized. To evaluate the DMC system in the biological cylinder, a simple on-off keying modulation scheme is adopted and corresponding error probability is derived. Particle based simulation results confirm the proposed analysis. Also, the effect of different system parameters on the concentration Green's function are examined. Our results reveal that the degradation reaction and the boundary covered by biological receptors may be utilized to mitigate intersymbol interference and outperform corresponding error probability.        △ Less","24 November, 2018",cs.IT,
              Predicting Infant Motor Development Status using Day Long Movement Data from Wearable Sensors          ,1807.02617,https://arxiv.org/abs/1807.02617,https://arxiv.org/pdf/1807.02617,"Authors:DavidGoodfellow,RuoyuZhi,RebeccaFunke,JoseCarlosPulido,MajaMataric,BethA.Smith","        Infants with a variety of complications at or before birth are classified as being at risk for developmental delays (AR). As they grow older, they are followed by healthcare providers in an effort to discern whether they are on a typical or impaired developmental trajectory. Often, it is difficult to make an accurate determination early in infancy as infants with typical development (TD) display high variability in their developmental trajectories both in content and timing. Studies have shown that spontaneous movements have the potential to differentiate typical and atypical trajectories early in life using sensors and kinematic analysis systems. In this study, machine learning classification algorithms are used to take inertial movement from wearable sensors placed on an infant for a day and predict if the infant is AR or TD, thus further establishing the connection between early spontaneous movement and developmental trajectory.        △ Less","14 October, 2018","cs.LG,stat.ML",
              From Text to Topics in Healthcare Records: An Unsupervised Graph Partitioning Methodology          ,1807.02599,https://arxiv.org/abs/1807.02599,https://arxiv.org/pdf/1807.02599,"Authors:M.TarikAltuncu,ErikMayer,SophiaN.Yaliraki,MauricioBarahona","        Electronic Healthcare Records contain large volumes of unstructured data, including extensive free text. Yet this source of detailed information often remains under-used because of a lack of methodologies to extract interpretable content in a timely manner. Here we apply network-theoretical tools to analyse free text in Hospital Patient Incident reports from the National Health Service, to find clusters of documents with similar content in an unsupervised manner at different levels of resolution. We combine deep neural network paragraph vector text-embedding with multiscale Markov Stability community detection applied to a sparsified similarity graph of document vectors, and showcase the approach on incident reports from Imperial College Healthcare NHS Trust, London. The multiscale community structure reveals different levels of meaning in the topics of the dataset, as shown by descriptive terms extracted from the clusters of records. We also compare a posteriori against hand-coded categories assigned by healthcare personnel, and show that our approach outperforms LDA-based models. Our content clusters exhibit good correspondence with two levels of hand-coded categories, yet they also provide further medical detail in certain areas and reveal complementary descriptors of incidents beyond the external classification taxonomy.        △ Less","6 July, 2018","cs.CL,cs.IR,cs.LG,cs.SI,math.SP",
              Multi-Task Learning with Incomplete Data for Healthcare,1807.02442,https://arxiv.org/abs/1807.02442,https://arxiv.org/pdf/1807.02442,"Authors:XinJ.Hunt,SabaEmrani,IlknurKaynarKabul,JorgeSilva","        Multi-task learning is a type of transfer learning that trains multiple tasks simultaneously and leverages the shared information between related tasks to improve the generalization performance. However, missing features in the input matrix is a much more difficult problem which needs to be carefully addressed. Removing records with missing values can significantly reduce the sample size, which is impractical for datasets with large percentage of missing values. Popular imputation methods often distort the covariance structure of the data, which causes inaccurate inference. In this paper we propose using plug-in covariance matrix estimators to tackle the challenge of missing features. Specifically, we analyze the plug-in estimators under the framework of robust multi-task learning with LASSO and graph regularization, which captures the relatedness between tasks via graph regularization. We use the Alzheimer's disease progression dataset as an example to show how the proposed framework is effective for prediction and model estimation when missing data is present.        △ Less","6 July, 2018","stat.ML,cs.LG",
              The TESTMED Project Experience. Process-aware Enactment of Clinical Guidelines through Multimodal Interfaces          ,1807.02022,https://arxiv.org/abs/1807.02022,https://arxiv.org/pdf/1807.02022,"Authors:AndreaMarrella,MassimoMecella,MahmoudSharf,TizianaCatarci","Healthcare is one of the largest business segments in the world and is a critical area for future growth. In order to ensure efficient access to medical and patient-related information, hospitals have invested heavily in improving clinical mobile technologies and spread their use among doctors. Notwithstanding the benefits of mobile technologies towards a more efficient and personalized delivery of care procedures, there are also indications that their use may have a negative impact on patient-centeredness and often places many cognitive and physical demands on doctors, making them prone to make medical errors. To tackle this issue, in this paper we present the main outcomes of the project TESTMED, which aimed at realizing a clinical system that provides operational support to doctors using mobile technologies for delivering care to patients, in a bid to minimize medical errors. The system exploits concepts from Business Process Management on how to manage a specific class of care procedures, called clinical guidelines, and how to support their execution and mobile orchestration among doctors. As a viable solution for doctors' interaction with the system, we investigated the use of vocal and touch interfaces. User evaluation results indicate a good usability of the system.        △ Less","5 July, 2018",cs.CY,
              Transfer Learning for Clinical Time Series Analysis using Recurrent Neural Networks          ,1807.01705,https://arxiv.org/abs/1807.01705,https://arxiv.org/pdf/1807.01705,"Authors:PriyankaGupta,PankajMalhotra,LovekeshVig,GautamShroff","        Deep neural networks have shown promising results for various clinical prediction tasks such as diagnosis, mortality prediction, predicting duration of stay in hospital, etc. However, training deep networks -- such as those based on Recurrent Neural Networks (RNNs) -- requires large labeled data, high computational resources, and significant hyperparameter tuning effort. In this work, we investigate as to what extent can transfer learning address these issues when using deep RNNs to model multivariate clinical time series. We consider transferring the knowledge captured in an RNN trained on several source tasks simultaneously using a large labeled dataset to build the model for a target task with limited labeled data. An RNN pre-trained on several tasks provides generic features, which are then used to build simpler linear models for new target tasks without training task-specific RNNs. For evaluation, we train a deep RNN to identify several patient phenotypes on time series from MIMIC-III database, and then use the features extracted using that RNN to build classifiers for identifying previously unseen phenotypes, and also for a seemingly unrelated task of in-hospital mortality. We demonstrate that (i) models trained on features extracted using pre-trained RNN outperform or, in the worst case, perform as well as task-specific RNNs; (ii) the models using features from pre-trained models are more robust to the size of labeled data than task-specific RNNs; and (iii) features extracted using pre-trained RNN are generic enough and perform better than typical statistical hand-crafted features.        △ Less","4 July, 2018","cs.LG,stat.ML",
              Ensemble learning with Conformal Predictors: Targeting credible predictions of conversion from Mild Cognitive Impairment to Alzheimer's Disease          ,1807.01619,https://arxiv.org/abs/1807.01619,https://arxiv.org/pdf/1807.01619,"Authors:TelmaPereira,SandraCardoso,DinaSilva,ManuelaGuerreiro,AlexandredeMendonça,SaraC.Madeira","        Most machine learning classifiers give predictions for new examples accurately, yet without indicating how trustworthy predictions are. In the medical domain, this hampers their integration in decision support systems, which could be useful in the clinical practice. We use a supervised learning approach that combines Ensemble learning with Conformal Predictors to predict conversion from Mild Cognitive Impairment to Alzheimer's Disease. Our goal is to enhance the classification performance (Ensemble learning) and complement each prediction with a measure of credibility (Conformal Predictors). Our results showed the superiority of the proposed approach over a similar ensemble framework with standard classifiers.        △ Less","5 July, 2018","cs.LG,stat.ML",
              Generating Synthetic but Plausible Healthcare Record Datasets          ,1807.01514,https://arxiv.org/abs/1807.01514,https://arxiv.org/pdf/1807.01514,"Authors:LauraAviñó,MatteoRuffini,RicardGavaldà","        Generating datasets that ""look like"" given real ones is an interesting tasks for healthcare applications of ML and many other fields of science and engineering. In this paper we propose a new method of general application to binary datasets based on a method for learning the parameters of a latent variable moment that we have previously used for clustering patient datasets. We compare our method with a recent proposal (MedGan) based on generative adversarial methods and find that the synthetic datasets we generate are globally more realistic in at least two senses: real and synthetic instances are harder to tell apart by Random Forests, and the MMD statistic. The most likely explanation is that our method does not suffer from the ""mode collapse"" which is an admitted problem of GANs. Additionally, the generative models we generate are easy to interpret, unlike the rather obscure GANs. Our experiments are performed on two patient datasets containing ICD-9 diagnostic codes: the publicly available MIMIC-III dataset and a dataset containing admissions for congestive heart failure during 7 years at Hospital de Sant Pau in Barcelona.        △ Less","4 July, 2018","stat.ML,cs.LG",
              ssDNA sequencing by rectification          ,1807.01215,https://arxiv.org/abs/1807.01215,https://arxiv.org/pdf/1807.01215,"Authors:IvanaDjurišić,MilošS.Dražić,AleksandarŽ.Tomović,MarkoSpasenović,VladimirP.Jovanović,RadomirZikic","        Fast, reliable and inexpensive DNA sequencing is an important pursuit in healthcare, especially in personalized medicine with possible deep societal impacts. Despite significant progress of various nanopore-based sequencing configurations, challenges remain in resolution (due to thermal fluctuations or to sensitivity to molecular orientation) and speed, which are calling for new approaches. Here we propose a sequencing protocol for DNA translocation through a nanopore with side-embedded N-terminated carbon nanotube electrodes. Employing DFT and Non-Equilibrium Green's Function formalism, we show that the rectification ratio (response to square pulses of alternating bias) bears high nucleobase specificity. The rectification arises due to bias-dependent resistance asymmetry on the deoxyribonucleotide-electrode interfaces. The asymmetry induces molecular charging and HOMO pinning to the electrochemical potential of one of the electrodes, assisted by an in-gap electric field effect caused by dipoles at the terminated electrode ends. This sequencing protocol is sensitive, selective with orders of magnitude, has high resolution, and it is robust to molecular orientation.        △ Less","10 October, 2018",cond-mat.mes-hall,10.1021/acsanm.0c00385 
              Modeling Mistrust in End-of-Life Care          ,1807.00124,https://arxiv.org/abs/1807.00124,https://arxiv.org/pdf/1807.00124,"Authors:WillieBoag,HariniSuresh,LeoAnthonyCeli,PeterSzolovits,MarzyehGhassemi","        In this work, we characterize the doctor-patient relationship using a machine learning-derived trust score. We show that this score has statistically significant racial associations, and that by modeling trust directly we find stronger disparities in care than by stratifying on race. We further demonstrate that mistrust is indicative of worse outcomes, but is only weakly associated with physiologically-created severity scores. Finally, we describe sentiment analysis experiments indicating patients with higher levels of mistrust have worse experiences and interactions with their caregivers. This work is a step towards measuring fairer machine learning in the healthcare domain.        △ Less","2 July, 2019","cs.AI,cs.CY",
              Measuring the quality of Synthetic data for use in competitions          ,1806.11345,https://arxiv.org/abs/1806.11345,https://arxiv.org/pdf/1806.11345,"Authors:JamesJordon,JinsungYoon,MihaelavanderSchaar","        Machine learning has the potential to assist many communities in using the large datasets that are becoming more and more available. Unfortunately, much of that potential is not being realized because it would require sharing data in a way that compromises privacy. In order to overcome this hurdle, several methods have been proposed that generate synthetic data while preserving the privacy of the real data. In this paper we consider a key characteristic that synthetic data should have in order to be useful for machine learning researchers - the relative performance of two algorithms (trained and tested) on the synthetic dataset should be the same as their relative performance (when trained and tested) on the original dataset.        △ Less","29 June, 2018","cs.LG,stat.ML",
              In Vivo WBAN Communication: Design and Implementation          ,1806.11224,https://arxiv.org/abs/1806.11224,https://arxiv.org/pdf/1806.11224,"Authors:HadeelElayan,RaedShubair,NawafAlmoosa","        The emerging in vivo communication and networking system is a prospective component in advancing healthcare delivery and empowering the development of new applications and services. In vivo communications is based on networked cyber-physical systems of embedded devices to allow rapid, correct and cost- effective responses under various conditions. This chapter presents the existing research which investigates the state of art of the in vivo communication. It focuses on characterizing and modeling the in vivo wireless channel and contrasting it with the other familiar channels. MIMO in vivo is also of cencern in this chapter since it significantly enhances the performance gain and data rates. Furthermore, this chapter addresses in vivo nano-communication which is presented for medical applications to provide fast and accurate disease diagnosis and treatment. Such communication paradigm is capable of operating inside the human body in real time and will be of great benefit for medical monitoring and medical implant communications. Consequently, propagation at the Terahertz (THz) frequency must be well understood as it is considered the most promising band for electromagnetic nano-communication models.        △ Less","20 July, 2018",eess.SP,
              A hybrid deep learning approach for medical relation extraction          ,1806.11189,https://arxiv.org/abs/1806.11189,https://arxiv.org/pdf/1806.11189,"Authors:VeeraRaghavendraChikka,KamalakarKarlapalem","        Mining relationships between treatment(s) and medical problem(s) is vital in the biomedical domain. This helps in various applications, such as decision support system, safety surveillance, and new treatment discovery. We propose a deep learning approach that utilizes both word level and sentence-level representations to extract the relationships between treatment and problem. While deep learning techniques demand a large amount of data for training, we make use of a rule-based system particularly for relationship classes with fewer samples. Our final relations are derived by jointly combining the results from deep learning and rule-based models. Our system achieved a promising performance on the relationship classes of I2b2 2010 relation extraction task.        △ Less","26 June, 2018","cs.LG,cs.IR,stat.ML",
              Mapping Unparalleled Clinical Professional and Consumer Languages with Embedding Alignment          ,1806.09542,https://arxiv.org/abs/1806.09542,https://arxiv.org/pdf/1806.09542,"Authors:Wei-HungWeng,PeterSzolovits","        Mapping and translating professional but arcane clinical jargons to consumer language is essential to improve the patient-clinician communication. Researchers have used the existing biomedical ontologies and consumer health vocabulary dictionary to translate between the languages. However, such approaches are limited by expert efforts to manually build the dictionary, which is hard to be generalized and scalable. In this work, we utilized the embeddings alignment method for the word mapping between unparalleled clinical professional and consumer language embeddings. To map semantically similar words in two different word embeddings, we first independently trained word embeddings on both the corpus with abundant clinical professional terms and the other with mainly healthcare consumer terms. Then, we aligned the embeddings by the Procrustes algorithm. We also investigated the approach with the adversarial training with refinement. We evaluated the quality of the alignment through the similar words retrieval both by computing the model precision and as well as judging qualitatively by human. We show that the Procrustes algorithm can be performant for the professional consumer language embeddings alignment, whereas adversarial training with refinement may find some relations between two languages.        △ Less","25 June, 2018","cs.LG,cs.CL,stat.ML",
              Exploring Adversarial Examples: Patterns of One-Pixel Attacks          ,1806.09410,https://arxiv.org/abs/1806.09410,https://arxiv.org/pdf/1806.09410,"Authors:DavidKügler,AlexanderDistergoft,ArjanKuijper,AnirbanMukhopadhyay","        Failure cases of black-box deep learning, e.g. adversarial examples, might have severe consequences in healthcare. Yet such failures are mostly studied in the context of real-world images with calibrated attacks. To demystify the adversarial examples, rigorous studies need to be designed. Unfortunately, complexity of the medical images hinders such study design directly from the medical images. We hypothesize that adversarial examples might result from the incorrect mapping of image space to the low dimensional generation manifold by deep networks. To test the hypothesis, we simplify a complex medical problem namely pose estimation of surgical tools into its barest form. An analytical decision boundary and exhaustive search of the one-pixel attack across multiple image dimensions let us localize the regions of frequent successful one-pixel attacks at the image space.        △ Less","13 November, 2018",cs.CV,10.1007/978-3-030-02628-8_8 
              A Community-Driven Validation Service for Standard Medical Imaging Objects          ,1806.08987,https://arxiv.org/abs/1806.08987,https://arxiv.org/pdf/1806.08987,"Authors:JorgeMiguelSilva,TiagoMarquesGodinho,DavidSilva,CarlosCosta","        Digital medical imaging laboratories contain many distinct types of equipment provided by different manufacturers. Interoperability is a critical issue and the DICOM protocol is a de facto standard in those environments. However, manufacturers' implementation of the standard may have non-conformities at several levels, which will hinder systems' integration. Moreover, medical staff may be responsible for data inconsistencies when entering data. Those situations severely affect the quality of healthcare services since they can disrupt system operations. The existence of software able to confirm data quality and compliance with the DICOM standard is important for programmers, IT staff and healthcare technicians. Although there are a few solutions that try to accomplish this goal, they are unable to deal with certain situations that require user input. Furthermore, these cases usually require the setup of a working environment, which makes the sharing of validation information more difficult. This article proposes and describes the development of a Web DICOM validation service for the community. This solution requires no configuration by the user, promotes validation results share-ability in the community and preserves patient data privacy since files are de-identified on the client side.        △ Less","26 June, 2018",cs.SE,10.1016/j.csi.2018.06.003 
              Forecasting Internally Displaced Population Migration Patterns in Syria and Yemen          ,1806.08819,https://arxiv.org/abs/1806.08819,https://arxiv.org/pdf/1806.08819,"Authors:BenjaminQ.Huynh,SanjayBasu","        Armed conflict has led to an unprecedented number of internally displaced persons (IDPs) - individuals who are forced out of their homes but remain within their country. IDPs often urgently require shelter, food, and healthcare, yet prediction of when large fluxes of IDPs will cross into an area remains a major challenge for aid delivery organizations. Accurate forecasting of IDP migration would empower humanitarian aid groups to more effectively allocate resources during conflicts. We show that monthly flow of IDPs from province to province in both Syria and Yemen can be accurately forecasted one month in advance, using publicly available data. We model monthly IDP flow using data on food price, fuel price, wage, geospatial, and news data. We find that machine learning approaches can more accurately forecast migration trends than baseline persistence models. Our findings thus potentially enable proactive aid allocation for IDPs in anticipation of forecasted arrivals.        △ Less","22 June, 2018","stat.AP,physics.soc-ph,stat.ML",
              A Scalable Machine Learning Approach for Inferring Probabilistic US-LI-RADS Categorization          ,1806.07346,https://arxiv.org/abs/1806.07346,https://arxiv.org/pdf/1806.07346,"Authors:ImonBanerjee,HaileyH.Choi,TerryDesser,DanielL.Rubin","        We propose a scalable computerized approach for large-scale inference of Liver Imaging Reporting and Data System (LI-RADS) final assessment categories in narrative ultrasound (US) reports. Although our model was trained on reports created using a LI-RADS template, it was also able to infer LI-RADS scoring for unstructured reports that were created before the LI-RADS guidelines were established. No human-labelled data was required in any step of this study; for training, LI-RADS scores were automatically extracted from those reports that contained structured LI-RADS scores, and it translated the derived knowledge to reasoning on unstructured radiology reports. By providing automated LI-RADS categorization, our approach may enable standardizing screening recommendations and treatment planning of patients at risk for hepatocellular carcinoma, and it may facilitate AI-based healthcare research with US images by offering large scale text mining and data gathering opportunities from standard hospital clinical data repositories.        △ Less","15 June, 2018","cs.CL,cs.AI",
              Detection of Bio-aerosols and COVID-19 Equivalent Particles Via On-chip Mid Infrared Photonic Spectroscopy          ,1806.06910,https://arxiv.org/abs/1806.06910,https://arxiv.org/pdf/1806.06910,"Authors:RobinSingh,PeterSu,LionelKimerling,AnuAgarwal,BrianWAnthony","        We propose an on-chip mid-infrared (MIR) photonic spectroscopy platform for aerosol characterization to obtain highly discriminatory information on the chemistry of aerosol particles. Sensing of aerosols is crucial for various environmental, climactic, warfare threat detection, and pulmonary healthcare applications. Further, there are a number of unintended situations for potential exposure to bioaerosols such as viruses, bacteria, and fungi. For instance, the current pandemic scenario of COVID-19 occurring across the world. Currently, chemical characterization of aerosols is performed using FTIR spectroscopy yielding chemical fingerprinting because most of the vibrational and rotational transitions of chemical molecules fall in the MIR range; and Raman spectroscopy. Both techniques use free space bench-top geometries. Here, we propose miniaturized on-chip MIR photonics-based aerosol spectroscopy consisting of a broadband spiral-waveguide sensor that significantly enhances particle-light interaction to improve sensitivity. The spiral waveguides are made of a chalcogenide glass material (Ge23Sb7S70) which shows a broad transparency over IR range. We demonstrate the sensing of N-methyl aniline-based aerosol particles with the device. We anticipate that the sensor will readily complement existing photonic resonator-based particle sizing and counting techniques to develop a unified framework for on-chip integrated photonic aerosol spectroscopy.        △ Less","9 May, 2020","physics.app-ph,physics.ins-det,physics.optics",
              A UV-cured nanofibrous membrane of vinylbenzylated gelatin-poly(ε-caprolactone) dimethacrylate co-network by scalable free surface electrospinning          ,1806.06667,https://arxiv.org/abs/1806.06667,https://arxiv.org/pdf/1806.06667,"Authors:MohamedBaselBazbouz,HeLiang,GiuseppeTronci","        Electrospun nanofibrous membranes of natural polymers, such as gelatin, are fundamental in the design of regenerative devices. Crosslinking of electrospun fibres from gelatin is required to prevent dissolution in water, to retain the original nanofibre morphology after immersion in water, and to improve the thermal and mechanical properties, although this is still challenging to accomplish in a controlled fashion. In this study, we have investigated the scalable manufacture and structural stability in aqueous environment of a UV-cured nanofibrous membrane fabricated by free surface electrospinning (FSES) of aqueous solutions containing vinylbenzylated gelatin and poly(epsilon-caprolactone) dimethacrylate (PCL-DMA). Vinylbenzylated gelatin was obtained via chemical functionalisation with photopolymerisable 4-vinylbenzyl chloride (4VBC) groups, so that the gelatin and PCL phase in electrospun fibres were integrated in a covalent UV-cured co-network at the molecular scale, rather than being simply physically mixed. UV-cured nanofibrous membranes did not dissolve in water and showed enhanced thermal and mechanical properties, with respect to as-spun samples, indicating the effectiveness of the photo-crosslinking reaction. In addition, UV-cured gelatin/PCL membranes displayed increased structural stability in water with respect to PCL-free samples and were highly tolerated by G292 osteosarcoma cells. These results therefore support the use of PCL DMA as hydrophobic, biodegradable crosslinker and provide new insight on the scalable design of water insoluble, mechanical competent gelatin membranes for healthcare applications.        △ Less","26 June, 2018","physics.app-ph,physics.chem-ph",10.1016/j.msec.2018.05.076 
              Blockchain Enabled Enhanced IoT Ecosystem Security          ,1806.05242,https://arxiv.org/abs/1806.05242,https://arxiv.org/pdf/1806.05242,"Authors:MahdiH.Miraz,MaarufAli","        Blockchain (BC), the technology behind the Bitcoin cryptocurrency system, is starting to be adopted for ensuring enhanced security and privacy in the Internet of Things (IoT) ecosystem. Fervent research is currently being focused in both academia and industry in this domain. Proof of Work (PoW), a cryptographic puzzle, plays a vital role in ensuring BC security by maintaining a digital ledger of transactions, which are considered to be incorruptible. Furthermore, BC uses a changeable Public Key (PK) to record the identity of users, thus providing an extra layer of privacy. Not only in cryptocurrency has the successful adoption of the BC been implemented, but also in multifaceted non-monetary systems, such as in: distributed storage systems, proof of location and healthcare. Recent research articles and projects or applications were surveyed to assess the implementation of the BC for IoT Security and identify associated challenges and propose solutions for BC enabled enhanced security for the IoT ecosystem.        △ Less","12 June, 2018","cs.CR,cs.NI",
              Human Activity Recognition Based on Wearable Sensor Data: A Standardization of the State-of-the-Art          ,1806.05226,https://arxiv.org/abs/1806.05226,https://arxiv.org/pdf/1806.05226,"Authors:ArturJordao,AntonioC.NazareJr.,JessicaSena,WilliamRobsonSchwartz","        Human activity recognition based on wearable sensor data has been an attractive research topic due to its application in areas such as healthcare and smart environments. In this context, many works have presented remarkable results using accelerometer, gyroscope and magnetometer data to represent the activities categories. However, current studies do not consider important issues that lead to skewed results, making it hard to assess the quality of sensor-based human activity recognition and preventing a direct comparison of previous works. These issues include the samples generation processes and the validation protocols used. We emphasize that in other research areas, such as image classification and object detection, these issues are already well-defined, which brings more efforts towards the application. Inspired by this, we conduct an extensive set of experiments that analyze different sample generation processes and validation protocols to indicate the vulnerable points in human activity recognition based on wearable sensor data. For this purpose, we implement and evaluate several top-performance methods, ranging from handcrafted-based approaches to convolutional neural networks. According to our study, most of the experimental evaluations that are currently employed are not adequate to perform the activity recognition in the context of wearable sensor data, in which the recognition accuracy drops considerably when compared to an appropriate evaluation approach. To the best of our knowledge, this is the first study that tackles essential issues that compromise the understanding of the performance in human activity recognition based on wearable sensor data.        △ Less","1 February, 2019",cs.CV,
              Neonatal EEG Interpretation and Decision Support Framework for Mobile Platforms          ,1806.04037,https://arxiv.org/abs/1806.04037,https://arxiv.org/pdf/1806.04037,"Authors:MarkO'Sullivan,SergiGomez,AlisonO'Shea,EduardSalgado,KevinHuillca,SeanMathieson,GeraldineBoylan,EmanuelPopovici,AndriyTemko","        This paper proposes and implements an intuitive and pervasive solution for neonatal EEG monitoring assisted by sonification and deep learning AI that provides information about neonatal brain health to all neonatal healthcare professionals, particularly those without EEG interpretation expertise. The system aims to increase the demographic of clinicians capable of diagnosing abnormalities in neonatal EEG. The proposed system uses a low-cost and low-power EEG acquisition system. An Android app provides single-channel EEG visualization, traffic-light indication of the presence of neonatal seizures provided by a trained, deep convolutional neural network and an algorithm for EEG sonification, designed to facilitate the perception of changes in EEG morphology specific to neonatal seizures. The multifaceted EEG interpretation framework is presented and the implemented mobile platform architecture is analyzed with respect to its power consumption and accuracy.        △ Less","8 June, 2018","q-bio.NC,cs.SE,stat.ML",
              Regional accuracy of ZTE-based attenuation correction in static and dynamic brain PET/MR          ,1806.03481,https://arxiv.org/abs/1806.03481,https://arxiv.org/pdf/1806.03481,"Authors:GeorgSchramm,MichelKoole,StefanieM.A.Willekens,AhmadrezaRezaei,DonatienneVanWeehaeghe,GasparDelso,RonaldPeeters,NathalieMertens,JohanNuyts,KoenVanLaere","        Accurate MR-based attenuation correction (MRAC) is essential for quantitative PET/MR imaging of the brain. In this study, we analyze the regional bias caused by MRAC based on Zero-Echo-Time MR images (ZTEAC) compared to CT-based AC (CTAC) in static and dynamic PET imaging. In addition the results are compared to the performance of the current default Atlas-based AC (AtlasAC) implemented in the GE SIGNA PET/MR.  Methods: Thirty static [18F]FDG and 11 dynamic [18}F]PE2I acquisitions from a GE SIGNA PET/MR were reconstructed using ZTEAC (using a research tool, GE Healthcare), single-subject AtlasAC (the current default AC in GE's SIGNA PET/MR) and CTAC (from a PET/CT acquisition of the same day). In the 30 static [18F]FDG reconstructions, the bias caused by ZTEAC and AtlasAC in the mean uptake of 85 anatomical volumes of interest (VOIs) of the Hammers' atlas was analyzed in PMOD. For the 11 dynamic [18}F]PE2I reconstructions, the bias caused by ZTEAC and AtlasAC in the non displaceable binding potential BPnd in the striatum was calculated with cerebellum as the reference region and a simplified reference tissue model.  Results: The regional bias caused by ZTEAC in the static [18F]FDG reconstructions ranged from -8.0% to +7.7% (mean 0.1%, SD 2.0%). For AtlasAC this bias ranged from -31.6% to +16.6% (mean -0.4%, SD 4.3%). The bias caused by AtlasAC showed a clear gradient in the cranio-caudal direction (-4.2% in the cerebellum, +6.6% in the left superior frontal gyrus). The bias in the striatal BPnd for the [18F]PE2I reconstructions ranged from -0.8% to +4.8% (mean 1.5%, SD 1.4%) using ZTEAC and from -0.6% to +9.4% using AtlasAC (mean 4.2%, SD 2.6%).  Conclusion: ZTEAC provides excellent quantitative accuracy for static and dynamic brain PET/MR, comparable to CTAC, and is clearly superior to the default AtlasAC currently implemented in the GE SIGNA PET/MR.        △ Less","9 June, 2018",physics.med-ph,
              Surgical Data Science: A Consensus Perspective          ,1806.03184,https://arxiv.org/abs/1806.03184,https://arxiv.org/pdf/1806.03184,"Authors:LenaMaier-Hein,MatthiasEisenmann,CarolinFeldmann,HubertusFeussner,GermainForestier,StamatiaGiannarou,BernardGibaud,GregoryD.Hager,MakotoHashizume,DarkoKatic,HannesKenngott,RonKikinis,MichaelKranzfelder,AnandMalpani,KenoMärz,BeatMüuller-Stich,NassirNavab,ThomasNeumuth,NicolasPadoy,AdrianPark,CarlaPugh,NicolaiSchoch,DanailStoyanov,RussellTaylor,MartinWagner,etal.(3additionalauthorsnotshown)","        Surgical data science is a scientific discipline with the objective of improving the quality of interventional healthcare and its value through capturing, organization, analysis, and modeling of data. The goal of the 1st workshop on Surgical Data Science was to bring together researchers working on diverse topics in surgical data science in order to discuss existing challenges, potential standards and new research directions in the field. Inspired by current open space and think tank formats, it was organized in June 2016 in Heidelberg. While the first day of the workshop, which was dominated by interactive sessions, was open to the public, the second day was reserved for a board meeting on which the information gathered on the public day was processed by (1) discussing remaining open issues, (2) deriving a joint definition for surgical data science and (3) proposing potential strategies for advancing the field. This document summarizes the key findings.        △ Less","8 June, 2018",cs.CY,
              System Level Framework for Assessing the Accuracy of Neonatal EEG Acquisition          ,1806.03045,https://arxiv.org/abs/1806.03045,https://arxiv.org/pdf/1806.03045,"Authors:MarkO'Sullivan,EmanuelPopovici,AndreaBocchino,ConorO'Mahony,GeraldineBoylan,AndriyTemko","        Significant research has been conducted in recent years to design low-cost alternatives to the current EEG monitoring systems used in healthcare facilities. Testing such systems on a vulnerable population such as newborns is complicated due to ethical and regulatory considerations that slow down the technical development. This paper presents and validates a method for quantifying the accuracy of neonatal EEG acquisition systems and electrode technologies via clinical data simulations that do not require neonatal participants. The proposed method uses an extensive neonatal EEG database to simulate analogue signals, which are subsequently passed through electrical models of the skin-electrode interface, which are developed using wet and dry EEG electrode designs. The signal losses in the system are quantified at each stage of the acquisition process for electrode and acquisition board losses. SNR, correlation and noise values were calculated. The results verify that low-cost EEG acquisition systems are capable of obtaining clinical grade EEG. Although dry electrodes result in a significant increase in the skin-electrode impedance, accurate EEG recordings are still achievable.        △ Less","8 June, 2018","physics.med-ph,stat.AP",
              Medical Concept Embedding with Time-Aware Attention          ,1806.02873,https://arxiv.org/abs/1806.02873,https://arxiv.org/pdf/1806.02873,"Authors:XiangruiCai,JinyangGao,KeeYuanNgiam,BengChinOoi,YingZhang,XiaojieYuan","        Embeddings of medical concepts such as medication, procedure and diagnosis codes in Electronic Medical Records (EMRs) are central to healthcare analytics. Previous work on medical concept embedding takes medical concepts and EMRs as words and documents respectively. Nevertheless, such models miss out the temporal nature of EMR data. On the one hand, two consecutive medical concepts do not indicate they are temporally close, but the correlations between them can be revealed by the time gap. On the other hand, the temporal scopes of medical concepts often vary greatly (e.g., \textit{common cold} and \textit{diabetes}). In this paper, we propose to incorporate the temporal information to embed medical codes. Based on the Continuous Bag-of-Words model, we employ the attention mechanism to learn a ""soft"" time-aware context window for each medical concept. Experiments on public and proprietary datasets through clustering and nearest neighbour search tasks demonstrate the effectiveness of our model, showing that it outperforms five state-of-the-art baselines.        △ Less","6 June, 2018","cs.CL,cs.AI",
              Predictive Analysis on Twitter: Techniques and Applications          ,1806.02377,https://arxiv.org/abs/1806.02377,https://arxiv.org/pdf/1806.02377,"Authors:UgurKursuncu,ManasGaur,UshaLokala,KrishnaprasadThirunarayan,AmitSheth,I.BudakArpinar","        Predictive analysis of social media data has attracted considerable attention from the research community as well as the business world because of the essential and actionable information it can provide. Over the years, extensive experimentation and analysis for insights have been carried out using Twitter data in various domains such as healthcare, public health, politics, social sciences, and demographics. In this chapter, we discuss techniques, approaches and state-of-the-art applications of predictive analysis of Twitter data. Specifically, we present fine-grained analysis involving aspects such as sentiment, emotion, and the use of domain knowledge in the coarse-grained analysis of Twitter data for making decisions and taking actions, and relate a few success stories.        △ Less","6 June, 2018",cs.SI,
              Deep Mixed Effect Model using Gaussian Processes: A Personalized and Reliable Prediction for Healthcare,1806.01551,https://arxiv.org/abs/1806.01551,https://arxiv.org/pdf/1806.01551,"Authors:IngyoChung,SaehoonKim,JuhoLee,KwangJoonKim,SungJuHwang,EunhoYang","        We present a personalized and reliable prediction model for healthcare, which can provide individually tailored medical services such as diagnosis, disease treatment, and prevention. Our proposed framework targets at making personalized and reliable predictions from time-series data, such as Electronic Health Records (EHR), by modeling two complementary components: i) a shared component that captures global trend across diverse patients and ii) a patient-specific component that models idiosyncratic variability for each patient. To this end, we propose a composite model of a deep neural network to learn complex global trends from the large number of patients, and Gaussian Processes (GP) to probabilistically model individual time-series given relatively small number of visits per patient. We evaluate our model on diverse and heterogeneous tasks from EHR datasets and show practical advantages over standard time-series deep models such as pure Recurrent Neural Network (RNN).        △ Less","24 November, 2019","stat.ML,cs.LG",
              Natural Language Generation for Electronic Health Records          ,1806.01353,https://arxiv.org/abs/1806.01353,https://arxiv.org/pdf/1806.01353,Authors:ScottLee,"        A variety of methods existing for generating synthetic electronic health records (EHRs), but they are not capable of generating unstructured text, like emergency department (ED) chief complaints, history of present illness or progress notes. Here, we use the encoder-decoder model, a deep learning algorithm that features in many contemporary machine translation systems, to generate synthetic chief complaints from discrete variables in EHRs, like age group, gender, and discharge diagnosis. After being trained end-to-end on authentic records, the model can generate realistic chief complaint text that preserves much of the epidemiological information in the original data. As a side effect of the model's optimization goal, these synthetic chief complaints are also free of relatively uncommon abbreviation and misspellings, and they include none of the personally-identifiable information (PII) that was in the training data, suggesting it may be used to support the de-identification of text in EHRs. When combined with algorithms like generative adversarial networks (GANs), our model could be used to generate fully-synthetic EHRs, facilitating data sharing between healthcare providers and researchers and improving our ability to develop machine learning methods tailored to the information in healthcare data.        △ Less","1 June, 2018","cs.CL,cs.LG,stat.ML",10.1038/s41746-018-0070-0 
              A Review of Challenges and Opportunities in Machine Learning for Health          ,1806.00388,https://arxiv.org/abs/1806.00388,https://arxiv.org/pdf/1806.00388,"Authors:MarzyehGhassemi,TristanNaumann,PeterSchulam,AndrewL.Beam,IreneY.Chen,RajeshRanganath","        Modern electronic health records (EHRs) provide data to answer clinically meaningful questions. The growing data in EHRs makes healthcare ripe for the use of machine learning. However, learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies. For example, diseases in EHRs are poorly labeled, conditions can encompass multiple underlying endotypes, and healthy individuals are underrepresented. This article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare.        △ Less","5 December, 2019","cs.LG,cs.CY,stat.ML",
              Evaluating Reinforcement Learning Algorithms in Observational Health Settings          ,1805.12298,https://arxiv.org/abs/1805.12298,https://arxiv.org/pdf/1805.12298,"Authors:OmerGottesman,FredrikJohansson,JoshuaMeier,JackDent,DonghunLee,SrivatsanSrinivasan,LinyingZhang,YiDing,DavidWihl,XuefengPeng,JiayuYao,IsaacLage,ChristopherMosch,Li-weiH.Lehman,MatthieuKomorowski,MatthieuKomorowski,AldoFaisal,LeoAnthonyCeli,DavidSontag,FinaleDoshi-Velez","        Much attention has been devoted recently to the development of machine learning algorithms with the goal of improving treatment policies in healthcare. Reinforcement learning (RL) is a sub-field within machine learning that is concerned with learning how to make sequences of decisions so as to optimize long-term effects. Already, RL algorithms have been proposed to identify decision-making strategies for mechanical ventilation, sepsis management and treatment of schizophrenia. However, before implementing treatment policies learned by black-box algorithms in high-stakes clinical decision problems, special care must be taken in the evaluation of these policies.  In this document, our goal is to expose some of the subtleties associated with evaluating RL algorithms in healthcare. We aim to provide a conceptual starting point for clinical and computational researchers to ask the right questions when designing and evaluating algorithms for new ways of treating patients. In the following, we describe how choices about how to summarize a history, variance of statistical estimators, and confounders in more ad-hoc measures can result in unreliable, even misleading estimates of the quality of a treatment policy. We also provide suggestions for mitigating these effects---for while there is much promise for mining observational health data to uncover better treatment policies, evaluation must be performed thoughtfully.        △ Less","30 May, 2018","cs.LG,stat.ML",
              Why Is My Classifier Discriminatory?          ,1805.12002,https://arxiv.org/abs/1805.12002,https://arxiv.org/pdf/1805.12002,"Authors:IreneChen,FredrikD.Johansson,DavidSontag","        Recent attempts to achieve fairness in predictive models focus on the balance between fairness and accuracy. In sensitive applications such as healthcare or criminal justice, this trade-off is often undesirable as any increase in prediction error could have devastating consequences. In this work, we argue that the fairness of predictions should be evaluated in context of the data, and that unfairness induced by inadequate samples sizes or unmeasured predictive variables should be addressed through data collection, rather than by constraining the model. We decompose cost-based metrics of discrimination into bias, variance, and noise, and propose actions aimed at estimating and reducing each term. Finally, we perform case-studies on prediction of income, mortality, and review ratings, confirming the value of this analysis. We find that data collection is often a means to reduce discrimination without sacrificing accuracy.        △ Less","10 December, 2018","stat.ML,cs.LG",
              Teaching Meaningful Explanations          ,1805.11648,https://arxiv.org/abs/1805.11648,https://arxiv.org/pdf/1805.11648,"Authors:NoelC.F.Codella,MichaelHind,KarthikeyanNatesanRamamurthy,MurrayCampbell,AmitDhurandhar,KushR.Varshney,DennisWei,AleksandraMojsilovic","        The adoption of machine learning in high-stakes applications such as healthcare and law has lagged in part because predictions are not accompanied by explanations comprehensible to the domain user, who often holds the ultimate responsibility for decisions and outcomes. In this paper, we propose an approach to generate such explanations in which training data is augmented to include, in addition to features and labels, explanations elicited from domain users. A joint model is then learned to produce both labels and explanations from the input features. This simple idea ensures that explanations are tailored to the complexity expectations and domain knowledge of the consumer. Evaluation spans multiple modeling techniques on a game dataset, a (visual) aesthetics dataset, a chemical odor dataset and a Melanoma dataset showing that our approach is generalizable across domains and algorithms. Results demonstrate that meaningful explanations can be reliably taught to machine learning algorithms, and in some cases, also improve modeling accuracy.        △ Less","10 September, 2018",cs.AI,
              The Actor Search Tree Critic (ASTC) for Off-Policy POMDP Learning in Medical Decision Making          ,1805.11548,https://arxiv.org/abs/1805.11548,https://arxiv.org/pdf/1805.11548,"Authors:LuchenLi,MatthieuKomorowski,AldoA.Faisal","        Off-policy reinforcement learning enables near-optimal policy from suboptimal experience, thereby provisions opportunity for artificial intelligence applications in healthcare. Previous works have mainly framed patient-clinician interactions as Markov decision processes, while true physiological states are not necessarily fully observable from clinical data. We capture this situation with partially observable Markov decision process, in which an agent optimises its actions in a belief represented as a distribution of patient states inferred from individual history trajectories. A Gaussian mixture model is fitted for the observed data. Moreover, we take into account the fact that nuance in pharmaceutical dosage could presumably result in significantly different effect by modelling a continuous policy through a Gaussian approximator directly in the policy space, i.e. the actor. To address the challenge of infinite number of possible belief states which renders exact value iteration intractable, we evaluate and plan for only every encountered belief, through heuristic search tree by tightly maintaining lower and upper bounds of the true value of belief. We further resort to function approximations to update value bounds estimation, i.e. the critic, so that the tree search can be improved through more compact bounds at the fringe nodes that will be back-propagated to the root. Both actor and critic parameters are learned via gradient-based approaches. Our proposed policy trained from real intensive care unit data is capable of dictating dosing on vasopressors and intravenous fluids for sepsis patients that lead to the best patient outcomes.        △ Less","3 June, 2018",cs.AI,
              Softwarization of Internet of Things Infrastructure for Secure and Smart Healthcare,1805.11011,https://arxiv.org/abs/1805.11011,https://arxiv.org/pdf/1805.11011,"Authors:MohammadA.Salahuddin,AlaAl-Fuqaha,MohsenGuizani,KhaledShuaib,FaragSallabi","        We propose an agile softwarized infrastructure for flexible, cost effective, secure and privacy preserving deployment of Internet of Things (IoT) for smart healthcare applications and services. It integrates state-of-the-art networking and virtualization techniques across IoT, fog and cloud domains, employing Blockchain, Tor and message brokers to provide security and privacy for patients and healthcare providers. We propose a novel platform using Machine-to-Machine (M2M) messaging and rule-based beacons for seamless data management and discuss the role of data and decision fusion in the cloud and the fog, respectively, for smart healthcare applications and services.        △ Less","28 May, 2018","cs.CY,cs.CR",10.1109/MC.2017.195 
              Scraping SERPs for Archival Seeds: It Matters When You Start          ,1805.10260,https://arxiv.org/abs/1805.10260,https://arxiv.org/pdf/1805.10260,"Authors:AlexanderC.Nwala,MicheleC.Weigle,MichaelL.Nelson","        Event-based collections are often started with a web search, but the search results you find on Day 1 may not be the same as those you find on Day 7. In this paper, we consider collections that originate from extracting URIs (Uniform Resource Identifiers) from Search Engine Result Pages (SERPs). Specifically, we seek to provide insight about the retrievability of URIs of news stories found on Google, and to answer two main questions: first, can one ""refind"" the same URI of a news story (for the same query) from Google after a given time? Second, what is the probability of finding a story on Google over a given period of time? To answer these questions, we issued seven queries to Google every day for over seven months (2017-05-25 to 2018-01-12) and collected links from the first five SERPs to generate seven collections for each query. The queries represent public interest stories: ""healthcare bill,"" ""manchester bombing,"" ""london terrorism,"" ""trump russia,"" ""travel ban,"" ""hurricane harvey,"" and ""hurricane irma."" We tracked each URI in all collections over time to estimate the discoverability of URIs from the first five SERPs. Our results showed that the daily average rate at which stories were replaced on the default Google SERP ranged from 0.21 -0.54, and a weekly rate of 0.39 - 0.79, suggesting the fast replacement of older stories by newer stories. The probability of finding the same URI of a news story after one day from the initial appearance on the SERP ranged from 0.34 - 0.44. After a week, the probability of finding the same news stories diminishes rapidly to 0.01 - 0.11. Our findings suggest that due to the difficulty in retrieving the URIs of news stories from Google, collection building that originates from search engines should begin as soon as possible in order to capture the first stages of events, and should persist in order to capture the evolution of the events...        △ Less","25 May, 2018",cs.DL,10.1145/3197026.3197056 
              Impact of Cooperation in Flow-Induced Diffusive Mobile Molecular Communication          ,1805.09976,https://arxiv.org/abs/1805.09976,https://arxiv.org/pdf/1805.09976,"Authors:NeerajVarshney,AdarshPatel,WernerHaselmayr,AdityaK.Jagannatham,PramodK.Varshney,WeisiGuo","        Motivated by the numerous healthcare applications of molecular communication (MC) inside blood vessels, this work considers relay/cooperative nanomachine (CN)-assisted mobile MC between a source nanomachine (SN) and a destination nanomachine (DN) where each nanomachine is mobile in a flow-induced diffusive channel. Using the first hitting time model, the impact of an intermediate CN on the performance of the CN-assisted diffusive mobile MC system with fully absorbing receivers is analyzed in the presence of inter-symbol interference, multi-source interference, and counting errors. For this purpose, the likelihood ratio test based optimal symbol detection scheme is obtained at the DN considering the non-ideal nature of CN, i.e., CN can be in error with a finite probability. Further, to characterize the system performance, closed-form expressions for the end-to-end probabilities of detection and false alarm at the DN are derived between the SN-DN pair incorporating the detection performance of the intermediate CN. In addition, the channel capacity expression is also derived for the aforementioned scenario. Simulation results are presented to corroborate the theoretical results derived and also, to yield insights into system performance.        △ Less","25 May, 2018",cs.IT,
              A Sentiment Analysis of Breast Cancer Treatment Experiences and Healthcare Perceptions Across Twitter          ,1805.09959,https://arxiv.org/abs/1805.09959,https://arxiv.org/pdf/1805.09959,"Authors:EricM.Clark,TedJames,ChrisA.Jones,AmulyaAlapati,PromiseUkandu,ChristopherM.Danforth,PeterSheridanDodds","        Background: Social media has the capacity to afford the healthcare industry with valuable feedback from patients who reveal and express their medical decision-making process, as well as self-reported quality of life indicators both during and post treatment. In prior work, [Crannell et. al.], we have studied an active cancer patient population on Twitter and compiled a set of tweets describing their experience with this disease. We refer to these online public testimonies as ""Invisible Patient Reported Outcomes"" (iPROs), because they carry relevant indicators, yet are difficult to capture by conventional means of self-report. Methods: Our present study aims to identify tweets related to the patient experience as an additional informative tool for monitoring public health. Using Twitter's public streaming API, we compiled over 5.3 million ""breast cancer"" related tweets spanning September 2016 until mid December 2017. We combined supervised machine learning methods with natural language processing to sift tweets relevant to breast cancer patient experiences. We analyzed a sample of 845 breast cancer patient and survivor accounts, responsible for over 48,000 posts. We investigated tweet content with a hedonometric sentiment analysis to quantitatively extract emotionally charged topics. Results: We found that positive experiences were shared regarding patient treatment, raising support, and spreading awareness. Further discussions related to healthcare were prevalent and largely negative focusing on fear of political legislation that could result in loss of coverage. Conclusions: Social media can provide a positive outlet for patients to discuss their needs and concerns regarding their healthcare coverage and treatment needs. Capturing iPROs from online communication can help inform healthcare professionals and lead to more connected and personalized treatment regimens.        △ Less","12 October, 2018","cs.CL,cs.SI",
              Affective computing using speech and eye gaze: a review and bimodal system proposal for continuous affect prediction          ,1805.06652,https://arxiv.org/abs/1805.06652,https://arxiv.org/pdf/1805.06652,"Authors:JonnyO'Dwyer,NiallMurray,RonanFlynn","        Speech has been a widely used modality in the field of affective computing. Recently however, there has been a growing interest in the use of multi-modal affective computing systems. These multi-modal systems incorporate both verbal and non-verbal features for affective computing tasks. Such multi-modal affective computing systems are advantageous for emotion assessment of individuals in audio-video communication environments such as teleconferencing, healthcare, and education. From a review of the literature, the use of eye gaze features extracted from video is a modality that has remained largely unexploited for continuous affect prediction. This work presents a review of the literature within the emotion classification and continuous affect prediction sub-fields of affective computing for both speech and eye gaze modalities. Additionally, continuous affect prediction experiments using speech and eye gaze modalities are presented. A baseline system is proposed using open source software, the performance of which is assessed on a publicly available audio-visual corpus. Further system performance is assessed in a cross-corpus and cross-lingual experiment. The experimental results suggest that eye gaze is an effective supportive modality for speech when used in a bimodal continuous affect prediction system. The addition of eye gaze to speech in a simple feature fusion framework yields a prediction improvement of 6.13% for valence and 1.62% for arousal.        △ Less","17 May, 2018",cs.HC,
              Electrically controlled water permeation through graphene oxide membranes          ,1805.06390,https://arxiv.org/abs/1805.06390,https://arxiv.org/pdf/1805.06390,"Authors:K.-G.Zhou,K.S.Vasu,C.T.Cherian,M.Neek-Amal,J.C.Zhang,H.Ghorbanfekr-Kalashami,K.Huang,O.P.Marshall,V.G.Kravets,J.Abraham,Y.Su,A.N.Grigorenko,A.Pratt,A.K.Geim,F.M.Peeters,K.S.Novoselov,R.R.Nair","        Developing 'smart' membranes that allow precise and reversible control of molecular permeation using external stimuli would be of intense interest for many areas of science: from physics and chemistry to life-sciences. In particular, electrical control of water permeation through membranes is a long-sought objective and is of crucial importance for healthcare and related areas. Currently, such adjustable membranes are limited to the modulation of wetting of the membranes and controlled ion transport, but not the controlled mass flow of water. Despite intensive theoretical work yielding conflicting results, the experimental realisation of electrically controlled water permeation has not yet been achieved. Here we report electrically controlled water permeation through micrometre-thick graphene oxide (GO) membranes. By controllable electric breakdown, conductive filaments are created in the GO membrane. The electric field concentrated around such current carrying filaments leads to controllable ionisation of water molecules in graphene capillaries, allowing precise control of water permeation: from ultrafast permeation to complete blocking. Our work opens up an avenue for developing smart membrane technologies and can revolutionize the field of artificial biological systems, tissue engineering and filtration.        △ Less","16 May, 2018",cond-mat.mtrl-sci,10.1038/s41586-018-0292-y 
              A Navigational Approach to Health          ,1805.05402,https://arxiv.org/abs/1805.05402,https://arxiv.org/pdf/1805.05402,Authors:RameshJain,"        What if an app could guide you to better health, similar to how GPS navigation directs you to your desired destination? What if the app could use real-time information to redirect you around a disease, just as you're rerouted to avoid traffic? What if the app could provide step-by-step directions to get you to your optimal health state, whether you're a professional athlete or retires school teacher? We discuss how this navigational approach to healthcare could become a reality by combining technology with well-established cybernetics principles.        △ Less","7 May, 2018",cs.CY,
              Automated Diagnosis of Clinic Workflows          ,1805.02264,https://arxiv.org/abs/1805.02264,https://arxiv.org/pdf/1805.02264,"Authors:AlexCheng,JulesWhite","        Outpatient clinics often run behind schedule due to patients who arrive late or appointments that run longer than expected. We sought to develop a generalizable method that would allow healthcare providers to diagnose problems in workflow that disrupt the schedule on any given provider clinic day. We use a constraint optimization problem to identify the least number of appointment modifications that make the rest of the schedule run on-time. We apply this method to an outpatient clinic at Vanderbilt. For patient seen in this clinic between March 27, 2017 and April 21, 2017, long cycle times tended to affect the overall schedule more than late patients. Results from this workflow diagnosis method could be used to inform interventions to help clinics run smoothly, thus decreasing patient wait times and increasing provider utilization.        △ Less","6 May, 2018",cs.AI,
              Automatic Documentation of ICD Codes with Far-Field Speech Recognition          ,1804.11046,https://arxiv.org/abs/1804.11046,https://arxiv.org/pdf/1804.11046,"Authors:AlbertHaque,CorinnaFukushima","        Documentation errors increase healthcare costs and cause unnecessary patient deaths. As the standard language for diagnoses and billing, ICD codes serve as the foundation for medical documentation worldwide. Despite the prevalence of electronic medical records, hospitals still witness high levels of ICD miscoding. In this paper, we propose to automatically document ICD codes with far-field speech recognition. Far-field speech occurs when the microphone is located several meters from the source, as is common with smart homes and security systems. Our method combines acoustic signal processing with recurrent neural networks to recognize and document ICD codes in real time. To evaluate our model, we collected a far-field speech dataset of ICD-10 codes and found our model to achieve 87% accuracy with a BLEU score of 85%. By sampling from an unsupervised medical language model, our method is able to outperform existing methods. Overall, this work shows the potential of automatic speech recognition to provide efficient, accurate, and cost-effective healthcare documentation.        △ Less","26 November, 2018","cs.SD,cs.CL,eess.AS",
              Eletronic Health Records using Blockchain Technology          ,1804.10078,https://arxiv.org/abs/1804.10078,https://arxiv.org/pdf/1804.10078,"Authors:ArlindoFlaviodaConceição,FlavioSoaresCorreadaSilva,VladimirRocha,AngelaLocoro,JoãoMarcosBarguil","        Data privacy refers to ensuring that users keep control over access to information, whereas data accessibility refers to ensuring that information access is unconstrained. Conflicts between privacy and accessibility of data are natural to occur, and healthcare is a domain in which they are particularly relevant. In the present article, we discuss how blockchain technology, and smart contracts, could help in some typical scenarios related to data access, data management and data interoperability for the specific healthcare domain. We then propose the implementation of a large-scale information architecture to access Electronic Health Records (EHRs) based on Smart Contracts as information mediators. Our main contribution is the framing of data privacy and accessibility issues in healthcare and the proposal of an integrated blockchain based architecture.        △ Less","26 April, 2018",cs.CY,
              PANDA: Facilitating Usable AI Development          ,1804.09997,https://arxiv.org/abs/1804.09997,https://arxiv.org/pdf/1804.09997,"Authors:JinyangGao,WeiWang,MeihuiZhang,GangChen,H.V.Jagadish,GuoliangLi,TeckKhimNg,BengChinOoi,ShengWang,JingrenZhou","        Recent advances in artificial intelligence (AI) and machine learning have created a general perception that AI could be used to solve complex problems, and in some situations over-hyped as a tool that can be so easily used. Unfortunately, the barrier to realization of mass adoption of AI on various business domains is too high because most domain experts have no background in AI. Developing AI applications involves multiple phases, namely data preparation, application modeling, and product deployment. The effort of AI research has been spent mostly on new AI models (in the model training stage) to improve the performance of benchmark tasks such as image recognition. Many other factors such as usability, efficiency and security of AI have not been well addressed, and therefore form a barrier to democratizing AI. Further, for many real world applications such as healthcare and autonomous driving, learning via huge amounts of possibility exploration is not feasible since humans are involved. In many complex applications such as healthcare, subject matter experts (e.g. Clinicians) are the ones who appreciate the importance of features that affect health, and their knowledge together with existing knowledge bases are critical to the end results. In this paper, we take a new perspective on developing AI solutions, and present a solution for making AI usable. We hope that this resolution will enable all subject matter experts (eg. Clinicians) to exploit AI like data scientists.        △ Less","26 April, 2018","cs.AI,cs.DB",
              The application of collagen in advanced wound dressings          ,1804.09256,https://arxiv.org/abs/1804.09256,https://arxiv.org/pdf/1804.09256,Authors:GiuseppeTronci,"        Chronic wounds fail to proceed through an orderly and timely self healing process, resulting in cutaneous damage with full thickness in depth and leading to a major healthcare and economic burden worldwide. In the UK alone, 200,000 patients suffer from a chronic wound, whilst the global advanced wound care market is expected to reach nearly $11 million in 2022. Despite extensive research efforts so far, clinically-approved chronic wound therapies are still time-consuming, economically unaffordable and present restricted customisation. In this chapter, the role of collagen in the extracellular matrix of biological tissues and wound healing will be discussed, together with its use as building block for the manufacture of advanced wound dressings. Commercially-available collagen dressings and respective clinical performance will be presented, followed by an overview on the latest research advances in the context of multifunctional collagen systems for advanced wound care.        △ Less","27 April, 2018",q-bio.TO,
              Matching Fingerphotos to Slap Fingerprint Images          ,1804.08122,https://arxiv.org/abs/1804.08122,https://arxiv.org/pdf/1804.08122,"Authors:DebayanDeb,TarangChugh,JoshuaEngelsma,KaiCao,NeetaNain,JakeKendall,AnilK.Jain","        We address the problem of comparing fingerphotos, fingerprint images from a commodity smartphone camera, with the corresponding legacy slap contact-based fingerprint images. Development of robust versions of these technologies would enable the use of the billions of standard Android phones as biometric readers through a simple software download, dramatically lowering the cost and complexity of deployment relative to using a separate fingerprint reader. Two fingerphoto apps running on Android phones and an optical slap reader were utilized for fingerprint collection of 309 subjects who primarily work as construction workers, farmers, and domestic helpers. Experimental results show that a True Accept Rate (TAR) of 95.79 at a False Accept Rate (FAR) of 0.1% can be achieved in matching fingerphotos to slaps (two thumbs and two index fingers) using a COTS fingerprint matcher. By comparison, a baseline TAR of 98.55% at 0.1% FAR is achieved when matching fingerprint images from two different contact-based optical readers. We also report the usability of the two smartphone apps, in terms of failure to acquire rate and fingerprint acquisition time. Our results show that fingerphotos are promising to authenticate individuals (against a national ID database) for banking, welfare distribution, and healthcare applications in developing countries.        △ Less","22 April, 2018",cs.CV,
              3D Human Pose Estimation on a Configurable Bed from a Pressure Image          ,1804.07873,https://arxiv.org/abs/1804.07873,https://arxiv.org/pdf/1804.07873,"Authors:HenryM.Clever,ArielKapusta,DaehyungPark,ZackoryErickson,YashChitalia,CharlesC.Kemp","        Robots have the potential to assist people in bed, such as in healthcare settings, yet bedding materials like sheets and blankets can make observation of the human body difficult for robots. A pressure-sensing mat on a bed can provide pressure images that are relatively insensitive to bedding materials. However, prior work on estimating human pose from pressure images has been restricted to 2D pose estimates and flat beds. In this work, we present two convolutional neural networks to estimate the 3D joint positions of a person in a configurable bed from a single pressure image. The first network directly outputs 3D joint positions, while the second outputs a kinematic model that includes estimated joint angles and limb lengths. We evaluated our networks on data from 17 human participants with two bed configurations: supine and seated. Our networks achieved a mean joint position error of 77 mm when tested with data from people outside the training set, outperforming several baselines. We also present a simple mechanical model that provides insight into ambiguity associated with limbs raised off of the pressure mat, and demonstrate that Monte Carlo dropout can be used to estimate pose confidence in these situations. Finally, we provide a demonstration in which a mobile manipulator uses our network's estimated kinematic model to reach a location on a person's body in spite of the person being seated in a bed and covered by a blanket.        △ Less","29 August, 2018","cs.RO,cs.CV",
              Detecting Regions of Maximal Divergence for Spatio-Temporal Anomaly Detection          ,1804.07091,https://arxiv.org/abs/1804.07091,https://arxiv.org/pdf/1804.07091,"Authors:BjörnBarz,ErikRodner,YaniraGuancheGarcia,JoachimDenzler","        Automatic detection of anomalies in space- and time-varying measurements is an important tool in several fields, e.g., fraud detection, climate analysis, or healthcare monitoring. We present an algorithm for detecting anomalous regions in multivariate spatio-temporal time-series, which allows for spotting the interesting parts in large amounts of data, including video and text data. In opposition to existing techniques for detecting isolated anomalous data points, we propose the ""Maximally Divergent Intervals"" (MDI) framework for unsupervised detection of coherent spatial regions and time intervals characterized by a high Kullback-Leibler divergence compared with all other data given. In this regard, we define an unbiased Kullback-Leibler divergence that allows for ranking regions of different size and show how to enable the algorithm to run on large-scale data sets in reasonable time using an interval proposal technique. Experiments on both synthetic and real data from various domains, such as climate analysis, video surveillance, and text forensics, demonstrate that our method is widely applicable and a valuable tool for finding interesting events in different types of data.        △ Less","23 July, 2019","stat.ML,cs.CV,cs.LG,stat.AP",10.1109/TPAMI.2018.2823766 
              A Study on Overfitting in Deep Reinforcement Learning          ,1804.06893,https://arxiv.org/abs/1804.06893,https://arxiv.org/pdf/1804.06893,"Authors:ChiyuanZhang,OriolVinyals,RemiMunos,SamyBengio","        Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen ""robustly"": commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.        △ Less","20 April, 2018","cs.LG,stat.ML",
              Estimating Treatment Effects in Mover Designs          ,1804.06721,https://arxiv.org/abs/1804.06721,https://arxiv.org/pdf/1804.06721,Authors:PeterHull,"        Researchers increasingly leverage movement across multiple treatments to estimate causal effects. While these ""mover regressions"" are often motivated by a linear constant-effects model, it is not clear what they capture under weaker quasi-experimental assumptions. I show that binary treatment mover regressions recover a convex average of four difference-in-difference comparisons and are thus causally interpretable under a standard parallel trends assumption. Estimates from multiple-treatment models, however, need not be causal without stronger restrictions on the heterogeneity of treatment effects and time-varying shocks. I propose a class of two-step estimators to isolate and combine the large set of difference-in-difference quasi-experiments generated by a mover design, identifying mover average treatment effects under conditional-on-covariate parallel trends and effect homogeneity restrictions. I characterize the efficient estimators in this class and derive specification tests based on the model's overidentifying restrictions. Future drafts will apply the theory to the Finkelstein et al. (2016) movers design, analyzing the causal effects of geography on healthcare utilization.        △ Less","18 April, 2018",econ.EM,
              Multi-modality Sensor Data Classification with Selective Attention          ,1804.05493,https://arxiv.org/abs/1804.05493,https://arxiv.org/pdf/1804.05493,"Authors:XiangZhang,LinaYao,ChaoranHuang,SenWang,MingkuiTan,GuodongLong,CanWang","        Multimodal wearable sensor data classification plays an important role in ubiquitous computing and has a wide range of applications in scenarios from healthcare to entertainment. However, most existing work in this field employs domain-specific approaches and is thus ineffective in complex sit- uations where multi-modality sensor data are col- lected. Moreover, the wearable sensor data are less informative than the conventional data such as texts or images. In this paper, to improve the adapt- ability of such classification methods across differ- ent application domains, we turn this classification task into a game and apply a deep reinforcement learning scheme to deal with complex situations dynamically. Additionally, we introduce a selective attention mechanism into the reinforcement learn- ing scheme to focus on the crucial dimensions of the data. This mechanism helps to capture extra information from the signal and thus it is able to significantly improve the discriminative power of the classifier. We carry out several experiments on three wearable sensor datasets and demonstrate the competitive performance of the proposed approach compared to several state-of-the-art baselines.        △ Less","1 May, 2018",cs.CV,
              Adversarial Attacks Against Medical Deep Learning Systems          ,1804.05296,https://arxiv.org/abs/1804.05296,https://arxiv.org/pdf/1804.05296,"Authors:SamuelG.Finlayson,HyungWonChung,IsaacS.Kohane,AndrewL.Beam","        The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we demonstrate that adversarial examples are capable of manipulating deep learning systems across three clinical domains. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. Our models are representative of the current state of the art in medical computer vision and, in some cases, directly reflect architectures already seeing deployment in real world clinical settings. In addition to the technical contribution of our paper, we synthesize a large body of knowledge about the healthcare system to argue that medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud and provide concrete examples of how and why such attacks could be realistically carried out. We urge practitioners to be aware of current vulnerabilities when deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.        △ Less","4 February, 2019","cs.CR,cs.CY,cs.LG,stat.ML",
              Towards Practical Privacy-Preserving Analytics for IoT and Cloud Based Healthcare Systems          ,1804.04250,https://arxiv.org/abs/1804.04250,https://arxiv.org/pdf/1804.04250,"Authors:SagarSharma,KekeChen,AmitSheth","        Modern healthcare systems now rely on advanced computing methods and technologies, such as Internet of Things (IoT) devices and clouds, to collect and analyze personal health data at an unprecedented scale and depth. Patients, doctors, healthcare providers, and researchers depend on analytical models derived from such data sources to remotely monitor patients, early-diagnose diseases, and find personalized treatments and medications. However, without appropriate privacy protection, conducting data analytics becomes a source of a privacy nightmare. In this article, we present the research challenges in developing practical privacy-preserving analytics in healthcare information systems. The study is based on kHealth - a personalized digital healthcare information system that is being developed and tested for disease monitoring. We analyze the data and analytic requirements for the involved parties, identify the privacy assets, analyze existing privacy substrates, and discuss the potential tradeoff among privacy, efficiency, and model quality.        △ Less","11 April, 2018","cs.CY,cs.CR",10.1109/MIC.2018.112102519 
              A Deep Active Survival Analysis Approach for Precision Treatment Recommendations: Application of Prostate Cancer          ,1804.03280,https://arxiv.org/abs/1804.03280,https://arxiv.org/pdf/1804.03280,"Authors:MiladZafarNezhad,NajibesadatSadati,KaiYang,DongxiaoZhu","        Survival analysis has been developed and applied in the number of areas including manufacturing, finance, economics and healthcare. In healthcare domain, usually clinical data are high-dimensional, sparse and complex and sometimes there exists few amount of time-to-event (labeled) instances. Therefore building an accurate survival model from electronic health records is challenging. With this motivation, we address this issue and provide a new survival analysis framework using deep learning and active learning with a novel sampling strategy. First, our approach provides better representation with lower dimensions from clinical features using labeled (time-to-event) and unlabeled (censored) instances and then actively trains the survival model by labeling the censored data using an oracle. As a clinical assistive tool, we introduce a simple effective treatment recommendation approach based on our survival model. In the experimental study, we apply our approach on SEER-Medicare data related to prostate cancer among African-Americans and white patients. The results indicate that our approach outperforms significantly than baseline models.        △ Less","9 April, 2018","cs.LG,cs.CY,stat.ML",10.1016/j.eswa.2018.07.070 
              Comparing Clinical Judgment with MySurgeryRisk Algorithm for Preoperative Risk Assessment: A Pilot Study          ,1804.03258,https://arxiv.org/abs/1804.03258,https://arxiv.org/pdf/1804.03258,"Authors:MeghanBrennan,SahilPuri,TezcanOzrazgat-Baslanti,RajendraBhat,ZhengFeng,PetarMomcilovic,XiaolinLi,DaisyZheWang,AzraBihorac","        Background: Major postoperative complications are associated with increased short and long-term mortality, increased healthcare cost, and adverse long-term consequences. The large amount of data contained in the electronic health record (EHR) creates barriers for physicians to recognize patients most at risk. We hypothesize, if presented in an optimal format, information from data-driven predictive risk algorithms for postoperative complications can improve physician risk assessment. Methods: Prospective, non-randomized, interventional pilot study of twenty perioperative physicians at a quarterly academic medical center. Using 150 clinical cases we compared physicians' risk assessment before and after interaction with MySurgeryRisk, a validated machine-learning algorithm predicting preoperative risk for six major postoperative complications using EHR data. Results: The area under the curve (AUC) of MySurgeryRisk algorithm ranged between 0.73 and 0.85 and was significantly higher than physicians' risk assessments (AUC between 0.47 and 0.69) for all postoperative complications except cardiovascular complications. The AUC for repeated physician's risk assessment improved by 2% to 5% for all complications with the exception of thirty-day mortality. Physicians' risk assessment for acute kidney injury and intensive care unit admission longer than 48 hours significantly improved after knowledge exchange, resulting in net reclassification improvement of 12.4% and 16%, respectively. Conclusions: The validated MySurgeryRisk algorithm predicted postoperative complications with equal or higher accuracy than pilot cohort of physicians using available clinical preoperative data. The interaction with algorithm significantly improved physicians' risk assessment.        △ Less","9 April, 2018",cs.HC,10.1016/j.surg.2019.01.002 
              Graphene deflectometry for sensing molecular processes at the nanoscale          ,1804.02701,https://arxiv.org/abs/1804.02701,https://arxiv.org/pdf/1804.02701,"Authors:DanielGruss,AlexSmolyanitsky,MichaelZwolak","        Single-molecule sensing is at the core of modern biophysics and nanoscale science, from revolutionizing healthcare through rapid, low-cost sequencing to understanding various physical, chemical, and biological processes at their most basic level. However, important processes at the molecular scale are often too fast for the detection bandwidth or otherwise outside the detection sensitivity. Moreover, most envisioned biophysical applications are at room temperature, which further limits detection due to significant thermal noise. Here, we theoretically demonstrate reliable transduction of forces into electronic currents via locally suspended graphene nanoribbons subject to ultra-low flexural deflection. The decay of electronic couplings with distance magnifies the effect of the deflection, giving rise to measurable electronic current changes even in aqueous solution. Due to thermal fluctuations, the characteristic charge carrier transmission peak follows a generalized Voigt profile, behavior which is reflected in the optimized sensor. The intrinsic sensitivity is less than 7 fN/Hz−−−√\sqrt{\mathbf{Hz}}, allowing for the detection of ultra-weak and fast processes at room temperature. Graphene deflectometry thus presents new opportunities in the sensing and detection of molecular-scale processes, from ion dynamics to DNA sequencing and protein folding, in their native environment.        △ Less","8 April, 2018","cond-mat.mes-hall,cond-mat.mtrl-sci,cond-mat.stat-mech",
              Module-less Synthesis on Cyberphysical Digital Microfluidic Biochip Ensuring Error Detection and Routing Performance Optimization          ,1804.02631,https://arxiv.org/abs/1804.02631,https://arxiv.org/pdf/1804.02631,"Authors:SaritChakraborty,SusantaChakraborty","        Digital Microfluidic Biochips consist of Two Dimensional microarrays that are integrated with different healthcare related cyberphysical systems and expected to be used extensively in near future. Thus, faster and error-free synthesis techniques need to be implemented on such chips. Various Bio protocols are performed based on different mixing modules present on the chip until now. In this work, the concept of such dedicated virtual modules has been eliminated and a novel Module Less Synthesis method is proposed for accomplishing bioassays for cyberphysical DMFBs. However, path congestion problem and operational errors are inevitable in MLS approach.        △ Less","8 April, 2018",cs.ET,
              Multi-view Banded Spectral Clustering with Application to ICD9 Clustering          ,1804.02097,https://arxiv.org/abs/1804.02097,https://arxiv.org/pdf/1804.02097,"Authors:LuwanZhang,KatherineLiao,IssacKohane,TianxiCai","        Despite recent development in methodology, community detection remains a challenging problem. Existing literature largely focuses on the standard setting where a network is learned using an observed adjacency matrix from a single data source. Constructing a shared network from multiple data sources is more challenging due to the heterogeneity across populations. Additionally, no existing method leverages the prior distance knowledge available in many domains to help the discovery of the network structure. To bridge this gap, in this paper we propose a novel spectral clustering method that optimally combines multiple data sources while leveraging the prior distance knowledge. The proposed method combines a banding step guided by the distance knowledge with a subsequent weighting step to maximize consensus across multiple sources. Its statistical performance is thoroughly studied under a multi-view stochastic block model. We also provide a simple yet optimal rule of choosing weights in practice. The efficacy and robustness of the method is fully demonstrated through extensive simulations. Finally, we apply the method to cluster the International classification of diseases, ninth revision (ICD9), codes and yield a very insightful clustering structure by integrating information from a large claim database and two healthcare systems.        △ Less","20 June, 2018","stat.ME,stat.AP,stat.ML",
              A Large-scale Concurrent Data Anonymous Batch Verification Scheme for Mobile Healthcare Crowd Sensing          ,1804.01822,https://arxiv.org/abs/1804.01822,https://arxiv.org/pdf/1804.01822,"Authors:JingweiLiu,HuijuanCao,QingqingLi,FanghuiCai,XiaojiangDu,MohsenGuizani","        Recently, with the rapid development of big data, Internet of Things (IoT) brings more and more intelligent and convenient services to people's daily lives. Mobile healthcare crowd sensing (MHCS), as a typical application of IoT, is becoming an effective approach to provide various medical and healthcare services to individual or organizations. However, MHCS still have to face to different security challenges in practice. For example, how to quickly and effectively authenticate masses of bio-information uploaded by IoT terminals without revealing the owners' sensitive information. Therefore, we propose a large-scale concurrent data anonymous batch verification scheme for MHCS based on an improved certificateless aggregate signature. The proposed scheme can authenticate all sensing bio-information at once in a privacy preserving way. The individual data generated by different users can be verified in batch, while the actual identity of participants is hidden. Moreover, assuming the intractability of CDHP, our scheme is proved to be secure. Finally, the performance evaluation shows that the proposed scheme is suitable for MHCS, due to its high efficiency.        △ Less","5 April, 2018",cs.CR,
              Processing of Electronic Health Records using Deep Learning: A review          ,1804.01758,https://arxiv.org/abs/1804.01758,https://arxiv.org/pdf/1804.01758,"Authors:VenetOsmani,LiLi,MatteoDanieletto,BenjaminGlicksberg,JoelDudley,OscarMayora","        Availability of large amount of clinical data is opening up new research avenues in a number of fields. An exciting field in this respect is healthcare, where secondary use of healthcare data is beginning to revolutionize healthcare. Except for availability of Big Data, both medical data from healthcare institutions (such as EMR data) and data generated from health and wellbeing devices (such as personal trackers), a significant contribution to this trend is also being made by recent advances on machine learning, specifically deep learning algorithms.        △ Less","5 April, 2018",cs.CY,
              Hospital Readmission Prediction - Applying Hierarchical Sparsity Norms for Interpretable Models          ,1804.01188,https://arxiv.org/abs/1804.01188,https://arxiv.org/pdf/1804.01188,"Authors:JialiangJiang,SharonHewner,VarunChandola","        Hospital readmissions have become one of the key measures of healthcare quality. Preventable readmissions have been identified as one of the primary targets for reducing costs and improving healthcare delivery. However, most data driven studies for understanding readmissions have produced black box classification and predictive models with moderate performance, which precludes them from being used effectively within the decision support systems in the hospitals. In this paper we present an application of structured sparsity-inducing norms for predicting readmission risk for patients based on their disease history and demographics. Most existing studies have focused on hospital utilization, test results, etc., to assign a readmission label to each episode of hospitalization. However, we focus on assigning a readmission risk label to a patient based on their disease history. Our emphasis is on interpreting the models to improve the understanding of the readmission problem. To achieve this, we exploit the domain induced hierarchical structure available for the disease codes which are the features for the classification algorithm. We use a tree based sparsity-inducing regularization strategy that explicitly uses the domain hierarchy. The resulting model not only outperforms standard regularization procedures but is also highly sparse and interpretable. We analyze the model and identify several significant factors that have an effect on readmission risk. Some of these factors conform to existing beliefs, e.g., impact of surgical complications and infections during hospital stay. Other factors, such as the impact of mental disorder and substance abuse on readmission, provide empirical evidence for several pre-existing but unverified hypotheses. The analysis also reveals previously undiscovered connections such as the influence of socioeconomic factors like lack of housing and malnutrition.        △ Less","3 April, 2018","cs.LG,stat.ML",
              Non-minimum phase viscoelastic properties of soft biological tissues          ,1804.00979,https://arxiv.org/abs/1804.00979,https://arxiv.org/pdf/1804.00979,"Authors:YoKobayashi,NaomiOkamura,MarikoTsukune,MasakatsuG.Fujie,MasaoTanaka","        Understanding the visocoelastic properties of soft biological tissues is important for progress in the field of human healthcare. This study analyzes the viscoelastic properties of soft biological tissues using a fractional dynamics model. We conducted a dynamic viscoelastic test on several porcine samples, namely liver, breast, and skeletal muscle tissues, using a plate--plate rheometer. We found that some soft biological tissues have non-minimum phase properties; that is, the relationship between compliance and phase delay is not uniquely related to the non-integer derivative order in the fractional dynamics model. The experimental results show that the actual phase delay is larger than that estimated from compliance. We propose a fractional dynamics model with the fractional Hilbert transform to represent these non-minimum phase properties. The model and experimental results were highly correlated in terms of compliance and phase diagrams and complex mechanical impedance. We also show that the amount of additional phase delay, defined as the increase in actual phase delay compared to that estimated from compliance, differs with tissue type.        △ Less","26 March, 2019",physics.bio-ph,
              DeepSigns: A Generic Watermarking Framework for IP Protection of Deep Learning Models          ,1804.00750,https://arxiv.org/abs/1804.00750,https://arxiv.org/pdf/1804.00750,"Authors:BitaDarvishRouhani,HuiliChen,FarinazKoushanfar","        Deep Learning (DL) models have caused a paradigm shift in our ability to comprehend raw data in various important fields, ranging from intelligence warfare and healthcare to autonomous transportation and automated manufacturing. A practical concern, in the rush to adopt DL models as a service, is protecting the models against Intellectual Property (IP) infringement. The DL models are commonly built by allocating significant computational resources that process vast amounts of proprietary training data. The resulting models are therefore considered to be the IP of the model builder and need to be protected to preserve the owner's competitive advantage.  This paper proposes DeepSigns, a novel end-to-end IP protection framework that enables insertion of coherent digital watermarks in contemporary DL models. DeepSigns, for the first time, introduces a generic watermarking methodology that can be used for protecting DL owner's IP rights in both white-box and black-box settings, where the adversary may or may not have the knowledge of the model internals. The suggested methodology is based on embedding the owner's signature (watermark) in the probability density function (pdf) of the data abstraction obtained in different layers of a DL model. DeepSigns can demonstrably withstand various removal and transformation attacks, including model compression, model fine-tuning, and watermark overwriting. Proof-of-concept evaluations on MNIST, and CIFAR10 datasets, as well as a wide variety of neural network architectures including Wide Residual Networks, Convolution Neural Networks, and Multi-Layer Perceptrons corroborate DeepSigns' effectiveness and applicability.        △ Less","31 May, 2018",cs.CR,
              Socioeconomic bias in influenza surveillance          ,1804.00327,https://arxiv.org/abs/1804.00327,https://arxiv.org/pdf/1804.00327,"Authors:SamuelV.Scarpino,JamesG.Scott,RosalindM.Eggo,BruceClements,NedialkoB.Dimitrov,LaurenAncelMeyers","        Individuals in low socioeconomic brackets are considered at-risk for developing influenza-related complications and often exhibit higher than average influenza-related hospitalization rates. This disparity has been attributed to various factors, including restricted access to preventative and therapeutic health care, limited sick leave, and household structure. Adequate influenza surveillance in these at-risk populations is a critical precursor to accurate risk assessments and effective intervention. However, the United States of America's primary national influenza surveillance system (ILINet) monitors outpatient healthcare providers, which may be largely inaccessible to lower socioeconomic populations. Recent initiatives to incorporate internet-source and hospital electronic medical records data into surveillance systems seek to improve the timeliness, coverage, and accuracy of outbreak detection and situational awareness. Here, we use a flexible statistical framework for integrating multiple surveillance data sources to evaluate the adequacy of traditional (ILINet) and next generation (BioSense 2.0 and Google Flu Trends) data for situational awareness of influenza across poverty levels. We find that zip codes in the highest poverty quartile are a critical blind-spot for ILINet that the integration of next generation data fails to ameliorate.        △ Less","1 April, 2018","stat.AP,q-bio.PE",
              Efficient and Robust Semi-Supervised Estimation of Average Treatment Effects in Electronic Medical Records Data          ,1804.00195,https://arxiv.org/abs/1804.00195,https://arxiv.org/pdf/1804.00195,"Authors:DavidCheng,AshwinAnanthakrishnan,TianxiCai","        There is strong interest in conducting comparative effectiveness research (CER) in electronic medical records (EMR) to evaluate treatment strategies among real-world patients. Inferring causal effects in EMR data, however, is challenging due to the lack of direct observation on pre-specified gold-standard outcomes, in addition to the observational nature of the data. Extracting gold-standard outcomes often requires labor-intensive medical chart review, which is unfeasible for large studies. While one may impute outcomes and estimate average treatment effects (ATE) based on imputed data, naive imputations may lead to biased ATE estimators. In this paper, we frame the problem of estimating the ATE in a semi-supervised learning setting, where a small set of observations is labeled with the true outcome via manual chart review and a large set of unlabeled observations with features predictive of the outcome are available. We develop an imputation-based approach for estimating the ATE that is robust to misspecification of the imputation model. This allows information from the predictive features to be safely leveraged to improve the efficiency in estimating the ATE. The estimator is additionally doubly-robust in that it is consistent under correct specification of either an initial propensity score model or a baseline outcome model. We show that it is locally semiparametric efficient under an ideal semi-supervised model where the distribution of unlabeled data is known. Simulations exhibit the efficiency and robustness of the proposed method compared to existing approaches in finite samples. We illustrate the method to compare rates of treatment response to two biologic agents for treating inflammatory bowel disease using EMR data from Partner's Healthcare.        △ Less","31 March, 2018",stat.ME,
              Improving confidence while predicting trends in temporal disease networks          ,1803.11462,https://arxiv.org/abs/1803.11462,https://arxiv.org/pdf/1803.11462,"Authors:DjordjeGligorijevic,JelenaStojanovic,ZoranObradovic","        For highly sensitive real-world predictive analytic applications such as healthcare and medicine, having good prediction accuracy alone is often not enough. These kinds of applications require a decision making process which uses uncertainty estimation as input whenever possible. Quality of uncertainty estimation is a subject of over or under confident prediction, which is often not addressed in many models. In this paper we show several extensions to the Gaussian Conditional Random Fields model, which aim to provide higher quality uncertainty estimation. These extensions are applied to the temporal disease graph built from the State Inpatient Database (SID) of California, acquired from the HCUP. Our experiments demonstrate benefits of using graph information in modeling temporal disease properties as well as improvements in uncertainty estimation provided by given extensions of the Gaussian Conditional Random Fields method.        △ Less","28 March, 2018","cs.LG,stat.ML",
"              Accelerating Materials Development via Automation, Machine Learning, and High-Performance Computing          ",1803.11246,https://arxiv.org/abs/1803.11246,https://arxiv.org/pdf/1803.11246,"Authors:JuanPabloCorrea-Baena,KedarHippalgaonkar,JeroenvanDuren,ShaffiqJaffer,VijayR.Chandrasekhar,VladanStevanovic,CyrusWadia,SupratikGuha,TonioBuonassisi","        Successful materials innovations can transform society. However, materials research often involves long timelines and low success probabilities, dissuading investors who have expectations of shorter times from bench to business. A combination of emergent technologies could accelerate the pace of novel materials development by 10x or more, aligning the timelines of stakeholders (investors and researchers), markets, and the environment, while increasing return-on-investment. First, tool automation enables rapid experimental testing of candidate materials. Second, high-throughput computing (HPC) concentrates experimental bandwidth on promising compounds by predicting and inferring bulk, interface, and defect-related properties. Third, machine learning connects the former two, where experimental outputs automatically refine theory and help define next experiments. We describe state-of-the-art attempts to realize this vision and identify resource gaps. We posit that over the coming decade, this combination of tools will transform the way we perform materials research. There are considerable first-mover advantages at stake, especially for grand challenges in energy and related fields, including computing, healthcare, urbanization, water, food, and the environment.        △ Less","20 March, 2018","cs.CY,cond-mat.mtrl-sci",10.1016/j.joule.2018.05.009 
              Artificial Intelligence and Robotics          ,1803.10813,https://arxiv.org/abs/1803.10813,https://arxiv.org/pdf/1803.10813,"Authors:JavierAndreu-Perez,FaniDeligianni,DanieleRavi,Guang-ZhongYang","        The recent successes of AI have captured the wildest imagination of both the scientific communities and the general public. Robotics and AI amplify human potentials, increase productivity and are moving from simple reasoning towards human-like cognitive abilities. Current AI technologies are used in a set area of applications, ranging from healthcare, manufacturing, transport, energy, to financial services, banking, advertising, management consulting and government agencies. The global AI market is around 260 billion USD in 2016 and it is estimated to exceed 3 trillion by 2024. To understand the impact of AI, it is important to draw lessons from it's past successes and failures and this white paper provides a comprehensive explanation of the evolution of AI, its current status and future directions.        △ Less","28 March, 2018",cs.AI,
              A systematic approach to improving the reliability and scale of evidence from health care data          ,1803.10791,https://arxiv.org/abs/1803.10791,https://arxiv.org/pdf/1803.10791,"Authors:MartijnJ.Schuemie,PatrickB.Ryan,GeorgeHripcsak,DavidMadigan,MarcA.Suchard","        Concerns over reproducibility in science extend to research using existing healthcare data; many observational studies investigating the same topic produce conflicting results, even when using the same data. To address this problem, we propose a paradigm shift. The current paradigm centers on generating one estimate at a time using a unique study design with unknown reliability and publishing (or not) one estimate at a time. The new paradigm advocates for high-throughput observational studies using consistent and standardized methods, allowing evaluation, calibration, and unbiased dissemination to generate a more reliable and complete evidence base. We demonstrate this new paradigm by comparing all depression treatments for a set of outcomes, producing 17,718 hazard ratios, each using methodology on par with state-of-the-art studies. We furthermore include control hypotheses to evaluate and calibrate our evidence generation process. Results show good transitivity and consistency between databases, and agree with four out of the five findings from clinical trials. The distribution of effect size estimates reported in literature reveals an absence of small or null effects, with a sharp cutoff at p = 0.05. No such phenomena were observed in our results, suggesting more complete and more reliable evidence.        △ Less","28 March, 2018",stat.AP,
              Disease-Atlas: Navigating Disease Trajectories with Deep Learning          ,1803.10254,https://arxiv.org/abs/1803.10254,https://arxiv.org/pdf/1803.10254,"Authors:BryanLim,MihaelavanderSchaar","        Joint models for longitudinal and time-to-event data are commonly used in longitudinal studies to forecast disease trajectories over time. While there are many advantages to joint modeling, the standard forms suffer from limitations that arise from a fixed model specification, and computational difficulties when applied to high-dimensional datasets. In this paper, we propose a deep learning approach to address these limitations, enhancing existing methods with the inherent flexibility and scalability of deep neural networks, while retaining the benefits of joint modeling. Using longitudinal data from a real-world medical dataset, we demonstrate improvements in performance and scalability, as well as robustness in the presence of irregularly sampled data.        △ Less","6 July, 2018","stat.ML,cs.LG",
              A Conversational Interface to Improve Medication Adherence: Towards AI Support in Patient's Treatment          ,1803.09844,https://arxiv.org/abs/1803.09844,https://arxiv.org/pdf/1803.09844,Authors:AhmedFadhil,"        Medication adherence is of utmost importance for many chronic conditions, regardless of the disease type. Engaging patients in self-tracking their medication is a big challenge. One way to potentially reduce this burden is to use reminders to promote wellness throughout all stages of life and improve medication adherence. Chatbots have proven effectiveness in triggering users to engage in certain activity, such as medication adherence. In this paper, we discuss ""Roborto"", a chatbot to create an engaging interactive and intelligent environment for patients and assist in positive lifestyle modification. We introduce a way for healthcare providers to track patients adherence and intervene whenever necessary. We describe the health, technical and behavioural approaches to the problem of medication non-adherence and propose a diagnostic and decision support tool. The proposed study will be implemented and validated through a pilot experiment with users to measure the efficacy of the proposed approach.        △ Less","3 March, 2018","cs.CY,cs.AI",
              Deep Representation for Patient Visits from Electronic Health Records          ,1803.09533,https://arxiv.org/abs/1803.09533,https://arxiv.org/pdf/1803.09533,"Authors:Jean-BaptisteEscudié,AlaaSaade,AliceCoucke,MarcLelarge",        We show how to learn low-dimensional representations (embeddings) of patient visits from the corresponding electronic health record (EHR) where International Classification of Diseases (ICD) diagnosis codes are removed. We expect that these embeddings will be useful for the construction of predictive statistical models anticipated to drive personalized medicine and improve healthcare quality. These embeddings are learned using a deep neural network trained to predict ICD diagnosis categories. We show that our embeddings capture relevant clinical informations and can be used directly as input to standard machine learning algorithms like multi-output classifiers for ICD code prediction. We also show that important medical informations correspond to particular directions in our embedding space.        △ Less,"26 March, 2018","cs.CY,cs.LG,stat.ML",
              Broad Learning for Healthcare,1803.08978,https://arxiv.org/abs/1803.08978,https://arxiv.org/pdf/1803.08978,Authors:BokaiCao,"        A broad spectrum of data from different modalities are generated in the healthcare domain every day, including scalar data (e.g., clinical measures collected at hospitals), tensor data (e.g., neuroimages analyzed by research institutes), graph data (e.g., brain connectivity networks), and sequence data (e.g., digital footprints recorded on smart sensors). Capability for modeling information from these heterogeneous data sources is potentially transformative for investigating disease mechanisms and for informing therapeutic interventions.  Our works in this thesis attempt to facilitate healthcare applications in the setting of broad learning which focuses on fusing heterogeneous data sources for a variety of synergistic knowledge discovery and machine learning tasks. We are generally interested in computer-aided diagnosis, precision medicine, and mobile health by creating accurate user profiles which include important biomarkers, brain connectivity patterns, and latent representations. In particular, our works involve four different data mining problems with application to the healthcare domain: multi-view feature selection, subgraph pattern mining, brain network embedding, and multi-view sequence prediction.        △ Less","23 March, 2018","cs.LG,stat.ML",
              Detection of Surgical Site Infection Utilizing Automated Feature Generation in Clinical Notes          ,1803.08850,https://arxiv.org/abs/1803.08850,https://arxiv.org/pdf/1803.08850,"Authors:FeichenShen,DavidWLarson,JamesM.Naessens,ElizabethB.Habermann,HongfangLiu,SunghwanSohn","        Postsurgical complications (PSCs) are known as a deviation from the normal postsurgical course and categorized by severity and treatment requirements. Surgical site infection (SSI) is one of major PSCs and the most common healthcare-associated infection, resulting in increased length of hospital stay and cost. In this work, we assessed an automated way to generate lexicon (i.e., keyword features) from clinical narratives using sublanguage analysis with heuristics to detect SSI and evaluated these keywords with medical experts. To further validate our approach, we also conducted decision tree algorithm on cohort using automatically generated keywords. The results show that our framework was able to identify SSI keywords from clinical narratives and to support search-based natural language processing (NLP) approaches by augmenting search queries.        △ Less","26 March, 2018",cs.IR,10.1007/s41666-018-0042-9 
              Paving the Way for Culturally Competent Robots: a Position Paper          ,1803.08812,https://arxiv.org/abs/1803.08812,https://arxiv.org/pdf/1803.08812,"Authors:BarbaraBruno,NakYoungChong,HirokoKamide,SanjeevKanoria,JaeryoungLee,YutoLim,AmitKumarPandey,ChrisPapadopoulos,IrenaPapadopoulos,FedericoPecora,AlessandroSaffiotti,AntonioSgorbissa","        Cultural competence is a well known requirement for an effective healthcare, widely investigated in the nursing literature. We claim that personal assistive robots should likewise be culturally competent, aware of general cultural characteristics and of the different forms they take in different individuals, and sensitive to cultural differences while perceiving, reasoning, and acting. Drawing inspiration from existing guidelines for culturally competent healthcare and the state-of-the-art in culturally competent robotics, we identify the key robot capabilities which enable culturally competent behaviours and discuss methodologies for their development and evaluation.        △ Less","22 March, 2018","cs.CY,cs.RO",10.1109/ROMAN.2017.8172357 
              Causal Inference for Survival Analysis          ,1803.08218,https://arxiv.org/abs/1803.08218,https://arxiv.org/pdf/1803.08218,Authors:VikasRamachandra,"        In this paper, we propose the use of causal inference techniques for survival function estimation and prediction for subgroups of the data, upto individual units. Tree ensemble methods, specifically random forests were modified for this purpose. A real world healthcare dataset was used with about 1800 patients with breast cancer, which has multiple patient covariates as well as disease free survival days (DFS) and a death event binary indicator (y). We use the type of cancer curative intervention as the treatment variable (T=0 or 1, binary treatment case in our example). The algorithm is a 2 step approach. In step 1, we estimate heterogeneous treatment effects using a causalTree with the DFS as the dependent variable. Next, in step 2, for each selected leaf of the causalTree with distinctly different average treatment effect (with respect to survival), we fit a survival forest to all the patients in that leaf, one forest each for treatment T=0 as well as T=1 to get estimated patient level survival curves for each treatment (more generally, any model can be used at this step). Then, we subtract the patient level survival curves to get the differential survival curve for a given patient, to compare the survival function as a result of the 2 treatments. The path to a selected leaf also gives us the combination of patient features and their values which are causally important for the treatment effect difference at the leaf.        △ Less","21 March, 2018",econ.EM,
              Contribution of Data Categories to Readmission Prediction Accuracy          ,1803.07850,https://arxiv.org/abs/1803.07850,https://arxiv.org/pdf/1803.07850,"Authors:WendongGe,HeeYeunKim,SonaliDesai,LeonidPerlovsky,AlexanderTurchin","        Identification of patients at high risk for readmission could help reduce morbidity and mortality as well as healthcare costs. Most of the existing studies on readmission prediction did not compare the contribution of data categories. In this study we analyzed relative contribution of 90,101 variables across 398,884 admission records corresponding to 163,468 patients, including patient demographics, historical hospitalization information, discharge disposition, diagnoses, procedures, medications and laboratory test results. We established an interpretable readmission prediction model based on Logistic Regression in scikit-learn, and added the available variables to the model one by one in order to analyze the influences of individual data categories on readmission prediction accuracy. Diagnosis related groups (c-statistic increment of 0.0933) and discharge disposition (c-statistic increment of 0.0269) were the strongest contributors to model accuracy. Additionally, we also identified the top ten contributing variables in every data category.        △ Less","22 March, 2018",q-bio.QM,
              Removing Confounding Factors Associated Weights in Deep Neural Networks Improves the Prediction Accuracy for Healthcare Applications          ,1803.07276,https://arxiv.org/abs/1803.07276,https://arxiv.org/pdf/1803.07276,"Authors:HaohanWang,ZhenglinWu,EricP.Xing","        The proliferation of healthcare data has brought the opportunities of applying data-driven approaches, such as machine learning methods, to assist diagnosis. Recently, many deep learning methods have been shown with impressive successes in predicting disease status with raw input data. However, the ""black-box"" nature of deep learning and the high-reliability requirement of biomedical applications have created new challenges regarding the existence of confounding factors. In this paper, with a brief argument that inappropriate handling of confounding factors will lead to models' sub-optimal performance in real-world applications, we present an efficient method that can remove the influences of confounding factors such as age or gender to improve the across-cohort prediction accuracy of neural networks. One distinct advantage of our method is that it only requires minimal changes of the baseline model's architecture so that it can be plugged into most of the existing neural networks. We conduct experiments across CT-scan, MRA, and EEG brain wave with convolutional neural networks and LSTM to verify the efficiency of our method.        △ Less","31 August, 2018","stat.ML,cs.LG",
              Quantifying the Contributions of Training Data and Algorithm Logic to the Performance of Automated Cause-assignment Algorithms for Verbal Autopsy          ,1803.07141,https://arxiv.org/abs/1803.07141,https://arxiv.org/pdf/1803.07141,"Authors:SamuelJ.Clark,ZehangLi,TylerH.McCormick","        A verbal autopsy (VA) consists of a survey with a relative or close contact of a person who has recently died. VA surveys are commonly used to infer likely causes of death for individuals when deaths happen outside of hospitals or healthcare facilities. Several statistical and algorithmic methods are available to assign cause of death using VA surveys. Each of these methods require as inputs some information about the joint distribution of symptoms and causes. In this note, we examine the generalizability of this symptom-cause information by comparing different automated coding methods using various combinations of inputs and evaluation data. VA algorithm performance is affected by both the specific SCI themselves and the logic of a given algorithm. Using a variety of performance metrics for all existing VA algorithms, we demonstrate that in general the adequacy of the information about the joint distribution between symptoms and cause affects performance at least as much or more than algorithm logic.        △ Less","15 November, 2018","stat.AP,stat.OT",
              A Robust AUC Maximization Framework with Simultaneous Outlier Detection and Feature Selection for Positive-Unlabeled Classification          ,1803.06604,https://arxiv.org/abs/1803.06604,https://arxiv.org/pdf/1803.06604,"Authors:KeRen,HaichuanYang,YuZhao,MingshanXue,HongyuMiao,ShuaiHuang,JiLiu","        The positive-unlabeled (PU) classification is a common scenario in real-world applications such as healthcare, text classification, and bioinformatics, in which we only observe a few samples labeled as ""positive"" together with a large volume of ""unlabeled"" samples that may contain both positive and negative samples. Building robust classifier for the PU problem is very challenging, especially for complex data where the negative samples overwhelm and mislabeled samples or corrupted features exist. To address these three issues, we propose a robust learning framework that unifies AUC maximization (a robust metric for biased labels), outlier detection (for excluding wrong labels), and feature selection (for excluding corrupted features). The generalization error bounds are provided for the proposed model that give valuable insight into the theoretical performance of the method and lead to useful practical guidance, e.g., to train a model, we find that the included unlabeled samples are sufficient as long as the sample size is comparable to the number of positive samples in the training process. Empirical comparisons and two real-world applications on surgical site infection (SSI) and EEG seizure detection are also conducted to show the effectiveness of the proposed model.        △ Less","18 March, 2018","cs.LG,stat.ML",
              Tell Me Why Is It So? Explaining Knowledge Graph Relationships by Finding Descriptive Support Passages          ,1803.06555,https://arxiv.org/abs/1803.06555,https://arxiv.org/pdf/1803.06555,"Authors:SumitBhatia,PurusharthDwivedi,AvneetKaur","        We address the problem of finding descriptive explanations of facts stored in a knowledge graph. This is important in high-risk domains such as healthcare, intelligence, etc. where users need additional information for decision making and is especially crucial for applications that rely on automatically constructed knowledge bases where machine learned systems extract facts from an input corpus and working of the extractors is opaque to the end-user. We follow an approach inspired from information retrieval and propose a simple and efficient, yet effective solution that takes into account passage level as well as document level properties to produce a ranked list of passages describing a given input relation. We test our approach using Wikidata as the knowledge base and Wikipedia as the source corpus and report results of user studies conducted to study the effectiveness of our proposed model.        △ Less","17 March, 2018","cs.AI,cs.IR",
              Attack Trees in Isabelle          ,1803.06494,https://arxiv.org/abs/1803.06494,https://arxiv.org/pdf/1803.06494,Authors:FlorianKammüller,"        In this paper, we present a proof theory for attack trees. Attack trees are a well established and useful model for the construction of attacks on systems since they allow a stepwise exploration of high level attacks in application scenarios. Using the expressiveness of Higher Order Logic in Isabelle, we succeed in developing a generic theory of attack trees with a state-based semantics based on Kripke structures and CTL. The resulting framework allows mechanically supported logic analysis of the meta-theory of the proof calculus of attack trees and at the same time the developed proof theory enables application to case studies. A central correctness and completeness result proved in Isabelle establishes a connection between the notion of attack tree validity and CTL. The application is illustrated on the example of a healthcare IoT system and GDPR compliance verification.        △ Less","15 May, 2018","cs.CR,cs.LO",
              Modeling the effects of telephone nursing on healthcare utilization          ,1803.06109,https://arxiv.org/abs/1803.06109,https://arxiv.org/pdf/1803.06109,"Authors:JesperMartinsson,SiljeGustafsson","        Background: Telephone nursing is the first line of contact for many care-seekers and aims at optimizing the performance of the healthcare system by supporting and guiding patients to the correct level of care and reduce the amount of unscheduled visits. Good statistical models that describe the effects of telephone nursing are important in order to study its impact on healthcare resources and evaluate changes in telephone nursing procedures.  Objective: To develop a valid model that captures the complex relationships between the nurse's recommendations, the patients' intended actions and the patients' health seeking behavior. Using the model to estimate the effects of telephone nursing on patient behavior, healthcare utilization, and infer potential cost savings.  Methods: Bayesian ordinal regression modeling of data from randomly selected patients that received telephone nursing. Inference is based on Markov Chain Monte Carlo methods, model selection using the Watanabe-Akaike Information Criteria, and model validation using posterior predictive checks on standard discrepancy measures.  Results and Conclusions: We present a robust Bayesian ordinal regression model that predicts 76% of the patients' healthcare utilization after telephone nursing and we found no evidence of model deficiencies. The model reveals a risk reducing behavior and the effect of the telephone nursing recommendation is 7 times higher than the effect of the patient's intended action prior to consultation if the recommendation is the highest level of care. But the effect of the nurse's recommendation is lower, or even non-existing, if the recommendation is self-care. Telephone nursing was found to have a constricting effect on healthcare utilization, however, the compliance to nurse's recommendation is closely tied to perceptions of risk, emphasizing the importance to address caller's needs of reassurance.        △ Less","16 March, 2018",stat.AP,10.1016/j.ijmedinf.2018.02.004 
              Beyond Patient Monitoring: Conversational Agents Role in Telemedicine & Healthcare Support For Home-Living Elderly Individuals          ,1803.06000,https://arxiv.org/abs/1803.06000,https://arxiv.org/pdf/1803.06000,Authors:AhmedFadhil,"        There is a need for systems to dynamically interact with ageing populations to gather information, monitor health condition and provide support, especially after hospital discharge or at-home settings. Several smart devices have been delivered by digital health, bundled with telemedicine systems, smartphone and other digital services. While such solutions offer personalised data and suggestions, the real disruptive step comes from the interaction of new digital ecosystem, represented by chatbots. Chatbots will play a leading role by embodying the function of a virtual assistant and bridging the gap between patients and clinicians. Powered by AI and machine learning algorithms, chatbots are forecasted to save healthcare costs when used in place of a human or assist them as a preliminary step of helping to assess a condition and providing self-care recommendations. This paper describes integrating chatbots into telemedicine systems intended for elderly patient after their hospital discharge. The paper discusses possible ways to utilise chatbots to assist healthcare providers and support patients with their condition.        △ Less","3 March, 2018","cs.CY,cs.AI",
              Distributed Data Vending on Blockchain          ,1803.05871,https://arxiv.org/abs/1803.05871,https://arxiv.org/pdf/1803.05871,"Authors:JiayuZhou,FengyiTang,HeZhu,NingNan,ZihengZhou","        Recent advances in blockchain technologies have provided exciting opportunities for decentralized applications. Specifically, blockchain-based smart contracts enable credible transactions without authorized third parties. The attractive properties of smart contracts facilitate distributed data vending, allowing for proprietary data to be securely exchanged on a blockchain. Distributed data vending can transform domains such as healthcare by encouraging data distribution from owners and enabling large-scale data aggregation. However, one key challenge in distributed data vending is the trade-off dilemma between the effectiveness of data retrieval, and the leakage risk from indexing the data. In this paper, we propose a framework for distributed data vending through a combination of data embedding and similarity learning. We illustrate our framework through a practical scenario of distributing and aggregating electronic medical records on a blockchain. Extensive empirical results demonstrate the effectiveness of our framework.        △ Less","15 March, 2018","cs.CR,cs.LG",
              MedShare: Medical Resource Sharing among Autonomous Healthcare Providers          ,1803.05353,https://arxiv.org/abs/1803.05353,https://arxiv.org/pdf/1803.05353,"Authors:YilongYang,XiaoshanLi,NafeesQamar,WeiKe,ZhimingLiu","        Legacy Electronic Health Records (EHRs) systems were not developed with the level of connectivity expected from them nowadays. Therefore, interoperability weakness inherent in the legacy systems can result in poor patient care and waste of financial resources. Large hospitals are less likely to share their data with external hospitals due to economic and political reasons. Motivated by these facts, we aim to provide a set of software implementation guidelines, i.e., MedShare to deal with interoperability issues among disconnected healthcare systems. The proposed integrated architecture includes: 1) a data extractor to fetch legacy medical data from a hemodialysis center, 2) converting it to a common data model, 3) indexing patient information using the HashMap technique, and 4) a set of services and tools that can be installed as a coherent environment on top of stand-alone EHRs systems. Our work enabled three cooperating but autonomous hospitals to mutually exchange medical data and helped them develop a common reference architecture. It lets stakeholders retain control over their patient data, winning the trust and confidence much needed towards a successful deployment of MedShare. Security concerns were effectively addressed that also included patient consent in the data exchange process. Thereby, the implemented toolset offered a collaborative environment to share EHRs by the healthcare providers.        △ Less","14 March, 2018",cs.SE,10.1109/ACCESS.2018.2865535 
              Efficient Determination of Equivalence for Encrypted Data          ,1803.03760,https://arxiv.org/abs/1803.03760,https://arxiv.org/pdf/1803.03760,"Authors:JasonN.Doctor,JaideepVaidya,XiaoqianJiang,ShuangWang,LisaM.Schilling,ToanOng,MichaelE.Matheny,LucilaOhno-Machado,DaniellaMeeker","        Secure computation of equivalence has fundamental application in many different areas, including healthcare. We study this problem in the context of matching an individual identity to link medical records across systems. We develop an efficient solution for equivalence based on existing work that can evaluate the greater than relation. We implement the approach and demonstrate its effectiveness on data, as well as demonstrate how it meets regulatory criteria for risk.        △ Less","10 March, 2018",cs.CR,
              City-wide Analysis of Electronic Health Records Reveals Gender and Age Biases in the Administration of Known Drug-Drug Interactions          ,1803.03571,https://arxiv.org/abs/1803.03571,https://arxiv.org/pdf/1803.03571,"Authors:RionBrattigCorreia,LucianaP.deAraújo,MauroM.Mattos,LuisM.Rocha","        The occurrence of drug-drug-interactions (DDI) from multiple drug dispensations is a serious problem, both for individuals and health-care systems, since patients with complications due to DDI are likely to reenter the system at a costlier level. We present a large-scale longitudinal study (18 months) of the DDI phenomenon at the primary- and secondary-care level using electronic health records (EHR) from the city of Blumenau in Southern Brazil (pop. ≈340,000\approx 340,000). We found that 181 distinct drug pairs known to interact were dispensed concomitantly to 12\% of the patients in the city's public health-care system. Further, 4\% of the patients were dispensed drug pairs that are likely to result in major adverse drug reactions (ADR)---with costs estimated to be much larger than previously reported in smaller studies. The large-scale analysis reveals that women have a 60\% increased risk of DDI as compared to men; the increase becomes 90\% when considering only DDI known to lead to major ADR. Furthermore, DDI risk increases substantially with age; patients aged 70-79 years have a 34\% risk of DDI when they are dispensed two or more drugs concomitantly. Interestingly, a statistical null model demonstrates that age- and female-specific risks from increased polypharmacy fail by far to explain the observed DDI risks in those populations, suggesting unknown social or biological causes. We also provide a network visualization of drugs and demographic factors that characterize the DDI phenomenon and demonstrate that accurate DDI prediction can be included in healthcare and public-health management, to reduce DDI-related ADR and costs.        △ Less","2 January, 2020","cs.SI,cs.CY,cs.IR,q-bio.QM,stat.ML",10.1038/s41746-019-0141-x 
              Towards Automatic & Personalised Mobile Health Interventions: An Interactive Machine Learning Perspective          ,1803.01842,https://arxiv.org/abs/1803.01842,https://arxiv.org/pdf/1803.01842,Authors:AhmedFadhil,"        Machine learning (ML) is the fastest growing field in computer science and healthcare, providing future benefits in improved medical diagnoses, disease analyses and prevention. In this paper, we introduce an application of interactive machine learning (iML) in a telemedicine system, to enable automatic and personalised interventions for lifestyle promotion. We first present the high level architecture of the system and the components forming the overall architecture. We then illustrate the interactive machine learning process design. Prediction models are expected to be trained through the participants' profiles, activity performance, and feedback from the caregiver. Finally, we show some preliminary results during the system implementation and discuss future directions. We envisage the proposed system to be digitally implemented, and behaviourally designed to promote healthy lifestyle and activities, and hence prevent users from the risk of chronic diseases.        △ Less","3 March, 2018","cs.CY,cs.AI,cs.HC",
              Developing a functional prototype master patient index (MPI) for interoperability of e-health systems in Sri Lanka          ,1803.00451,https://arxiv.org/abs/1803.00451,https://arxiv.org/pdf/1803.00451,"Authors:W.G.P.TJayathissa,VajiraHWDissanayake,RoshanHewapathirana","        Introduction: A Master Patient Index(MPI) is a centralized index of all patients in a healthcare system. This index is composed of a unique identifier for each patient link to his/her demographic data and clinical encounters. A MPI is essential to ensure data interoperability in the different healthcare institution. The The health ministry of Sri Lanka planning to develop MPI for the country. This project focused on developing the prototype MPI for Sri Lanka with the view to implementing and scaling up at the national level. Methods: This project consisted of 3 phases. Phase 1: requirement analysis using focus group discussions (FGD) with information system users. Phase 2: identification of the suitable Application Programming interface (API) model. Phase 3: development of the prototype MPI. Results: FGD were conducted in 6 hospitals. There were 78 interviewers (Male -36, and female - 42). They highlighted the key requirements for the MPI. Which were the unique identification method and different searching criteria and merging records to avoid duplication. Using this information, the requirements specification for MPI was developed. A combination of monolithic and microservices architecture was selected to develop the MPI. The API using the Personal Health Number (PHN) as the unique patient identifier and HL7 standard was developed and implemented. Conclusions: Development and implementation of a MPI has facilitated the long due need for interoperability among health information systems in Sri Lankan. KEYWORDS MPI, Interoperability, Unique Identifier, PHN, API        △ Less","2 March, 2018",cs.CY,
              RF Energy Harvesting Sensor Networks for Healthcare of Animals: Opportunities and Challenges          ,1803.00106,https://arxiv.org/abs/1803.00106,https://arxiv.org/pdf/1803.00106,"Authors:YuLuo,LinaPu,YanxiaoZhao","        In recent years, the radio frequency (RF) energy harvesting technique is considered as a favorable alternative to supply power for the next-generation wireless sensor networks. Due to the features of energy self-sustainability and long lifetime, the energy harvesting sensor network (RF-EHSN) becomes a promising solution to provide smart healthcare services. Nowadays, many energy harvesting based applications have been developed for monitoring the health status of human beings; how to benefit animals, however, has not yet drawn people's attention. This article explores the potential of applying RF-EHSNs to monitoring the health level of animals. The unique challenges and potential solutions at different layers of an RF-EHSN for animals' healthcare service are studied.        △ Less","28 February, 2018",cs.NI,
              A High GOPs/Slice Time Series Classifier for Portable and Embedded Biomedical Applications          ,1802.10458,https://arxiv.org/abs/1802.10458,https://arxiv.org/pdf/1802.10458,"Authors:HamidSoleimani,Aliasghar,Makhlooghpour,WiltenNicola,ClaudiaClopath,Emmanuel.M.Drakakis","        Nowadays a diverse range of physiological data can be captured continuously for various applications in particular wellbeing and healthcare. Such data require efficient methods for classification and analysis. Deep learning algorithms have shown remarkable potential regarding such analyses, however, the use of these algorithms on low-power wearable devices is challenged by resource constraints such as area and power consumption. Most of the available on-chip deep learning processors contain complex and dense hardware architectures in order to achieve the highest possible throughput. Such a trend in hardware design may not be efficient in applications where on-node computation is required and the focus is more on the area and power efficiency as in the case of portable and embedded biomedical devices. This paper presents an efficient time-series classifier capable of automatically detecting effective features and classifying the input signals in real-time. In the proposed classifier, throughput is traded off with hardware complexity and cost using resource sharing techniques. A Convolutional Neural Network (CNN) is employed to extract input features and then a Long-Short-Term-Memory (LSTM) architecture with ternary weight precision classifies the input signals according to the extracted features. Hardware implementation on a Xilinx FPGA confirm that the proposed hardware can accurately classify multiple complex biomedical time series data with low area and power consumption and outperform all previously presented state-of-the-art records. Most notably, our classifier reaches 1.3×\times higher GOPs/Slice than similar state of the art FPGA-based accelerators.        △ Less","1 November, 2018","cs.LG,eess.SP",
              Domain Modelling in Computational Persuasion for Behaviour Change in Healthcare,1802.10054,https://arxiv.org/abs/1802.10054,https://arxiv.org/pdf/1802.10054,"Authors:LisaChalaguine,EmmanuelHadoux,FionaHamilton,AndrewHayward,AnthonyHunter,SylwiaPolberg,HenryW.W.Potts","        The aim of behaviour change is to help people to change aspects of their behaviour for the better (e.g., to decrease calorie intake, to drink in moderation, to take more exercise, to complete a course of antibiotics once started, etc.). In current persuasion technology for behaviour change, the emphasis is on helping people to explore their issues (e.g., through questionnaires or game playing) or to remember to follow a behaviour change plan (e.g., diaries and email reminders). However, recent developments in computational persuasion are leading to an argument-centric approach to persuasion that can potentially be harnessed in behaviour change applications. In this paper, we review developments in computational persuasion, and then focus on domain modelling as a key component. We present a multi-dimensional approach to domain modelling. At the core of this proposal is an ontology which provides a representation of key factors, in particular kinds of belief, which we have identified in the behaviour change literature as being important in diverse behaviour change initiatives. Our proposal for domain modelling is intended to facilitate the acquisition and representation of the arguments that can be used in persuasion dialogues, together with meta-level information about them which can be used by the persuader to make strategic choices of argument to present.        △ Less","27 February, 2018",cs.AI,
              Missing Data in Sparse Transition Matrix Estimation for Sub-Gaussian Vector Autoregressive Processes          ,1802.09511,https://arxiv.org/abs/1802.09511,https://arxiv.org/pdf/1802.09511,"Authors:AminJalali,RebeccaWillett","        High-dimensional time series data exist in numerous areas such as finance, genomics, healthcare, and neuroscience. An unavoidable aspect of all such datasets is missing data, and dealing with this issue has been an important focus in statistics, control, and machine learning. In this work, we consider a high-dimensional estimation problem where a dynamical system, governed by a stable vector autoregressive model, is randomly and only partially observed at each time point. Our task amounts to estimating the transition matrix, which is assumed to be sparse. In such a scenario, where covariates are highly interdependent and partially missing, new theoretical challenges arise. While transition matrix estimation in vector autoregressive models has been studied previously, the missing data scenario requires separate efforts. Moreover, while transition matrix estimation can be studied from a high-dimensional sparse linear regression perspective, the covariates are highly dependent and existing results on regularized estimation with missing data from i.i.d.~covariates are not applicable. At the heart of our analysis lies 1) a novel concentration result when the innovation noise satisfies the convex concentration property, as well as 2) a new quantity for characterizing the interactions of the time-varying observation process with the underlying dynamical system.        △ Less","26 February, 2018","stat.ML,cs.LG",
              Forecasting the impact of state pension reforms in post-Brexit England and Wales using microsimulation and deep learning          ,1802.09427,https://arxiv.org/abs/1802.09427,https://arxiv.org/pdf/1802.09427,Authors:AgnieszkaWerpachowska,"        We employ stochastic dynamic microsimulations to analyse and forecast the pension cost dependency ratio for England and Wales from 1991 to 2061, evaluating the impact of the ongoing state pension reforms and changes in international migration patterns under different Brexit scenarios. To fully account for the recently observed volatility in life expectancies, we propose mortality rate model based on deep learning techniques, which discovers complex patterns in data and extrapolated trends. Our results show that the recent reforms can effectively stave off the ""pension crisis"" and bring back the system on a sounder fiscal footing. At the same time, increasingly more workers can expect to spend greater share of their lifespan in retirement, despite the eligibility age rises. The population ageing due to the observed postponement of death until senectitude often occurs with the compression of morbidity, and thus will not, perforce, intrinsically strain healthcare costs. To a lesser degree, the future pension cost dependency ratio will depend on the post-Brexit relations between the UK and the EU, with ""soft"" alignment on the free movement lowering the relative cost of the pension system compared to the ""hard"" one. In the long term, however, the ratio has a rising tendency.        △ Less","21 April, 2018","econ.EM,q-fin.GN",
              A Deep Learning Approach for Privacy Preservation in Assisted Living          ,1802.09359,https://arxiv.org/abs/1802.09359,https://arxiv.org/pdf/1802.09359,"Authors:IsminiPsychoula,ErincMerdivan,DeepikaSingh,LimingChen,FengChen,StenHanke,JohannesKropf,AndreasHolzinger,MatthieuGeist","        In the era of Internet of Things (IoT) technologies the potential for privacy invasion is becoming a major concern especially in regards to healthcare data and Ambient Assisted Living (AAL) environments. Systems that offer AAL technologies make extensive use of personal data in order to provide services that are context-aware and personalized. This makes privacy preservation a very important issue especially since the users are not always aware of the privacy risks they could face. A lot of progress has been made in the deep learning field, however, there has been lack of research on privacy preservation of sensitive personal data with the use of deep learning. In this paper we focus on a Long Short Term Memory (LSTM) Encoder-Decoder, which is a principal component of deep learning, and propose a new encoding technique that allows the creation of different AAL data views, depending on the access level of the end user and the information they require access to. The efficiency and effectiveness of the proposed method are demonstrated with experiments on a simulated AAL dataset. Qualitatively, we show that the proposed model learns privacy operations such as disclosure, deletion and generalization and can perform encoding and decoding of the data with almost perfect recovery.        △ Less","22 February, 2018","eess.SP,cs.LG",
              AutoPrognosis: Automated Clinical Prognostic Modeling via Bayesian Optimization with Structured Kernel Learning          ,1802.07207,https://arxiv.org/abs/1802.07207,https://arxiv.org/pdf/1802.07207,"Authors:AhmedM.Alaa,MihaelavanderSchaar","        Clinical prognostic models derived from largescale healthcare data can inform critical diagnostic and therapeutic decisions. To enable off-theshelf usage of machine learning (ML) in prognostic research, we developed AUTOPROGNOSIS: a system for automating the design of predictive modeling pipelines tailored for clinical prognosis. AUTOPROGNOSIS optimizes ensembles of pipeline configurations efficiently using a novel batched Bayesian optimization (BO) algorithm that learns a low-dimensional decomposition of the pipelines high-dimensional hyperparameter space in concurrence with the BO procedure. This is achieved by modeling the pipelines performances as a black-box function with a Gaussian process prior, and modeling the similarities between the pipelines baseline algorithms via a sparse additive kernel with a Dirichlet prior. Meta-learning is used to warmstart BO with external data from similar patient cohorts by calibrating the priors using an algorithm that mimics the empirical Bayes method. The system automatically explains its predictions by presenting the clinicians with logical association rules that link patients features to predicted risk strata. We demonstrate the utility of AUTOPROGNOSIS using 10 major patient cohorts representing various aspects of cardiovascular patient care.        △ Less","20 February, 2018","cs.LG,stat.ML",
              Simultaneous Modeling of Multiple Complications for Risk Profiling in Diabetes Care          ,1802.06476,https://arxiv.org/abs/1802.06476,https://arxiv.org/pdf/1802.06476,"Authors:BinLiu,YingLi,SoumyaGhosh,ZhaonanSun,KenneyNg,JianyingHu","        Type 2 diabetes mellitus (T2DM) is a chronic disease that often results in multiple complications. Risk prediction and profiling of T2DM complications is critical for healthcare professionals to design personalized treatment plans for patients in diabetes care for improved outcomes. In this paper, we study the risk of developing complications after the initial T2DM diagnosis from longitudinal patient records. We propose a novel multi-task learning approach to simultaneously model multiple complications where each task corresponds to the risk modeling of one complication. Specifically, the proposed method strategically captures the relationships (1) between the risks of multiple T2DM complications, (2) between the different risk factors, and (3) between the risk factor selection patterns. The method uses coefficient shrinkage to identify an informative subset of risk factors from high-dimensional data, and uses a hierarchical Bayesian framework to allow domain knowledge to be incorporated as priors. The proposed method is favorable for healthcare applications because in additional to improved prediction performance, relationships among the different risks and risk factors are also identified. Extensive experimental results on a large electronic medical claims database show that the proposed method outperforms state-of-the-art models by a significant margin. Furthermore, we show that the risk associations learned and the risk factors identified lead to meaningful clinical insights.        △ Less","18 February, 2018","cs.LG,cs.AI,stat.ML",10.1109/TKDE.2019.2904060 
              Benchmarking Framework for Performance-Evaluation of Causal Inference Analysis          ,1802.05046,https://arxiv.org/abs/1802.05046,https://arxiv.org/pdf/1802.05046,"Authors:YishaiShimoni,ChenYanover,EhudKaravani,YaaraGoldschmnidt","        Causal inference analysis is the estimation of the effects of actions on outcomes. In the context of healthcare data this means estimating the outcome of counter-factual treatments (i.e. including treatments that were not observed) on a patient's outcome. Compared to classic machine learning methods, evaluation and validation of causal inference analysis is more challenging because ground truth data of counter-factual outcome can never be obtained in any real-world scenario. Here, we present a comprehensive framework for benchmarking algorithms that estimate causal effect. The framework includes unlabeled data for prediction, labeled data for validation, and code for automatic evaluation of algorithm predictions using both established and novel metrics. The data is based on real-world covariates, and the treatment assignments and outcomes are based on simulations, which provides the basis for validation. In this framework we address two questions: one of scaling, and the other of data-censoring. The framework is available as open source code at https://github.com/IBM-HRL-MLHLS/IBM-Causal-Inference-Benchmarking-Framework        △ Less","20 March, 2018","stat.ME,cs.LG,stat.ML",
              Recovering Loss to Followup Information Using Denoising Autoencoders          ,1802.04664,https://arxiv.org/abs/1802.04664,https://arxiv.org/pdf/1802.04664,"Authors:LovedeepGondara,KeWang","        Loss to followup is a significant issue in healthcare and has serious consequences for a study's validity and cost. Methods available at present for recovering loss to followup information are restricted by their expressive capabilities and struggle to model highly non-linear relations and complex interactions. In this paper we propose a model based on overcomplete denoising autoencoders to recover loss to followup information. Designed to work with high volume data, results on various simulated and real life datasets show our model is appropriate under varying dataset and loss to followup conditions and outperforms the state-of-the-art methods by a wide margin (≥20%\ge 20\% in some scenarios) while preserving the dataset utility for final analysis.        △ Less","11 February, 2018","cs.LG,stat.AP,stat.ML",10.1109/BigData.2017.8258139 
              Design of Experiments for Model Discrimination Hybridising Analytical and Data-Driven Approaches          ,1802.04170,https://arxiv.org/abs/1802.04170,https://arxiv.org/pdf/1802.04170,"Authors:SimonOlofsson,MarcPeterDeisenroth,RuthMisener","Healthcare companies must submit pharmaceutical drugs or medical devices to regulatory bodies before marketing new technology. Regulatory bodies frequently require transparent and interpretable computational modelling to justify a new healthcare technology, but researchers may have several competing models for a biological system and too little data to discriminate between the models. In design of experiments for model discrimination, the goal is to design maximally informative physical experiments in order to discriminate between rival predictive models. Prior work has focused either on analytical approaches, which cannot manage all functions, or on data-driven approaches, which may have computational difficulties or lack interpretable marginal predictive distributions. We develop a methodology introducing Gaussian process surrogates in lieu of the original mechanistic models. We thereby extend existing design and model discrimination methods developed for analytical models to cases of non-analytical models in a computationally efficient manner.        △ Less","31 May, 2018","stat.AP,stat.ML",
              Scalable Architecture for Personalized Healthcare Service Recommendation using Big Data Lake          ,1802.04105,https://arxiv.org/abs/1802.04105,https://arxiv.org/pdf/1802.04105,"Authors:SarathkumarRangarajan,HuaiLiu,HuaWang,Chuan-LongWang","        The personalized health care service utilizes the relational patient data and big data analytics to tailor the medication recommendations. However, most of the health care data are in unstructured form and it consumes a lot of time and effort to pull them into relational form. This study proposes a novel data lake architecture to reduce the data ingestion time and improve the precision of healthcare analytics. It also removes the data silos and enhances the analytics by allowing the connectivity to the third-party data providers (such as clinical lab results, chemist, insurance company,etc.). The data lake architecture uses the Hadoop Distributed File System (HDFS) to provide the storage for both structured and unstructured data. This study uses K-means clustering algorithm to find the patient clusters with similar health conditions. Subsequently, it employs a support vector machine to find the most successful healthcare recommendations for the each cluster. Our experiment results demonstrate the ability of data lake to reduce the time for ingesting data from various data vendors regardless of its format. Moreover, it is evident that the data lake poses the potential to generate clusters of patients more precisely than the existing approaches. It is obvious that the data lake provides a unified storage location for the data in its native format. It can also improve the personalized healthcare medication recommendations by removing the data silos.        △ Less","1 February, 2018",cs.CY,
              Sample Design for Audit Populations          ,1802.03778,https://arxiv.org/abs/1802.03778,https://arxiv.org/pdf/1802.03778,Authors:MichelleNorris,"        We develop several tools for the determination of sample size and design for MediCal audits. This audit setting involves a population of claims for reimbursement by a healthcare provider which need to be reviewed by an auditor to determine the correct amount for each claim. The existing literature regarding sample planning for audits is incomplete and often includes restrictive assumptions. To fill these gaps, we exploit the special relationship between the known claim amounts and the unknown post-audit amounts. We propose a hypergeometric generative process for audit populations which we use to derive estimators of variances needed for sample size determination. We further develop a criterion for choosing between simple expansion and ratio estimation and an efficient method for determining exact optimal strata breakpoints in populations with repeated values. We also derive a variance estimator under a more general ""partial error"" model than previous researchers have used. These tools apply more generally to audits where an overstated book/claim amount is the primary concern and estimation of the total dollar value of the claim errors is the goal. The sample design methods we develop are illustrated on two simulated audit populations.        △ Less","11 February, 2018","stat.ME,stat.AP",
"              The Effect of IoT New Features on Security and Privacy: New Threats, Existing Solutions, and Challenges Yet to Be Solved          ",1802.03110,https://arxiv.org/abs/1802.03110,https://arxiv.org/pdf/1802.03110,"Authors:WeiZhou,YuqingZhang,PengLiu","        The future of Internet of Things (IoT) is already upon us. IoT applications have been widely used in many field of social production and social living such as healthcare, energy and industrial automation. While enjoying the convenience and efficiency that IoT brings to us, new threats from IoT also have emerged. There are increasing research works to ease these threats, but many problems remain open. To better understand the essential reasons of new threats and the challenges in current research, this survey first proposes the concept of ""IoT features"". Then, the security and privacy effects of eight IoT new features were discussed including the threats they cause, existing solutions and challenges yet to be solved. To help researchers follow the up-to-date works in this field, this paper finally illustrates the developing trend of IoT security research and reveals how IoT features affect existing security research by investigating most existing research works related to IoT security from 2013 to 2017.        △ Less","8 February, 2018",cs.CR,10.1109/JIOT.2018.2847733 
              Cognitive Business Process Management for Adaptive Cyber-Physical Processes          ,1802.02986,https://arxiv.org/abs/1802.02986,https://arxiv.org/pdf/1802.02986,"Authors:AndreaMarrella,MassimoMecella","        In the era of Big Data and Internet-of-Things (IoT), all real-world environments are gradually becoming cyber-physical (e.g., emergency management, healthcare, smart manufacturing, etc.), with the presence of connected devices and embedded ICT systems (e.g., smartphones, sensors, actuators) producing huge amounts of data and events that influence the enactment of the Cyber Physical Processes (CPPs) enacted in such environments. A Process Management System (PMS) employed for executing CPPs is required to automatically adapt its running processes to anomalous situations and exogenous events by minimising any human intervention at run-time. In this paper, we tackle this issue by introducing an approach and an adaptive Cognitive PMS that combines process execution monitoring, unanticipated exception detection and automated resolution strategies leveraging on well-established action-based formalisms in Artificial Intelligence, which allow to interpret the ever-changing knowledge of cyber-physical environments and to adapt CPPs by preserving their base structure.        △ Less","8 February, 2018",cs.SE,10.1007/978-3-319-74030-0_33 
              Automatic construction of Chinese herbal prescription from tongue image via CNNs and auxiliary latent therapy topics          ,1802.02203,https://arxiv.org/abs/1802.02203,https://arxiv.org/pdf/1802.02203,"Authors:YangHu,GuihuaWen,HuiqiangLiao,ChangjunWang,DanDai,ZhiwenYu","        The tongue image provides important physical information of humans. It is of great importance for diagnoses and treatments in clinical medicine. Herbal prescriptions are simple, noninvasive and have low side effects. Thus, they are widely applied in China. Studies on the automatic construction technology of herbal prescriptions based on tongue images have great significance for deep learning to explore the relevance of tongue images for herbal prescriptions, it can be applied to healthcare services in mobile medical systems. In order to adapt to the tongue image in a variety of photographic environments and construct herbal prescriptions, a neural network framework for prescription construction is designed. It includes single/double convolution channels and fully connected layers. Furthermore, it proposes the auxiliary therapy topic loss mechanism to model the therapy of Chinese doctors and alleviate the interference of sparse output labels on the diversity of results. The experiment use the real world tongue images and the corresponding prescriptions and the results can generate prescriptions that are close to the real samples, which verifies the feasibility of the proposed method for the automatic construction of herbal prescriptions from tongue images. Also, it provides a reference for automatic herbal prescription construction from more physical information.        △ Less","6 May, 2019","cs.CV,cs.LG,cs.NE",
              ModelChain: Decentralized Privacy-Preserving Healthcare Predictive Modeling Framework on Private Blockchain Networks          ,1802.01746,https://arxiv.org/abs/1802.01746,https://arxiv.org/pdf/1802.01746,"Authors:Tsung-TingKuo,LucilaOhno-Machado","        Cross-institutional healthcare predictive modeling can accelerate research and facilitate quality improvement initiatives, and thus is important for national healthcare delivery priorities. For example, a model that predicts risk of re-admission for a particular set of patients will be more generalizable if developed with data from multiple institutions. While privacy-protecting methods to build predictive models exist, most are based on a centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure (and single-point-of-breach) and accidental or malicious modification of records. In this article, we describe a new framework, ModelChain, to adapt Blockchain technology for privacy-preserving machine learning. Each participating site contributes to model parameter estimation without revealing any patient health information (i.e., only model data, no observation-level data, are exchanged across institutions). We integrate privacy-preserving online machine learning with a private Blockchain network, apply transaction metadata to disseminate partial models, and design a new proof-of-information algorithm to determine the order of the online learning process. We also discuss the benefits and potential issues of applying Blockchain technology to solve the privacy-preserving healthcare predictive modeling task and to increase interoperability between institutions, to support the Nationwide Interoperability Roadmap and national healthcare delivery priorities such as Patient-Centered Outcomes Research (PCOR).        △ Less","5 February, 2018","cs.CY,cs.CR",
              Internet - assisted risk assessment of infectious diseases in women sexual and reproductive health          ,1802.01733,https://arxiv.org/abs/1802.01733,https://arxiv.org/pdf/1802.01733,"Authors:AndrzejJarynowski,DamianMarchewka,AndrzejBuda","        We develop open source infection risk calculators for patients and healthcare professionals as apps for hospital acquired infections (during child-delivery) and sexually transmitted infections (like HIV). Advanced versions of ehealth in non-communicable diseases do not apply to epidemiology much. There is, however, no infection risk calculator in the Polish Internet so far, despite the existence of data that may be applied to create such a tool.  The algorithms involve data from Information Systems (like HIS in hospitals) and surveys by applying mathematical modelling, Bayesian inference, logistic regressions, covariance analysis and social network analysis. Finally, user may fill or import data from Information System to obtain risk assessment and test different settings to learn overall risk.  The most promising risk calculator is developed for Healthcare-associated infections in modes for patient hospital sanitary inspection. The most extended version for hospital epidemiologists may include many layers of hospital interactions by agent-based modeling. Simplified version of calculator is dedicated to patients that require personalized hospitalization history of pregnancy described by questions represented by quantitative and qualitative variables. Patients receive risk assessment from interactive web application with additional description about modifiable risk factors.  We also provide solution for sexually transmitted infections like HIV. The results of calculations with meaningful description and percentage chances are presented in real-time to interested users. Finally, user fills the form to obtain risk assessment for given settings.        △ Less","5 February, 2018","stat.AP,physics.soc-ph,q-bio.PE",
              Resset: A Recurrent Model for Sequence of Sets with Applications to Electronic Medical Records          ,1802.00948,https://arxiv.org/abs/1802.00948,https://arxiv.org/pdf/1802.00948,"Authors:PhuocNguyen,TruyenTran,SvethaVenkatesh","        Modern healthcare is ripe for disruption by AI. A game changer would be automatic understanding the latent processes from electronic medical records, which are being collected for billions of people worldwide. However, these healthcare processes are complicated by the interaction between at least three dynamic components: the illness which involves multiple diseases, the care which involves multiple treatments, and the recording practice which is biased and erroneous. Existing methods are inadequate in capturing the dynamic structure of care. We propose Resset, an end-to-end recurrent model that reads medical record and predicts future risk. The model adopts the algebraic view in that discrete medical objects are embedded into continuous vectors lying in the same space. We formulate the problem as modeling sequences of sets, a novel setting that have rarely, if not, been addressed. Within Resset, the bag of diseases recorded at each clinic visit is modeled as function of sets. The same hold for the bag of treatments. The interaction between the disease bag and the treatment bag at a visit is modeled in several, one of which as residual of diseases minus the treatments. Finally, the health trajectory, which is a sequence of visits, is modeled using a recurrent neural network. We report results on over a hundred thousand hospital visits by patients suffered from two costly chronic diseases -- diabetes and mental health. Resset shows promises in multiple predictive tasks such as readmission prediction, treatments recommendation and diseases progression.        △ Less","3 February, 2018",cs.NE,
              Dual Memory Neural Computer for Asynchronous Two-view Sequential Learning          ,1802.00662,https://arxiv.org/abs/1802.00662,https://arxiv.org/pdf/1802.00662,"Authors:HungLe,TruyenTran,SvethaVenkatesh","        One of the core tasks in multi-view learning is to capture relations among views. For sequential data, the relations not only span across views, but also extend throughout the view length to form long-term intra-view and inter-view interactions. In this paper, we present a new memory augmented neural network model that aims to model these complex interactions between two asynchronous sequential views. Our model uses two encoders for reading from and writing to two external memories for encoding input views. The intra-view interactions and the long-term dependencies are captured by the use of memories during this encoding process. There are two modes of memory accessing in our system: late-fusion and early-fusion, corresponding to late and early inter-view interactions. In the late-fusion mode, the two memories are separated, containing only view-specific contents. In the early-fusion mode, the two memories share the same addressing space, allowing cross-memory accessing. In both cases, the knowledge from the memories will be combined by a decoder to make predictions over the output space. The resulting dual memory neural computer is demonstrated on a comprehensive set of experiments, including a synthetic task of summing two sequences and the tasks of drug prescription and disease progression in healthcare. The results demonstrate competitive performance over both traditional algorithms and deep learning methods designed for multi-view problems.        △ Less","10 February, 2018","cs.LG,stat.ML",
              Preserving Patient-centred Controls in Electronic Health Record Systems: A Reliance-based Model Implication          ,1802.00575,https://arxiv.org/abs/1802.00575,https://arxiv.org/pdf/1802.00575,"Authors:PasupathyVimalachandran,HuaWang,YanchunZhang,BenHeyward,YueaiZhao","        As a consequence of the huge advancement of the Electronic Health Record (EHR) in healthcare settings, the My Health Record (MHR) is introduced in Australia. However security and privacy of the MHR system have been encumbering the development of the system. Even though the MHR system is claimed as patient-cenred and patient-controlled, there are several instances where healthcare providers (other than the usual provider) and system operators who maintain the system can easily access the system and these unauthorised accesses can lead to a breach of the privacy of the patients. This is one of the main concerns of the consumers that affect the uptake of the system. In this paper, we propose a patient centred MHR framework which requests authorisation from the patient to access their sensitive health information. The proposed model increases the involvement and satisfaction of the patients in their healthcare and also suggests mobile security system to give an online permission to access the MHR system.        △ Less","2 February, 2018",cs.CY,
              FoodRepo: An Open Food Repository of Barcoded Food Products          ,1801.10195,https://arxiv.org/abs/1801.10195,https://arxiv.org/pdf/1801.10195,"Authors:GianroccoLazzari,YannisJaquet,DjilaniKebaili,LauraSymul,MarcelSalathé","        In the past decade, digital technologies have started to profoundly influence healthcare systems. Digital self-tracking has facilitated more precise epidemiological studies, and in the field of nutritional epidemiology, mobile apps have the potential to alleviate a significant part of the journaling burden by, for example, allowing users to record their food intake via a simple scan of packaged products barcodes. Such studies thus rely on databases of commercialized products, their barcodes, ingredients, and nutritional values, which are not yet openly available with sufficient geographical and product coverage. In this paper, we present FoodRepo (https://www.foodrepo.org), an open food repository of barcoded food items, whose database is programmatically accessible through an application programming interface (API). Furthermore, an open source license gives the appropriate rights to anyone to share and reuse FoodRepo data, including for commercial purposes. With currently more than 21,000 items available on the Swiss market, our database represents a solid starting point for large-scale studies in the field of digital nutrition, with the aim to lead to a better understanding of the intricate connections between diets and health in general, and metabolic disorders in particular.        △ Less","25 January, 2018",q-bio.QM,10.3389/fnut.2018.00057 
              Personalized Survival Prediction with Contextual Explanation Networks          ,1801.09810,https://arxiv.org/abs/1801.09810,https://arxiv.org/pdf/1801.09810,"Authors:MaruanAl-Shedivat,AvinavaDubey,EricP.Xing","        Accurate and transparent prediction of cancer survival times on the level of individual patients can inform and improve patient care and treatment practices. In this paper, we design a model that concurrently learns to accurately predict patient-specific survival distributions and to explain its predictions in terms of patient attributes such as clinical tests or assessments. Our model is flexible and based on a recurrent network, can handle various modalities of data including temporal measurements, and yet constructs and uses simple explanations in the form of patient- and time-specific linear regression. For analysis, we use two publicly available datasets and show that our networks outperform a number of baselines in prediction while providing a way to inspect the reasons behind each prediction.        △ Less","29 January, 2018","cs.LG,cs.AI",
              Foursquare to The Rescue: Predicting Ambulance Calls Across Geographies          ,1801.09524,https://arxiv.org/abs/1801.09524,https://arxiv.org/pdf/1801.09524,"Authors:AnastasiosNoulas,ColinMoffatt,DesislavaHristova,BrunoGonçalves","        Understanding how ambulance incidents are spatially distributed can shed light to the epidemiological dynamics of geographic areas and inform healthcare policy design. Here we analyze a longitudinal dataset of more than four million ambulance calls across a region of twelve million residents in the North West of England. With the aim to explain geographic variations in ambulance call frequencies, we employ a wide range of data layers including open government datasets describing population demographics and socio-economic characteristics, as well as geographic activity in online services such as Foursquare. Working at a fine level of spatial granularity we demonstrate that daytime population levels and the deprivation status of an area are the most important variables when it comes to predicting the volume of ambulance calls at an area. Foursquare check-ins on the other hand complement these government sourced indicators, offering a novel view to population nightlife and commercial activity locally. We demonstrate how check-in activity can provide an edge when predicting certain types of emergency incidents in a multi-variate regression model.        △ Less","26 February, 2018","physics.soc-ph,cs.CY,cs.SI",
              Phonocardiographic Sensing using Deep Learning for Abnormal Heartbeat Detection          ,1801.08322,https://arxiv.org/abs/1801.08322,https://arxiv.org/pdf/1801.08322,"Authors:SiddiqueLatif,MuhammadUsman,RajibRana,JunaidQadir","        Cardiac auscultation involves expert interpretation of abnormalities in heart sounds using stethoscope. Deep learning based cardiac auscultation is of significant interest to the healthcare community as it can help reducing the burden of manual auscultation with automated detection of abnormal heartbeats. However, the problem of automatic cardiac auscultation is complicated due to the requirement of reliability and high accuracy, and due to the presence of background noise in the heartbeat sound. In this work, we propose a Recurrent Neural Networks (RNNs) based automated cardiac auscultation solution. Our choice of RNNs is motivated by the great success of deep learning in medical applications and by the observation that RNNs represent the deep learning configuration most suitable for dealing with sequential or temporal data even in the presence of noise. We explore the use of various RNN models, and demonstrate that these models deliver the abnormal heartbeat classification score with significant improvement. Our proposed approach using RNNs can be potentially be used for real-time abnormal heartbeat detection in the Internet of Medical Things for remote monitoring applications.        △ Less","27 July, 2020",cs.CV,
              Scalable and accurate deep learning for electronic health records          ,1801.07860,https://arxiv.org/abs/1801.07860,https://arxiv.org/pdf/1801.07860,"Authors:AlvinRajkomar,EyalOren,KaiChen,AndrewM.Dai,NissanHajaj,PeterJ.Liu,XiaobingLiu,MimiSun,PatrikSundberg,HectorYee,KunZhang,GavinE.Duggan,GerardoFlores,MichaelaHardt,JamieIrvine,QuocLe,KurtLitsch,JakeMarcus,AlexanderMossin,JustinTansuwan,DeWang,JamesWexler,JimboWilson,DanaLudwig,SamuelL.Volchenboum,etal.(9additionalauthorsnotshown)","        Predictive modeling with electronic health record (EHR) data is anticipated to drive personalized medicine and improve healthcare quality. Constructing predictive statistical models typically requires extraction of curated predictor variables from normalized EHR data, a labor-intensive process that discards the vast majority of information in each patient's record. We propose a representation of patients' entire, raw EHR records based on the Fast Healthcare Interoperability Resources (FHIR) format. We demonstrate that deep learning methods using this representation are capable of accurately predicting multiple medical events from multiple centers without site-specific data harmonization. We validated our approach using de-identified EHR data from two U.S. academic medical centers with 216,221 adult patients hospitalized for at least 24 hours. In the sequential format we propose, this volume of EHR data unrolled into a total of 46,864,534,945 data points, including clinical notes. Deep learning models achieved high accuracy for tasks such as predicting in-hospital mortality (AUROC across sites 0.93-0.94), 30-day unplanned readmission (AUROC 0.75-0.76), prolonged length of stay (AUROC 0.85-0.86), and all of a patient's final discharge diagnoses (frequency-weighted AUROC 0.90). These models outperformed state-of-the-art traditional predictive models in all cases. We also present a case-study of a neural-network attribution system, which illustrates how clinicians can gain some transparency into the predictions. We believe that this approach can be used to create accurate and scalable predictions for a variety of clinical scenarios, complete with explanations that directly highlight evidence in the patient's chart.        △ Less","11 May, 2018","cs.CY,cs.LG",10.1038/s41746-018-0029-1 
              Secure Mobile Crowdsensing with Deep Learning          ,1801.07379,https://arxiv.org/abs/1801.07379,https://arxiv.org/pdf/1801.07379,"Authors:LiangXiao,DonghuaJiang,DongjinXu,NingAn","        In order to stimulate secure sensing for Internet of Things (IoT) applications such as healthcare and traffic monitoring, mobile crowdsensing (MCS) systems have to address security threats, such as jamming, spoofing and faked sensing attacks, during both the sensing and the information exchange processes in large-scale dynamic and heterogenous networks. In this article, we investigate secure mobile crowdsensing and present how to use deep learning (DL) methods such as stacked autoencoder (SAE), deep neural network (DNN), and convolutional neural network (CNN) to improve the MCS security approaches including authentication, privacy protection, faked sensing countermeasures, intrusion detection and anti-jamming transmissions in MCS. We discuss the performance gain of these DL-based approaches compared with traditional security schemes and identify the challenges that need to be addressed to implement them in practical MCS systems.        △ Less","22 January, 2018","cs.CR,cs.LG,cs.NI",
              Fostering Bilateral Patient-Clinician Engagement in Active Self-Tracking of Subjective Experience          ,1801.06352,https://arxiv.org/abs/1801.06352,https://arxiv.org/pdf/1801.06352,"Authors:JakobEgLarsen,ThomasBlomsethChristiansen,KasperEskelund","        In this position paper we describe select aspects of our experience with health-related self-tracking, the data generated, and processes surrounding those. In particular we focus on how bilateral patient-clinician engagement may be fostered by the combination of technology and method. We exemplify with a case study where a PTSD-suffering veteran has been self-tracking a specific symptom precursor. The availability of high-resolution self-tracking data on the occurrences of even a single symptom created new opportunities in the therapeutic process for identifying underlying triggers of symptoms. The patient was highly engaged in self-tracking and sharing the collected data. We suggest a key reason was the collaborative effort in defining the data collection protocol and discussion of the data. The therapist also engaged highly in the self-tracking data, as it supported the existing therapeutic process in reaching insights otherwise unobtainable.        △ Less","19 January, 2018",cs.HC,10.1145/3154862.3154918 
              Multi-Label Learning from Medical Plain Text with Convolutional Residual Models          ,1801.05062,https://arxiv.org/abs/1801.05062,https://arxiv.org/pdf/1801.05062,"Authors:XinyuanZhang,RicardoHenao,ZheGan,YitongLi,LawrenceCarin","        Predicting diagnoses from Electronic Health Records (EHRs) is an important medical application of multi-label learning. We propose a convolutional residual model for multi-label classification from doctor notes in EHR data. A given patient may have multiple diagnoses, and therefore multi-label learning is required. We employ a Convolutional Neural Network (CNN) to encode plain text into a fixed-length sentence embedding vector. Since diagnoses are typically correlated, a deep residual network is employed on top of the CNN encoder, to capture label (diagnosis) dependencies and incorporate information directly from the encoded sentence vector. A real EHR dataset is considered, and we compare the proposed model with several well-known baselines, to predict diagnoses based on doctor notes. Experimental results demonstrate the superiority of the proposed convolutional residual model.        △ Less","8 August, 2018","stat.ML,cs.LG,stat.AP",
              Hire the Experts: Combinatorial Auction Based Scheme for Experts Selection in E-Healthcare,1801.04544,https://arxiv.org/abs/1801.04544,https://arxiv.org/pdf/1801.04544,"Authors:VikashKumarSingh,SajalMukhopadhyay,FatosXhafa","        During the last decade, scheduling the healthcare services (such as staffs and OTs) inside the hospitals have assumed a central role in healthcare. Recently, some works are addressed in the direction of hiring the expert consultants (mainly doctors) for the critical healthcare scenarios from outside of the medical unit, in both strategic and non-strategic settings under monetary and non-monetary perspectives. In this paper, we have tried to investigate the experts hiring problem with multiple patients and multiple experts; where each patient reports a preferred set of experts which is private information alongwith their private cost for consultancy. To the best of our knowledge, this is the first step in the direction of modeling the experts hiring problem in the combinatorial domain. In this paper, the combinatorial auction based scheme is proposed for hiring experts from outside of the hospitals to have expertise by the preferred doctors set to the patients.        △ Less","14 January, 2018",cs.GT,
              Applications of Blockchain Technology beyond Cryptocurrency          ,1801.03528,https://arxiv.org/abs/1801.03528,https://arxiv.org/pdf/1801.03528,"Authors:MahdiH.Miraz,MaarufAli","        Blockchain (BC), the technology behind the Bitcoin crypto-currency system, is considered to be both alluring and critical for ensuring enhanced security and (in some implementations, non-traceable) privacy for diverse applications in many other domains including in the Internet of Things (IoT) eco-system. Intensive research is currently being conducted in both academia and industry applying the Blockchain technology in multifarious applications. Proof-of-Work (PoW), a cryptographic puzzle, plays a vital role in ensuring BC security by maintaining a digital ledger of transactions, which is considered to be incorruptible. Furthermore, BC uses a changeable Public Key (PK) to record the users' identity, which provides an extra layer of privacy. Not only in cryptocurrency has the successful adoption of BC been implemented but also in multifaceted non-monetary systems such as in: distributed storage systems, proof-of-location, healthcare, decentralized voting and so forth. Recent research articles and projects/applications were surveyed to assess the implementation of BC for enhanced security, to identify associated challenges and to propose solutions for BC enabled enhanced security systems.        △ Less","3 January, 2018",cs.CR,10.33166/AETiC.2018.01.001 
              Representation Learning with Autoencoders for Electronic Health Records: A Comparative Study          ,1801.02961,https://arxiv.org/abs/1801.02961,https://arxiv.org/pdf/1801.02961,"Authors:NajibesadatSadati,MiladZafarNezhad,RatnaBabuChinnam,DongxiaoZhu","        Increasing volume of Electronic Health Records (EHR) in recent years provides great opportunities for data scientists to collaborate on different aspects of healthcare research by applying advanced analytics to these EHR clinical data. A key requirement however is obtaining meaningful insights from high dimensional, sparse and complex clinical data. Data science approaches typically address this challenge by performing feature learning in order to build more reliable and informative feature representations from clinical data followed by supervised learning. In this paper, we propose a predictive modeling approach based on deep learning based feature representations and word embedding techniques. Our method uses different deep architectures (stacked sparse autoencoders, deep belief network, adversarial autoencoders and variational autoencoders) for feature representation in higher-level abstraction to obtain effective and robust features from EHRs, and then build prediction models on top of them. Our approach is particularly useful when the unlabeled data is abundant whereas labeled data is scarce. We investigate the performance of representation learning through a supervised learning approach. Our focus is to present a comparative study to evaluate the performance of different deep architectures through supervised learning and provide insights in the choice of deep feature representation techniques. Our experiments demonstrate that for small data sets, stacked sparse autoencoder demonstrates a superior generality performance in prediction due to sparsity regularization whereas variational autoencoders outperform the competing approaches for large data sets due to its capability of learning the representation distribution.        △ Less","29 September, 2019","cs.LG,stat.ML",
              Semi-automated Annotation of Signal Events in Clinical EEG Data          ,1801.02476,https://arxiv.org/abs/1801.02476,https://arxiv.org/pdf/1801.02476,"Authors:ScottYang,SilviaLopez,MeysamGolmohammadi,IyadObeid,JosephPicone","        To be effective, state of the art machine learning technology needs large amounts of annotated data. There are numerous compelling applications in healthcare that can benefit from high performance automated decision support systems provided by deep learning technology, but they lack the comprehensive data resources required to apply sophisticated machine learning models. Further, for economic reasons, it is very difficult to justify the creation of large annotated corpora for these applications. Hence, automated annotation techniques become increasingly important. In this study, we investigated the effectiveness of using an active learning algorithm to automatically annotate a large EEG corpus. The algorithm is designed to annotate six types of EEG events. Two model training schemes, namely threshold-based and volume-based, are evaluated. In the threshold-based scheme the threshold of confidence scores is optimized in the initial training iteration, whereas for the volume-based scheme only a certain amount of data is preserved after each iteration. Recognition performance is improved 2% absolute and the system is capable of automatically annotating previously unlabeled data. Given that the interpretation of clinical EEG data is an exceedingly difficult task, this study provides some evidence that the proposed method is a viable alternative to expensive manual annotation.        △ Less","2 January, 2018","eess.SP,cs.DB,cs.LG,stat.ML",10.1109/SPMB.2016.7846855 
              How will the Internet of Things enable Augmented Personalized Health?          ,1801.00356,https://arxiv.org/abs/1801.00356,https://arxiv.org/pdf/1801.00356,"Authors:AmitSheth,UtkarshaniJaimini,HongYungYip","        Internet-of-Things (IoT) is profoundly redefining the way we create, consume, and share information. Health aficionados and citizens are increasingly using IoT technologies to track their sleep, food intake, activity, vital body signals, and other physiological observations. This is complemented by IoT systems that continuously collect health-related data from the environment and inside the living quarters. Together, these have created an opportunity for a new generation of healthcare solutions. However, interpreting data to understand an individual's health is challenging. It is usually necessary to look at that individual's clinical record and behavioral information, as well as social and environmental information affecting that individual. Interpreting how well a patient is doing also requires looking at his adherence to respective health objectives, application of relevant clinical knowledge and the desired outcomes.  We resort to the vision of Augmented Personalized Healthcare (APH) to exploit the extensive variety of relevant data and medical knowledge using Artificial Intelligence (AI) techniques to extend and enhance human health to presents various stages of augmented health management strategies: self-monitoring, self-appraisal, self-management, intervention, and disease progress tracking and prediction. kHealth technology, a specific incarnation of APH, and its application to Asthma and other diseases are used to provide illustrations and discuss alternatives for technology-assisted health management. Several prominent efforts involving IoT and patient-generated health data (PGHD) with respect converting multimodal data into actionable information (big data to smart data) are also identified. Roles of three components in an evidence-based semantic perception approach- Contextualization, Abstraction, and Personalization are discussed.        △ Less","31 December, 2017","cs.CY,cs.AI",
              Co-Morbidity Exploration on Wearables Activity Data Using Unsupervised Pre-training and Multi-Task Learning          ,1712.09527,https://arxiv.org/abs/1712.09527,https://arxiv.org/pdf/1712.09527,"Authors:KaranAggarwal,ShafiqJoty,LuisF.Luque,JaideepSrivastava","        Physical activity and sleep play a major role in the prevention and management of many chronic conditions. It is not a trivial task to understand their impact on chronic conditions. Currently, data from electronic health records (EHRs), sleep lab studies, and activity/sleep logs are used. The rapid increase in the popularity of wearable health devices provides a significant new data source, making it possible to track the user's lifestyle real-time through web interfaces, both to consumer as well as their healthcare provider, potentially. However, at present there is a gap between lifestyle data (e.g., sleep, physical activity) and clinical outcomes normally captured in EHRs. This is a critical barrier for the use of this new source of signal for healthcare decision making. Applying deep learning to wearables data provides a new opportunity to overcome this barrier.  To address the problem of the unavailability of clinical data from a major fraction of subjects and unrepresentative subject populations, we propose a novel unsupervised (task-agnostic) time-series representation learning technique called act2vec. act2vec learns useful features by taking into account the co-occurrence of activity levels along with periodicity of human activity patterns. The learned representations are then exploited to boost the performance of disorder-specific supervised learning models. Furthermore, since many disorders are often related to each other, a phenomenon referred to as co-morbidity, we use a multi-task learning framework for exploiting the shared structure of disorder inducing life-style choices partially captured in the wearables data. Empirical evaluation using actigraphy data from 4,124 subjects shows that our proposed method performs and generalizes substantially better than the conventional time-series symbolic representational methods and task-specific deep learning models.        △ Less","27 December, 2017","cs.LG,cs.CY",
              Smart Fog: Fog Computing Framework for Unsupervised Clustering Analytics in Wearable Internet of Things          ,1712.09347,https://arxiv.org/abs/1712.09347,https://arxiv.org/pdf/1712.09347,"Authors:DebanjanBorthakur,HarishchandraDubey,NicholasConstant,LeslieMahler,KunalMankodiya","        The increasing use of wearables in smart telehealth generates heterogeneous medical big data. Cloud and fog services process these data for assisting clinical procedures. IoT based ehealthcare have greatly benefited from efficient data processing. This paper proposed and evaluated use of low resource machine learning on Fog devices kept close to the wearables for smart healthcare. In state of the art telecare systems, the signal processing and machine learning modules are deployed in the cloud for processing physiological data. We developed a prototype of Fog-based unsupervised machine learning big data analysis for discovering patterns in physiological data. We employed Intel Edison and Raspberry Pi as Fog computer in proposed architecture. We performed validation studies on real-world pathological speech data from in home monitoring of patients with Parkinson's disease (PD). Proposed architecture employed machine learning for analysis of pathological speech data obtained from smartwatches worn by the patients with PD. Results showed that proposed architecture is promising for low-resource clinical machine learning. It could be useful for other applications within wearable IoT for smart telehealth scenarios by translating machine learning approaches from the cloud backend to edge computing devices such as Fog.        △ Less","24 December, 2017",cs.CY,
              SoA-Fog: Secure Service-Oriented Edge Computing Architecture for Smart Health Big Data Analytics          ,1712.09098,https://arxiv.org/abs/1712.09098,https://arxiv.org/pdf/1712.09098,"Authors:RabindraK.Barik,HarishchandraDubey,KunalMankodiya","        The smart health paradigms employ Internet-connected wearables for telemonitoring, diagnosis for providing inexpensive healthcare solutions. Fog computing reduces latency and increases throughput by processing data near the body sensor network. In this paper, we proposed a secure serviceorientated edge computing architecture that is validated on recently released public dataset. Results and discussions support the applicability of proposed architecture for smart health applications. We proposed SoA-Fog i.e. a three-tier secure framework for efficient management of health data using fog devices. It discuss the security aspects in client layer, fog layer and the cloud layer. We design the prototype by using win-win spiral model with use case and sequence diagram. Overlay analysis was performed using proposed framework on malaria vector borne disease positive maps of Maharastra state in India from 2011 to 2014. The mobile clients were taken as test case. We performed comparative analysis between proposed secure fog framework and state-of-the art cloud-based framework.        △ Less","25 December, 2017",cs.DC,
              Bayesian Nonparametric Causal Inference: Information Rates and Learning Algorithms          ,1712.08914,https://arxiv.org/abs/1712.08914,https://arxiv.org/pdf/1712.08914,"Authors:AhmedM.Alaa,MihaelavanderSchaar","        We investigate the problem of estimating the causal effect of a treatment on individual subjects from observational data, this is a central problem in various application domains, including healthcare, social sciences, and online advertising. Within the Neyman Rubin potential outcomes model, we use the Kullback Leibler (KL) divergence between the estimated and true distributions as a measure of accuracy of the estimate, and we define the information rate of the Bayesian causal inference procedure as the (asymptotic equivalence class of the) expected value of the KL divergence between the estimated and true distributions as a function of the number of samples. Using Fano method, we establish a fundamental limit on the information rate that can be achieved by any Bayesian estimator, and show that this fundamental limit is independent of the selection bias in the observational data. We characterize the Bayesian priors on the potential (factual and counterfactual) outcomes that achieve the optimal information rate. As a consequence, we show that a particular class of priors that have been widely used in the causal inference literature cannot achieve the optimal information rate. On the other hand, a broader class of priors can achieve the optimal information rate. We go on to propose a prior adaptation procedure (which we call the information based empirical Bayes procedure) that optimizes the Bayesian prior by maximizing an information theoretic criterion on the recovered causal effects rather than maximizing the marginal likelihood of the observed (factual) data. Building on our analysis, we construct an information optimal Bayesian causal inference algorithm.        △ Less","21 January, 2018","stat.ME,cs.LG,stat.ML",10.1109/JSTSP.2018.2848230 
              Dropout Feature Ranking for Deep Learning Models          ,1712.08645,https://arxiv.org/abs/1712.08645,https://arxiv.org/pdf/1712.08645,"Authors:Chun-HaoChang,LadislavRampasek,AnnaGoldenberg","        Deep neural networks (DNNs) achieve state-of-the-art results in a variety of domains. Unfortunately, DNNs are notorious for their non-interpretability, and thus limit their applicability in hypothesis-driven domains such as biology and healthcare. Moreover, in the resource-constraint setting, it is critical to design tests relying on fewer more informative features leading to high accuracy performance within reasonable budget. We aim to close this gap by proposing a new general feature ranking method for deep learning. We show that our simple yet effective method performs on par or compares favorably to eight strawman, classical and deep-learning feature ranking methods in two simulations and five very different datasets on tasks ranging from classification to regression, in both static and time series scenarios. We also illustrate the use of our method on a drug response dataset and show that it identifies genes relevant to the drug-response.        △ Less","9 March, 2018","cs.LG,stat.ML",
              Predicting physiological developments from human gait using smartphone sensor data          ,1712.07958,https://arxiv.org/abs/1712.07958,https://arxiv.org/pdf/1712.07958,"Authors:UmairAhmed,MuhammadFaizyabAli,KashifJaved,HaroonAtiqueBabri","        Coronary artery disease, heart failure, angina pectoris and diabetes are among the leading causes of morbidity and mortality over the globe. Susceptibility to such disorders is compounded by changing lifestyles, poor dietary routines, aging and obesity. Besides, conventional diagnostics are limited in their capability to detect such pathologies at an early stage. This generates demand for automatic recommender systems that could effectively monitor and predict pathogenic behaviors in the body. To this end, we propose human gait analysis for predicting two important physiological parameters associated with different diseases, body mass index and age. Predicting age and body mass index by actively profiling the gait samples, could be further used for providing suitable healthcare recommendations. Existing strategies for predicting age and body mass index, however, necessitate stringent experimental settings for achieving appropriate performance. For instance, precisely recorded speech signals were recently used for predicting body mass indices of different subjects. Similarly, age groups were predicted by recording gait samples from on-body and wearable sensors. Such specialized methods limit active and convenient profiling of human age and body mass indices. We address these issues, by introducing smartphone sensors as a means for recording gait signals. Using on-board accelerometer and gyroscope helps in developing easy-to-use and accessible systems for predicting body mass index and age. To empirically show the effectiveness of our proposed methodology, we collected gait samples from sixty-three different subjects that were classified in body mass index and age groups using six well-known machine learning classifiers. We evaluated our system using two different windowing operations for feature extraction, namely Gaussian and Square.        △ Less","13 December, 2017",eess.SP,
              Optimal Cross Slice Orchestration for 5G Mobile Services          ,1712.05912,https://arxiv.org/abs/1712.05912,https://arxiv.org/pdf/1712.05912,"Authors:DinhThaiHoang,DusitNiyato,PingWang,AntonioDeDomenico,EmilioCalvaneseStrinati","        5G mobile networks encompass the capabilities of hosting a variety of services such as mobile social networks, multimedia delivery, healthcare, transportation, and public safety. Therefore, the major challenge in designing the 5G networks is how to support different types of users and applications with different quality-of-service requirements under a single physical network infrastructure. Recently, network slicing has been introduced as a promising solution to address this challenge. Network slicing allows programmable network instances which match the service requirements by using network virtualization technologies. However, how to efficiently allocate resources across network slices has not been well studied in the literature. Therefore, in this paper, we first introduce a model for orchestrating network slices based on the service requirements and available resources. Then, we propose a Markov decision process framework to formulate and determine the optimal policy that manages cross-slice admission control and resource allocation for the 5G networks. Through simulation results, we show that the proposed framework and solution are efficient not only in providing slice-as-a-service based on the service requirements, but also in maximizing the provider's revenue.        △ Less","16 December, 2017",cs.NI,
              IT Risk Assessment for Group6 Healthcare Clinic Report          ,1712.04560,https://arxiv.org/abs/1712.04560,https://arxiv.org/pdf/1712.04560,"Authors:IsraelCole,LiswaniSibamba,AbdiHilowle,JayeJallow","        Information security and privacy in the healthcare sector is an issue of growing importance. The adoption of digital patient records, increased regulation, provider consolidation and the increasing need for information exchange between patients, providers and payers, all point towards the need for better information security. We critically survey the literature on information security and privacy in healthcare, published in information systems journals as well as many other related disciplines including health informatics, public health, law, medicine, and the trade press and industry reports        △ Less","12 December, 2017",cs.CR,
              Exploiting WiFi Channel State Information for Residential Healthcare Informatics          ,1712.03401,https://arxiv.org/abs/1712.03401,https://arxiv.org/pdf/1712.03401,"Authors:BoTan,QingchaoChen,KevinChetty,KarlWoodbridge,WendaLi,RobertPiechocki","        Detection and interpretation of human activities have emerged as a challenging healthcare problem in areas such as assisted living and remote monitoring. Besides traditional approaches that rely on wearable devices and camera systems, WiFi based technologies are evolving as a promising solution for indoor monitoring and activity recognition. This is, in part, due to the pervasive nature of WiFi in residential settings such as homes and care facilities, and unobtrusive nature of WiFi based sensing. Advanced signal processing techniques can accurately extract WiFi channel status information (CSI) using commercial off-the-shelf (COTS) devices or bespoke hardware. This includes phase variations, frequency shifts and signal levels. In this paper, we describe the healthcare application of Doppler shifts in the WiFi CSI, caused by human activities which take place in the signal coverage area. The technique is shown to recognize different types of human activities and behaviour and be very suitable for applications in healthcare. Three experimental case studies are presented to illustrate the capabilities of WiFi CSI Doppler sensing in assisted living and residential care environments. We also discuss the potential opportunities and practical challenges for real-world scenarios.        △ Less","9 December, 2017",eess.SP,
              Systematizing Genome Privacy Research: A Privacy-Enhancing Technologies Perspective          ,1712.02193,https://arxiv.org/abs/1712.02193,https://arxiv.org/pdf/1712.02193,"Authors:AlexandrosMittos,BradleyMalin,EmilianoDeCristofaro","        Rapid advances in human genomics are enabling researchers to gain a better understanding of the role of the genome in our health and well-being, stimulating hope for more effective and cost efficient healthcare. However, this also prompts a number of security and privacy concerns stemming from the distinctive characteristics of genomic data. To address them, a new research community has emerged and produced a large number of publications and initiatives.  In this paper, we rely on a structured methodology to contextualize and provide a critical analysis of the current knowledge on privacy-enhancing technologies used for testing, storing, and sharing genomic data, using a representative sample of the work published in the past decade. We identify and discuss limitations, technical challenges, and issues faced by the community, focusing in particular on those that are inherently tied to the nature of the problem and are harder for the community alone to address. Finally, we report on the importance and difficulty of the identified challenges based on an online survey of genome data privacy experts        △ Less","17 August, 2018",cs.CR,
              A survey of a hurdle model for heavy-tailed data based on the generalized lambda distribution          ,1712.02183,https://arxiv.org/abs/1712.02183,https://arxiv.org/pdf/1712.02183,"Authors:DiegoMarcondes,CláudiaPeixoto,AnaCarolinaMaia","        In this survey we present an extensive research of the vast literature about the Generalized Lambda Distribution (GLD) and propose a hurdle, or two-way, model whose associated distribution is the GLD in order to meet the demand for a highly flexible model of heavy-tailed data with excess of zeros. We apply the developed models to a dataset consisting of yearly healthcare expenses, a typical example of heavy-tailed data with excess of zeros. The fitted models are compared with models based on the Generalised Pareto Distribution and it is established that the GLD models perform best.        △ Less","20 September, 2018",stat.AP,10.1080/03610926.2018.1549251 
              Memory-based Combination PUFs for Device Authentication in Embedded Systems          ,1712.01611,https://arxiv.org/abs/1712.01611,https://arxiv.org/pdf/1712.01611,"Authors:SoubhagyaSutar,ArnabRaha,VijayRaghunathan","        Embedded systems play a crucial role in fueling the growth of the Internet-of-Things (IoT) in application domains such as healthcare, home automation, transportation, etc. However, their increasingly network-connected nature, coupled with their ability to access potentially sensitive/confidential information, has given rise to many security and privacy concerns. An additional challenge is the growing number of counterfeit components in these devices, resulting in serious reliability and financial implications. Physically Unclonable Functions (PUFs) are a promising security primitive to help address these concerns. Memory-based PUFs are particularly attractive as they require minimal or no additional hardware for their operation. However, current memory-based PUFs utilize only a single memory technology for constructing the PUF, which has several disadvantages including making them vulnerable to security attacks. In this paper, we propose the design of a new memory-based combination PUF that intelligently combines two memory technologies, SRAM and DRAM, to overcome these shortcomings. The proposed combination PUF exhibits high entropy, supports a large number of challenge-response pairs, and is intrinsically reconfigurable. We have implemented the proposed combination PUF using a Terasic TR4-230 FPGA board and several off-the-shelf SRAMs and DRAMs. Experimental results demonstrate substantial improvements over current memory-based PUFs including the ability to resist various attacks. Extensive authentication tests across a wide temperature range (20 - 60 deg. Celsius) and accelerated aging (12 months) demonstrate the robustness of the proposed design, which achieves a 100% true-positive rate and 0% false-positive rate for authentication across these parameter ranges.        △ Less","5 December, 2017",cs.CR,
              Approximating the Sum of Independent Non-Identical Binomial Random Variables          ,1712.01410,https://arxiv.org/abs/1712.01410,https://arxiv.org/pdf/1712.01410,"Authors:BoxiangLiu,ThomasQuertermous","        The distribution of sum of independent non-identical binomial random variables is frequently encountered in areas such as genomics, healthcare, and operations research. Analytical solutions to the density and distribution are usually cumbersome to find and difficult to compute. Several methods have been developed to approximate the distribution, and among these is the saddlepoint approximation. However, implementation of the saddlepoint approximation is non-trivial and, to our knowledge, an R package is still lacking. In this paper, we implemented the saddlepoint approximation in the \textbf{sinib} package. We provide two examples to illustrate its usage. One example uses simulated data while the other uses real-world healthcare data. The \textbf{sinib} package addresses the gap between the theory and the implementation of approximating the sum of independent non-identical binomials.        △ Less","4 December, 2017",stat.CO,
              Learning the Probability of Activation in the Presence of Latent Spreaders          ,1712.00643,https://arxiv.org/abs/1712.00643,https://arxiv.org/pdf/1712.00643,"Authors:MaggieMakar,JohnGuttag,JennaWiens","        When an infection spreads in a community, an individual's probability of becoming infected depends on both her susceptibility and exposure to the contagion through contact with others. While one often has knowledge regarding an individual's susceptibility, in many cases, whether or not an individual's contacts are contagious is unknown. We study the problem of predicting if an individual will adopt a contagion in the presence of multiple modes of infection (exposure/susceptibility) and latent neighbor influence. We present a generative probabilistic model and a variational inference method to learn the parameters of our model. Through a series of experiments on synthetic data, we measure the ability of the proposed model to identify latent spreaders, and predict the risk of infection. Applied to a real dataset of 20,000 hospital patients, we demonstrate the utility of our model in predicting the onset of a healthcare associated infection using patient room-sharing and nurse-sharing networks. Our model outperforms existing benchmarks and provides actionable insights for the design and implementation of targeted interventions to curb the spread of infection.        △ Less","2 December, 2017","cs.SI,physics.soc-ph",
              Novel Exploration Techniques (NETs) for Malaria Policy Interventions          ,1712.00428,https://arxiv.org/abs/1712.00428,https://arxiv.org/pdf/1712.00428,"Authors:OliverBent,SekouL.Remy,StephenRoberts,AishaWalcott-Bryant","        The task of decision-making under uncertainty is daunting, especially for problems which have significant complexity. Healthcare policy makers across the globe are facing problems under challenging constraints, with limited tools to help them make data driven decisions. In this work we frame the process of finding an optimal malaria policy as a stochastic multi-armed bandit problem, and implement three agent based strategies to explore the policy space. We apply a Gaussian Process regression to the findings of each agent, both for comparison and to account for stochastic results from simulating the spread of malaria in a fixed population. The generated policy spaces are compared with published results to give a direct reference with human expert decisions for the same simulated population. Our novel approach provides a powerful resource for policy makers, and a platform which can be readily extended to capture future more nuanced policy spaces.        △ Less","1 December, 2017",cs.AI,
              Distributed Stratified Locality Sensitive Hashing for Critical Event Prediction in the Cloud          ,1712.00206,https://arxiv.org/abs/1712.00206,https://arxiv.org/pdf/1712.00206,"Authors:AlessandroDePalma,ErikHemberg,Una-MayO'Reilly","        The availability of massive healthcare data repositories calls for efficient tools for data-driven medicine. We introduce a distributed system for Stratified Locality Sensitive Hashing to perform fast similarity-based prediction on large medical waveform datasets. Our implementation, for an ICU use case, prioritizes latency over throughput and is targeted at a cloud environment. We demonstrate our system on Acute Hypotensive Episode prediction from Arterial Blood Pressure waveforms. On a dataset of 1.371.37 million points, we show scaling up to 4040 processors and a 21×21\times speedup in number of comparisons to parallel exhaustive search at the price of a 10%10\% Matthews correlation coefficient (MCC) loss. Furthermore, if additional MCC loss can be tolerated, our system achieves speedups up to two orders of magnitude.        △ Less","1 December, 2017",cs.LG,
              Generative Adversarial Networks for Electronic Health Records: A Framework for Exploring and Evaluating Methods for Predicting Drug-Induced Laboratory Test Trajectories          ,1712.00164,https://arxiv.org/abs/1712.00164,https://arxiv.org/pdf/1712.00164,"Authors:AlexandreYahi,RamiVanguri,NoémieElhadad,NicholasP.Tatonetti","        Generative Adversarial Networks (GANs) represent a promising class of generative networks that combine neural networks with game theory. From generating realistic images and videos to assisting musical creation, GANs are transforming many fields of arts and sciences. However, their application to healthcare has not been fully realized, more specifically in generating electronic health records (EHR) data. In this paper, we propose a framework for exploring the value of GANs in the context of continuous laboratory time series data. We devise an unsupervised evaluation method that measures the predictive power of synthetic laboratory test time series. Further, we show that when it comes to predicting the impact of drug exposure on laboratory test data, incorporating representation learning of the training cohorts prior to training GAN models is beneficial.        △ Less","30 November, 2017","cs.LG,stat.ML",
              Towards Personalized Modeling of the Female Hormonal Cycle: Experiments with Mechanistic Models and Gaussian Processes          ,1712.00117,https://arxiv.org/abs/1712.00117,https://arxiv.org/pdf/1712.00117,"Authors:IñigoUrteaga,DavidJ.Albers,MarijaVlajicWheeler,AnnaDruet,HansRaffauf,NoémieElhadad","        In this paper, we introduce a novel task for machine learning in healthcare, namely personalized modeling of the female hormonal cycle. The motivation for this work is to model the hormonal cycle and predict its phases in time, both for healthy individuals and for those with disorders of the reproductive system. Because there are individual differences in the menstrual cycle, we are particularly interested in personalized models that can account for individual idiosyncracies, towards identifying phenotypes of menstrual cycles. As a first step, we consider the hormonal cycle as a set of observations through time. We use a previously validated mechanistic model to generate realistic hormonal patterns, and experiment with Gaussian process regression to estimate their values over time. Specifically, we are interested in the feasibility of predicting menstrual cycle phases under varying learning conditions: number of cycles used for training, hormonal measurement noise and sampling rates, and informed vs. agnostic sampling of hormonal measurements. Our results indicate that Gaussian processes can help model the female menstrual cycle. We discuss the implications of our experiments in the context of modeling the female menstrual cycle.        △ Less","30 November, 2017","stat.ML,cs.LG,stat.AP",
              Access control management for e-Healthcare in cloud environment          ,1711.10553,https://arxiv.org/abs/1711.10553,https://arxiv.org/pdf/1711.10553,"Authors:LiliSun,JianmingYong,JeffreySoar","        Semantic web technologies represent much richer forms of relationships among users, resources and actions among different web applications such as clouding computing. However, Semantic web applications pose new requirements for security mechanisms especially in the access control models. This paper addresses existing access control methods and presents a semantic based access control model which considers semantic relations among different entities in cloud computing environment. We have enriched the research for semantic web technology with role-based access control that is able to be applied in the field of medical information system or e-Healthcare system. This work shows how the semantic web technology provides efficient solutions for the management of complex and distributed data in heterogeneous systems, and it can be used in the medical information systems as well.        △ Less","25 October, 2017","cs.CR,cs.CY",10.4108/sis.1.2.e3 
              DFUNet: Convolutional Neural Networks for Diabetic Foot Ulcer Classification          ,1711.10448,https://arxiv.org/abs/1711.10448,https://arxiv.org/pdf/1711.10448,"Authors:ManuGoyal,NeilD.Reeves,AdrianK.Davison,SatyanRajbhandari,JenniferSpragg,MoiHoonYap","        Globally, in 2016, one out of eleven adults suffered from Diabetes Mellitus. Diabetic Foot Ulcers (DFU) are a major complication of this disease, which if not managed properly can lead to amputation. Current clinical approaches to DFU treatment rely on patient and clinician vigilance, which has significant limitations such as the high cost involved in the diagnosis, treatment and lengthy care of the DFU. We collected an extensive dataset of foot images, which contain DFU from different patients. In this paper, we have proposed the use of traditional computer vision features for detecting foot ulcers among diabetic patients, which represent a cost-effective, remote and convenient healthcare solution. Furthermore, we used Convolutional Neural Networks (CNNs) for the first time in DFU classification. We have proposed a novel convolutional neural network architecture, DFUNet, with better feature extraction to identify the feature differences between healthy skin and the DFU. Using 10-fold cross-validation, DFUNet achieved an AUC score of 0.962. This outperformed both the machine learning and deep learning classifiers we have tested. Here we present the development of a novel and highly sensitive DFUNet for objectively detecting the presence of DFUs. This novel approach has the potential to deliver a paradigm shift in diabetic foot care.        △ Less","10 December, 2017",cs.CV,
              Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram Completion          ,1711.10388,https://arxiv.org/abs/1711.10388,https://arxiv.org/pdf/1711.10388,"Authors:RushilAnirudh,HyojinKim,JayaramanJ.Thiagarajan,K.AdityaMohan,KyleChampley,TimoBremer","        Computed Tomography (CT) reconstruction is a fundamental component to a wide variety of applications ranging from security, to healthcare. The classical techniques require measuring projections, called sinograms, from a full 180∘^\circ view of the object. This is impractical in a limited angle scenario, when the viewing angle is less than 180∘^\circ, which can occur due to different factors including restrictions on scanning time, limited flexibility of scanner rotation, etc. The sinograms obtained as a result, cause existing techniques to produce highly artifact-laden reconstructions. In this paper, we propose to address this problem through implicit sinogram completion, on a challenging real world dataset containing scans of common checked-in luggage. We propose a system, consisting of 1D and 2D convolutional neural networks, that operates on a limited angle sinogram to directly produce the best estimate of a reconstruction. Next, we use the x-ray transform on this reconstruction to obtain a ""completed"" sinogram, as if it came from a full 180∘^\circ measurement. We feed this to standard analytical and iterative reconstruction techniques to obtain the final reconstruction. We show with extensive experimentation that this combined strategy outperforms many competitive baselines. We also propose a measure of confidence for the reconstruction that enables a practitioner to gauge the reliability of a prediction made by our network. We show that this measure is a strong indicator of quality as measured by the PSNR, while not requiring ground truth at test time. Finally, using a segmentation experiment, we show that our reconstruction preserves the 3D structure of objects effectively.        △ Less","11 July, 2018","cs.CV,stat.ML",
              Design of an Integrated Analytics Platform for Healthcare Assessment Centered on the Episode of Care          ,1711.09729,https://arxiv.org/abs/1711.09729,https://arxiv.org/pdf/1711.09729,"Authors:DouglasTeodoro,NilsRotgans,LucasOliveira,LilianCorreia","        Assessing care quality and performance is essential to improve healthcare processes and population health management. However, due to bad system design and lack of access to required data, this assessment is often delayed or not done at all. The goal of our research is to investigate an advanced analytics platform that enables healthcare quality and performance assessment. We used a user-centered design approach to identify the system requirements and have the concept of episode of care as the building block of information for a key performance indicator analytics system. We implemented architecture and interface prototypes, and performed a usability test with hospital users with managerial roles. The results show that by using user-centered design we created an analytical platform that provides a holistic and integrated view of the clinical, financial and operational aspects of the institution. Our encouraging results warrant further studies to understand other aspects of usability.        △ Less","14 November, 2017",cs.CY,
              Interpretable Convolutional Neural Networks for Effective Translation Initiation Site Prediction          ,1711.09558,https://arxiv.org/abs/1711.09558,https://arxiv.org/pdf/1711.09558,"Authors:JasperZuallaert,MijungKim,YvanSaeys,WesleyDeNeve","        Thanks to rapidly evolving sequencing techniques, the amount of genomic data at our disposal is growing increasingly large. Determining the gene structure is a fundamental requirement to effectively interpret gene function and regulation. An important part in that determination process is the identification of translation initiation sites. In this paper, we propose a novel approach for automatic prediction of translation initiation sites, leveraging convolutional neural networks that allow for automatic feature extraction. Our experimental results demonstrate that we are able to improve the state-of-the-art approaches with a decrease of 75.2% in false positive rate and with a decrease of 24.5% in error rate on chosen datasets. Furthermore, an in-depth analysis of the decision-making process used by our predictive model shows that our neural network implicitly learns biologically relevant features from scratch, without any prior knowledge about the problem at hand, such as the Kozak consensus sequence, the influence of stop and start codons in the sequence and the presence of donor splice site patterns. In summary, our findings yield a better understanding of the internal reasoning of a convolutional neural network when applying such a neural network to genomic data.        △ Less","27 November, 2017","q-bio.GN,cs.LG",
"              An Extremely Flexible, Energy, and Spectral Effective Green PHY-MAC for Profitable Ubiquitous Rural and Remote 5G/B5G IoT/M2M Communications          ",1711.09093,https://arxiv.org/abs/1711.09093,https://arxiv.org/pdf/1711.09093,Authors:AlexanderMarkhasin,"        In this paper, the fundamental throughput limits and extremums of the invariant criteria of energy, power and spectral efficiency of the physical layer (PHY) and medium access control (MAC) sublayer are proved. The invariant criteria are constructed relying on Shannon m-ary digital channel capacity which a rich palette of the technically interpreted physical and control parameters consider. Therefore, the invariant criteria as very suitable for research and design of the fifth generation (5G) communications extremely performance problems are found. The smart distributed control techniques which able implements on-the-fly the limits close and invariant criterion optimization or trade-off is proposed. Such smart control techniques represent a key disruptive technology meet the 5G and Beyond 5G network challenges.        △ Less","26 November, 2017",eess.SP,10.1007/978-3-319-46301-8 
              Interactive Complexity: Software Metrics from an Ecosystem Perspective          ,1711.09054,https://arxiv.org/abs/1711.09054,https://arxiv.org/pdf/1711.09054,"Authors:CharlesHathaway,RonEglash,MukkaiKrishnamoorthy","        With even the most trivial of applications now being written on top of millions of lines code of libraries, API's, and programming languages, much of the complexity that used to exist when designing software has been abstracted away to allow programmers to focus on primarily business logic. With each application relying so heavily on the ecosystem it was designed to run in, whether that is limited to a local system or includes dependencies on machines connected by networks, measuring the complexity of these systems can no longer be done simply by observing the code internal to the application; we also need to account for its external interactions. This is especially important when considering issues of security, which becomes more vital as our healthcare, financial, and automobiles rely on complicated software systems. We propose Interactive Complexity, which provide a quantitative measure of how intertwined parts of the system are. Some of the most well-known software complexity metrics out there are the metrics in the CK-metric suite; these metrics are designed for use in measuring object oriented systems, but we believe they can be adapted to help measure the interaction of software systems. Our experimental results show strong correlations between the number of bugs fixed in a release and the value of some of these metrics in systems of sufficient scale.        △ Less","28 November, 2017",cs.SE,
              Finding Algebraic Structure of Care in Time: A Deep Learning Approach          ,1711.07980,https://arxiv.org/abs/1711.07980,https://arxiv.org/pdf/1711.07980,"Authors:PhuocNguyen,TruyenTran,SvethaVenkatesh","        Understanding the latent processes from Electronic Medical Records could be a game changer in modern healthcare. However, the processes are complex due to the interaction between at least three dynamic components: the illness, the care and the recording practice. Existing methods are inadequate in capturing the dynamic structure of care. We propose an end-to-end model that reads medical record and predicts future risk. The model adopts the algebraic view in that discrete medical objects are embedded into continuous vectors lying in the same space. The bag of disease and comorbidities recorded at each hospital visit are modeled as function of sets. The same holds for the bag of treatments. The interaction between diseases and treatments at a visit is modeled as the residual of the diseases minus the treatments. Finally, the health trajectory, which is a sequence of visits, is modeled using a recurrent neural network. We report preliminary results on chronic diseases - diabetes and mental health - for predicting unplanned readmission.        △ Less","20 November, 2017",cs.LG,
              Effective Use of Bidirectional Language Modeling for Transfer Learning in Biomedical Named Entity Recognition          ,1711.07908,https://arxiv.org/abs/1711.07908,https://arxiv.org/pdf/1711.07908,"Authors:DevendraSinghSachan,PengtaoXie,MrinmayaSachan,EricPXing","        Biomedical named entity recognition (NER) is a fundamental task in text mining of medical documents and has many applications. Deep learning based approaches to this task have been gaining increasing attention in recent years as their parameters can be learned end-to-end without the need for hand-engineered features. However, these approaches rely on high-quality labeled data, which is expensive to obtain. To address this issue, we investigate how to use unlabeled text data to improve the performance of NER models. Specifically, we train a bidirectional language model (BiLM) on unlabeled data and transfer its weights to ""pretrain"" an NER model with the same architecture as the BiLM, which results in a better parameter initialization of the NER model. We evaluate our approach on four benchmark datasets for biomedical NER and show that it leads to a substantial improvement in the F1 scores compared with the state-of-the-art approaches. We also show that BiLM weight transfer leads to a faster model training and the pretrained model requires fewer training examples to achieve a particular F1 score.        △ Less","14 August, 2018",cs.CL,
"              Fundamentals of the Extremely Green, Flexible, and Profitable 5G M2M Ubiquitous Communications for Remote e-Healthcare and other Social e-Applications          ",1711.06469,https://arxiv.org/abs/1711.06469,https://arxiv.org/pdf/1711.06469,Authors:AlexanderMarkhasin,"        The revolutionary trend of the up-to-date medicine can be formulated as wide introduction into basic medicine fields of electronic (e-health) and mobile (m-health) healthcare services and information applications. Unfortunately, all list of qualified m/e-healthcare services can be provided cost-effectively only in urban areas very good covered by broadband 4G/5G wireless communications. Unacceptably high investments are required into deployment of the optic core infrastructure for ubiquitous wide covering of sparsely populated rural, remote, and difficult for access (RRD) areas using the recent (4G) and forthcoming (5G) broadband radio access (RAN) centralized techniques, characterized by short cells ranges, because their profitability boundary exceeds several hundred residents per square km. Furthermore, the unprecedented requirements and new features of the forthcoming Internet of Things (IoT), machine-to-machine (M2M), and many other machine type IT-systems lead to a breakthrough in designing extremely green, flexible, and cost-effective technologies for future 5G wireless systems which will be able to reach in real time the performance extremums, trade-off optimums and fundamental limits. This paper examines the 5G PHY-MAC fundamentals and extremely approaches to creation of the profitable ubiquitous remote e/m-health services and telemedicine as the main innovation technology of popular healthcare and other social e-Applications for RRD territories. Proposed approaches lean on summarizing and develop the results of our previous works on RRD-adapted profitable ubiquitous green 4G/5G wireless multifunctional technologies.        △ Less","17 November, 2017",cs.NI,
              Improving Palliative Care with Deep Learning          ,1711.06402,https://arxiv.org/abs/1711.06402,https://arxiv.org/pdf/1711.06402,"Authors:AnandAvati,KennethJung,StephanieHarman,LanceDowning,AndrewNg,NigamH.Shah","        Improving the quality of end-of-life care for hospitalized patients is a priority for healthcare organizations. Studies have shown that physicians tend to over-estimate prognoses, which in combination with treatment inertia results in a mismatch between patients wishes and actual care at the end of life. We describe a method to address this problem using Deep Learning and Electronic Health Record (EHR) data, which is currently being piloted, with Institutional Review Board approval, at an academic medical center. The EHR data of admitted patients are automatically evaluated by an algorithm, which brings patients who are likely to benefit from palliative care services to the attention of the Palliative Care team. The algorithm is a Deep Neural Network trained on the EHR data from previous years, to predict all-cause 3-12 month mortality of patients as a proxy for patients that could benefit from palliative care. Our predictions enable the Palliative Care team to take a proactive approach in reaching out to such patients, rather than relying on referrals from treating physicians, or conduct time consuming chart reviews of all patients. We also present a novel interpretation technique which we use to provide explanations of the model's predictions.        △ Less","16 November, 2017","cs.CY,cs.LG,stat.ML",
              Towards Deep Learning Models for Psychological State Prediction using Smartphone Data: Challenges and Opportunities          ,1711.06350,https://arxiv.org/abs/1711.06350,https://arxiv.org/pdf/1711.06350,"Authors:GatisMikelsons,MatthewSmith,AbhinavMehrotra,MircoMusolesi","        There is an increasing interest in exploiting mobile sensing technologies and machine learning techniques for mental health monitoring and intervention. Researchers have effectively used contextual information, such as mobility, communication and mobile phone usage patterns for quantifying individuals' mood and wellbeing. In this paper, we investigate the effectiveness of neural network models for predicting users' level of stress by using the location information collected by smartphones. We characterize the mobility patterns of individuals using the GPS metrics presented in the literature and employ these metrics as input to the network. We evaluate our approach on the open-source StudentLife dataset. Moreover, we discuss the challenges and trade-offs involved in building machine learning models for digital mental health and highlight potential future work in this direction.        △ Less","16 November, 2017","cs.LG,cs.AI,stat.ML",
              When Mobile Blockchain Meets Edge Computing          ,1711.05938,https://arxiv.org/abs/1711.05938,https://arxiv.org/pdf/1711.05938,"Authors:ZehuiXiong,YangZhang,DusitNiyato,PingWang,ZhuHan","        Blockchain, as the backbone technology of the current popular Bitcoin digital currency, has become a promising decentralized data management framework. Although blockchain has been widely adopted in many applications, e.g., finance, healthcare, and logistics, its application in mobile services is still limited. This is due to the fact that blockchain users need to solve preset proof-of-work puzzles to add new data, i.e., a block, to the blockchain. Solving the proof-of-work, however, consumes substantial resources in terms of CPU time and energy, which is not suitable for resource-limited mobile devices. To facilitate blockchain applications in future mobile Internet of Things systems, multiple access mobile edge computing appears to be an auspicious solution to solve the proof-of-work puzzles for mobile users. We first introduce a novel concept of edge computing for mobile blockchain. Then, we introduce an economic approach for edge computing resource management. Moreover, a prototype of mobile edge computing enabled blockchain systems is presented with experimental results to justify the proposed concept.        △ Less","11 April, 2018",cs.DC,
              Markov Decision Processes with Continuous Side Information          ,1711.05726,https://arxiv.org/abs/1711.05726,https://arxiv.org/pdf/1711.05726,"Authors:AdityaModi,NanJiang,SatinderSingh,AmbujTewari","        We consider a reinforcement learning (RL) setting in which the agent interacts with a sequence of episodic MDPs. At the start of each episode the agent has access to some side-information or context that determines the dynamics of the MDP for that episode. Our setting is motivated by applications in healthcare where baseline measurements of a patient at the start of a treatment episode form the context that may provide information about how the patient might respond to treatment decisions. We propose algorithms for learning in such Contextual Markov Decision Processes (CMDPs) under an assumption that the unobserved MDP parameters vary smoothly with the observed context. We also give lower and upper PAC bounds under the smoothness assumption. Because our lower bound has an exponential dependence on the dimension, we consider a tractable linear setting where the context is used to create linear combinations of a finite set of MDPs. For the linear setting, we give a PAC learning algorithm based on KWIK learning techniques.        △ Less","15 November, 2017","stat.ML,cs.AI,cs.LG",
              Analysis of the U.S. Patient Referral Network          ,1711.03245,https://arxiv.org/abs/1711.03245,https://arxiv.org/pdf/1711.03245,"Authors:ChuankaiAn,A.JamesO'Malley,DanielN.Rockmore,CoreyD.Stock","        In this paper we analyze the US Patient Referral Network (also called the Shared Patient Network) and various subnetworks for the years 2009--2015. In these networks two physicians are linked if a patient encounters both of them within a specified time-interval, according to the data made available by the Centers for Medicare and Medicaid Services. We find power law distributions on most state-level data as well as a core-periphery structure. On a national and state level, we discover a so-called small-world structure as well as a ""gravity law"" of the type found in some large-scale economic networks. Some physicians play the role of hubs for interstate referral. Strong correlations between certain network statistics with healthcare system statistics at both the state and national levels are discovered. The patterns in the referral network evinced using several statistical analyses involving key metrics derived from the network illustrate the potential for using network analysis to provide new insights into the healthcare system and opportunities or mechanisms for catalyzing improvements.        △ Less","8 November, 2017","cs.SI,physics.soc-ph",
              A Review of Privacy and Consent Management in Healthcare: A Focus on Emerging Data Sources          ,1711.00546,https://arxiv.org/abs/1711.00546,https://arxiv.org/pdf/1711.00546,"Authors:MuhammadRizwanAsghar,TzeHoweLee,MirzaMansoorBaig,EhsanUllah,GiovanniRussello,GillianDobbie","        The emergence of New Data Sources (NDS) in healthcare is revolutionising traditional electronic health records in terms of data availability, storage, and access. Increasingly, clinicians are using NDS to build a virtual holistic image of a patient's health condition. This research is focused on a review and analysis of the current legislation and privacy rules available for healthcare professionals. NDS in this project refers to and includes patient-generated health data, consumer device data, wearable health and fitness data, and data from social media.  This project reviewed legal and regulatory requirements for New Zealand, Australia, the European Union, and the United States to establish the ground reality of existing mechanisms in place concerning the use of NDS. The outcome of our research is to recommend changes and enhancements required to better prepare for the 'tsunami' of NDS and applications in the currently evolving data-driven healthcare area and precision or personalised health initiatives such as Precision Driven Health (PDH) in New Zealand.        △ Less","1 November, 2017",cs.CY,
              Re-DPoctor: Real-time health data releasing with w-day differential privacy          ,1711.00232,https://arxiv.org/abs/1711.00232,https://arxiv.org/pdf/1711.00232,"Authors:JiajunZhang,XiaohuiLiang,ZhikunZhang,ShiboHe,ZhiguoShi","        Wearable devices enable users to collect health data and share them with healthcare providers for improved health service. Since health data contain privacy-sensitive information, unprotected data release system may result in privacy leakage problem. Most of the existing work use differential privacy for private data release. However, they have limitations in healthcare scenarios because they do not consider the unique features of health data being collected from wearables, such as continuous real-time collection and pattern preservation. In this paper, we propose Re-DPoctor, a real-time health data releasing scheme with ww-day differential privacy where the privacy of health data collected from any consecutive ww days is preserved. We improve utility by using a specially-designed partition algorithm to protect the health data patterns. Meanwhile, we improve privacy preservation by applying newly proposed adaptive sampling technique and budget allocation method. We prove that Re-DPoctor satisfies ww-day differential privacy. Experiments on real health data demonstrate that our method achieves better utility with strong privacy guarantee than existing state-of-the-art methods.        △ Less","1 November, 2017",cs.CR,
              Synth-Validation: Selecting the Best Causal Inference Method for a Given Dataset          ,1711.00083,https://arxiv.org/abs/1711.00083,https://arxiv.org/pdf/1711.00083,"Authors:AlejandroSchuler,KenJung,RobertTibshirani,TrevorHastie,NigamShah","        Many decisions in healthcare, business, and other policy domains are made without the support of rigorous evidence due to the cost and complexity of performing randomized experiments. Using observational data to answer causal questions is risky: subjects who receive different treatments also differ in other ways that affect outcomes. Many causal inference methods have been developed to mitigate these biases. However, there is no way to know which method might produce the best estimate of a treatment effect in a given study. In analogy to cross-validation, which estimates the prediction error of predictive models applied to a given dataset, we propose synth-validation, a procedure that estimates the estimation error of causal inference methods applied to a given dataset. In synth-validation, we use the observed data to estimate generative distributions with known treatment effects. We apply each causal inference method to datasets sampled from these distributions and compare the effect estimates with the known effects to estimate error. Using simulations, we show that using synth-validation to select a causal inference method for each study lowers the expected estimation error relative to consistently using any single method.        △ Less","31 October, 2017",stat.ML,
              Trajectory Deformations from Physical Human-Robot Interaction          ,1710.09871,https://arxiv.org/abs/1710.09871,https://arxiv.org/pdf/1710.09871,"Authors:DylanP.Losey,MarciaK.O'Malley","        Robots are finding new applications where physical interaction with a human is necessary: manufacturing, healthcare, and social tasks. Accordingly, the field of physical human-robot interaction (pHRI) has leveraged impedance control approaches, which support compliant interactions between human and robot. However, a limitation of traditional impedance control is that---despite provisions for the human to modify the robot's current trajectory---the human cannot affect the robot's future desired trajectory through pHRI. In this paper, we present an algorithm for physically interactive trajectory deformations which, when combined with impedance control, allows the human to modulate both the actual and desired trajectories of the robot. Unlike related works, our method explicitly deforms the future desired trajectory based on forces applied during pHRI, but does not require constant human guidance. We present our approach and verify that this method is compatible with traditional impedance control. Next, we use constrained optimization to derive the deformation shape. Finally, we describe an algorithm for real time implementation, and perform simulations to test the arbitration parameters. Experimental results demonstrate reduction in the human's effort and improvement in the movement quality when compared to pHRI with impedance control alone.        △ Less","26 October, 2017",cs.RO,10.1109/TRO.2017.2765335 
              Benchmark of Deep Learning Models on Large Healthcare MIMIC Datasets          ,1710.08531,https://arxiv.org/abs/1710.08531,https://arxiv.org/pdf/1710.08531,"Authors:SanjayPurushotham,ChuizhengMeng,ZhengpingChe,YanLiu","        Deep learning models (aka Deep Neural Networks) have revolutionized many fields including computer vision, natural language processing, speech recognition, and is being increasingly used in clinical healthcare applications. However, few works exist which have benchmarked the performance of the deep learning models with respect to the state-of-the-art machine learning models and prognostic scoring systems on publicly available healthcare datasets. In this paper, we present the benchmarking results for several clinical prediction tasks such as mortality prediction, length of stay prediction, and ICD-9 code group prediction using Deep Learning models, ensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA scores. We used the Medical Information Mart for Intensive Care III (MIMIC-III) (v1.4) publicly available dataset, which includes all patients admitted to an ICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the benchmarking tasks. Our results show that deep learning models consistently outperform all the other approaches especially when the `raw' clinical time series data is used as input features to the models.        △ Less","23 October, 2017","cs.LG,cs.CY,stat.ML",
              Bringing Semantic Structures to User Intent Detection in Online Medical Queries          ,1710.08015,https://arxiv.org/abs/1710.08015,https://arxiv.org/pdf/1710.08015,"Authors:ChenweiZhang,NanDu,WeiFan,YaliangLi,Chun-TaLu,PhilipS.Yu","        The Internet has revolutionized healthcare by offering medical information ubiquitously to patients via web search. The healthcare status, complex medical information needs of patients are expressed diversely and implicitly in their medical text queries. Aiming to better capture a focused picture of user's medical-related information search and shed insights on their healthcare information access strategies, it is challenging yet rewarding to detect structured user intentions from their diversely expressed medical text queries. We introduce a graph-based formulation to explore structured concept transitions for effective user intent detection in medical queries, where each node represents a medical concept mention and each directed edge indicates a medical concept transition. A deep model based on multi-task learning is introduced to extract structured semantic transitions from user queries, where the model extracts word-level medical concept mentions as well as sentence-level concept transitions collectively. A customized graph-based mutual transfer loss function is designed to impose explicit constraints and further exploit the contribution of mentioning a medical concept word to the implication of a semantic transition. We observe an 8% relative improvement in AUC and 23% relative reduction in coverage error by comparing the proposed model with the best baseline model for the concept transition inference task on real-world medical text queries.        △ Less","22 October, 2017",cs.CL,
              Modeling Graphs Using a Mixture of Kronecker Models          ,1710.07231,https://arxiv.org/abs/1710.07231,https://arxiv.org/pdf/1710.07231,"Authors:SuchismitMahapatra,VarunChandola","        Generative models for graphs are increasingly becoming a popular tool for researchers to generate realistic approximations of graphs. While in the past, focus was on generating graphs which follow general laws, such as the power law for degree distribution, current models have the ability to learn from observed graphs and generate synthetic approximations. The primary emphasis of existing models has been to closely match different properties of a single observed graph. Such models, though stochastic, tend to generate samples which do not have significant variance in terms of the various graph properties. We argue that in many cases real graphs are sampled drawn from a graph population (e.g., networks sampled at various time points, social networks for individual schools, healthcare networks for different geographic regions, etc.). Such populations typically exhibit significant variance. However, existing models are not designed to model this variance, which could lead to issues such as overfitting. We propose a graph generative model that focuses on matching the properties of real graphs and the natural variance expected for the corresponding population. The proposed model adopts a mixture-model strategy to expand the expressiveness of Kronecker product based graph models (KPGM), while building upon the two strengths of KPGM, viz., ability to model several key properties of graphs and to scale to massive graph sizes using its elegant fractal growth based formulation. The proposed model, called x-Kronecker Product Graph Model, or xKPGM, allows scalable learning from observed graphs and generates samples that match the mean and variance of several salient graph properties. We experimentally demonstrate the capability of the proposed model to capture the inherent variability in real world graphs on a variety of publicly available graph data sets.        △ Less","19 October, 2017","cs.SI,physics.soc-ph",10.1109/BigData.2015.7363817 
              Robots as-a-Service in Cloud Computing: Search and Rescue in Large-scale Disasters Case Study          ,1710.04919,https://arxiv.org/abs/1710.04919,https://arxiv.org/pdf/1710.04919,"Authors:CarlaMouradian,SamiYangui,RochH.Glitho","        Internet of Things (IoT) is expected to enable a myriad of applications by interconnecting objects - such as sensors and robots - over the Internet. IoT applications range from healthcare to autonomous vehicles and include disaster management. Enabling these applications in cloud environments requires the design of appropriate IoT Infrastructure-as-a-Service (IoT IaaS) to ease the provisioning of the IoT objects as cloud services. This paper discusses a case study on search and rescue IoT applications in large-scale disaster scenarios. It proposes an IoT IaaS architecture that virtualizes robots (IaaS for robots) and provides them to the upstream applications as-a-Service. Node- and Network-level robots virtualization are supported. The proposed architecture meets a set of identified requirements, such as the need for a unified description model for heterogeneous robots, publication/discovery mechanism, and federation with other IaaS for robots when needed. A validating proof of concept is built and experiments are made to evaluate its performance. Lessons learned and prospective research directions are discussed.        △ Less","3 November, 2017",cs.DC,
              Big Data Analytics and Its Applications          ,1710.04135,https://arxiv.org/abs/1710.04135,https://arxiv.org/pdf/1710.04135,"Authors:MashooqueAhmedMemon,SafeeullahSoomro,AwaisKhanJumani,MuneerAhmedKartio","        The term, Big Data, has been authored to refer to the extensive heave of data that can't be managed by traditional data handling methods or techniques. The field of Big Data plays an indispensable role in various fields, such as agriculture, banking, data mining, education, chemistry, finance, cloud computing, marketing, health care stocks. Big data analytics is the method for looking at big data to reveal hidden patterns, incomprehensible relationship and other important data that can be utilize to resolve on enhanced decisions. There has been a perpetually expanding interest for big data because of its fast development and since it covers different areas of applications. Apache Hadoop open source technology created in Java and keeps running on Linux working framework was used. The primary commitment of this exploration is to display an effective and free solution for big data application in a distributed environment, with its advantages and indicating its easy use. Later on, there emerge to be a required for an analytical review of new developments in the big data technology. Healthcare is one of the best concerns of the world. Big data in healthcare imply to electronic health data sets that are identified with patient healthcare and prosperity. Data in the healthcare area is developing past managing limit of the healthcare associations and is relied upon to increment fundamentally in the coming years.        △ Less","9 October, 2017",cs.CY,
"              CHIPS: A Service for Collecting, Organizing, Processing, and Sharing Medical Image Data in the Cloud          ",1710.00734,https://arxiv.org/abs/1710.00734,https://arxiv.org/pdf/1710.00734,"Authors:RudolphPienaar,AtaTurk,JorgeBernal-Rusiel,NicolasRannou,DanielHaehn,P.EllenGrant,OrranKrieger","        Web browsers are increasingly used as middleware platforms offering a central access point for service provision. Using backend containerization, RESTful APIs, and distributed computing allows for complex systems to be realized that address the needs of modern compute intense environments. In this paper, we present a web-based medical image data and information management software platform called CHIPS (Cloud Healthcare Image Processing Service). This cloud-based services allows for authenticated and secure retrieval of medical image data from resources typically found in hospitals, organizes and presents information in a modern feed-like interface, provides access to a growing library of plugins that process these data, allows for easy data sharing between users and provides powerful 3D visualization and real-time collaboration. Image processing is orchestrated across additional cloud-based resources using containerization technologies.        △ Less","2 October, 2017","cs.DC,cs.CY",
              Checking and Enforcing Security through Opacity in Healthcare Applications          ,1710.00011,https://arxiv.org/abs/1710.00011,https://arxiv.org/pdf/1710.00011,"Authors:RymZrelli,MoezYeddes,NejibBenHadj-Alouane","        The Internet of Things (IoT) is a paradigm that can tremendously revolutionize health care thus benefiting both hospitals, doctors and patients. In this context, protecting the IoT in health care against interference, including service attacks and malwares, is challenging. Opacity is a confidentiality property capturing a system's ability to keep a subset of its behavior hidden from passive observers. In this work, we seek to introduce an IoT-based heart attack detection system, that could be life-saving for patients without risking their need for privacy through the verification and enforcement of opacity. Our main contributions are the use of a tool to verify opacity in three of its forms, so as to detect privacy leaks in our system. Furthermore, we develop an efficient, Symbolic Observation Graph (SOG)-based algorithm for enforcing opacity.        △ Less","29 September, 2017",cs.CR,
              Antibacterial properties of nonwoven wound dressings coated with Manuka honey or methylglyoxal          ,1709.09497,https://arxiv.org/abs/1709.09497,https://arxiv.org/pdf/1709.09497,"Authors:SophieE.L.Bulman,GiuseppeTronci,ParikshitGoswami,ChrisCarr,StephenJ.Russell","        Manuka honey (MH) is used as an antibacterial agent in bioactive wound dressings via direct impregnation onto a suitable substrate. MH provides unique antibacterial activity when compared with conventional honeys, owing partly to one of its constituents, methylglyoxal (MGO). Aiming to investigate an antibiotic-free antimicrobial strategy, we studied the antibacterial activity of both MH and MGO (at equivalent MGO concentrations) when applied as a physical coating to a nonwoven fabric wound dressing. When physically coated on to a cellulosic hydroentangled nonwoven fabric, it was found that concentrations of 0.0054 mg cm-2 of MGO in the form of MH and MGO was sufficient to achieve 100 CFU% bacteria reduction against gram-positive Staphylococcus aureus and gram-negative Klebsiella pneumoniae, based on BS EN ISO 20743:2007. A 3- to 20- fold increase in MGO concentration (0.0170 - 0.1 mg cm-2) was required to facilitate a good antibacterial effect (based on BS EN ISO 20645:2004) in terms of zone of inhibition and lack of growth under the sample. The minimum inhibitory concentration (MIC) and minimum bactericidal concentration (MBC) was also assessed for MGO in liquid form against three prevalent wound and healthcare-associated pathogens, i.e. Staphylococcus aureus, gram-negative Pseudomonas aeruginosa and gram-positive Enterococcus faecalis. Other than the case of MGO-containing fabrics, solutions with much higher MGO concentrations (128 mg L-1 - 1024 mg L-1) were required to provide either a bacteriostatic or bactericidal effect. The results presented in this study therefore demonstrate the relevance of MGO-based coating as an environment-friendly strategy for the design of functional dressings with antibiotic-free antimicrobial chemistries.        △ Less","13 August, 2017",q-bio.QM,
              Multi-Person Brain Activity Recognition via Comprehensive EEG Signal Analysis          ,1709.09077,https://arxiv.org/abs/1709.09077,https://arxiv.org/pdf/1709.09077,"Authors:XiangZhang,LinaYao,DalinZhang,XianzhiWang,QuanZ.Sheng,TaoGu","        An electroencephalography (EEG) based brain activity recognition is a fundamental field of study for a number of significant applications such as intention prediction, appliance control, and neurological disease diagnosis in smart home and smart healthcare domains. Existing techniques mostly focus on binary brain activity recognition for a single person, which limits their deployment in wider and complex practical scenarios. Therefore, multi-person and multi-class brain activity recognition has obtained popularity recently. Another challenge faced by brain activity recognition is the low recognition accuracy due to the massive noises and the low signal-to-noise ratio in EEG signals. Moreover, the feature engineering in EEG processing is time-consuming and highly re- lies on the expert experience. In this paper, we attempt to solve the above challenges by proposing an approach which has better EEG interpretation ability via raw Electroencephalography (EEG) signal analysis for multi-person and multi-class brain activity recognition. Specifically, we analyze inter-class and inter-person EEG signal characteristics, based on which to capture the discrepancy of inter-class EEG data. Then, we adopt an Autoencoder layer to automatically refine the raw EEG signals by eliminating various artifacts. We evaluate our approach on both a public and a local EEG datasets and conduct extensive experiments to explore the effect of several factors (such as normalization methods, training data size, and Autoencoder hidden neuron size) on the recognition results. The experimental results show that our approach achieves a high accuracy comparing to competitive state-of-the-art methods, indicating its potential in promoting future research on multi-person EEG recognition.        △ Less","26 September, 2017","cs.HC,cs.CV",
              On the economics of knowledge creation and sharing          ,1709.07390,https://arxiv.org/abs/1709.07390,https://arxiv.org/pdf/1709.07390,Authors:OmarMetwally,"        This work bridges the technical concepts underlying distributed computing and blockchain technologies with their profound socioeconomic and sociopolitical implications, particularly on academic research and the healthcare industry. Several examples from academia, industry, and healthcare are explored throughout this paper. The limiting factor in contemporary life sciences research is often funding: for example, to purchase expensive laboratory equipment and materials, to hire skilled researchers and technicians, and to acquire and disseminate data through established academic channels. In the case of the U.S. healthcare system, hospitals generate massive amounts of data, only a small minority of which is utilized to inform current and future medical practice. Similarly, corporations too expend large amounts of money to collect, secure and transmit data from one centralized source to another. In all three scenarios, data moves under the traditional paradigm of centralization, in which data is hosted and curated by individuals and organizations and of benefit to only a small subset of people.        △ Less","11 September, 2017",cs.CY,
              Secure and Trustable Electronic Medical Records Sharing using Blockchain          ,1709.06528,https://arxiv.org/abs/1709.06528,https://arxiv.org/pdf/1709.06528,"Authors:AlevtinaDubovitskaya,ZhigangXu,SamuelRyu,MichaelSchumacher,FushengWang","        Electronic medical records (EMRs) are critical, highly sensitive private information in healthcare, and need to be frequently shared among peers. Blockchain provides a shared, immutable and transparent history of all the transactions to build applications with trust, accountability and transparency. This provides a unique opportunity to develop a secure and trustable EMR data management and sharing system using blockchain. In this paper, we present our perspectives on blockchain based healthcare data management, in particular, for EMR data sharing between healthcare providers and for research studies. We propose a framework on managing and sharing EMR data for cancer patient care. In collaboration with Stony Brook University Hospital, we implemented our framework in a prototype that ensures privacy, security, availability, and fine-grained access control over EMR data. The proposed work can significantly reduce the turnaround time for EMR sharing, improve decision making for medical care, and reduce the overall cost        △ Less","2 August, 2017","cs.CY,cs.CR",
              Personalizing Path-Specific Effects          ,1709.03862,https://arxiv.org/abs/1709.03862,https://arxiv.org/pdf/1709.03862,"Authors:IlyaShpitser,SourjyaSarkar","        Unlike classical causal inference, which often has an average causal effect of a treatment within a population as a target, in settings such as personalized medicine, the goal is to map a given unit's characteristics to a treatment tailored to maximize the expected outcome for that unit. Obtaining high-quality mappings of this type is the goal of the dynamic regime literature (Chakraborty and Moodie 2013), with connections to reinforcement learning and experimental design. Aside from the average treatment effects, mechanisms behind causal relationships are also of interest. A well-studied approach to mechanism analysis is establishing average effects along with a particular set of causal pathways, in the simplest case the direct and indirect effects. Estimating such effects is the subject of the mediation analysis literature (Robins and Greenland 1992; Pearl 2001).  In this paper, we consider how unit characteristics may be used to tailor a treatment assignment strategy that maximizes a particular path-specific effect. In healthcare applications, finding such a policy is of interest if, for instance, we are interested in maximizing the chemical effect of a drug on an outcome (corresponding to the direct effect), while assuming drug adherence (corresponding to the indirect effect) is set to some reference level. To solve our problem, we define counterfactuals associated with path-specific effects of a policy, give a general identification algorithm for these counterfactuals, give a proof of completeness, and show how classification algorithms in machine learning (Chen, Zeng, and Kosorok 2016) may be used to find a high-quality policy. We validate our approach via a simulation study.        △ Less","12 September, 2017",stat.ME,
              Give Me a Like: How HIV/AIDS Nonprofit Organizations Can Engage Their Audience on Facebook          ,1709.03220,https://arxiv.org/abs/1709.03220,https://arxiv.org/pdf/1709.03220,"Authors:Yu-ChaoHuang,Yi-PinLin,GregoryD.Saxton","        With the rapid proliferation and adoption of social media among healthcare professionals and organizations, social media-based HIV/AIDS intervention programs have become increasingly popular. However, the question of the effectiveness of the HIV/AIDS messages disseminated via social media has received scant attention in the literature. The current study applies content analysis to examine the relationship between Facebook messaging strategies employed by 110 HIV/AIDS nonprofit organizations and audience reactions in the form of liking, commenting, and sharing behavior. The results reveal that HIV/AIDS nonprofit organizations often use informational messages as one-way communication with their audience instead of dialogic interactions. Some specific types of messages, such as medication-focused messages, engender better audience engagement, in contrast, event-related messages and call-to-action messages appear to translate into lower corresponding audience reactions. The findings provide guidance to HIV/AIDS organizations in developing effective social media communication strategies.        △ Less","10 September, 2017",cs.CY,
              Interakt---A Multimodal Multisensory Interactive Cognitive Assessment Tool          ,1709.01796,https://arxiv.org/abs/1709.01796,https://arxiv.org/pdf/1709.01796,Authors:DanielSonntag,"        Cognitive assistance may be valuable in applications for doctors and therapists that reduce costs and improve quality in healthcare systems. Use cases and scenarios include the assessment of dementia. In this paper, we present our approach to the (semi-)automatic assessment of dementia.        △ Less","6 September, 2017",cs.HC,
              Boosting Deep Learning Risk Prediction with Generative Adversarial Networks for Electronic Health Records          ,1709.01648,https://arxiv.org/abs/1709.01648,https://arxiv.org/pdf/1709.01648,"Authors:ZhengpingChe,YuCheng,ShuangfeiZhai,ZhaonanSun,YanLiu","        The rapid growth of Electronic Health Records (EHRs), as well as the accompanied opportunities in Data-Driven Healthcare (DDH), has been attracting widespread interests and attentions. Recent progress in the design and applications of deep learning methods has shown promising results and is forcing massive changes in healthcare academia and industry, but most of these methods rely on massive labeled data. In this work, we propose a general deep learning framework which is able to boost risk prediction performance with limited EHR data. Our model takes a modified generative adversarial network namely ehrGAN, which can provide plausible labeled EHR data by mimicking real patient records, to augment the training dataset in a semi-supervised learning manner. We use this generative model together with a convolutional neural network (CNN) based prediction model to improve the onset prediction performance. Experiments on two real healthcare datasets demonstrate that our proposed framework produces realistic data samples and achieves significant improvements on classification tasks with the generated data over several stat-of-the-art baselines.        △ Less","5 September, 2017","cs.LG,stat.ML",
              Virtual Reality: Blessings and Risk Assessment          ,1708.09540,https://arxiv.org/abs/1708.09540,https://arxiv.org/pdf/1708.09540,"Authors:AyushSharma,PiyushBajpai,SukhdevSingh,KiranKhatter","        Objectives: This paper presents an up-to-date overview of research performed in the Virtual Reality (VR) environment ranging from definitions, its presence in the various fields, and existing market players and their projects in the VR technology. Further an attempt is made to gain an insight on the psychological mechanism underlying experience in using VR device. Methods: Our literature survey is based on the research articles, analysis of the projects of various companies and their findings for different areas of interest. Findings: In our literature survey we observed that the recent advances in virtual reality enabling technologies have led to variety of virtual devices that facilitate people to interact with the digital world. In fact in the past two decades researchers have tried to integrate reality and VR in the form of intuitive computer interface. Improvements: This has led to variety of potential benefits of VR in many applications such as News, Healthcare, Entertainment, Tourism, Military and Defence etc. However despite the extensive research efforts in creating virtual system environments it is yet to become apparent in normal daily life.        △ Less","30 August, 2017",cs.HC,
"              Watch Me, but Don't Touch Me! Contactless Control Flow Monitoring via Electromagnetic Emanations          ",1708.09099,https://arxiv.org/abs/1708.09099,https://arxiv.org/pdf/1708.09099,"Authors:YiHan,SriharshaEtigowni,HuaLi,SamanZonouz,AthinaPetropulu","        Trustworthy operation of industrial control systems depends on secure and real-time code execution on the embedded programmable logic controllers (PLCs). The controllers monitor and control the critical infrastructures, such as electric power grids and healthcare platforms, and continuously report back the system status to human operators. We present Zeus, a contactless embedded controller security monitor to ensure its execution control flow integrity. Zeus leverages the electromagnetic emission by the PLC circuitry during the execution of the controller programs. Zeus's contactless execution tracking enables non-intrusive monitoring of security-critical controllers with tight real-time constraints. Those devices often cannot tolerate the cost and performance overhead that comes with additional traditional hardware or software monitoring modules. Furthermore, Zeus provides an air-gap between the monitor (trusted computing base) and the target (potentially compromised) PLC. This eliminates the possibility of the monitor infection by the same attack vectors. Zeus monitors for control flow integrity of the PLC program execution. Zeus monitors the communications between the human-machine interface and the PLC, and captures the control logic binary uploads to the PLC. Zeus exercises its feasible execution paths, and fingerprints their emissions using an external electromagnetic sensor. Zeus trains a neural network for legitimate PLC executions, and uses it at runtime to identify the control flow based on PLC's electromagnetic emissions. We implemented Zeus on a commercial Allen Bradley PLC, which is widely used in industry, and evaluated it on real-world control program executions. Zeus was able to distinguish between different legitimate and malicious executions with 98.9% accuracy and with zero overhead on PLC execution by design.        △ Less","29 August, 2017",cs.CR,10.1145/3133956.3134081 
              Clustering Patients with Tensor Decomposition          ,1708.08994,https://arxiv.org/abs/1708.08994,https://arxiv.org/pdf/1708.08994,"Authors:MatteoRuffini,RicardGavaldà,EstherLimón","        In this paper we present a method for the unsupervised clustering of high-dimensional binary data, with a special focus on electronic healthcare records. We present a robust and efficient heuristic to face this problem using tensor decomposition. We present the reasons why this approach is preferable for tasks such as clustering patient records, to more commonly used distance-based methods. We run the algorithm on two datasets of healthcare records, obtaining clinically meaningful results.        △ Less","29 August, 2017","stat.ML,cs.LG",
              How 5G (and concomitant technologies) will revolutionize healthcare,1708.08746,https://arxiv.org/abs/1708.08746,https://arxiv.org/pdf/1708.08746,"Authors:SiddiqueLatif,JunaidQadir,ShahzadFarooq,MuhammadAliImran","        In this paper, we build the case that 5G and concomitant emerging technologies (such as IoT, big data, artificial intelligence, and machine learning) will transform global healthcare systems in the near future. Our optimism around 5G-enabled healthcare stems from a confluence of significant technical pushes that are already at play: apart from the availability of high-throughput low-latency wireless connectivity, other significant factors include the democratization of computing through cloud computing; the democratization of AI and cognitive computing (e.g., IBM Watson); and the commoditization of data through crowdsourcing and digital exhaust. These technologies together can finally crack a dysfunctional healthcare system that has largely been impervious to technological innovations. We highlight the persistent deficiencies of the current healthcare system, and then demonstrate how the 5G-enabled healthcare revolution can fix these deficiencies. We also highlight open technical research challenges, and potential pitfalls, that may hinder the development of such a 5G-enabled health revolution.        △ Less","17 August, 2017",cs.CY,
              m-TSNE: A Framework for Visualizing High-Dimensional Multivariate Time Series          ,1708.07942,https://arxiv.org/abs/1708.07942,https://arxiv.org/pdf/1708.07942,"Authors:MinhNguyen,SanjayPurushotham,HienTo,CyrusShahabi","        Multivariate time series (MTS) have become increasingly common in healthcare domains where human vital signs and laboratory results are collected for predictive diagnosis. Recently, there have been increasing efforts to visualize healthcare MTS data based on star charts or parallel coordinates. However, such techniques might not be ideal for visualizing a large MTS dataset, since it is difficult to obtain insights or interpretations due to the inherent high dimensionality of MTS. In this paper, we propose 'm-TSNE': a simple and novel framework to visualize high-dimensional MTS data by projecting them into a low-dimensional (2-D or 3-D) space while capturing the underlying data properties. Our framework is easy to use and provides interpretable insights for healthcare professionals to understand MTS data. We evaluate our visualization framework on two real-world datasets and demonstrate that the results of our m-TSNE show patterns that are easy to understand while the other methods' visualization may have limitations in interpretability.        △ Less","26 August, 2017","cs.LG,stat.ME",
"              Modular Learning Component Attacks: Today's Reality, Tomorrow's Challenge          ",1708.07807,https://arxiv.org/abs/1708.07807,https://arxiv.org/pdf/1708.07807,"Authors:XinyangZhang,YujieJi,TingWang","        Many of today's machine learning (ML) systems are not built from scratch, but are compositions of an array of {\em modular learning components} (MLCs). The increasing use of MLCs significantly simplifies the ML system development cycles. However, as most MLCs are contributed and maintained by third parties, their lack of standardization and regulation entails profound security implications.  In this paper, for the first time, we demonstrate that potentially harmful MLCs pose immense threats to the security of ML systems. We present a broad class of {\em logic-bomb} attacks in which maliciously crafted MLCs trigger host systems to malfunction in a predictable manner. By empirically studying two state-of-the-art ML systems in the healthcare domain, we explore the feasibility of such attacks. For example, we show that, without prior knowledge about the host ML system, by modifying only 3.3{\textperthousand} of the MLC's parameters, each with distortion below 10−310^{-3}, the adversary is able to force the misdiagnosis of target victims' skin cancers with 100\% success rate. We provide analytical justification for the success of such attacks, which points to the fundamental characteristics of today's ML models: high dimensionality, non-linearity, and non-convexity. The issue thus seems fundamental to many ML systems. We further discuss potential countermeasures to mitigate MLC-based attacks and their potential technical challenges.        △ Less","25 August, 2017","cs.CR,cs.LG,stat.ML",
              Emotion Detection Using Noninvasive Low Cost Sensors          ,1708.06664,https://arxiv.org/abs/1708.06664,https://arxiv.org/pdf/1708.06664,"Authors:DanielaGirardi,FilippoLanubile,NicoleNovielli","        Emotion recognition from biometrics is relevant to a wide range of application domains, including healthcare. Existing approaches usually adopt multi-electrodes sensors that could be expensive or uncomfortable to be used in real-life situations. In this study, we investigate whether we can reliably recognize high vs. low emotional valence and arousal by relying on noninvasive low cost EEG, EMG, and GSR sensors. We report the results of an empirical study involving 19 subjects. We achieve state-of-the- art classification performance for both valence and arousal even in a cross-subject classification setting, which eliminates the need for individual training and tuning of classification models.        △ Less","22 August, 2017",cs.HC,
              The CARESSES EU-Japan project: making assistive robots culturally competent          ,1708.06276,https://arxiv.org/abs/1708.06276,https://arxiv.org/pdf/1708.06276,"Authors:BarbaraBruno,NakYoungChong,HirokoKamide,SanjeevKanoria,JaeryoungLee,YutoLim,AmitKumarPandey,ChrisPapadopoulos,IrenaPapadopoulos,FedericoPecora,AlessandroSaffiotti,AntonioSgorbissa","        The nursing literature shows that cultural competence is an important requirement for effective healthcare. We claim that personal assistive robots should likewise be culturally competent, that is, they should be aware of general cultural characteristics and of the different forms they take in different individuals, and take these into account while perceiving, reasoning, and acting. The CARESSES project is an Europe-Japan collaborative effort that aims at designing, developing and evaluating culturally competent assistive robots. These robots will be able to adapt the way they behave, speak and interact to the cultural identity of the person they assist. This paper describes the approach taken in the CARESSES project, its initial steps, and its future plans.        △ Less","21 August, 2017","cs.RO,cs.AI,cs.CY,cs.HC",
              An Improved Multi-Output Gaussian Process RNN with Real-Time Validation for Early Sepsis Detection          ,1708.05894,https://arxiv.org/abs/1708.05894,https://arxiv.org/pdf/1708.05894,"Authors:JosephFutoma,SanjayHariharan,MarkSendak,NathanBrajer,MeredithClement,ArmandoBedoya,CaraO'Brien,KatherineHeller","        Sepsis is a poorly understood and potentially life-threatening complication that can occur as a result of infection. Early detection and treatment improves patient outcomes, and as such it poses an important challenge in medicine. In this work, we develop a flexible classifier that leverages streaming lab results, vitals, and medications to predict sepsis before it occurs. We model patient clinical time series with multi-output Gaussian processes, maintaining uncertainty about the physiological state of a patient while also imputing missing values. The mean function takes into account the effects of medications administered on the trajectories of the physiological variables. Latent function values from the Gaussian process are then fed into a deep recurrent neural network to classify patient encounters as septic or not, and the overall model is trained end-to-end using back-propagation. We train and validate our model on a large dataset of 18 months of heterogeneous inpatient stays from the Duke University Health System, and develop a new ""real-time"" validation scheme for simulating the performance of our model as it will actually be used. Our proposed method substantially outperforms clinical baselines, and improves on a previous related model for detecting sepsis. Our model's predictions will be displayed in a real-time analytics dashboard to be used by a sepsis rapid response team to help detect and improve treatment of sepsis.        △ Less","19 August, 2017","stat.ML,stat.AP,stat.ME",
              Identifying Harm Events in Clinical Care through Medical Narratives          ,1708.04681,https://arxiv.org/abs/1708.04681,https://arxiv.org/pdf/1708.04681,"Authors:ArmanCohan,AllanFong,RajRatwani,NazliGoharian","        Preventable medical errors are estimated to be among the leading causes of injury and death in the United States. To prevent such errors, healthcare systems have implemented patient safety and incident reporting systems. These systems enable clinicians to report unsafe conditions and cases where patients have been harmed due to errors in medical care. These reports are narratives in natural language and while they provide detailed information about the situation, it is non-trivial to perform large scale analysis for identifying common causes of errors and harm to the patients. In this work, we present a method based on attentive convolutional and recurrent networks for identifying harm events in patient care and categorize the harm based on its severity level. We demonstrate that our methods can significantly improve the performance over existing methods in identifying harm in clinical care.        △ Less","15 August, 2017","cs.CL,cs.IR",10.1145/3107411.3107485 
              Real Time Analytics: Algorithms and Systems          ,1708.02621,https://arxiv.org/abs/1708.02621,https://arxiv.org/pdf/1708.02621,"Authors:ArunKejariwal,SanjeevKulkarni,KarthikRamasamy","        Velocity is one of the 4 Vs commonly used to characterize Big Data. In this regard, Forrester remarked the following in Q3 2014: ""The high velocity, white-water flow of data from innumerable real-time data sources such as market data, Internet of Things, mobile, sensors, click-stream, and even transactions remain largely unnavigated by most firms. The opportunity to leverage streaming analytics has never been greater."" Example use cases of streaming analytics include, but not limited to: (a) visualization of business metrics in real-time (b) facilitating highly personalized experiences (c) expediting response during emergencies. Streaming analytics is extensively used in a wide variety of domains such as healthcare, e-commerce, financial services, telecommunications, energy and utilities, manufacturing, government and transportation.  In this tutorial, we shall present an in-depth overview of streaming analytics - applications, algorithms and platforms - landscape. We shall walk through how the field has evolved over the last decade and then discuss the current challenges - the impact of the other three Vs, viz., Volume, Variety and Veracity, on Big Data streaming analytics. The tutorial is intended for both researchers and practitioners in the industry. We shall also present state-of-the-affairs of streaming analytics at Twitter.        △ Less","7 August, 2017","cs.DB,cs.LG",
              On the relative role of different age groups during epidemics associated with the respiratory syncytial virus          ,1708.02560,https://arxiv.org/abs/1708.02560,https://arxiv.org/pdf/1708.02560,"Authors:EdwardGoldstein,HieuH.Nguyen,PatrickLiu,CecileViboud,ClaudiaA.Steiner,ColinJ.Worby,MarcLipsitch","        Background: While RSV circulation results in high burden of hospitalization, particularly among infants, young children and the elderly, little is known about the role of different age groups in propagating annual RSV epidemics in the community.  Methods: During a communicable disease outbreak, some subpopulations may play a disproportionate role during the outbreak's ascent due to increased susceptibility and/or contact rates. Such subpopulations can be identified by considering the proportion that cases in a subpopulation represent among all cases in the population occurring before (Bp) and after the epidemic peak (Ap) to calculate the subpopulation's relative risk, RR=Bp/Ap. We estimated RR for several age groups using data on RSV hospitalizations in the US between 2001-2012 from the Healthcare Cost and Utilization Project (HCUP).  Results: Children aged 3-4y and 5-6y each had the highest RR estimate for 5/11 seasons in the data, with RSV hospitalization rates in infants being generally higher during seasons when children aged 5-6y had the highest RR estimates. Children aged 2y had the highest RR estimate during one season. RR estimates in infants and individuals aged 11y and older were mostly lower than in children aged 1-10y.  Conclusions: The RR estimates suggest that preschool and young school-age children have the leading relative roles during RSV epidemics. We hope that those results will aid in the design of RSV vaccination policies.        △ Less","8 August, 2017",q-bio.PE,
              Ensuring patients privacy in a cryptographic-based-electronic health records using bio-cryptography          ,1708.01643,https://arxiv.org/abs/1708.01643,https://arxiv.org/pdf/1708.01643,"Authors:AdebayoOmotosho,JusticeEmuoyibofarhe,ChristophMeinel","        Several recent works have proposed and implemented cryptography as a means to preserve privacy and security of patients health data. Nevertheless, the weakest point of electronic health record (EHR) systems that relied on these cryptographic schemes is key management. Thus, this paper presents the development of privacy and security system for cryptography-based-EHR by taking advantage of the uniqueness of fingerprint and iris characteristic features to secure cryptographic keys in a bio-cryptography framework. The results of the system evaluation showed significant improvements in terms of time efficiency of this approach to cryptographic-based-EHR. Both the fuzzy vault and fuzzy commitment demonstrated false acceptance rate (FAR) of 0%, which reduces the likelihood of imposters gaining successful access to the keys protecting patients protected health information. This result also justifies the feasibility of implementing fuzzy key binding scheme in real applications, especially fuzzy vault which demonstrated a better performance during key reconstruction.        △ Less","26 July, 2017",cs.CY,10.1504/IJEH.2017.10003030 
              Hashtag Healthcare: From Tweets to Mental Health Journals Using Deep Transfer Learning          ,1708.01372,https://arxiv.org/abs/1708.01372,https://arxiv.org/pdf/1708.01372,"Authors:BenjaminShickel,MartinHeesacker,SherryBenton,ParisaRashidi","        As the popularity of social media platforms continues to rise, an ever-increasing amount of human communication and self- expression takes place online. Most recent research has focused on mining social media for public user opinion about external entities such as product reviews or sentiment towards political news. However, less attention has been paid to analyzing users' internalized thoughts and emotions from a mental health perspective. In this paper, we quantify the semantic difference between public Tweets and private mental health journals used in online cognitive behavioral therapy. We will use deep transfer learning techniques for analyzing the semantic gap between the two domains. We show that for the task of emotional valence prediction, social media can be successfully harnessed to create more accurate, robust, and personalized mental health models. Our results suggest that the semantic gap between public and private self-expression is small, and that utilizing the abundance of available social media is one way to overcome the small sample sizes of mental health data, which are commonly limited by availability and privacy concerns.        △ Less","3 August, 2017","cs.CL,cs.CY",
              Towards Vision-Based Smart Hospitals: A System for Tracking and Monitoring Hand Hygiene Compliance          ,1708.00163,https://arxiv.org/abs/1708.00163,https://arxiv.org/pdf/1708.00163,"Authors:AlbertHaque,MichelleGuo,AlexandreAlahi,SerenaYeung,ZelunLuo,AlishaRege,JeffreyJopling,LanceDowning,WilliamBeninati,AmitSingh,TerryPlatchek,ArnoldMilstein,LiFei-Fei","        One in twenty-five patients admitted to a hospital will suffer from a hospital acquired infection. If we can intelligently track healthcare staff, patients, and visitors, we can better understand the sources of such infections. We envision a smart hospital capable of increasing operational efficiency and improving patient care with less spending. In this paper, we propose a non-intrusive vision-based system for tracking people's activity in hospitals. We evaluate our method for the problem of measuring hand hygiene compliance. Empirically, our method outperforms existing solutions such as proximity-based techniques and covert in-person observational studies. We present intuitive, qualitative results that analyze human movement patterns and conduct spatial analytics which convey our method's interpretability. This work is a step towards a computer-vision based smart hospital and demonstrates promising results for reducing hospital acquired infections.        △ Less","24 April, 2018",cs.CV,
              On Designing of a Low Leakage Patient-Centric Provider Network          ,1707.09675,https://arxiv.org/abs/1707.09675,https://arxiv.org/pdf/1707.09675,"Authors:YuchenZheng,KunLin,ThomasWhite,JeremyPickereign,GigiYuen-Reed","        When a patient in a provider network seeks services outside of their community, the community experiences a leakage. Leakage is undesirable as it typically leads to higher out-of-network cost for patient and increases barrier for care coordination, which is particularly problematic for Accountable Care Organization (ACO) as the in-network providers are financially responsible for patient quality and outcome. We aim to design a data-driven method to identify naturally occurring provider networks driven by diabetic patient choices, and understand the relationship among provider composition, patient composition, and service leakage pattern. We construct a healthcare provider network based on patients' historical medical insurance claims. A community detection algorithm is used to identify naturally occurring communities of collaborating providers. Finally, import-export analysis is conducted to benchmark their leakage pattern and identify further leakage reduction opportunity. The design yields six major provider communities with diverse profiles. Some communities are geographically concentrated, while others tend to draw patients with certain diabetic co-morbidities. Providers from the same healthcare institution are likely to be assigned to the same community. While most communities have high within-community utilization and spending, at 85% and 86% respectively, leakage still persists. Hence, we utilize a metric from import-export analysis to detect leakage, gaining insight on how to minimizing leakage. In conclusion, we identify patient-driven provider organization by surfacing providers who share a large number of patients. By analyzing the import-export behavior of each identified community using a novel approach and profiling community patient and provider composition we understand the key features of having a balanced number of PCP and specialists and provider heterogeneity.        △ Less","30 July, 2017",stat.AP,
              A Comparative Study of the Clinical use of Motion Analysis from Kinect Skeleton Data          ,1707.08813,https://arxiv.org/abs/1707.08813,https://arxiv.org/pdf/1707.08813,"Authors:SeanMaudsley-Barton,JamieMcPheey,AnthonyBukowski,DanielLeightley,MoiHoonYap","        The analysis of human motion as a clinical tool can bring many benefits such as the early detection of disease and the monitoring of recovery, so in turn helping people to lead independent lives. However, it is currently under used. Developments in depth cameras, such as Kinect, have opened up the use of motion analysis in settings such as GP surgeries, care homes and private homes. To provide an insight into the use of Kinect in the healthcare domain, we present a review of the current state of the art. We then propose a method that can represent human motions from time-series data of arbitrary length, as a single vector. Finally, we demonstrate the utility of this method by extracting a set of clinically significant features and using them to detect the age related changes in the motions of a set of 54 individuals, with a high degree of certainty (F1- score between 0.9 - 1.0). Indicating its potential application in the detection of a range of age-related motion impairments.        △ Less","31 July, 2017",cs.CV,10.1109/SMC.2017.8123052 
              Non-Stationary Bandits with Habituation and Recovery Dynamics          ,1707.08423,https://arxiv.org/abs/1707.08423,https://arxiv.org/pdf/1707.08423,"Authors:YonatanMintz,AnilAswani,PhilipKaminsky,ElenaFlowers,YoshimiFukuoka","        Many settings involve sequential decision-making where a set of actions can be chosen at each time step, each action provides a stochastic reward, and the distribution for the reward of each action is initially unknown. However, frequent selection of a specific action may reduce its expected reward, while abstaining from choosing an action may cause its expected reward to increase. Such non-stationary phenomena are observed in many real world settings such as personalized healthcare-adherence improving interventions and targeted online advertising. Though finding an optimal policy for general models with non-stationarity is PSPACE-complete, we propose and analyze a new class of models called ROGUE (Reducing or Gaining Unknown Efficacy) bandits, which we show in this paper can capture these phenomena and are amenable to the design of effective policies. We first present a consistent maximum likelihood estimator for the parameters of these models. Next, we construct finite sample concentration bounds that lead to an upper confidence bound policy called the ROGUE Upper Confidence Bound (ROGUE-UCB) algorithm. We prove that under proper conditions the ROGUE-UCB algorithm achieves logarithmic in time regret, unlike existing algorithms which result in linear regret. We conclude with a numerical experiment using real data from a personalized healthcare-adherence improving intervention to increase physical activity. In this intervention, the goal is to optimize the selection of messages (e.g., confidence increasing vs. knowledge increasing) to send to each individual each day to increase adherence and physical activity. Our results show that ROGUE-UCB performs better in terms of regret and average reward as compared to state of the art algorithms, and the use of ROGUE-UCB increases daily step counts by roughly 1,000 steps a day (about a half-mile more of walking) as compared to other algorithms.        △ Less","18 October, 2019","math.OC,cs.LG",
              NuCypher KMS: Decentralized key management system          ,1707.06140,https://arxiv.org/abs/1707.06140,https://arxiv.org/pdf/1707.06140,"Authors:MichaelEgorov,MacLaneWilkison,DavidNunez","        NuCypher KMS is a decentralized Key Management System (KMS) that addresses the limitations of using consensus networks to securely store and manipulate private, encrypted data. It provides encryption and cryptographic access controls, performed by a decentralized network, leveraging proxy re-encryption. Unlike centralized KMS as a service solutions, it doesn't require trusting a service provider. NuCypher KMS enables sharing of sensitive data for both decentralized and centralized applications, providing security infrastructure for applications from healthcare to identity management to decentralized content marketplaces. NuCypher KMS will be an essential part of decentralized applications, just as SSL/TLS is an essential part of every secure web application.        △ Less","12 November, 2017","cs.CR,cs.DC",
              A Flexible Approach for Finding Optimal Paths with Minimal Conflicts          ,1707.05383,https://arxiv.org/abs/1707.05383,https://arxiv.org/pdf/1707.05383,"Authors:JulianaBowles,MarcoB.Caminati","        Complex systems are usually modelled through a combination of structural and behavioural models, where separate behavioural models make it easier to design and understand partial behaviour. When partial models are combined, we need to guarantee that they are consistent, and several automated techniques have been developed to check this. We argue that in some cases it is impossible to guarantee total consistency, and instead we want to find execution paths across such models with minimal conflicts with respect to a certain metric of interest. We present an efficient and scalable solution to find optimal paths through a combination of the theorem prover Isabelle with the constraint solver Z3. Our approach has been inspired by a healthcare problem, namely how to detect conflicts between medications taken by patients with multiple chronic conditions, and how to find preferable alternatives automatically.        △ Less","13 July, 2017","cs.SE,cs.LO",
              Recognizing Abnormal Heart Sounds Using Deep Learning          ,1707.04642,https://arxiv.org/abs/1707.04642,https://arxiv.org/pdf/1707.04642,"Authors:JonathanRubin,RuiAbreu,AnuragGanguli,SaigopalNelaturi,IonMatei,KumarSricharan","        The work presented here applies deep learning to the task of automated cardiac auscultation, i.e. recognizing abnormalities in heart sounds. We describe an automated heart sound classification algorithm that combines the use of time-frequency heat map representations with a deep convolutional neural network (CNN). Given the cost-sensitive nature of misclassification, our CNN architecture is trained using a modified loss function that directly optimizes the trade-off between sensitivity and specificity. We evaluated our algorithm at the 2016 PhysioNet Computing in Cardiology challenge where the objective was to accurately classify normal and abnormal heart sounds from single, short, potentially noisy recordings. Our entry to the challenge achieved a final specificity of 0.95, sensitivity of 0.73 and overall score of 0.84. We achieved the greatest specificity score out of all challenge entries and, using just a single CNN, our algorithm differed in overall score by only 0.02 compared to the top place finisher, which used an ensemble approach.        △ Less","19 October, 2017","cs.SD,cs.CV",
              Arianna: towards a new paradigm for assistive technology at home          ,1707.03988,https://arxiv.org/abs/1707.03988,https://arxiv.org/pdf/1707.03988,"Authors:LucaBuoncompagni,BarbaraBruno,AntonellaGiuni,FulvioMastrogiovanni,RenatoZaccaria","        Providing elderly and people with special needs to retain their independence as long as possible is one of the biggest challenges of the society of tomorrow. Teseo, a startup company spinoff from the University of Genoa, aims at accelerating the transition towards a sustainable healthcare system. Teseo's first concept and product, Arianna, allows for the automated recognition of activities of daily living at home and acts as a wellbeing and healthcare personalized assistant. This abstract outlines the main concepts underlying its features and capabilities.        △ Less","13 July, 2017","cs.CY,cs.HC",
              Catching Zika Fever: Application of Crowdsourcing and Machine Learning for Tracking Health Misinformation on Twitter          ,1707.03778,https://arxiv.org/abs/1707.03778,https://arxiv.org/pdf/1707.03778,"Authors:AmiraGhenai,YelenaMejova","        In February 2016, World Health Organization declared the Zika outbreak a Public Health Emergency of International Concern. With developing evidence it can cause birth defects, and the Summer Olympics coming up in the worst affected country, Brazil, the virus caught fire on social media. In this work, use Zika as a case study in building a tool for tracking the misinformation around health concerns on Twitter. We collect more than 13 million tweets -- spanning the initial reports in February 2016 and the Summer Olympics -- regarding the Zika outbreak and track rumors outlined by the World Health Organization and Snopes fact checking website. The tool pipeline, which incorporates health professionals, crowdsourcing, and machine learning, allows us to capture health-related rumors around the world, as well as clarification campaigns by reputable health organizations. In the case of Zika, we discover an extremely bursty behavior of rumor-related topics, and show that, once the questionable topic is detected, it is possible to identify rumor-bearing tweets using automated techniques. Thus, we illustrate insights the proposed tools provide into potentially harmful information on social media, allowing public health researchers and practitioners to respond with a targeted and timely action.        △ Less","12 July, 2017","cs.SI,cs.CY",
              Mobile Quantification and Therapy Course Tracking for Gait Rehabilitation          ,1707.03275,https://arxiv.org/abs/1707.03275,https://arxiv.org/pdf/1707.03275,"Authors:JavierConteAlcaraz,SanamMoghaddamnia,JürgenPeissig","        This paper presents a novel autonomous quality metric to quantify the rehabilitations progress of subjects with knee/hip operations. The presented method supports digital analysis of human gait patterns using smartphones. The algorithm related to the autonomous metric utilizes calibrated acceleration, gyroscope and magnetometer signals from seven Inertial Measurement Unit attached on the lower body in order to classify and generate the grading system values. The developed Android application connects the seven Inertial Measurement Units via Bluetooth and performs the data acquisition and processing in real-time. In total nine features per acceleration direction and lower body joint angle are calculated and extracted in real-time to achieve a fast feedback to the user. We compare the classification accuracy and quantification capabilities of Linear Discriminant Analysis, Principal Component Analysis and Naive Bayes algorithms. The presented system is able to classify patients and control subjects with an accuracy of up to 100\%. The outcomes can be saved on the device or transmitted to treating physicians for later control of the subject's improvements and the efficiency of physiotherapy treatments in motor rehabilitation. The proposed autonomous quality metric solution bears great potential to be used and deployed to support digital healthcare and therapy.        △ Less","7 July, 2017","cs.CV,cs.IT",
              Achieving Secure and Differentially Private Computations in Multiparty Settings          ,1707.01871,https://arxiv.org/abs/1707.01871,https://arxiv.org/pdf/1707.01871,"Authors:AbbasAcar,Z.BerkayCelik,HidayetAksu,A.SelcukUluagac,PatrickMcDaniel","        Sharing and working on sensitive data in distributed settings from healthcare to finance is a major challenge due to security and privacy concerns. Secure multiparty computation (SMC) is a viable panacea for this, allowing distributed parties to make computations while the parties learn nothing about their data, but the final result. Although SMC is instrumental in such distributed settings, it does not provide any guarantees not to leak any information about individuals to adversaries. Differential privacy (DP) can be utilized to address this; however, achieving SMC with DP is not a trivial task, either. In this paper, we propose a novel Secure Multiparty Distributed Differentially Private (SM-DDP) protocol to achieve secure and private computations in a multiparty environment. Specifically, with our protocol, we simultaneously achieve SMC and DP in distributed settings focusing on linear regression on horizontally distributed data. That is, parties do not see each others' data and further, can not infer information about individuals from the final constructed statistical model. Any statistical model function that allows independent calculation of local statistics can be computed through our protocol. The protocol implements homomorphic encryption for SMC and functional mechanism for DP to achieve the desired security and privacy guarantees. In this work, we first introduce the theoretical foundation for the SM-DDP protocol and then evaluate its efficacy and performance on two different datasets. Our results show that one can achieve individual-level privacy through the proposed protocol with distributed DP, which is independently applied by each party in a distributed fashion. Moreover, our results also show that the SM-DDP protocol incurs minimal computational overhead, is scalable, and provides security and privacy guarantees.        △ Less","6 July, 2017",cs.CR,
              A Robust Interrupted Time Series Model for Analyzing Complex Healthcare Intervention Data          ,1707.01861,https://arxiv.org/abs/1707.01861,https://arxiv.org/pdf/1707.01861,"Authors:MaricelaCruz,MiriamBender,HernandoOmbao","        Current health policy calls for greater use of evidence based care delivery services to improve patient quality and safety outcomes. Care delivery is complex, with interacting and interdependent components that challenge traditional statistical analytic techniques, in particular when modeling a time series of outcomes data that might be ""interrupted"" by a change in a particular method of health care delivery. Interrupted time series (ITS) is a robust quasi-experimental design with the ability to infer the effectiveness of an intervention that accounts for data dependency. Current standardized methods for analyzing ITS data do not model changes in variation and correlation following the intervention. This is a key limitation since it is plausible for data variability and dependency to change because of the intervention. Moreover, present methodology either assumes a pre-specified interruption time point with an instantaneous effect or removes data for which the effect of intervention is not fully realized. In this paper, we describe and develop a novel `Robust-ITS' model that overcomes these omissions and limitations. The Robust-ITS model formally performs inference on: (a) identifying the change point; (b) differences in pre- and post-intervention correlation; (c) differences in the outcome variance pre- and post-intervention; and (d) differences in the mean pre- and post-intervention. We illustrate the proposed method by analyzing patient satisfaction data from a hospital that implemented and evaluated a new nursing care delivery model as the intervention of interest. The Robust-ITS model is implemented in a R Shiny toolbox which is freely available to the community.        △ Less","25 March, 2019",stat.AP,10.1002/sim.7443 
              Dynamic Shrinkage Processes          ,1707.00763,https://arxiv.org/abs/1707.00763,https://arxiv.org/pdf/1707.00763,"Authors:DanielR.Kowal,DavidS.Matteson,DavidRuppert","        We propose a novel class of dynamic shrinkage processes for Bayesian time series and regression analysis. Building upon a global-local framework of prior construction, in which continuous scale mixtures of Gaussian distributions are employed for both desirable shrinkage properties and computational tractability, we model dependence among the local scale parameters. The resulting processes inherit the desirable shrinkage behavior of popular global-local priors, such as the horseshoe prior, but provide additional localized adaptivity, which is important for modeling time series data or regression functions with local features. We construct a computationally efficient Gibbs sampling algorithm based on a Pólya-Gamma scale mixture representation of the proposed process. Using dynamic shrinkage processes, we develop a Bayesian trend filtering model that produces more accurate estimates and tighter posterior credible intervals than competing methods, and apply the model for irregular curve-fitting of minute-by-minute Twitter CPU usage data. In addition, we develop an adaptive time-varying parameter regression model to assess the efficacy of the Fama-French five-factor asset pricing model with momentum added as a sixth factor. Our dynamic analysis of manufacturing and healthcare industry data shows that with the exception of the market risk, no other risk factors are significant except for brief periods.        △ Less","23 February, 2018",stat.ME,10.1111/rssb.12325 
              Employing Emotion Cues to Verify Speakers in Emotional Talking Environments          ,1707.00137,https://arxiv.org/abs/1707.00137,https://arxiv.org/pdf/1707.00137,Authors:IsmailShahin,"        Usually, people talk neutrally in environments where there are no abnormal talking conditions such as stress and emotion. Other emotional conditions that might affect people talking tone like happiness, anger, and sadness. Such emotions are directly affected by the patient health status. In neutral talking environments, speakers can be easily verified, however, in emotional talking environments, speakers cannot be easily verified as in neutral talking ones. Consequently, speaker verification systems do not perform well in emotional talking environments as they do in neutral talking environments. In this work, a two-stage approach has been employed and evaluated to improve speaker verification performance in emotional talking environments. This approach employs speaker emotion cues (text-independent and emotion-dependent speaker verification problem) based on both Hidden Markov Models (HMMs) and Suprasegmental Hidden Markov Models (SPHMMs) as classifiers. The approach is comprised of two cascaded stages that combines and integrates emotion recognizer and speaker recognizer into one recognizer. The architecture has been tested on two different and separate emotional speech databases: our collected database and Emotional Prosody Speech and Transcripts database. The results of this work show that the proposed approach gives promising results with a significant improvement over previous studies and other approaches such as emotion-independent speaker verification approach and emotion-dependent speaker verification approach based completely on HMMs.        △ Less","1 July, 2017",cs.SD,10.1515/jisys-2014-0118 
              Physiology-Aware Rural Ambulance Routing          ,1706.10290,https://arxiv.org/abs/1706.10290,https://arxiv.org/pdf/1706.10290,"Authors:MohammadHosseini,RichardB.BerlinJr.,LuiSha","        In emergency patient transport from rural medical facility to center tertiary hospital, real-time monitoring of the patient in the ambulance by a physician expert at the tertiary center is crucial. While telemetry healthcare services using mobile networks may enable remote real-time monitoring of transported patients, physiologic measures and tracking are at least as important and requires the existence of high-fidelity communication coverage. However, the wireless networks along the roads especially in rural areas can range from 4G to low-speed 2G, some parts with communication breakage. From a patient care perspective, transport during critical illness can make route selection patient state dependent. Prompt decisions with the relative advantage of a longer more secure bandwidth route versus a shorter, more rapid transport route but with less secure bandwidth must be made. The trade-off between route selection and the quality of wireless communication is an important optimization problem which unfortunately has remained unaddressed by prior work.  In this paper, we propose a novel physiology-aware route scheduling approach for emergency ambulance transport of rural patients with acute, high risk diseases in need of continuous remote monitoring. We mathematically model the problem into an NP-hard graph theory problem, and approximate a solution based on a trade-off between communication coverage and shortest path. We profile communication along two major routes in a large rural hospital settings in Illinois, and use the traces to manifest the concept. Further, we design our algorithms and run preliminary experiments for scalability analysis. We believe that our scheduling techniques can become a compelling aid that enables an always-connected remote monitoring system in emergency patient transfer scenarios aimed to prevent morbidity and mortality with early diagnosis treatment.        △ Less","30 June, 2017",cs.NI,
              Collaborative-controlled LASSO for Constructing Propensity Score-based Estimators in High-Dimensional Data          ,1706.10029,https://arxiv.org/abs/1706.10029,https://arxiv.org/pdf/1706.10029,"Authors:ChengJu,RichardWyss,JessicaM.Franklin,SebastianSchneeweiss,JennyHäggström,MarkJ.vanderLaan","        Propensity score (PS) based estimators are increasingly used for causal inference in observational studies. However, model selection for PS estimation in high-dimensional data has received little attention. In these settings, PS models have traditionally been selected based on the goodness-of-fit for the treatment mechanism itself, without consideration of the causal parameter of interest. Collaborative minimum loss-based estimation (C-TMLE) is a novel methodology for causal inference that takes into account information on the causal parameter of interest when selecting a PS model. This ""collaborative learning"" considers variable associations with both treatment and outcome when selecting a PS model in order to minimize a bias-variance trade off in the estimated treatment effect. In this study, we introduce a novel approach for collaborative model selection when using the LASSO estimator for PS estimation in high-dimensional covariate settings. To demonstrate the importance of selecting the PS model collaboratively, we designed quasi-experiments based on a real electronic healthcare database, where only the potential outcomes were manually generated, and the treatment and baseline covariates remained unchanged. Results showed that the C-TMLE algorithm outperformed other competing estimators for both point estimation and confidence interval coverage. In addition, the PS model selected by C-TMLE could be applied to other PS-based estimators, which also resulted in substantive improvement for both point estimation and confidence interval coverage. We illustrate the discussed concepts through an empirical example comparing the effects of non-selective nonsteroidal anti-inflammatory drugs with selective COX-2 inhibitors on gastrointestinal complications in a population of Medicare beneficiaries.        △ Less","30 June, 2017","stat.ME,stat.CO,stat.ML",
              Summarization of ICU Patient Motion from Multimodal Multiview Videos          ,1706.09430,https://arxiv.org/abs/1706.09430,https://arxiv.org/pdf/1706.09430,"Authors:CarlosTorres,KennethRose,JeffreyC.Fried,B.S.Manjunath","        Clinical observations indicate that during critical care at the hospitals, patients sleep positioning and motion affect recovery. Unfortunately, there is no formal medical protocol to record, quantify, and analyze patient motion. There is a small number of clinical studies, which use manual analysis of sleep poses and motion recordings to support medical benefits of patient positioning and motion monitoring. Manual processes are not scalable, are prone to human errors, and strain an already taxed healthcare workforce. This study introduces DECU (Deep Eye-CU): an autonomous mulitmodal multiview system, which addresses these issues by autonomously monitoring healthcare environments and enabling the recording and analysis of patient sleep poses and motion. DECU uses three RGB-D cameras to monitor patient motion in a medical Intensive Care Unit (ICU). The algorithms in DECU estimate pose direction at different temporal resolutions and use keyframes to efficiently represent pose transition dynamics. DECU combines deep features computed from the data with a modified version of Hidden Markov Model to more flexibly model sleep pose duration, analyze pose patterns, and summarize patient motion. Extensive experimental results are presented. The performance of DECU is evaluated in ideal (BC: Bright and Clear/occlusion-free) and natural (DO: Dark and Occluded) scenarios at two motion resolutions in a mock-up and a real ICU. The results indicate that deep features allow DECU to match the classification performance of engineered features in BC scenes and increase the accuracy by up to 8% in DO scenes. In addition, the overall pose history summarization tracing accuracy shows an average detection rate of 85% in BC and of 76% in DO scenes. The proposed keyframe estimation algorithm allows DECU to reach an average 78% transition classification accuracy.        △ Less","28 June, 2017",cs.CV,
              Enabling Prescription-based Health Apps          ,1706.09407,https://arxiv.org/abs/1706.09407,https://arxiv.org/pdf/1706.09407,"Authors:VenetOsmani,StefanoForti,OscarMayora,DiegoConforti","        We describe an innovative framework for prescription of personalised health apps by integrating Personal Health Records (PHR) with disease-specific mobile applications for managing medical conditions and the communication with clinical professionals. The prescribed apps record multiple variables including medical history enriched with innovative features such as integration with medical monitoring devices and wellbeing trackers to provide patients and clinicians with a personalised support on disease management. Our framework is based on an existing PHR ecosystem called TreC, uniquely positioned between healthcare provider and the patients, which is being used by over 70.000 patients in Trentino region in Northern Italy. We also describe three important aspects of health app prescription and how medical information is automatically encoded through the TreC framework and is prescribed as a personalised app, ready to be installed in the patients' smartphone.        △ Less","29 June, 2017",cs.CY,
              Research Opportunities and Visions for Smart and Pervasive Health          ,1706.09372,https://arxiv.org/abs/1706.09372,https://arxiv.org/pdf/1706.09372,"Authors:ElizabethMynatt,GregoryD.Hager,SantoshKumar,MingLin,ShwetakPatel,JackStankovic,HelenWright","        Improving the health of the nation's population and increasing the capabilities of the US healthcare system to support diagnosis, treatment, and prevention of disease is a critical national and societal priority. In the past decade, tremendous advances in expanding computing capabilities--sensors, data analytics, networks, advanced imaging, and cyber-physical systems--have, and will continue to, enhance healthcare and health research, with resulting improvements in health and wellness. However, the cost and complexity of healthcare continues to rise alongside the impact of poor health on productivity and quality of life. What is lacking are transformative capabilities that address significant health and healthcare trends: the growing demands and costs of chronic disease, the greater responsibility placed on patients and informal caregivers, and the increasing complexity of health challenges in the US, including mental health, that are deeply rooted in a person's social and environmental context.        △ Less","28 June, 2017",cs.CY,
              Preserving Differential Privacy in Convolutional Deep Belief Networks          ,1706.08839,https://arxiv.org/abs/1706.08839,https://arxiv.org/pdf/1706.08839,"Authors:NhatHaiPhan,XintaoWu,DejingDou","        The remarkable development of deep learning in medicine and healthcare domain presents obvious privacy issues, when deep neural networks are built on users' personal and highly sensitive data, e.g., clinical records, user profiles, biomedical images, etc. However, only a few scientific studies on preserving privacy in deep learning have been conducted. In this paper, we focus on developing a private convolutional deep belief network (pCDBN), which essentially is a convolutional deep belief network (CDBN) under differential privacy. Our main idea of enforcing epsilon-differential privacy is to leverage the functional mechanism to perturb the energy-based objective functions of traditional CDBNs, rather than their results. One key contribution of this work is that we propose the use of Chebyshev expansion to derive the approximate polynomial representation of objective functions. Our theoretical analysis shows that we can further derive the sensitivity and error bounds of the approximate polynomial representation. As a result, preserving differential privacy in CDBNs is feasible. We applied our model in a health social network, i.e., YesiWell data, and in a handwriting digit dataset, i.e., MNIST data, for human behavior prediction, human behavior classification, and handwriting digit recognition tasks. Theoretical analysis and rigorous experimental evaluations show that the pCDBN is highly effective. It significantly outperforms existing solutions.        △ Less","22 April, 2018","cs.LG,stat.ML",
"              Fog Computing in Medical Internet-of-Things: Architecture, Implementation, and Applications          ",1706.08012,https://arxiv.org/abs/1706.08012,https://arxiv.org/pdf/1706.08012,"Authors:HarishchandraDubey,AdmirMonteiro,NicholasConstant,MohammadrezaAbtahi,DebanjanBorthakur,LeslieMahler,YanSun,QingYang,UmerAkbar,KunalMankodiya","        In the era when the market segment of Internet of Things (IoT) tops the chart in various business reports, it is apparently envisioned that the field of medicine expects to gain a large benefit from the explosion of wearables and internet-connected sensors that surround us to acquire and communicate unprecedented data on symptoms, medication, food intake, and daily-life activities impacting one's health and wellness. However, IoT-driven healthcare would have to overcome many barriers, such as: 1) There is an increasing demand for data storage on cloud servers where the analysis of the medical big data becomes increasingly complex, 2) The data, when communicated, are vulnerable to security and privacy issues, 3) The communication of the continuously collected data is not only costly but also energy hungry, 4) Operating and maintaining the sensors directly from the cloud servers are non-trial tasks. This book chapter defined Fog Computing in the context of medical IoT. Conceptually, Fog Computing is a service-oriented intermediate layer in IoT, providing the interfaces between the sensors and cloud servers for facilitating connectivity, data transfer, and queryable local database. The centerpiece of Fog computing is a low-power, intelligent, wireless, embedded computing node that carries out signal conditioning and data analytics on raw data collected from wearables or other medical sensors and offers efficient means to serve telehealth interventions. We implemented and tested an fog computing system using the Intel Edison and Raspberry Pi that allows acquisition, computing, storage and communication of the various medical data such as pathological speech data of individuals with speech disorders, Phonocardiogram (PCG) signal for heart rate estimation, and Electrocardiogram (ECG)-based Q, R, S detection.        △ Less","24 June, 2017",cs.DC,10.1007/978-3-319-58280-1_11 
              Heterogeneous MPSoCs for Mixed Criticality Systems: Challenges and Opportunities          ,1706.07429,https://arxiv.org/abs/1706.07429,https://arxiv.org/pdf/1706.07429,Authors:MohamedHassan,"        Due to their cost, performance, area, and energy efficiency, MPSoCs offer appealing architecture for emerging mixed criticality systems (MCS) such as driverless cars, smart power grids, and healthcare devices. Furthermore, heterogeneity of MPSoCs presents exceptional opportunities to satisfy the conflicting requirements of MCS. Seizing these opportunities is unattainable without addressing the associated challenges. We focus on four aspects of MCS that we believe are of most importance upon adopting MPSoCs: theoretical model, interference, data sharing, and security. We outline existing solutions, highlight the necessary considerations for MPSoCs including both opportunities they create and research directions yet to be explored.        △ Less","23 June, 2017",cs.DC,
              A Computer Vision Pipeline for Automated Determination of Cardiac Structure and Function and Detection of Disease by Two-Dimensional Echocardiography          ,1706.07342,https://arxiv.org/abs/1706.07342,https://arxiv.org/pdf/1706.07342,"Authors:JeffreyZhang,SravaniGajjala,PulkitAgrawal,GeoffreyH.Tison,LauraA.Hallock,LaurenBeussink-Nelson,EugeneFan,MandarA.Aras,ChaRandleJordan,KirstenE.Fleischmann,MichelleMelisko,AtifQasim,AlexeiEfros,SanjivJ.Shah,RuzenaBajcsy,RahulC.Deo","        Automated cardiac image interpretation has the potential to transform clinical practice in multiple ways including enabling low-cost serial assessment of cardiac function in the primary care and rural setting. We hypothesized that advances in computer vision could enable building a fully automated, scalable analysis pipeline for echocardiogram (echo) interpretation. Our approach entailed: 1) preprocessing; 2) convolutional neural networks (CNN) for view identification, image segmentation, and phasing of the cardiac cycle; 3) quantification of chamber volumes and left ventricular mass; 4) particle tracking to compute longitudinal strain; and 5) targeted disease detection. CNNs accurately identified views (e.g. 99% for apical 4-chamber) and segmented individual cardiac chambers. Cardiac structure measurements agreed with study report values (e.g. mean absolute deviations (MAD) of 7.7 mL/kg/m2 for left ventricular diastolic volume index, 2918 studies). We computed automated ejection fraction and longitudinal strain measurements (within 2 cohorts), which agreed with commercial software-derived values [for ejection fraction, MAD=5.3%, N=3101 studies; for strain, MAD=1.5% (n=197) and 1.6% (n=110)], and demonstrated applicability to serial monitoring of breast cancer patients for trastuzumab cardiotoxicity. Overall, we found that, compared to manual measurements, automated measurements had superior performance across seven internal consistency metrics with an average increase in the Spearman correlation coefficient of 0.05 (p=0.02). Finally, we developed disease detection algorithms for hypertrophic cardiomyopathy and cardiac amyloidosis, with C-statistics of 0.93 and 0.84, respectively. Our pipeline lays the groundwork for using automated interpretation to support point-of-care handheld cardiac ultrasound and large-scale analysis of the millions of echos archived within healthcare systems.        △ Less","12 January, 2018",cs.CV,
              Topic Modeling for Classification of Clinical Reports          ,1706.06177,https://arxiv.org/abs/1706.06177,https://arxiv.org/pdf/1706.06177,"Authors:EfsunSariogluKayi,KabirYadav,JamesM.Chamberlain,Hyeong-AhChoi","        Electronic health records (EHRs) contain important clinical information about patients. Efficient and effective use of this information could supplement or even replace manual chart review as a means of studying and improving the quality and safety of healthcare delivery. However, some of these clinical data are in the form of free text and require pre-processing before use in automated systems. A common free text data source is radiology reports, typically dictated by radiologists to explain their interpretations. We sought to demonstrate machine learning classification of computed tomography (CT) imaging reports into binary outcomes, i.e. positive and negative for fracture, using regular text classification and classifiers based on topic modeling. Topic modeling provides interpretable themes (topic distributions) in reports, a representation that is more compact than the commonly used bag-of-words representation and can be processed faster than raw text in subsequent automated processes. We demonstrate new classifiers based on this topic modeling representation of the reports. Aggregate topic classifier (ATC) and confidence-based topic classifier (CTC) use a single topic that is determined from the training dataset based on different measures to classify the reports on the test dataset. Alternatively, similarity-based topic classifier (STC) measures the similarity between the reports' topic distributions to determine the predicted class. Our proposed topic modeling-based classifier systems are shown to be competitive with existing text classification techniques and provides an efficient and interpretable representation.        △ Less","19 June, 2017",cs.CL,
              Dipole: Diagnosis Prediction in Healthcare via Attention-based Bidirectional Recurrent Neural Networks          ,1706.05764,https://arxiv.org/abs/1706.05764,https://arxiv.org/pdf/1706.05764,"Authors:FenglongMa,RadhaChitta,JingZhou,QuanzengYou,TongSun,JingGao","        Predicting the future health information of patients from the historical Electronic Health Records (EHR) is a core research task in the development of personalized healthcare. Patient EHR data consist of sequences of visits over time, where each visit contains multiple medical codes, including diagnosis, medication, and procedure codes. The most important challenges for this task are to model the temporality and high dimensionality of sequential EHR data and to interpret the prediction results. Existing work solves this problem by employing recurrent neural networks (RNNs) to model EHR data and utilizing simple attention mechanism to interpret the results. However, RNN-based approaches suffer from the problem that the performance of RNNs drops when the length of sequences is large, and the relationships between subsequent visits are ignored by current RNN-based approaches. To address these issues, we propose {\sf Dipole}, an end-to-end, simple and robust model for predicting patients' future health information. Dipole employs bidirectional recurrent neural networks to remember all the information of both the past visits and the future visits, and it introduces three attention mechanisms to measure the relationships of different visits for the prediction. With the attention mechanisms, Dipole can interpret the prediction results effectively. Dipole also allows us to interpret the learned medical code representations which are confirmed positively by medical experts. Experimental results on two real world EHR datasets show that the proposed Dipole can significantly improve the prediction accuracy compared with the state-of-the-art diagnosis prediction approaches and provide clinically meaningful interpretation.        △ Less","18 June, 2017",cs.LG,10.1145/3097983.3098088 
              A Large-Scale CNN Ensemble for Medication Safety Analysis          ,1706.05549,https://arxiv.org/abs/1706.05549,https://arxiv.org/pdf/1706.05549,"Authors:LiliyaAkhtyamova,AndreyIgnatov,JohnCardiff","        Revealing Adverse Drug Reactions (ADR) is an essential part of post-marketing drug surveillance, and data from health-related forums and medical communities can be of a great significance for estimating such effects. In this paper, we propose an end-to-end CNN-based method for predicting drug safety on user comments from healthcare discussion forums. We present an architecture that is based on a vast ensemble of CNNs with varied structural parameters, where the prediction is determined by the majority vote. To evaluate the performance of the proposed solution, we present a large-scale dataset collected from a medical website that consists of over 50 thousand reviews for more than 4000 drugs. The results demonstrate that our model significantly outperforms conventional approaches and predicts medicine safety with an accuracy of 87.17% for binary and 62.88% for multi-classification tasks.        △ Less","17 June, 2017","cs.IR,cs.CL",
              Applying Software Patterns to Address Interoperability in Blockchain-based Healthcare Apps          ,1706.03700,https://arxiv.org/abs/1706.03700,https://arxiv.org/pdf/1706.03700,"Authors:PengZhang,JulesWhite,DouglasC.Schmidt,GuntherLenz","        Since the inception of the Bitcoin technology, its underlying data structure--the blockchain--has garnered much attention due to properties such as decentralization, transparency, and immutability. These properties make blockchains suitable for apps that require disintermediation through trustless exchange, consistent and incorruptible transaction records, and operational models beyond cryptocurrency. In particular, blockchain and its smart contract capabilities have the potential to address healthcare interoperability issues, such as enabling effective interactions between users and medical applications, delivering patient data securely to a variety of organizations and devices, and improving the overall efficiency of medical practice workflow. Despite the interest in using blockchain for healthcare interoperability, however, little information is available on the concrete architectural styles and patterns for applying blockchain to healthcare apps. This paper provides an initial step in filling this gap by showing: (1) the features and implementation challenges in healthcare interoperability, (2) an end-to-end case study of a blockchain-based healthcare app we are developing, and (3) how applying foundational software patterns can help address common interoperability challenges faced by blockchain-based healthcare apps.        △ Less","5 June, 2017",cs.CY,
              Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis          ,1706.03446,https://arxiv.org/abs/1706.03446,https://arxiv.org/pdf/1706.03446,"Authors:BenjaminShickel,PatrickTighe,AzraBihorac,ParisaRashidi","        The past decade has seen an explosion in the amount of digital information stored in electronic health records (EHR). While primarily designed for archiving patient clinical information and administrative healthcare tasks, many researchers have found secondary use of these records for various clinical informatics tasks. Over the same period, the machine learning community has seen widespread advances in deep learning techniques, which also have been successfully applied to the vast amount of EHR data. In this paper, we review these deep EHR systems, examining architectures, technical aspects, and clinical applications. We also identify shortcomings of current techniques and discuss avenues of future research for EHR-based deep learning.        △ Less","23 February, 2018","cs.LG,stat.ML",10.1109/JBHI.2017.2767063 
              Group-Server Queues          ,1706.03341,https://arxiv.org/abs/1706.03341,https://arxiv.org/pdf/1706.03341,"Authors:Quan-LinLi,Jing-YuMa,MingzhouXie,LiXia","        By analyzing energy-efficient management of data centers, this paper proposes and develops a class of interesting {\it Group-Server Queues}, and establishes two representative group-server queues through loss networks and impatient customers, respectively. Furthermore, such two group-server queues are given model descriptions and necessary interpretation. Also, simple mathematical discussion is provided, and simulations are made to study the expected queue lengths, the expected sojourn times and the expected virtual service times. In addition, this paper also shows that this class of group-server queues are often encountered in many other practical areas including communication networks, manufacturing systems, transportation networks, financial networks and healthcare systems. Note that the group-server queues are always used to design effectively dynamic control mechanisms through regrouping and recombining such many servers in a large-scale service system by means of, for example, bilateral threshold control, and customers transfer to the buffer or server groups. This leads to the large-scale service system that is divided into several adaptive and self-organizing subsystems through scheduling of batch customers and regrouping of service resources, which make the middle layer of this service system more effectively managed and strengthened under a dynamic, real-time and even reward optimal framework. Based on this, performance of such a large-scale service system may be improved greatly in terms of introducing and analyzing such group-server queues. Therefore, not only analysis of group-server queues is regarded as a new interesting research direction, but there also exists many theoretical challenges, basic difficulties and open problems in the area of queueing networks.        △ Less","21 July, 2017","cs.PF,cs.IT,math.PR",
              Study of Vital Data Analysis Platform Using Wearable Sensor          ,1706.02557,https://arxiv.org/abs/1706.02557,https://arxiv.org/pdf/1706.02557,Authors:YojiYamato,"        In this paper, we propose a vital data analysis platform which resolves existing problems to utilize vital data for real-time actions. Recently, IoT technologies have been progressed but in the healthcare area, real-time actions based on analyzed vital data are not considered sufficiently yet. The causes are proper use of analyzing methods of stream / micro batch processing and network cost. To resolve existing problems, we propose our vital data analysis platform. Our platform collects vital data of Electrocardiograph and acceleration using an example of wearable vital sensor and analyzes them to extract posture, fatigue and relaxation in smart phones or cloud. Our platform can show analyzed dangerous posture or fatigue level change. We implemented the platform and we are now preparing a field test.        △ Less","6 June, 2017","cs.DC,cs.CY",
              Unlocking the Potential of Simulators: Design with RL in Mind          ,1706.02501,https://arxiv.org/abs/1706.02501,https://arxiv.org/pdf/1706.02501,"Authors:RikaAntonova,SilviaCruciani","        Using Reinforcement Learning (RL) in simulation to construct policies useful in real life is challenging. This is often attributed to the sequential decision making aspect: inaccuracies in simulation accumulate over multiple steps, hence the simulated trajectories diverge from what would happen in reality.  In our work we show the need to consider another important aspect: the mismatch in simulating control. We bring attention to the need for modeling control as well as dynamics, since oversimplifying assumptions about applying actions of RL policies could make the policies fail on real-world systems.  We design a simulator for solving a pivoting task (of interest in Robotics) and demonstrate that even a simple simulator designed with RL in mind outperforms high-fidelity simulators when it comes to learning a policy that is to be deployed on a real robotic system. We show that a phenomenon that is hard to model - friction - could be exploited successfully, even when RL is performed using a simulator with a simple dynamics and noise model. Hence, we demonstrate that as long as the main sources of uncertainty are identified, it could be possible to learn policies applicable to real systems even using a simple simulator.  RL-compatible simulators could open the possibilities for applying a wide range of RL algorithms in various fields. This is important, since currently data sparsity in fields like healthcare and education frequently forces researchers and engineers to only consider sample-efficient RL approaches. Successful simulator-aided RL could increase flexibility of experimenting with RL algorithms and help applying RL policies to real-world settings in fields where data is scarce. We believe that lessons learned in Robotics could help other fields design RL-compatible simulators, so we summarize our experience and conclude with suggestions.        △ Less","8 June, 2017","cs.LG,cs.RO",
              Beyond Volume: The Impact of Complex Healthcare Data on the Machine Learning Pipeline          ,1706.01513,https://arxiv.org/abs/1706.01513,https://arxiv.org/pdf/1706.01513,"Authors:KeithFeldman,LouisFaust,XianWu,ChaoHuang,NiteshV.Chawla","        From medical charts to national census, healthcare has traditionally operated under a paper-based paradigm. However, the past decade has marked a long and arduous transformation bringing healthcare into the digital age. Ranging from electronic health records, to digitized imaging and laboratory reports, to public health datasets, today, healthcare now generates an incredible amount of digital information. Such a wealth of data presents an exciting opportunity for integrated machine learning solutions to address problems across multiple facets of healthcare practice and administration. Unfortunately, the ability to derive accurate and informative insights requires more than the ability to execute machine learning models. Rather, a deeper understanding of the data on which the models are run is imperative for their success. While a significant effort has been undertaken to develop models able to process the volume of data obtained during the analysis of millions of digitalized patient records, it is important to remember that volume represents only one aspect of the data. In fact, drawing on data from an increasingly diverse set of sources, healthcare data presents an incredibly complex set of attributes that must be accounted for throughout the machine learning pipeline. This chapter focuses on highlighting such challenges, and is broken down into three distinct components, each representing a phase of the pipeline. We begin with attributes of the data accounted for during preprocessing, then move to considerations during model building, and end with challenges to the interpretation of model output. For each component, we present a discussion around data as it relates to the healthcare domain and offer insight into the challenges each may impose on the efficiency of machine learning techniques.        △ Less","26 January, 2018","cs.CY,cs.LG,stat.ML",10.1007/978-3-319-69775-8_9 
"              Stochastic Model Predictive Control: Output-Feedback, Duality and Guaranteed Performance          ",1706.00733,https://arxiv.org/abs/1706.00733,https://arxiv.org/pdf/1706.00733,"Authors:MartinASehr,RobertRBitmead","        A new formulation of Stochastic Model Predictive Output Feedback Control is presented and analyzed as a translation of Stochastic Optimal Output Feedback Control into a receding horizon setting. This requires lifting the design into a framework involving propagation of the conditional state density, the information state, via the Bayesian Filter and solution of the Stochastic Dynamic Programming Equation for an optimal feedback policy, both stages of which are computationally challenging in the general, nonlinear setup. The upside is that the clearance of three bottleneck aspects of Model Predictive Control is connate to the optimality: output feedback is incorporated naturally; dual regulation and probing of the control signal is inherent; closed-loop performance relative to infinite-horizon optimal control is guaranteed. While the methods are numerically formidable, our aim is to develop an approach to Stochastic Model Predictive Control with guarantees and, from there, to seek a less onerous approximation. To this end, we discuss in particular the class of Partially Observable Markov Decision Processes, to which our results extend seamlessly, and demonstrate applicability with an example in healthcare decision making, where duality and associated optimality in the control signal are required for satisfactory closed-loop behavior.        △ Less","2 June, 2017",math.OC,10.1016/j.automatica.2018.04.013 
              A Comparison of Spatial-based Targeted Disease Containment Strategies using Mobile Phone Data          ,1706.00690,https://arxiv.org/abs/1706.00690,https://arxiv.org/pdf/1706.00690,"Authors:StefaniaRubrichi,ZbigniewSmoreda,MircoMusolesi","        Epidemic outbreaks are an important healthcare challenge, especially in developing countries where they represent one of the major causes of mortality. Approaches that can rapidly target subpopulations for surveillance and control are critical for enhancing containment processes during epidemics.  Using a real-world dataset from Ivory Coast, this work presents an attempt to unveil the socio-geographical heterogeneity of disease transmission dynamics. By employing a spatially explicit meta-population epidemic model derived from mobile phone Call Detail Records (CDRs), we investigate how the differences in mobility patterns may affect the course of a realistic infectious disease outbreak. We consider different existing measures of the spatial dimension of human mobility and interactions, and we analyse their relevance in identifying the highest risk sub-population of individuals, as the best candidates for isolation countermeasures. The approaches presented in this paper provide further evidence that mobile phone data can be effectively exploited to facilitate our understanding of individuals' spatial behaviour and its relationship with the risk of infectious diseases' contagion. In particular, we show that CDRs-based indicators of individuals' spatial activities and interactions hold promise for gaining insight of contagion heterogeneity and thus for developing containment strategies to support decision-making during country-level pandemics.        △ Less","3 July, 2018","cs.SI,physics.soc-ph",
              Learning Bundled Care Opportunities from Electronic Medical Records          ,1706.00487,https://arxiv.org/abs/1706.00487,https://arxiv.org/pdf/1706.00487,"Authors:YouChen,AbelN.Kho,DavidLiebovitz,CatherineIvory,SarahOsmundson,JiangBian,BradleyA.Malin","        Objectives: The fee-for-service approach to healthcare leads to the management of a patient's conditions in an independent manner, inducing various negative consequences. It is recognized that a bundled care approach to healthcare-one that manages a collection of health conditions together-may enable greater efficacy and cost savings. However, it is not always evident which sets of conditions should be managed in a bundled program. Study Design: Retrospective inference of clusters of health conditions from an electronic medical record (EMR) system. A survey of healthcare experts to ascertain the plausibility of the clusters for bundled care programs. Methods: We designed a data-driven framework to infer clusters of health conditions via their shared clinical workflows according to EMR utilization by healthcare employees. We evaluated the framework with approximately 16,500 inpatient stays from a large medical center. The plausibility of the clusters for bundled care was assessed through a survey of a panel of healthcare experts using an analysis of variance (ANOVA) under a 95% confidence interval. Results: The framework inferred four condition clusters: 1) fetal abnormalities, 2) late pregnancies, 3) prostate problems, and 4) chronic diseases (with congestive heart failure featuring prominently). Each cluster was deemed plausible by the experts for bundled care. Conclusions: The findings suggest that data from EMRs may provide a basis for discovering new directions in bundled care. Still, translating such findings into actual care management will require further refinement, implementation, and evaluation.        △ Less","26 May, 2017",cs.CY,
              RFID-based Solutions for Smarter Healthcare,1705.09855,https://arxiv.org/abs/1705.09855,https://arxiv.org/pdf/1705.09855,Authors:CristinaTurcu.CornelTurcu,"        This paper proposes the application of RFID technology in healthcare industry based on its increased functionality, high reliability, easy-to-use capabilities and low cost. After a brief presentation of RFID technologies and their applications, the paper describes an RFID-based system that can provide efficient facilities to allow essential information management for emergency care across hospital boundaries. This system performs RFID-based identification of the patients, querying and retrieving medical data from various existing healthcare information systems, as well as storing and giving the most clinically significant information to the clinicians. Also, the system allows identifying and tracking RFID- tagged objects in order to provide new quality services for the mobility of objects.        △ Less","27 May, 2017",cs.CY,
              A Data-Driven Analysis of the Influence of Care Coordination on Trauma Outcome          ,1705.09713,https://arxiv.org/abs/1705.09713,https://arxiv.org/pdf/1705.09713,"Authors:YouChen,MayurB.Patel,CandaceD.McNaughton,BradleyA.Malin","        OBJECTIVE: To test the hypothesis that variation in care coordination is related to LOS. DESIGN We applied a spectral co-clustering methodology to simultaneously infer groups of patients and care coordination patterns, in the form of interaction networks of health care professionals, from electronic medical record (EMR) utilization data. The care coordination pattern for each patient group was represented by standard social network characteristics and its relationship with hospital LOS was assessed via a negative binomial regression with a 95% confidence interval. SETTING AND PATIENTS This study focuses on 5,588 adult patients hospitalized for trauma at the Vanderbilt University Medical Center. The EMRs were accessed by healthcare professionals from 179 operational areas during 158,467 operational actions. MAIN OUTCOME MEASURES: Hospital LOS for trauma inpatients, as an indicator of care coordination efficiency. RESULTS: Three general types of care coordination patterns were discovered, each of which was affiliated with a specific patient group. The first patient group exhibited the shortest hospital LOS and was managed by a care coordination pattern that involved the smallest number of operational areas (102 areas, as opposed to 125 and 138 for the other patient groups), but exhibited the largest number of collaborations between operational areas (e.g., an average of 27.1 connections per operational area compared to 22.5 and 23.3 for the other two groups). The hospital LOS for the second and third patient groups was 14 hours (P = 0.024) and 10 hours (P = 0.042) longer than the first patient group, respectively.        △ Less","26 May, 2017",cs.CY,
              FRAMR-EMR: Framework for Prognostic Predictive Model Development Using Electronic Medical Record Data with a Case Study in Osteoarthritis Risk          ,1705.09563,https://arxiv.org/abs/1705.09563,https://arxiv.org/pdf/1705.09563,"Authors:JasonBlack,AmandaTerry,DanielLizotte","        Background-Prognostic predictive models are used in the delivery of primary care to estimate a patients risk of future disease development. Electronic medical record, EMR, data can be used for the construction of these models. Objectives- To provide a framework for those seeking to develop prognostic predictive models using EMR data, and to illustrate these steps using osteoarthritis risk estimation as an example. FRAMR-EMR-The FRAmework for Modelling Risk from EMR data, FRAMR-EMR, was created, which outlines step-by-step guidance for the construction of a prognostic predictive model using EMR data. Throughout these steps, several potential pitfalls specific to using EMR data for predictive purposes are described and methods for addressing them are suggested. Case Study-We used the DELPHI, DELiver Primary Healthcare Information, database to develop our prognostic predictive model for estimation of osteoarthritis risk. We constructed a retrospective cohort of 28447 eligible primary care patients. Patients were included if they had an encounter with their primary care practitioner between 1 January 2008 and 31 December 2009. Patients were excluded if they had a diagnosis of osteoarthritis prior to baseline. Construction of a prognostic predictive model following FRAMR-EMR yielded a predictive model capable of estimating 5-year risk of osteoarthritis diagnosis. Logistic regression was used to predict osteoarthritis based on age, sex, BMI, previous leg injury, and osteoporosis. Internal validation of the models performance demonstrated good discrimination and moderate calibration. Conclusions-This study provides guidance to those interested in developing prognostic predictive models based on EMR data. The production of high quality prognostic predictive models allows for practitioner communication of accurately estimated risks of developing future disease among primary care patients.        △ Less","5 September, 2017",stat.AP,
              Grounded Recurrent Neural Networks          ,1705.08557,https://arxiv.org/abs/1705.08557,https://arxiv.org/pdf/1705.08557,"Authors:AnkitVani,YacineJernite,DavidSontag","        In this work, we present the Grounded Recurrent Neural Network (GRNN), a recurrent neural network architecture for multi-label prediction which explicitly ties labels to specific dimensions of the recurrent hidden state (we call this process ""grounding""). The approach is particularly well-suited for extracting large numbers of concepts from text. We apply the new model to address an important problem in healthcare of understanding what medical concepts are discussed in clinical text. Using a publicly available dataset derived from Intensive Care Units, we learn to label a patient's diagnoses and procedures from their discharge summary. Our evaluation shows a clear advantage to using our proposed architecture over a variety of strong baselines.        △ Less","23 May, 2017","stat.ML,cs.CL,cs.LG,cs.NE",
              TwiInsight: Discovering Topics and Sentiments from Social Media Datasets          ,1705.08094,https://arxiv.org/abs/1705.08094,https://arxiv.org/pdf/1705.08094,"Authors:ZhengkuiWang,GuangdongBai,SoumyadebChowdhury,QuanqingXu,ZhiLinSeow","        Social media platforms contain a great wealth of information which provides opportunities for us to explore hidden patterns or unknown correlations, and understand people's satisfaction with what they are discussing. As one showcase, in this paper, we present a system, TwiInsight which explores the insight of Twitter data. Different from other Twitter analysis systems, TwiInsight automatically extracts the popular topics under different categories (e.g., healthcare, food, technology, sports and transport) discussed in Twitter via topic modeling and also identifies the correlated topics across different categories. Additionally, it also discovers the people's opinions on the tweets and topics via the sentiment analysis. The system also employs an intuitive and informative visualization to show the uncovered insight. Furthermore, we also develop and compare six most popular algorithms - three for sentiment analysis and three for topic modeling.        △ Less","23 May, 2017","cs.IR,cs.CL",
              BAMHealthCloud: A Biometric Authentication and Data Management System for Healthcare Data in Cloud          ,1705.07121,https://arxiv.org/abs/1705.07121,https://arxiv.org/pdf/1705.07121,"Authors:KashishA.Shakil,FarhanaJ.Zareen,MansafAlam,SuraiyaJabin","        Advancements in healthcare industry with new technology and population growth has given rise to security threat to our most personal data. The healthcare data management system consists of records in different formats such as text, numeric, pictures and videos leading to data which is big and unstructured. Also, hospitals have several branches at different locations throughout a country and overseas. In view of these requirements a cloud based healthcare management system can be an effective solution for efficient health care data management. One of the major concerns of a cloud based healthcare system is the security aspect. It includes theft to identity, tax fraudulence, insurance frauds, medical frauds and defamation of high profile patients. Hence, a secure data access and retrieval is needed in order to provide security of critical medical records in health care management system. Biometric authentication mechanism is suitable in this scenario since it overcomes the limitations of token theft and forgetting passwords in conventional token id-password mechanism used for providing security. It also has high accuracy rate for secure data access and retrieval. In this paper we propose BAMHealthCloud which is a cloud based system for management of healthcare data, it ensures security of data through biometric authentication. It has been developed after performing a detailed case study on healthcare sector in a developing country. Training of the signature samples for authentication purpose has been performed in parallel on hadoop MapReduce framework using Resilient Backpropagation neural network. From rigorous experiments it can be concluded that it achieves a speedup of 9x, Equal error rate (EER) of 0.12, sensitivity of 0.98 and specificity of 0.95 as compared to other approaches existing in literature.        △ Less","19 May, 2017","cs.CR,cs.CY,cs.DC",
              Robust tracking of respiratory rate in high-dynamic range scenes using mobile thermal imaging          ,1705.06628,https://arxiv.org/abs/1705.06628,https://arxiv.org/pdf/1705.06628,"Authors:YoungjunCho,SimonJ.Julier,NicolaiMarquardt,NadiaBianchi-Berthouze","        The ability to monitor respiratory rate is extremely important for medical treatment, healthcare and fitness sectors. In many situations, mobile methods, which allow users to undertake every day activities, are required. However, current monitoring systems can be obtrusive, requiring users to wear respiration belts or nasal probes. Recent advances in thermographic systems have shrunk their size, weight and cost, to the point where it is possible to create smart-phone based respiration rate monitoring devices that are not affected by lighting conditions. However, mobile thermal imaging is challenged in scenes with high thermal dynamic ranges. This challenge is further amplified by general problems such as motion artifacts and low spatial resolution, leading to unreliable breathing signals. In this paper, we propose a novel and robust approach for respiration tracking which compensates for the negative effects of variations in the ambient temperature and motion artifacts and can accurately extract breathing rates in highly dynamic thermal scenes. It has three main contributions. The first is a novel Optimal Quantization technique which adaptively constructs a color mapping of absolute temperature to improve segmentation, classification and tracking. The second is the Thermal Gradient Flow method that computes thermal gradient magnitude maps to enhance accuracy of the nostril region tracking. Finally, we introduce the Thermal Voxel method to increase the reliability of the captured respiration signals compared to the traditional averaging method. We demonstrate the extreme robustness of our system to track the nostril-region and measure the respiratory rate in high dynamic range scenes.        △ Less","20 September, 2017","cs.CV,physics.med-ph",10.1364/BOE.8.004480 
              Impact on the Usage of Wireless Sensor Networks in Healthcare Sector          ,1705.06021,https://arxiv.org/abs/1705.06021,https://arxiv.org/pdf/1705.06021,"Authors:AhsanHumayun,MuneebNiaz,MuhammadUmar,MuhammadMujahid","        Recent advancement in the wireless sensor networks has provided a platform to numerous applications in healthcare sector. It has become an active research area due to its large scale potential. This research focuses on the application areas of wireless sensor networks specifically in the healthcare sector. In this work, we have tried to explain the different challenges faced by the WSNs in order to implement them. The different pros and cons of the WSNs in healthcare sector are also discussed. Some important parameters which can be used to evaluate the performance of the wireless sensor networks are also presented in this work. Wireless sensor networks have a tremendous future and it should be taken at its earliest because of the significant importance of the healthcare issues.        △ Less","17 May, 2017",cs.CY,
              ShortFuse: Biomedical Time Series Representations in the Presence of Structured Information          ,1705.04790,https://arxiv.org/abs/1705.04790,https://arxiv.org/pdf/1705.04790,"Authors:MadalinaFiterau,SuvratBhooshan,JasonFries,CharlesBournhonesque,JenniferHicks,EniHalilaj,ChristopherRé,ScottDelp","        In healthcare applications, temporal variables that encode movement, health status and longitudinal patient evolution are often accompanied by rich structured information such as demographics, diagnostics and medical exam data. However, current methods do not jointly optimize over structured covariates and time series in the feature extraction process. We present ShortFuse, a method that boosts the accuracy of deep learning models for time series by explicitly modeling temporal interactions and dependencies with structured covariates. ShortFuse introduces hybrid convolutional and LSTM cells that incorporate the covariates via weights that are shared across the temporal domain. ShortFuse outperforms competing models by 3% on two biomedical applications, forecasting osteoarthritis-related cartilage degeneration and predicting surgical outcomes for cerebral palsy patients, matching or exceeding the accuracy of models that use features engineered by domain experts.        △ Less","15 May, 2017",stat.ML,
              A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations          ,1705.01968,https://arxiv.org/abs/1705.01968,https://arxiv.org/pdf/1705.01968,"Authors:JosuaKrause,AritraDasgupta,JordanSwartz,YindalonAphinyanaphongs,EnricoBertini","        Human-in-the-loop data analysis applications necessitate greater transparency in machine learning models for experts to understand and trust their decisions. To this end, we propose a visual analytics workflow to help data scientists and domain experts explore, diagnose, and understand the decisions made by a binary classifier. The approach leverages ""instance-level explanations"", measures of local feature relevance that explain single instances, and uses them to build a set of visual representations that guide the users in their investigation. The workflow is based on three main visual representations and steps: one based on aggregate statistics to see how data distributes across correct / incorrect decisions; one based on explanations to understand which features are used to make these decisions; and one based on raw data, to derive insights on potential root causes for the observed patterns. The workflow is derived from a long-term collaboration with a group of machine learning and healthcare professionals who used our method to make sense of machine learning models they developed. The case study from this collaboration demonstrates that the proposed workflow helps experts derive useful knowledge about the model and the phenomena it describes, thus experts can generate useful hypotheses on how a model can be improved.        △ Less","1 October, 2017","stat.ML,cs.AI",
              A Time Series Analysis-Based Forecasting Framework for the Indian Healthcare Sector          ,1705.01144,https://arxiv.org/abs/1705.01144,https://arxiv.org/pdf/1705.01144,"Authors:JaydipSen,TamalDattaChaudhuri","        Designing efficient and robust algorithms for accurate prediction of stock market prices is one of the most exciting challenges in the field of time series analysis and forecasting. With the exponential rate of development and evolution of sophisticated algorithms and with the availability of fast computing platforms, it has now become possible to effectively and efficiently extract, store, process and analyze high volume of stock market data with diversity in its contents. Availability of complex algorithms which can execute very fast on parallel architecture over the cloud has made it possible to achieve higher accuracy in forecasting results while reducing the time required for computation. In this paper, we use the time series data of the healthcare sector of India for the period January 2010 till December 2016. We first demonstrate a decomposition approach of the time series and then illustrate how the decomposition results provide us with useful insights into the behavior and properties exhibited by the time series. Further, based on the structural analysis of the time series, we propose six different methods of forecasting for predicting the time series index of the healthcare sector. Extensive results are provided on the performance of the forecasting methods to demonstrate their effectiveness.        △ Less","25 April, 2017",q-fin.ST,
              Performance of Model Predictive Control of POMDPs          ,1704.07773,https://arxiv.org/abs/1704.07773,https://arxiv.org/pdf/1704.07773,"Authors:MartinA.Sehr,RobertR.Bitmead","        We revisit closed-loop performance guarantees for Model Predictive Control in the deterministic and stochastic cases, which extend to novel performance results applicable to receding horizon control of Partially Observable Markov Decision Processes. While performance guarantees similar to those achievable in deterministic Model Predictive Control can be obtained even in the stochastic case, the presumed stochastic optimal control law is intractable to obtain in practice. However, this intractability relaxes for a particular instance of stochastic systems, namely Partially Observable Markov Decision Processes, provided reasonable problem dimensions are taken. This motivates extending available performance guarantees to this particular class of systems, which may also be used to approximate general nonlinear dynamics via gridding of state, observation, and control spaces. We demonstrate applicability of the novel closed-loop performance results on a particular example in healthcare decision making, which relies explicitly on the duality of the control decisions associated with Stochastic Optimal Control in weighing appropriate appointment times, diagnostic tests, and medical intervention for treatment of a disease modeled by a Markov Chain.        △ Less","25 April, 2017",math.OC,10.23919/ECC.2018.8550570 
              Tractable Dual Optimal Stochastic Model Predictive Control: An Example in Healthcare,1704.07770,https://arxiv.org/abs/1704.07770,https://arxiv.org/pdf/1704.07770,"Authors:MartinA.Sehr,RobertR.Bitmead","        Output-Feedback Stochastic Model Predictive Control based on Stochastic Optimal Control for nonlinear systems is computationally intractable because of the need to solve a Finite Horizon Stochastic Optimal Control Problem. However, solving this problem leads to an optimal probing nature of the resulting control law, called dual control, which trades off benefits of exploration and exploitation. In practice, intractability of Stochastic Model Predictive Control is typically overcome by replacement of the underlying Stochastic Optimal Control problem by more amenable approximate surrogate problems, which however come at a loss of the optimal probing nature of the control signals. While probing can be superimposed in some approaches, this is done sub-optimally. In this paper, we examine approximation of the system dynamics by a Partially Observable Markov Decision Process with its own Finite Horizon Stochastic Optimal Control Problem, which can be solved for an optimal control policy, implemented in receding horizon fashion. This procedure enables maintaining probing in the control actions. We further discuss a numerical example in healthcare decision making, highlighting the duality in stochastic optimal receding horizon control.        △ Less","25 April, 2017",math.OC,10.1109/CCTA.2017.8062626 
              Surrogate-based artifact removal from single-channel EEG          ,1704.07603,https://arxiv.org/abs/1704.07603,https://arxiv.org/pdf/1704.07603,"Authors:MarioChavez,FannyGrosselin,AuroreBussalb,FabrizioDeVicoFallani,XavierNavarro-Sune","        The recent emergence and success of electroencephalography (EEG) in low-cost portable devices, has opened the door to a new generation of applications processing a small number of EEG channels for health monitoring and brain-computer interfacing. These recordings are, however, contaminated by many sources of noise degrading the signals of interest, thus compromising the interpretation of the underlying brain state. In this work, we propose a new data-driven algorithm to effectively remove ocular and muscular artifacts from single-channel EEG: the surrogate-based artifact removal (SuBAR). Methods: By means of the time-frequency analysis of surrogate data, our approach is able to identify and filter automatically ocular and muscular artifacts embedded in single-channel EEG. Results: In a comparative study using artificially contami- nated EEG signals, the efficacy of the algorithm in terms of noise removal and signal distortion was superior to other traditionally-employed single-channel EEG denoising techniques: wavelet thresholding and the canonical correlation analysis combined with an advanced version of the empirical mode decomposition. Even in the presence of mild and severe artifacts, our artifact removal method provides a relative error 4 to 5 times lower than traditional techniques. Significance: In view of these results, the SuBAR method is a promising solution for mobile environments, such as ambulatory healthcare systems, sleep stage scoring or anesthesia monitoring, where very few EEG channels or even a single channel is available.        △ Less","25 January, 2018","physics.data-an,physics.med-ph",10.1109/TNSRE.2018.2794184 
              PPMF: A Patient-based Predictive Modeling Framework for Early ICU Mortality Prediction          ,1704.07499,https://arxiv.org/abs/1704.07499,https://arxiv.org/pdf/1704.07499,"Authors:MohammadAminMorid,OliviaR.LiuSheng,SamirAbdelrahman","        To date, developing a good model for early intensive care unit (ICU) mortality prediction is still challenging. This paper presents a patient based predictive modeling framework (PPMF) to improve the performance of ICU mortality prediction using data collected during the first 48 hours of ICU admission. PPMF consists of three main components verifying three related research hypotheses. The first component captures dynamic changes of patients status in the ICU using their time series data (e.g., vital signs and laboratory tests). The second component is a local approximation algorithm that classifies patients based on their similarities. The third component is a Gradient Decent wrapper that updates feature weights according to the classification feedback. Experiments using data from MIMICIII show that PPMF significantly outperforms: (1) the severity score systems, namely SASP III, APACHE IV, and MPM0III, (2) the aggregation based classifiers that utilize summarized time series, and (3) baseline feature selection methods.        △ Less","24 April, 2017","cs.LG,cs.AI",
              Leveraging Patient Similarity and Time Series Data in Healthcare Predictive Models          ,1704.07498,https://arxiv.org/abs/1704.07498,https://arxiv.org/pdf/1704.07498,"Authors:MohammadAminMorid,OliviaR.LiuSheng,SamirAbdelrahman","        Patient time series classification faces challenges in high degrees of dimensionality and missingness. In light of patient similarity theory, this study explores effective temporal feature engineering and reduction, missing value imputation, and change point detection methods that can afford similarity-based classification models with desirable accuracy enhancement. We select a piecewise aggregation approximation method to extract fine-grain temporal features and propose a minimalist method to impute missing values in temporal features. For dimensionality reduction, we adopt a gradient descent search method for feature weight assignment. We propose new patient status and directional change definitions based on medical knowledge or clinical guidelines about the value ranges for different patient status levels, and develop a method to detect change points indicating positive or negative patient status changes. We evaluate the effectiveness of the proposed methods in the context of early Intensive Care Unit mortality prediction. The evaluation results show that the k-Nearest Neighbor algorithm that incorporates methods we select and propose significantly outperform the relevant benchmarks for early ICU mortality prediction. This study makes contributions to time series classification and early ICU mortality prediction via identifying and enhancing temporal feature engineering and reduction methods for similarity-based time series classification.        △ Less","30 April, 2017","cs.AI,cs.LG",
"              Deep Learning for Medical Image Processing: Overview, Challenges and Future          ",1704.06825,https://arxiv.org/abs/1704.06825,https://arxiv.org/pdf/1704.06825,"Authors:MuhammadImranRazzak,SaeedaNaz,AhmadZaib","Healthcare sector is totally different from other industry. It is on high priority sector and people expect highest level of care and services regardless of cost. It did not achieve social expectation even though it consume huge percentage of budget. Mostly the interpretations of medical data is being done by medical expert. In terms of image interpretation by human expert, it is quite limited due to its subjectivity, the complexity of the image, extensive variations exist across different interpreters, and fatigue. After the success of deep learning in other real world application, it is also providing exciting solutions with good accuracy for medical imaging and is seen as a key method for future applications in health secotr. In this chapter, we discussed state of the art deep learning architecture and its optimization used for medical image segmentation and classification. In the last section, we have discussed the challenges deep learning based methods for medical imaging and open research issue.        △ Less","22 April, 2017",cs.CV,
              On-a-chip biosensing based on all-dielectric nanoresonators          ,1704.06465,https://arxiv.org/abs/1704.06465,https://arxiv.org/pdf/1704.06465,"Authors:OzlemYavas,MikaelSvedendahl,PaulinaDobosz,VanesaSanz,RomainQuidant","        Nanophotonics has become a key enabling technology in biomedicine with great promises in early diagnosis and less invasive therapies. In this context, the unique capability of plasmonic noble metal nanoparticles to concentrate light on the nanometer scale has widely contributed to biosensing and enhanced spectroscopy. Recently, high-refractive index dielectric nanostructures featuring low loss resonances have been proposed as a promising alternative to nanoplasmonics, potentially offering better sensing performances along with full compatibility with the microelectronics industry. In this letter we report the first demonstration of biosensing with silicon nanoresonators integrated in state-of-the-art microfluidics. Our lab-on-a-chip platform enables detecting Prostate Specific Antigen (PSA) cancer marker in human serum with a sensitivity that meets clinical needs. These performances are directly compared with its plasmonic counterpart based on gold nanorods. Our work opens new opportunities in the development of future point-of-care devices towards a more personalized healthcare.        △ Less","21 April, 2017",physics.app-ph,10.1021/acs.nanolett.7b01518 
              Proposal of Vital Data Analysis Platform using Wearable Sensor          ,1704.05573,https://arxiv.org/abs/1704.05573,https://arxiv.org/pdf/1704.05573,Authors:YojiYamato,"        In this paper, we propose a vital data analysis platform which resolves existing problems to utilize vital data for real-time actions. Recently, IoT technologies have been progressed but in the healthcare area, real-time actions based on analyzed vital data are not considered sufficiently yet. The causes are proper use of analyzing methods of stream / micro batch processing and network cost. To resolve existing problems, we propose our vital data analysis platform. Our platform collects vital data of Electrocardiograph and acceleration using an example of wearable vital sensor and analyzes them to extract posture, fatigue and relaxation in smart phones or cloud. Our platform can show analyzed dangerous posture or fatigue level change. We implemented the platform. And we are now preparing a field test.        △ Less","18 April, 2017","cs.DC,cs.CY",
Healthcare Robotics          ,1704.03931,https://arxiv.org/abs/1704.03931,https://arxiv.org/pdf/1704.03931,Authors:LaurelD.Riek,"        Robots have the potential to be a game changer in healthcare: improving health and well-being, filling care gaps, supporting care givers, and aiding health care workers. However, before robots are able to be widely deployed, it is crucial that both the research and industrial communities work together to establish a strong evidence-base for healthcare robotics, and surmount likely adoption barriers. This article presents a broad contextualization of robots in healthcare by identifying key stakeholders, care settings, and tasks; reviewing recent advances in healthcare robotics; and outlining major challenges and opportunities to their adoption.        △ Less","12 April, 2017","cs.RO,cs.HC",10.1145/3127874 
              Modelling collaborative services: The COSEMO model          ,1704.03740,https://arxiv.org/abs/1704.03740,https://arxiv.org/pdf/1704.03740,"Authors:ThanhThoaPhamThi,ThangLeDinh,MarkusHelfert,MichelLeonard","        Despite the dominance of the service sector in the last decades, there is still a need for a strong foundation on service design and innovation. Little attention has paid on service modelling, particularly in the collaboration context. Collaboration is considered as one of solutions for surviving or sustaining the business in the high competitive atmosphere. Collaborative services require various service providers working together according to agreements between them, along with service consumers, in order to co-produce services. In this paper, we address crucial issues in collaborative services such as collaboration levels, sharing data and processes due to business inter-dependencies between service stakeholders. Afterward, we propose a model for Collaborative Service Modelling, which is able to cover identified issues. We also apply our proposed model to modelling an example of healthcare services in order to illustrate the relevance of our modelling approach to the matter in hand.        △ Less","11 April, 2017",cs.SE,10.5220/0002929800790082 
              Next Generation Business Intelligence and Analytics: A Survey          ,1704.03402,https://arxiv.org/abs/1704.03402,https://arxiv.org/pdf/1704.03402,"Authors:QuocDuyVo,JayaThomas,ShinyoungCho,PradiptaDe,BongJunChoi,LeeSael","        Business Intelligence and Analytics (BI&A) is the process of extracting and predicting business-critical insights from data. Traditional BI focused on data collection, extraction, and organization to enable efficient query processing for deriving insights from historical data. With the rise of big data and cloud computing, there are many challenges and opportunities for the BI. Especially with the growing number of data sources, traditional BI\&A are evolving to provide intelligence at different scales and perspectives - operational BI, situational BI, self-service BI. In this survey, we review the evolution of business intelligence systems in full scale from back-end architecture to and front-end applications. We focus on the changes in the back-end architecture that deals with the collection and organization of the data. We also review the changes in the front-end applications, where analytic services and visualization are the core components. Using a uses case from BI in Healthcare, which is one of the most complex enterprises, we show how BI\&A will play an important role beyond the traditional usage. The survey provides a holistic view of Business Intelligence and Analytics for anyone interested in getting a complete picture of the different pieces in the emerging next generation BI\&A solutions.        △ Less","11 April, 2017",cs.AI,
              A Brief Introduction to the Temporal Group LASSO and its Potential Applications in Healthcare,1704.02370,https://arxiv.org/abs/1704.02370,https://arxiv.org/pdf/1704.02370,Authors:DiegoSaldanaMiranda,"        The Temporal Group LASSO is an example of a multi-task, regularized regression approach for the prediction of response variables that vary over time. The aim of this work is to introduce the reader to the concepts behind the Temporal Group LASSO and its related methods, as well as to the type of potential applications in a healthcare setting that the method has. We argue that the method is attractive because of its ability to reduce overfitting, select predictors, learn smooth effect patterns over time, and finally, its simplicity        △ Less","12 April, 2017",stat.ML,
              Comparing Rule-Based and Deep Learning Models for Patient Phenotyping          ,1703.08705,https://arxiv.org/abs/1703.08705,https://arxiv.org/pdf/1703.08705,"Authors:SebastianGehrmann,FranckDernoncourt,YeranLi,EricT.Carlson,JoyT.Wu,JonathanWelt,JohnFooteJr.,EdwardT.Moseley,DavidW.Grant,PatrickD.Tyler,LeoAnthonyCeli","        Objective: We investigate whether deep learning techniques for natural language processing (NLP) can be used efficiently for patient phenotyping. Patient phenotyping is a classification task for determining whether a patient has a medical condition, and is a crucial part of secondary analysis of healthcare data. We assess the performance of deep learning algorithms and compare them with classical NLP approaches.  Materials and Methods: We compare convolutional neural networks (CNNs), n-gram models, and approaches based on cTAKES that extract pre-defined medical concepts from clinical notes and use them to predict patient phenotypes. The performance is tested on 10 different phenotyping tasks using 1,610 discharge summaries extracted from the MIMIC-III database.  Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The average F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our model having an F1-score up to 37 points higher than alternative approaches. We additionally assess the interpretability of our model by presenting a method that extracts the most salient phrases for a particular prediction.  Conclusion: We show that NLP methods based on deep learning improve the performance of patient phenotyping. Our CNN-based algorithm automatically learns the phrases associated with each patient phenotype. As such, it reduces the annotation complexity for clinical domain experts, who are normally required to develop task-specific annotation rules and identify relevant phrases. Our method performs well in terms of both performance and interpretability, which indicates that deep learning is an effective approach to patient phenotyping based on clinicians' notes.        △ Less","25 March, 2017","cs.CL,cs.AI,cs.NE,stat.ML",
              Hiring Expert Consultants in E-Healthcare: A Two Sided Matching Approach          ,1703.08698,https://arxiv.org/abs/1703.08698,https://arxiv.org/pdf/1703.08698,"Authors:VikashKumarSingh,SajalMukhopadhyay,AniruddhSharma,ArpanRoy","        Very often in some censorious healthcare scenario, there may be a need to have some expert consultancies (especially by doctors) that are not available in-house to the hospital. With the advancement in technologies (such as video conferencing, smartphone, etc.), it has become reality that, for the critical medical cases in the hospitals, expert consultants (ECs) from around the world could be hired, who will serve the patients by their physical or virtual presence. Earlier, this interesting healthcare scenario of hiring the ECs (mainly doctors) from outside of the hospitals had been studied with the robust concepts of mechanism design with or without money. We have tried to model the ECs (mainly doctors) hiring problem as a two-sided matching problem. In this paper, for the first time, to the best of our knowledge, we explore the more realistic two-sided matching in our set-up, where the members of the two participating communities, namely patients and doctors are revealing the strict preference ordering over all the members of the opposite community for a stipulated amount of time. We assume that patients and doctors are strategic in nature. With the theoretical analysis, we demonstrate that the proposed mechanism that results in a stable allocation of doctors to patients is strategy-proof (or truthful) and optimal. The proposed mechanism is also validated with exhaustive experiments.        △ Less","19 September, 2017",cs.GT,10.1007/978-3-319-99810-7_9 
              Sperm-hybrid micromotor for drug delivery in the female reproductive tract          ,1703.08510,https://arxiv.org/abs/1703.08510,https://arxiv.org/pdf/1703.08510,"Authors:HaifengXu,MarianaMedinaSanchez,VeronikaMagdanz,LukasSchwarz,FranziskaHebenstreit,OliverG.Schmidt","        A sperm-driven micromotor is presented as cargo-delivery system for the treatment of gynecological cancers. This particular hybrid micromotor is appealing to treat diseases in the female reproductive tract, the physiological environment that sperm cells are naturally adapted to swim in. Here, the single sperm cell serves as an active drug carrier and as driving force, taking advantage of its swimming capability, while a laser-printed microstructure coated with a nanometric layer of iron is used to guide and release the sperm in the desired area by an external magnet and structurally imposed mechanical actuation, respectively. The printed tubular microstructure features four arms which release the drug-loaded sperm cell in situ when they bend upon pushing against a tumor spheroid, resulting in the drug delivery, which occurs when the sperm squeezes through the cancer cells and fuses with cell membrane. Sperms also offer higher drug encapsulation capability and carrying stability compared to other nano and microcarriers, minimizing toxic effects and unwanted drug accumulation. Moreover, sperms neither express pathogenic proteins nor proliferate to form undesirable colonies, unlike other cells or microorganisms do, making this bio-hybrid system a unique and biocompatible cargo delivery platform for various biomedical applications, especially in gynecological healthcare.        △ Less","24 March, 2017","physics.med-ph,physics.bio-ph,q-bio.CB",10.1021/acsnano.7b06398 
              Multitask learning and benchmarking with clinical time series data          ,1703.07771,https://arxiv.org/abs/1703.07771,https://arxiv.org/pdf/1703.07771,"Authors:HrayrHarutyunyan,HrantKhachatrian,DavidC.Kale,GregVerSteeg,AramGalstyan","        Health care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data available for analysis, but progress in machine learning for healthcare research has been difficult to measure because of the absence of publicly available benchmark data sets. To address this problem, we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database. These tasks cover a range of clinical problems including modeling risk of mortality, forecasting length of stay, detecting physiologic decline, and phenotype classification. We propose strong linear and neural baselines for all four tasks and evaluate the effect of deep supervision, multitask training and data-specific architectural modifications on the performance of neural models.        △ Less","9 August, 2019","stat.ML,cs.LG",10.1038/s41597-019-0103-9 
              Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning          ,1703.07710,https://arxiv.org/abs/1703.07710,https://arxiv.org/pdf/1703.07710,"Authors:ChristophDann,TorLattimore,EmmaBrunskill","        Statistical performance bounds for reinforcement learning (RL) algorithms can be critical for high-stakes applications like healthcare. This paper introduces a new framework for theoretically measuring the performance of such algorithms called Uniform-PAC, which is a strengthening of the classical Probably Approximately Correct (PAC) framework. In contrast to the PAC framework, the uniform version may be used to derive high probability regret guarantees and so forms a bridge between the two setups that has been missing in the literature. We demonstrate the benefits of the new framework for finite-state episodic MDPs with a new algorithm that is Uniform-PAC and simultaneously achieves optimal regret and PAC guarantees except for a factor of the horizon.        △ Less","2 January, 2018","cs.LG,cs.AI,stat.ML",
              The Use of Autoencoders for Discovering Patient Phenotypes          ,1703.07004,https://arxiv.org/abs/1703.07004,https://arxiv.org/pdf/1703.07004,"Authors:HariniSuresh,PeterSzolovits,MarzyehGhassemi","        We use autoencoders to create low-dimensional embeddings of underlying patient phenotypes that we hypothesize are a governing factor in determining how different patients will react to different interventions. We compare the performance of autoencoders that take fixed length sequences of concatenated timesteps as input with a recurrent sequence-to-sequence autoencoder. We evaluate our methods on around 35,500 patients from the latest MIMIC III dataset from Beth Israel Deaconess Hospital.        △ Less","20 March, 2017",cs.LG,
              A Fully-Automated Pipeline for Detection and Segmentation of Liver Lesions and Pathological Lymph Nodes          ,1703.06418,https://arxiv.org/abs/1703.06418,https://arxiv.org/pdf/1703.06418,"Authors:AssafHoogi,JohnW.Lambert,YefengZheng,DorinComaniciu,DanielL.Rubin","        We propose a fully-automated method for accurate and robust detection and segmentation of potentially cancerous lesions found in the liver and in lymph nodes. The process is performed in three steps, including organ detection, lesion detection and lesion segmentation. Our method applies machine learning techniques such as marginal space learning and convolutional neural networks, as well as active contour models. The method proves to be robust in its handling of extremely high lesion diversity. We tested our method on volumetric computed tomography (CT) images, including 42 volumes containing liver lesions and 86 volumes containing 595 pathological lymph nodes. Preliminary results under 10-fold cross validation show that for both the liver lesions and the lymph nodes, a total detection sensitivity of 0.53 and average Dice score of 0.71±0.150.71 \pm 0.15 for segmentation were obtained.        △ Less","19 March, 2017",cs.CV,
              Engineering Trustworthy Self-Adaptive Software with Dynamic Assurance Cases          ,1703.06350,https://arxiv.org/abs/1703.06350,https://arxiv.org/pdf/1703.06350,"Authors:RaduCalinescu,DannyWeyns,SimosGerasimou,M.UsmanIftikhar,IbrahimHabli,TimKelly","        Building on concepts drawn from control theory, self-adaptive software handles environmental and internal uncertainties by dynamically adjusting its architecture and parameters in response to events such as workload changes and component failures. Self-adaptive software is increasingly expected to meet strict functional and non-functional requirements in applications from areas as diverse as manufacturing, healthcare and finance. To address this need, we introduce a methodology for the systematic ENgineering of TRUstworthy Self-adaptive sofTware (ENTRUST). ENTRUST uses a combination of (1) design-time and runtime modelling and verification, and (2) industry-adopted assurance processes to develop trustworthy self-adaptive software and assurance cases arguing the suitability of the software for its intended application. To evaluate the effectiveness of our methodology, we present a tool-supported instance of ENTRUST and its use to develop proof-of-concept self-adaptive software for embedded and service-based systems from the oceanic monitoring and e-finance domains, respectively. The experimental results show that ENTRUST can be used to engineer self-adaptive software systems in different application domains and to generate dynamic assurance cases for these systems.        △ Less","22 November, 2018",cs.SE,
              Do pay-for-performance incentives lead to a better health outcome?          ,1703.05103,https://arxiv.org/abs/1703.05103,https://arxiv.org/pdf/1703.05103,"Authors:AlinaPeluso,PaoloBerta,VeronicaVinciotti","        Pay-for-performance approaches have been widely adopted in order to drive improvements in the quality of healthcare provision. Previous studies evaluating the impact of these programs are either limited by the number of health outcomes or of medical conditions considered. In this paper, we evaluate the effectiveness of a pay-for-performance program on the basis of five health outcomes and across a wide range of medical conditions. The context of the study is the Lombardy region in Italy, where a rewarding program was introduced in 2012. The policy evaluation is based on a difference-in-differences approach. The model includes multiple dependent outcomes, that allow quantifying the joint effect of the program, and random effects, that account for the heterogeneity of the data at the ward and hospital level. Our results show that the policy had a positive effect on the hospitals' performance in terms of those outcomes that can be more influenced by a managerial activity, namely the number of readmissions, transfers and returns to the surgery room. No significant changes which can be related to the pay-for-performance introduction are observed for the number of voluntary discharges and for mortality. Finally, our study shows evidence that the medical wards have reacted more strongly to the pay-for-performance program than the surgical ones, whereas only limited evidence is found in support of a different policy reaction across different types of hospital ownership.        △ Less","15 March, 2017",stat.AP,
              Effects of pressure impulse and peak pressure of a shock wave on microjet velocity and the onset of cavitation in a microchannel          ,1703.04843,https://arxiv.org/abs/1703.04843,https://arxiv.org/pdf/1703.04843,"Authors:KeisukeHayasaka,AkihitoKiyama,YoshiyukiTagawa","        The development of needle-free injection systems utilizing high-speed microjets is of great importance to world healthcare. It is thus crucial to control the microjets, which are often induced by underwater shock waves. In this contribution from fluid-mechanics point of view, we experimentally investigate the effect of a shock wave on the velocity of a free surface (microjet) and underwater cavitation onset in a microchannel, focusing on the pressure impulse and peak pressure of the shock wave. The shock wave used had a non-spherically-symmetric peak pressure distribution and a spherically symmetric pressure impulse distribution [Tagawa et al., J. Fluid Mech., 2016, 808, 5-18]. First, we investigate the effect of the shock wave on the jet velocity by installing a narrow tube and a hydrophone in different configurations in a large water tank, and measuring the shock wave pressure and the jet velocity simultaneously. The results suggest that the jet velocity depends only on the pressure impulse of the shock wave. We then investigate the effect of the shock wave on the cavitation onset by taking measurements in an L-shaped microchannel. The results suggest that the probability of cavitation onset depends only on the peak pressure of the shock wave. In addition, the jet velocity varies according to the presence or absence of cavitation. The above findings provide new insights for advancing a control method for high-speed microjets.        △ Less","16 April, 2017",physics.flu-dyn,
Healthcare,1703.04524,https://arxiv.org/abs/1703.04524,https://arxiv.org/pdf/1703.04524,"Authors:ShoumenPalitAustinDatta,JulianMGoldman","        The complexity of the healthcare ecosystem and the trans-disciplinary convergence which is essential for its function, makes it difficult to address healthcare as one domain. Data curation and analysis of the information may boost our health related knowledge. Increasing connectivity and improving infrastructure may help, among other things, to uncover facts and observations which may influence the future of global health.        △ Less","2 March, 2017",cs.CY,
              Face-to-BMI: Using Computer Vision to Infer Body Mass Index on Social Media          ,1703.03156,https://arxiv.org/abs/1703.03156,https://arxiv.org/pdf/1703.03156,"Authors:EnesKocabey,MustafaCamurcu,FerdaOfli,YusufAytar,JavierMarin,AntonioTorralba,IngmarWeber","        A person's weight status can have profound implications on their life, ranging from mental health, to longevity, to financial income. At the societal level, ""fat shaming"" and other forms of ""sizeism"" are a growing concern, while increasing obesity rates are linked to ever raising healthcare costs. For these reasons, researchers from a variety of backgrounds are interested in studying obesity from all angles. To obtain data, traditionally, a person would have to accurately self-report their body-mass index (BMI) or would have to see a doctor to have it measured. In this paper, we show how computer vision can be used to infer a person's BMI from social media images. We hope that our tool, which we release, helps to advance the study of social aspects related to body weight.        △ Less","9 March, 2017","cs.HC,cs.CV,cs.CY",
              SAFETY: Secure gwAs in Federated Environment Through a hYbrid solution with Intel SGX and Homomorphic Encryption          ,1703.02577,https://arxiv.org/abs/1703.02577,https://arxiv.org/pdf/1703.02577,"Authors:MdNazmusSadat,MdMominAlAziz,NomanMohammed,FengChen,ShuangWang,XiaoqianJiang","        Recent studies demonstrate that effective healthcare can benefit from using the human genomic information. For instance, analysis of tumor genomes has revealed 140 genes whose mutations contribute to cancer. As a result, many institutions are using statistical analysis of genomic data, which are mostly based on genome-wide association studies (GWAS). GWAS analyze genome sequence variations in order to identify genetic risk factors for diseases. These studies often require pooling data from different sources together in order to unravel statistical patterns or relationships between genetic variants and diseases. In this case, the primary challenge is to fulfill one major objective: accessing multiple genomic data repositories for collaborative research in a privacy-preserving manner. Due to the sensitivity and privacy concerns regarding the genomic data, multi-jurisdictional laws and policies of cross-border genomic data sharing are enforced among different regions of the world. In this article, we present SAFETY, a hybrid framework, which can securely perform GWAS on federated genomic datasets using homomorphic encryption and recently introduced secure hardware component of Intel Software Guard Extensions (Intel SGX) to ensure high efficiency and privacy at the same time. Different experimental settings show the efficacy and applicability of such hybrid framework in secure conduction of GWAS. To the best of our knowledge, this hybrid use of homomorphic encryption along with Intel SGX is not proposed or experimented to this date. Our proposed framework, SAFETY is up to 4.82 times faster than the best existing secure computation technique.        △ Less","7 March, 2017",cs.CR,
              Deep Learning for Automated Quality Assessment of Color Fundus Images in Diabetic Retinopathy Screening          ,1703.02511,https://arxiv.org/abs/1703.02511,https://arxiv.org/pdf/1703.02511,"Authors:SajibKumarSaha,BasuraFernando,JorgeCuadros,DiXiao,YogesanKanagasingam",        Purpose To develop a computer based method for the automated assessment of image quality in the context of diabetic retinopathy (DR) to guide the photographer.  Methods A deep learning framework was trained to grade the images automatically. A large representative set of 7000 color fundus images were used for the experiment which were obtained from the EyePACS that were made available by the California Healthcare Foundation. Three retinal image analysis experts were employed to categorize these images into Accept and Reject classes based on the precise definition of image quality in the context of DR. A deep learning framework was trained using 3428 images.  Results A total of 3572 images were used for the evaluation of the proposed method. The method shows an accuracy of 100% to successfully categorise Accept and Reject images.  Conclusion Image quality is an essential prerequisite for the grading of DR. In this paper we have proposed a deep learning based automated image quality assessment method in the context of DR. The method can be easily incorporated with the fundus image capturing system and thus can guide the photographer whether a recapture is necessary or not.        △ Less,"7 March, 2017",cs.CV,
              Propensity score prediction for electronic healthcare databases using Super Learner and High-dimensional Propensity Score Methods          ,1703.02236,https://arxiv.org/abs/1703.02236,https://arxiv.org/pdf/1703.02236,"Authors:ChengJu,MaryCombs,SamuelDLendle,JessicaMFranklin,RichardWyss,SebastianSchneeweiss,MarkJ.vanderLaan","        The optimal learner for prediction modeling varies depending on the underlying data-generating distribution. Super Learner (SL) is a generic ensemble learning algorithm that uses cross-validation to select among a ""library"" of candidate prediction models. The SL is not restricted to a single prediction model, but uses the strengths of a variety of learning algorithms to adapt to different databases. While the SL has been shown to perform well in a number of settings, it has not been thoroughly evaluated in large electronic healthcare databases that are common in pharmacoepidemiology and comparative effectiveness research. In this study, we applied and evaluated the performance of the SL in its ability to predict treatment assignment using three electronic healthcare databases. We considered a library of algorithms that consisted of both nonparametric and parametric models. We also considered a novel strategy for prediction modeling that combines the SL with the high-dimensional propensity score (hdPS) variable selection algorithm. Predictive performance was assessed using three metrics: the negative log-likelihood, area under the curve (AUC), and time complexity. Results showed that the best individual algorithm, in terms of predictive performance, varied across datasets. The SL was able to adapt to the given dataset and optimize predictive performance relative to any individual learner. Combining the SL with the hdPS was the most consistent prediction method and may be promising for PS estimation and prediction modeling in electronic healthcare databases.        △ Less","14 March, 2017","stat.AP,stat.ML",
              A Policy Model and Framework for Context-Aware Access Control to Information Resources          ,1703.02162,https://arxiv.org/abs/1703.02162,https://arxiv.org/pdf/1703.02162,"Authors:A.S.M.Kayes,JunHan,WennyRahayu,Md.SaifulIslam,AlanColman","        In today's dynamic ICT environments, the ability to control users' access to resources becomes ever important. On the one hand, it should adapt to the users' changing needs; on the other hand, it should not be compromised. Therefore, it is essential to have a flexible access control model, incorporating dynamically changing context information. Towards this end, this paper introduces a policy framework for context-aware access control (CAAC) applications that extends the role-based access control model with both dynamic associations of user-role and role-permission capabilities. We first present a formal model of CAAC policies for our framework. Using this model, we then introduce an ontology-based approach and a software prototype for modelling and enforcing CAAC policies. In addition, we evaluate our policy ontology model and framework by considering (i) the completeness of the ontology concepts, specifying different context-aware user-role and role-permission assignment policies from the healthcare scenarios; (ii) the correctness and consistency of the ontology semantics, assessing the core and domain-specific ontologies through the healthcare case study; and (iii) the performance of the framework by means of response time. The evaluation results demonstrate the feasibility of our framework and quantify the performance overhead of achieving context-aware access control to information resources.        △ Less","6 March, 2017",cs.CR,
              Finding Statistically Significant Interactions between Continuous Features          ,1702.08694,https://arxiv.org/abs/1702.08694,https://arxiv.org/pdf/1702.08694,"Authors:MahitoSugiyama,KarstenBorgwardt","        The search for higher-order feature interactions that are statistically significantly associated with a class variable is of high relevance in fields such as Genetics or Healthcare, but the combinatorial explosion of the candidate space makes this problem extremely challenging in terms of computational efficiency and proper correction for multiple testing. While recent progress has been made regarding this challenge for binary features, we here present the first solution for continuous features. We propose an algorithm which overcomes the combinatorial explosion of the search space of higher-order interactions by deriving a lower bound on the p-value for each interaction, which enables us to massively prune interactions that can never reach significance and to thereby gain more statistical power. In our experiments, our approach efficiently detects all significant interactions in a variety of synthetic and real-world datasets.        △ Less","10 May, 2019","stat.ML,cs.LG,stat.ME",
              Preventing Hospital Acquired Infections Through a Workflow-Based Cyber-Physical System          ,1702.08010,https://arxiv.org/abs/1702.08010,https://arxiv.org/pdf/1702.08010,"Authors:MariaIulianaBocicor,Arthur-JozsefMolnar,CristianTaslitchi","        Hospital acquired infections (HAI) are infections acquired within the hospital from healthcare workers, patients or from the environment, but which have no connection to the initial reason for the patient's hospital admission. HAI are a serious world-wide problem, leading to an increase in mortality rates, duration of hospitalisation as well as significant economic burden on hospitals. Although clear preventive guidelines exist, studies show that compliance to them is frequently poor. This paper details the software perspective for an innovative, business process software based cyber-physical system that will be implemented as part of a European Union-funded research project. The system is composed of a network of sensors mounted in different sites around the hospital, a series of wearables used by the healthcare workers and a server side workflow engine. For better understanding, we describe the system through the lens of a single, simple clinical workflow that is responsible for a significant portion of all hospital infections. The goal is that when completed, the system will be configurable in the sense of facilitating the creation and automated monitoring of those clinical workflows that when combined, account for over 90\% of hospital infections.        △ Less","26 February, 2017",cs.OH,10.5220/0005916900630068 
              Simulation of Patient Flow in Multiple Healthcare Units using Process and Data Mining Techniques for Model Identification          ,1702.07733,https://arxiv.org/abs/1702.07733,https://arxiv.org/pdf/1702.07733,"Authors:SergeyV.Kovalchuk,AnastasiaA.Funkner,OlegG.Metsker,AlekseyN.Yakovlev","        Introduction: An approach to building a hybrid simulation of patient flow is introduced with a combination of data-driven methods for automation of model identification. The approach is described with a conceptual framework and basic methods for combination of different techniques. The implementation of the proposed approach for simulation of acute coronary syndrome (ACS) was developed and used within an experimental study. Methods: Combination of data, text, and process mining techniques and machine learning approaches for analysis of electronic health records (EHRs) with discrete-event simulation (DES) and queueing theory for simulation of patient flow was proposed. The performed analysis of EHRs for ACS patients enable identification of several classes of clinical pathways (CPs) which were used to implement a more realistic simulation of the patient flow. The developed solution was implemented using Python libraries (SimPy, SciPy, and others). Results: The proposed approach enables more realistic and detailed simulation of the patient flow within a group of related departments. Experimental study shows that the improved simulation of patient length of stay for ACS patient flow obtained from EHRs in Federal Almazov North-west Medical Research Centre in Saint Petersburg, Russia. Conclusion: The proposed approach, methods, and solutions provide a conceptual, methodological, and programming framework for implementation of simulation of complex and diverse scenarios within a flow of patients for different purposes: decision making, training, management optimization, and others.        △ Less","22 January, 2018",cs.CY,10.1016/j.jbi.2018.05.004 
              Diverse Weighted Bipartite b-Matching          ,1702.07134,https://arxiv.org/abs/1702.07134,https://arxiv.org/pdf/1702.07134,"Authors:FaezAhmed,JohnP.Dickerson,MarkFuge","        Bipartite matching, where agents on one side of a market are matched to agents or items on the other, is a classical problem in computer science and economics, with widespread application in healthcare, education, advertising, and general resource allocation. A practitioner's goal is typically to maximize a matching market's economic efficiency, possibly subject to some fairness requirements that promote equal access to resources. A natural balancing act exists between fairness and efficiency in matching markets, and has been the subject of much research.  In this paper, we study a complementary goal---balancing diversity and efficiency---in a generalization of bipartite matching where agents on one side of the market can be matched to sets of agents on the other. Adapting a classical definition of the diversity of a set, we propose a quadratic programming-based approach to solving a supermodular minimization problem that balances diversity and total weight of the solution. We also provide a scalable greedy algorithm with theoretical performance bounds. We then define the price of diversity, a measure of the efficiency loss due to enforcing diversity, and give a worst-case theoretical bound. Finally, we demonstrate the efficacy of our methods on three real-world datasets, and show that the price of diversity is not bad in practice.        △ Less","15 August, 2017","cs.DS,cs.AI",10.24963/ijcai.2017/6 
              Peak Transmission Rate Resilient Crosslayer Broadcast for Body Area Networks          ,1702.05031,https://arxiv.org/abs/1702.05031,https://arxiv.org/pdf/1702.05031,"Authors:WafaBadreddine,MariaPotop-Butucaru","        Wireless Body Area Networks (WBAN) open an interdisciplinary area within Wireless Sensor Networks (WSN) research, with a tremendous impact in healthcare area where sensors are used to monitor, collect and transmit biological parameters of the human body. We propose the rst network-MAC cross-layer broadcast protocol in WBAN. Our protocol, evaluated in the OMNET++ simulator enriched with realistic human body mobility models and channel models issued from the recent research on biomedical and health informatics, outperforms existing at broadcast strategies in terms of percentage of covered nodes, energy consumption and correct reception of causally-ordered packets (i.e. packets are received in the same order as they were sent by the sink). Furthermore, we investigate the resilience of both existing at broadcast strategies and our new protocol face to various transmission rates and human body mobility. Existing at broadcast strategies without exception start to have a drastic drop of performances for transmission rates above 11Kb/s while our cross-layer protocol performances maintains its good performances for transmission rates up to 190Kb/s.        △ Less","16 February, 2017",cs.NI,
              Predicting Hospital Re-admissions from Nursing Care Data of Hospitalized Patients          ,1702.04036,https://arxiv.org/abs/1702.04036,https://arxiv.org/pdf/1702.04036,"Authors:MuhammadKLodhi,RashidAnsari,YingweiYao,GailMKeenan,DianaWilkie,AshfaqAKhokhar","        Readmission rates in the hospitals are increasingly being used as a benchmark to determine the quality of healthcare delivery to hospitalized patients. Around three-fourths of all hospital re-admissions can be avoided, saving billions of dollars. Many hospitals have now deployed electronic health record (EHR) systems that can be used to study issues that trigger readmission. However, most of the EHRs are high dimen-sional and sparsely populated, and analyzing such data sets is a Big Data challenge. The effect of some of the well-known dimension reduction techniques is minimized due to presence of non-linear variables. We use association mining as a dimension reduction method and the results are used to develop models, using data from an existing nursing EHR system, for predicting risk of re-admission to the hospitals. These models can help in determining effective treatments for patients to minimize the possibility of re-admission, bringing down the cost and increasing the quality of care provided to the patients. Results from the models show significantly accurate predictions of patient re-admission.        △ Less","13 February, 2017",cs.CY,
              Elements of estimation theory for causal effects in the presence of network interference          ,1702.03578,https://arxiv.org/abs/1702.03578,https://arxiv.org/pdf/1702.03578,"Authors:DanielL.Sussman,EdoardoM.Airoldi","        Randomized experiments in which the treatment of a unit can affect the outcomes of other units are becoming increasingly common in healthcare, economics, and in the social and information sciences. From a causal inference perspective, the typical assumption of no interference becomes untenable in such experiments. In many problems, however, the patterns of interference may be informed by the observation of network connections among the units of analysis. Here, we develop elements of optimal estimation theory for causal effects leveraging an observed network, by assuming that the potential outcomes of an individual depend only on the individual's treatment and on the treatment of the neighbors. We propose a collection of exclusion restrictions on the potential outcomes, and show how subsets of these restrictions lead to various parameterizations. Considering the class of linear unbiased estimators of the average direct treatment effect, we derive conditions on the design that lead to the existence of unbiased estimators, and offer analytical insights on the weights that lead to minimum integrated variance estimators. We illustrate the improved performance of these estimators when compared to more standard biased and unbiased estimators, using simulations.        △ Less","12 February, 2017",stat.ME,
"              RSPP: A Reliable, Searchable and Privacy-Preserving e-Healthcare System for Cloud-Assisted Body Area Networks          ",1702.03467,https://arxiv.org/abs/1702.03467,https://arxiv.org/pdf/1702.03467,"Authors:LeiYang,QingjiZheng,XinxinFan","        The integration of cloud computing and Internet of Things (IoT) is quickly becoming the key enabler for the digital transformation of the healthcare industry by offering comprehensive improvements in patient engagements, productivity and risk mitigation. This paradigm shift, while bringing numerous benefits and new opportunities to healthcare organizations, has raised a lot of security and privacy concerns. In this paper, we present a reliable, searchable and privacy-preserving e-healthcare system, which takes advantage of emerging cloud storage and IoT infrastructure and enables healthcare service providers (HSPs) to realize remote patient monitoring in a secure and regulatory compliant manner. Our system is built upon a novel dynamic searchable symmetric encryption scheme with forward privacy and delegated verifiability for periodically generated healthcare data. While the forward privacy is achieved by maintaining an increasing counter for each keyword at an IoT gateway, the data owner delegated verifiability comes from the combination of the Bloom filter and aggregate message authentication code. Moreover, our system is able to support multiple HSPs through either data owner assistance or delegation. The detailed security analysis as well as the extensive simulations on a large data set with millions of records demonstrate the practical efficiency of the proposed system for real world healthcare applications.        △ Less","11 February, 2017",cs.CR,
              Mining Electronic Health Records: A Survey          ,1702.03222,https://arxiv.org/abs/1702.03222,https://arxiv.org/pdf/1702.03222,"Authors:PranjulYadav,MichaelSteinbach,VipinKumar,GyorgySimon","        The continuously increasing cost of the US healthcare system has received significant attention. Central to the ideas aimed at curbing this trend is the use of technology, in the form of the mandate to implement electronic health records (EHRs). EHRs consist of patient information such as demographics, medications, laboratory test results, diagnosis codes and procedures. Mining EHRs could lead to improvement in patient health management as EHRs contain detailed information related to disease prognosis for large patient populations. In this manuscript, we provide a structured and comprehensive overview of data mining techniques for modeling EHR data. We first provide a detailed understanding of the major application areas to which EHR mining has been applied and then discuss the nature of EHR data and its accompanying challenges. Next, we describe major approaches used for EHR mining, the metrics associated with EHRs, and the various study designs. With this foundation, we then provide a systematic and methodological organization of existing data mining techniques used to model EHRs and discuss ideas for future research. We conclude this survey with a comprehensive summary of clinical data mining applications of EHR data, as illustrated in the online supplement.        △ Less","23 March, 2017",cs.IR,
              Causal Regularization          ,1702.02604,https://arxiv.org/abs/1702.02604,https://arxiv.org/pdf/1702.02604,"Authors:MohammadTahaBahadori,KrzysztofChalupka,EdwardChoi,RobertChen,WalterF.Stewart,JimengSun","        In application domains such as healthcare, we want accurate predictive models that are also causally interpretable. In pursuit of such models, we propose a causal regularizer to steer predictive models towards causally-interpretable solutions and theoretically study its properties. In a large-scale analysis of Electronic Health Records (EHR), our causally-regularized model outperforms its L1-regularized counterpart in causal accuracy and is competitive in predictive performance. We perform non-linear causality analysis by causally regularizing a special neural network architecture. We also show that the proposed causal regularizer can be used together with neural representation learning algorithms to yield up to 20% improvement over multilayer perceptron in detecting multivariate causation, a situation common in healthcare, where many causal factors should occur simultaneously to have an effect on the target variable.        △ Less","23 February, 2017","cs.LG,cs.AI,cs.NE,stat.ML",
              RFID Localisation For Internet Of Things Smart Homes: A Survey          ,1702.02311,https://arxiv.org/abs/1702.02311,https://arxiv.org/pdf/1702.02311,"Authors:BelalAlsinglawi,MahmoudElkhodr,QuangVinhNguyen,UpulGunawardana,AnthonyMaeder,SimeonSimoff","        The Internet of Things (IoT) enables numerous business opportunities in fields as diverse as e-health, smart cities, smart homes, among many others. The IoT incorporates multiple long-range, short-range, and personal area wireless networks and technologies into the designs of IoT applications. Localisation in indoor positioning systems plays an important role in the IoT. Location Based IoT applications range from tracking objects and people in real-time, assets management, agriculture, assisted monitoring technologies for healthcare, and smart homes, to name a few. Radio Frequency based systems for indoor positioning such as Radio Frequency Identification (RFID) is a key enabler technology for the IoT due to its costeffective, high readability rates, automatic identification and, importantly, its energy efficiency characteristic. This paper reviews the state-of-the-art RFID technologies in IoT Smart Homes applications. It presents several comparable studies of RFID based projects in smart homes and discusses the applications, techniques, algorithms, and challenges of adopting RFID technologies in IoT smart home systems.        △ Less","8 February, 2017",cs.NI,
              Probabilistic Sensor Fusion for Ambient Assisted Living          ,1702.01209,https://arxiv.org/abs/1702.01209,https://arxiv.org/pdf/1702.01209,"Authors:TomDiethe,NiallTwomey,MeelisKull,PeterFlach,IanCraddock","        There is a widely-accepted need to revise current forms of health-care provision, with particular interest in sensing systems in the home. Given a multiple-modality sensor platform with heterogeneous network connectivity, as is under development in the Sensor Platform for HEalthcare in Residential Environment (SPHERE) Interdisciplinary Research Collaboration (IRC), we face specific challenges relating to the fusion of the heterogeneous sensor modalities.  We introduce Bayesian models for sensor fusion, which aims to address the challenges of fusion of heterogeneous sensor modalities. Using this approach we are able to identify the modalities that have most utility for each particular activity, and simultaneously identify which features within that activity are most relevant for a given activity.  We further show how the two separate tasks of location prediction and activity recognition can be fused into a single model, which allows for simultaneous learning an prediction for both tasks.  We analyse the performance of this model on data collected in the SPHERE house, and show its utility. We also compare against some benchmark models which do not have the full structure,and show how the proposed model compares favourably to these methods        △ Less","3 February, 2017","stat.ML,cs.HC",
              Drug-Drug Interaction Extraction from Biomedical Text Using Long Short Term Memory Network          ,1701.08303,https://arxiv.org/abs/1701.08303,https://arxiv.org/pdf/1701.08303,"Authors:SunilKumarSahu,AshishAnand","        Simultaneous administration of multiple drugs can have synergistic or antagonistic effects as one drug can affect activities of other drugs. Synergistic effects lead to improved therapeutic outcomes, whereas, antagonistic effects can be life-threatening, may lead to increased healthcare cost, or may even cause death. Thus identification of unknown drug-drug interaction (DDI) is an important concern for efficient and effective healthcare. Although multiple resources for DDI exist, they are often unable to keep pace with rich amount of information available in fast growing biomedical texts. Most existing methods model DDI extraction from text as a classification problem and mainly rely on handcrafted features. Some of these features further depend on domain specific tools. Recently neural network models using latent features have been shown to give similar or better performance than the other existing models dependent on handcrafted features. In this paper, we present three models namely, {\it B-LSTM}, {\it AB-LSTM} and {\it Joint AB-LSTM} based on long short-term memory (LSTM) network. All three models utilize word and position embedding as latent features and thus do not rely on explicit feature engineering. Further use of bidirectional long short-term memory (Bi-LSTM) networks allow implicit feature extraction from the whole sentence. The two models, {\it AB-LSTM} and {\it Joint AB-LSTM} also use attentive pooling in the output of Bi-LSTM layer to assign weights to features. Our experimental results on the SemEval-2013 DDI extraction dataset show that the {\it Joint AB-LSTM} model outperforms all the existing methods, including those relying on handcrafted features. The other two proposed LSTM models also perform competitively with state-of-the-art methods.        △ Less","13 August, 2017",cs.CL,
              Biomedical Data Warehouses          ,1701.08028,https://arxiv.org/abs/1701.08028,https://arxiv.org/pdf/1701.08028,"Authors:JérômeDarmont,EmersonOlivier","        The aim of this article is to present an overview of the existing biomedical data warehouses and to discuss the issues and future trends in this area. We illustrate this topic by presenting the design of an innovative, complex data warehouse for personal, anticipative medicine.        △ Less","27 January, 2017",cs.DB,
              Deep Reinforcement Learning: An Overview          ,1701.07274,https://arxiv.org/abs/1701.07274,https://arxiv.org/pdf/1701.07274,Authors:YuxiLi,"        We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.  Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.        △ Less","25 November, 2018",cs.LG,
              Rare Disease Physician Targeting: A Factor Graph Approach          ,1701.05644,https://arxiv.org/abs/1701.05644,https://arxiv.org/pdf/1701.05644,"Authors:YongCai,YunlongWang,DongDai","        In rare disease physician targeting, a major challenge is how to identify physicians who are treating diagnosed or underdiagnosed rare diseases patients. Rare diseases have extremely low incidence rate. For a specified rare disease, only a small number of patients are affected and a fractional of physicians are involved. The existing targeting methodologies, such as segmentation and profiling, are developed under mass market assumption. They are not suitable for rare disease market where the target classes are extremely imbalanced. The authors propose a graphical model approach to predict targets by jointly modeling physician and patient features from different data spaces and utilizing the extra relational information. Through an empirical example with medical claim and prescription data, the proposed approach demonstrates better accuracy in finding target physicians. The graph representation also provides visual interpretability of relationship among physicians and patients. The model can be extended to incorporate more complex dependency structures. This article contributes to the literature of exploring the benefit of utilizing relational dependencies among entities in healthcare industry.        △ Less","19 January, 2017","stat.ML,cs.LG",
              Surrogate Aided Unsupervised Recovery of Sparse Signals in Single Index Models for Binary Outcomes          ,1701.05230,https://arxiv.org/abs/1701.05230,https://arxiv.org/pdf/1701.05230,"Authors:AbhishekChakrabortty,MateyNeykov,RaymondCarroll,TianxiCai","        We consider the recovery of regression coefficients, denoted by β0\boldsymbolβ_0, for a single index model (SIM) relating a binary outcome YY to a set of possibly high dimensional covariates X\boldsymbol{X}, based on a large but 'unlabeled' dataset U\mathcal{U}, with YY never observed. On U\mathcal{U}, we fully observe X\boldsymbol{X} and additionally, a surrogate SS which, while not being strongly predictive of YY throughout the entirety of its support, can forecast it with high accuracy when it assumes extreme values. Such datasets arise naturally in modern studies involving large databases such as electronic medical records (EMR) where YY, unlike (X,S)(\boldsymbol{X}, S), is difficult and/or expensive to obtain. In EMR studies, an example of YY and SS would be the true disease phenotype and the count of the associated diagnostic codes respectively. Assuming another SIM for SS given X\boldsymbol{X}, we show that under sparsity assumptions, we can recover β0\boldsymbolβ_0 proportionally by simply fitting a least squares LASSO estimator to the subset of the observed data on (X,S)(\boldsymbol{X}, S) restricted to the extreme sets of SS, with YY imputed using the surrogacy of SS. We obtain sharp finite sample performance bounds for our estimator, including deterministic deviation bounds and probabilistic guarantees. We demonstrate the effectiveness of our approach through multiple simulation studies, as well as by application to real data from an EMR study conducted at the Partners HealthCare Systems.        △ Less","30 June, 2018","stat.ME,math.ST,stat.ML",
              Towards a Smarter organization for a Self-servicing Society          ,1701.04616,https://arxiv.org/abs/1701.04616,https://arxiv.org/pdf/1701.04616,"Authors:VincenzoDeFlorio,MohamedBakhouya,D.Eloudghiri,ChrisBlondia","        Traditional social organizations such as those for the management of healthcare are the result of designs that matched well with an operational context considerably different from the one we are experiencing today. The new context reveals all the fragility of our societies. In this paper, a platform is introduced by combining social-oriented communities and complex-event processing concepts: SELFSERV. Its aim is to complement the ""old recipes"" with smarter forms of social organization based on the self-service paradigm and by exploring culture-specific aspects and technological challenges.        △ Less","17 January, 2017",cs.CY,10.1145/3019943.3019980 
              A test for monitoring under- and overtreatment in Dutch hospitals          ,1701.03959,https://arxiv.org/abs/1701.03959,https://arxiv.org/pdf/1701.03959,"Authors:OliverUrsLenz,DanielLOberski","        Over- and undertreatment harm patients and society and confound other healthcare quality measures. Despite a growing body of research covering specific conditions, we lack tools to systematically detect and measure over- and undertreatment in hospitals. We demonstrate a test used to monitor over- and undertreatment in Dutch hospitals, and illustrate its results applied to the aggregated administrative treatment data of 1,836,349 patients at 89 hospitals in 2013. We employ a random effects model to create risk-adjusted funnel plots that account for natural variation among hospitals, allowing us to estimate a measure of overtreatment and undertreatment when hospitals fall outside the control limits. The results of this test are not definitive, findings were discussed with hospitals to improve the model and to enable the hospitals to make informed treatment decisions.        △ Less","14 January, 2017",stat.AP,
              Scalable Multi-Database Privacy-Preserving Record Linkage using Counting Bloom Filters          ,1701.01232,https://arxiv.org/abs/1701.01232,https://arxiv.org/pdf/1701.01232,"Authors:DinushaVatsalan,PeterChristen,ErhardRahm","        Privacy-preserving record linkage (PPRL) aims at integrating sensitive information from multiple disparate databases of different organizations. PPRL approaches are increasingly required in real-world application areas such as healthcare, national security, and business. Previous approaches have mostly focused on linking only two databases as well as the use of a dedicated linkage unit. Scaling PPRL to more databases (multi-party PPRL) is an open challenge since privacy threats as well as the computation and communication costs for record linkage increase significantly with the number of databases. We thus propose the use of a new encoding method of sensitive data based on Counting Bloom Filters (CBF) to improve privacy for multi-party PPRL. We also investigate optimizations to reduce communication and computation costs for CBF-based multi-party PPRL with and without the use of a dedicated linkage unit. Empirical evaluations conducted with real datasets show the viability of the proposed approaches and demonstrate their scalability, linkage quality, and privacy protection.        △ Less","5 January, 2017",cs.DB,
              Toward sensitive document release with privacy guarantees          ,1701.00436,https://arxiv.org/abs/1701.00436,https://arxiv.org/pdf/1701.00436,"Authors:DavidSánchez,MontserratBatet","        Privacy has become a serious concern for modern Information Societies. The sensitive nature of much of the data that are daily exchanged or released to untrusted parties requires that responsible organizations undertake appropriate privacy protection measures. Nowadays, much of these data are texts (e.g., emails, messages posted in social media, healthcare outcomes, etc.) that, because of their unstructured and semantic nature, constitute a challenge for automatic data protection methods. In fact, textual documents are usually protected manually, in a process known as document redaction or sanitization. To do so, human experts identify sensitive terms (i.e., terms that may reveal identities and/or confidential information) and protect them accordingly (e.g., via removal or, preferably, generalization). To relieve experts from this burdensome task, in a previous work we introduced the theoretical basis of C-sanitization, an inherently semantic privacy model that provides the basis to the development of automatic document redaction/sanitization algorithms and offers clear and a priori privacy guarantees on data protection; even though its potential benefits C-sanitization still presents some limitations when applied to practice (mainly regarding flexibility, efficiency and accuracy). In this paper, we propose a new more flexible model, named (C, g(C))-sanitization, which enables an intuitive configuration of the trade-off between the desired level of protection (i.e., controlled information disclosure) and the preservation of the utility of the protected data (i.e., amount of semantics to be preserved). Moreover, we also present a set of technical solutions and algorithms that provide an efficient and scalable implementation of the model and improve its practical accuracy, as we also illustrate through empirical experiments.        △ Less","2 January, 2017",cs.CR,10.1016/j.engappai.2016.12.013 
              Constructing Effective Personalized Policies Using Counterfactual Inference from Biased Data Sets with Many Features          ,1612.08082,https://arxiv.org/abs/1612.08082,https://arxiv.org/pdf/1612.08082,"Authors:OnurAtan,WilliamR.Zame,QiaojunFeng,MihaelavanderSchaar","        This paper proposes a novel approach for constructing effective personalized policies when the observed data lacks counter-factual information, is biased and possesses many features. The approach is applicable in a wide variety of settings from healthcare to advertising to education to finance. These settings have in common that the decision maker can observe, for each previous instance, an array of features of the instance, the action taken in that instance, and the reward realized -- but not the rewards of actions that were not taken: the counterfactual information. Learning in such settings is made even more difficult because the observed data is typically biased by the existing policy (that generated the data) and because the array of features that might affect the reward in a particular instance -- and hence should be taken into account in deciding on an action in each particular instance -- is often vast. The approach presented here estimates propensity scores for the observed data, infers counterfactuals, identifies a (relatively small) number of features that are (most) relevant for each possible action and instance, and prescribes a policy to be followed. Comparison of the proposed algorithm against the state-of-art algorithm on actual datasets demonstrates that the proposed algorithm achieves a significant improvement in performance.        △ Less","10 July, 2018","stat.ML,cs.LG",
              Development of UMLS Based Health Care Web Services for Android Platform          ,1612.07688,https://arxiv.org/abs/1612.07688,https://arxiv.org/pdf/1612.07688,"Authors:NareenaSoomro,SafeeulahSoomro,ZainabAlansari,SuhniAbbasi,MohammadRiyazBelgaum,AbdulBaqiKhakwani","        In this fast developing world of information, the amount of medical knowledge is rising at an exponential level. The UMLS (Unified Medical Language Systems), is rich knowledge base consisting files and software that provides many health and biomedical vocabularies and standards. A Web service is a web solution to facilitate machine-to-machine interaction over a network. Few UMLS web services are currently available for portable devices, but most of them lack in efficiency and performance. It is proposed to develop Android-based web services for healthcare systems underlying rich knowledge source of UMLS. The experimental evaluation was made to analyse the efficiency and performance effect with and without using the designed prototype. The understand-ability and interaction with the prototype were greater than those who used the alternate sources to obtain the answers to their questions. The overall performance indicates that the system is convenient and easy to use. The result of the evaluation clearly proved that designed system retrieves all the pertinent information better than syntactic searches.        △ Less","22 December, 2016","cs.CY,cs.IR,cs.SE",
              The Price of Political Uncertainty: Evidence from the 2016 U.S. Presidential Election and the U.S. Stock Markets          ,1612.06200,https://arxiv.org/abs/1612.06200,https://arxiv.org/pdf/1612.06200,"Authors:JamalBouoiyour,RefkSelmi","        There is bountiful evidence that political uncertainty stemming from presidential elections or doubt about the direction of future policy make financial markets significantly volatile, especially in proximity to close elections or elections that may prompt radical policy changes. Although several studies have examined the association between presidential elections and stock returns, very little attention has been given to the impacts of elections and election induced uncertainty on stock markets. This paper explores, at sectoral level, the uncertain information hypothesis (UIH) as a means of explaining the reaction of markets to the arrival of unanticipated information. This hypothesis postulates that political uncertainty is greater prior to the elections (relative to pre-election period) but is resolved once the outcome of the elections is determined (relative to post-election period). To this end, we adopt an event-study methodology that examines abnormal return behavior around the election date. We show that collapsing stock returns around the election result is reversed by positive abnormal return on the next day, except some cases where we note negative responses following the vote count. Although Trump's win plunges US into uncertain future, positive reactions of abnormal return are found. Therefore, our results do not support the UIH hypothesis. Besides, the effect of political uncertainty is sector-specific. While some sectors emerged winners (healthcare, oil and gas, real estate, defense, financials and consumer goods and services), others took the opposite route (technology and utilities). The winning industries are generally those that will benefit from the new administration's focus on rebuilding infrastructure, renegotiating trade agreements, reforming tax policy and labour laws, increasing defense funding, easing restrictions on energy production, and rolling back Obamacare.        △ Less","1 March, 2017",q-fin.GN,
              A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal Data: Learning and Inference          ,1612.06007,https://arxiv.org/abs/1612.06007,https://arxiv.org/pdf/1612.06007,"Authors:AhmedM.Alaa,MihaelavanderSchaar","        Modeling continuous-time physiological processes that manifest a patient's evolving clinical states is a key step in approaching many problems in healthcare. In this paper, we develop the Hidden Absorbing Semi-Markov Model (HASMM): a versatile probabilistic model that is capable of capturing the modern electronic health record (EHR) data. Unlike exist- ing models, an HASMM accommodates irregularly sampled, temporally correlated, and informatively censored physiological data, and can describe non-stationary clinical state transitions. Learning an HASMM from the EHR data is achieved via a novel forward- filtering backward-sampling Monte-Carlo EM algorithm that exploits the knowledge of the end-point clinical outcomes (informative censoring) in the EHR data, and implements the E-step by sequentially sampling the patients' clinical states in the reverse-time direction while conditioning on the future states. Real-time inferences are drawn via a forward- filtering algorithm that operates on a virtually constructed discrete-time embedded Markov chain that mirrors the patient's continuous-time state trajectory. We demonstrate the di- agnostic and prognostic utility of the HASMM in a critical care prognosis setting using a real-world dataset for patients admitted to the Ronald Reagan UCLA Medical Center.        △ Less","27 December, 2016","cs.AI,stat.ML",
              Towards Wide Learning: Experiments in Healthcare,1612.05730,https://arxiv.org/abs/1612.05730,https://arxiv.org/pdf/1612.05730,"Authors:SnehasisBanerjee,TanushyamChattopadhyay,SwagataBiswas,RohanBanerjee,AnirbanDuttaChoudhury,ArpanPal,UtpalGarain","        In this paper, a Wide Learning architecture is proposed that attempts to automate the feature engineering portion of the machine learning (ML) pipeline. Feature engineering is widely considered as the most time consuming and expert knowledge demanding portion of any ML task. The proposed feature recommendation approach is tested on 3 healthcare datasets: a) PhysioNet Challenge 2016 dataset of phonocardiogram (PCG) signals, b) MIMIC II blood pressure classification dataset of photoplethysmogram (PPG) signals and c) an emotion classification dataset of PPG signals. While the proposed method beats the state of the art techniques for 2nd and 3rd dataset, it reaches 94.38% of the accuracy level of the winner of PhysioNet Challenge 2016. In all cases, the effort to reach a satisfactory performance was drastically less (a few days) than manual feature engineering.        △ Less","21 December, 2016","stat.ML,cs.LG",
"              Prerequisites for International Exchanges of Health Information: Comparison of Australian, Austrian, Finnish, Swiss, and US Privacy Policies          ",1612.04902,https://arxiv.org/abs/1612.04902,https://arxiv.org/pdf/1612.04902,"Authors:HannaSuominen,HenningMüller,LucilaOhno-Machado,SannaSalanterä,GünterSchreier,LeifHanlen","        Capabilities to exchange health information are critical to accelerate discovery and its diffusion to healthcare practice. However, the same ethical and legal policies that protect privacy hinder these data exchanges, and the issues accumulate if moving data across geographical or organizational borders. This can be seen as one of the reasons why many health technologies and research findings are limited to very narrow domains. In this paper, we compare how using and disclosing personal data for research purposes is addressed in Australian, Austrian, Finnish, Swiss, and US policies with a focus on text data analytics. Our goal is to identify approaches and issues that enable or hinder international health information exchanges. As expected, the policies within each country are not as diverse as across countries. Most policies apply the principles of accountability and/or adequacy and are thereby fundamentally similar. Their following requirements create complications with re-using and re-disclosing data and even secondary data: 1) informing data subjects about the purposes of data collection and use, before the dataset is collected; 2) assurance that the subjects are no longer identifiable; and 3) destruction of data when the research activities are finished. Using storage and compute cloud services as well as other exchange technologies on the Internet without proper permissions is technically not allowed if the data are stored in another country. Both legislation and technologies are available as vehicles for overcoming these barriers. The resulting richness in information variety will contribute to the development and evaluation of new clinical hypotheses and technologies.        △ Less","14 December, 2016","cs.CY,cs.DL",
              Direct-written polymer field-effect transistors operating at 20 MHz          ,1612.04232,https://arxiv.org/abs/1612.04232,https://arxiv.org/pdf/1612.04232,"Authors:AndreaPerinot,PrakashKshirsagar,MariaAdaMalvindi,PierPaoloPompa,RobertoFiammengo,MarioCaironi","        Printed polymer electronics has held for long the promise of revolutionizing technology by delivering distributed, flexible, lightweight and cost-effective applications for wearables, healthcare, diagnostic, automation and portable devices. While impressive progresses have been registered in terms of organic semiconductors mobility, field-effect transistors (FET), the basic building block of any circuit, are still showing limited speed of operation, thus limiting their real applicability. So far, attempts with organic FET to achieve the tens of MHz regime, a threshold for many applications comprising the driving of high resolution displays, have relied on the adoption of sophisticated lithographic techniques and/or complex architectures, undermining the whole concept. In this work we demonstrate polymer FETs which can operate up to 20 MHz and are fabricated by means only of scalable printing techniques and direct-writing methods with a completely mask-less procedure. This is achieved by combining a fs-laser process for the sintering of high resolution metal electrodes, thus easily achieving micron-scale channels with reduced parasitism down to 0.19 pF mm-1, and a large area coating technique of a high mobility polymer semiconductor, according to a simple and scalable process flow.        △ Less","13 December, 2016",cond-mat.mtrl-sci,10.1038/srep38941 
              Bridging Medical Data Inference to Achilles Tendon Rupture Rehabilitation          ,1612.02490,https://arxiv.org/abs/1612.02490,https://arxiv.org/pdf/1612.02490,"Authors:AnQu,ChengZhang,PaulAckermann,HedvigKjellström","        Imputing incomplete medical tests and predicting patient outcomes are crucial for guiding the decision making for therapy, such as after an Achilles Tendon Rupture (ATR). We formulate the problem of data imputation and prediction for ATR relevant medical measurements into a recommender system framework. By applying MatchBox, which is a collaborative filtering approach, on a real dataset collected from 374 ATR patients, we aim at offering personalized medical data imputation and prediction. In this work, we show the feasibility of this approach and discuss potential research directions by conducting initial qualitative evaluations.        △ Less","7 December, 2016","cs.LG,stat.AP",
              Who is Mistaken?          ,1612.01175,https://arxiv.org/abs/1612.01175,https://arxiv.org/pdf/1612.01175,"Authors:BenjaminEysenbach,CarlVondrick,AntonioTorralba","        Recognizing when people have false beliefs is crucial for understanding their actions. We introduce the novel problem of identifying when people in abstract scenes have incorrect beliefs. We present a dataset of scenes, each visually depicting an 8-frame story in which a character has a mistaken belief. We then create a representation of characters' beliefs for two tasks in human action understanding: predicting who is mistaken, and when they are mistaken. Experiments suggest that our method for identifying mistaken characters performs better on these tasks than simple baselines. Diagnostics on our model suggest it learns important cues for recognizing mistaken beliefs, such as gaze. We believe models of people's beliefs will have many applications in action understanding, robotics, and healthcare.        △ Less","31 March, 2017",cs.CV,
              Skin Cancer Detection and Tracking using Data Synthesis and Deep Learning          ,1612.01074,https://arxiv.org/abs/1612.01074,https://arxiv.org/pdf/1612.01074,"Authors:YunzhuLi,AndreEsteva,BrettKuprel,RobNovoa,JustinKo,SebastianThrun","        Dense object detection and temporal tracking are needed across applications domains ranging from people-tracking to analysis of satellite imagery over time. The detection and tracking of malignant skin cancers and benign moles poses a particularly challenging problem due to the general uniformity of large skin patches, the fact that skin lesions vary little in their appearance, and the relatively small amount of data available. Here we introduce a novel data synthesis technique that merges images of individual skin lesions with full-body images and heavily augments them to generate significant amounts of data. We build a convolutional neural network (CNN) based system, trained on this synthetic data, and demonstrate superior performance to traditional detection and tracking techniques. Additionally, we compare our system to humans trained with simple criteria. Our system is intended for potential clinical use to augment the capabilities of healthcare providers. While domain-specific, we believe the methods invoked in this work will be useful in applying CNNs across domains that suffer from limited data availability.        △ Less","4 December, 2016",cs.CV,
              Large scale modeling of antimicrobial resistance with interpretable classifiers          ,1612.01030,https://arxiv.org/abs/1612.01030,https://arxiv.org/pdf/1612.01030,"Authors:AlexandreDrouin,FrédéricRaymond,GaëlLetarteSt-Pierre,MarioMarchand,JacquesCorbeil,FrançoisLaviolette","        Antimicrobial resistance is an important public health concern that has implications in the practice of medicine worldwide. Accurately predicting resistance phenotypes from genome sequences shows great promise in promoting better use of antimicrobial agents, by determining which antibiotics are likely to be effective in specific clinical cases. In healthcare, this would allow for the design of treatment plans tailored for specific individuals, likely resulting in better clinical outcomes for patients with bacterial infections. In this work, we present the recent work of Drouin et al. (2016) on using Set Covering Machines to learn highly interpretable models of antibiotic resistance and complement it by providing a large scale application of their method to the entire PATRIC database. We report prediction results for 36 new datasets and present the Kover AMR platform, a new web-based tool allowing the visualization and interpretation of the generated models.        △ Less","3 December, 2016","q-bio.GN,cs.LG,stat.ML",
              Canonical Correlation Analysis for Analyzing Sequences of Medical Billing Codes          ,1612.00516,https://arxiv.org/abs/1612.00516,https://arxiv.org/pdf/1612.00516,"Authors:CorinneL.Jones,ShamM.Kakade,LucasW.Thornblade,DavidR.Flum,AbrahamD.Flaxman","        We propose using canonical correlation analysis (CCA) to generate features from sequences of medical billing codes. Applying this novel use of CCA to a database of medical billing codes for patients with diverticulitis, we first demonstrate that the CCA embeddings capture meaningful relationships among the codes. We then generate features from these embeddings and establish their usefulness in predicting future elective surgery for diverticulitis, an important marker in efforts for reducing costs in healthcare.        △ Less","6 January, 2017","stat.ML,cs.LG",
              Transfer Learning Across Patient Variations with Hidden Parameter Markov Decision Processes          ,1612.00475,https://arxiv.org/abs/1612.00475,https://arxiv.org/pdf/1612.00475,"Authors:TaylorKillian,GeorgeKonidaris,FinaleDoshi-Velez","        Due to physiological variation, patients diagnosed with the same condition may exhibit divergent, but related, responses to the same treatments. Hidden Parameter Markov Decision Processes (HiP-MDPs) tackle this transfer-learning problem by embedding these tasks into a low-dimensional space. However, the original formulation of HiP-MDP had a critical flaw: the embedding uncertainty was modeled independently of the agent's state uncertainty, requiring an unnatural training procedure in which all tasks visited every part of the state space---possible for robots that can be moved to a particular location, impossible for human patients. We update the HiP-MDP framework and extend it to more robustly develop personalized medicine strategies for HIV treatment.        △ Less","1 December, 2016","stat.ML,cs.AI,cs.LG",
              A Data Fusion System to Study Synchronization in Social Activities          ,1611.10061,https://arxiv.org/abs/1611.10061,https://arxiv.org/pdf/1611.10061,"Authors:LoïcSevrin,BertrandMassot,NorbertNoury,NacerAbouchi,FabriceJumel,JacquesSaraydaryan","        As the world population gets older, the healthcare system must be adapted, among others by providing continuous health monitoring at home and in the city. The social activities have a significant role in everyone health status. Hence, this paper proposes a system to perform a data fusion of signals sampled on several subjects during social activities. This study implies the time synchronization of data coming from several sensors whether these are embedded on people or integrated in the environment. The data fusion is applied to several experiments including physical, cognitive and rest activities, with social aspects. The simultaneous and continuous analysis of four subjects cardiac activity and GPS coordinates provides a new way to distinguish different collaborative activities comparing the measurements between the subjects and along time.        △ Less","30 November, 2016",cs.CY,10.1109/HealthCom.2016.7749486 
              Predicting drug recalls from Internet search engine queries          ,1611.08848,https://arxiv.org/abs/1611.08848,https://arxiv.org/pdf/1611.08848,Authors:EladYom-Tov,"        Batches of pharmaceutical are sometimes recalled from the market when a safety issue or a defect is detected in specific production runs of a drug. Such problems are usually detected when patients or healthcare providers report abnormalities to medical authorities. Here we test the hypothesis that defective production lots can be detected earlier by monitoring queries to Internet search engines.  We extracted queries from the USA to the Bing search engine which mentioned one of 5,195 pharmaceutical drugs during 2015 and all recall notifications issued by the Food and Drug Administration (FDA) during that year. By using attributes that quantify the change in query volume at the state level, we attempted to predict if a recall of a specific drug will be ordered by FDA in a time horizon ranging from one to 40 days in future.  Our results show that future drug recalls can indeed be identified with an AUC of 0.791 and a lift at 5% of approximately 6 when predicting a recall will occur one day ahead. This performance degrades as prediction is made for longer periods ahead. The most indicative attributes for prediction are sudden spikes in query volume about a specific medicine in each state. Recalls of prescription drugs and those estimated to be of medium-risk are more likely to be identified using search query data.  These findings suggest that aggregated Internet search engine data can be used to facilitate in early warning of faulty batches of medicines.        △ Less","27 November, 2016","cs.IR,stat.AP",10.1109/JTEHM.2017.2732945 
              Patient-Driven Privacy Control through Generalized Distillation          ,1611.08648,https://arxiv.org/abs/1611.08648,https://arxiv.org/pdf/1611.08648,"Authors:Z.BerkayCelik,DavidLopez-Paz,PatrickMcDaniel","        The introduction of data analytics into medicine has changed the nature of patient treatment. In this, patients are asked to disclose personal information such as genetic markers, lifestyle habits, and clinical history. This data is then used by statistical models to predict personalized treatments. However, due to privacy concerns, patients often desire to withhold sensitive information. This self-censorship can impede proper diagnosis and treatment, which may lead to serious health complications and even death over time. In this paper, we present privacy distillation, a mechanism which allows patients to control the type and amount of information they wish to disclose to the healthcare providers for use in statistical models. Meanwhile, it retains the accuracy of models that have access to all patient data under a sufficient but not full set of privacy-relevant information. We validate privacy distillation using a corpus of patients prescribed to warfarin for a personalized dosage. We use a deep neural network to implement privacy distillation for training and making dose predictions. We find that privacy distillation with sufficient privacy-relevant information i) retains accuracy almost as good as having all patient data (only 3\% worse), and ii) is effective at preventing errors that introduce health-related risks (only 3.9\% worse under- or over-prescriptions).        △ Less","13 October, 2017","cs.CR,cs.CY,cs.LG,stat.ML",
              GRAM: Graph-based Attention Model for Healthcare Representation Learning          ,1611.07012,https://arxiv.org/abs/1611.07012,https://arxiv.org/pdf/1611.07012,"Authors:EdwardChoi,MohammadTahaBahadori,LeSong,WalterF.Stewart,JimengSun","        Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain: -Data insufficiency:Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. -Interpretation:The representations learned by deep learning methods should align with medical knowledge. To address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. Based on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.        △ Less","1 April, 2017","cs.LG,stat.ML",
"              The ββ-model for Random Graphs --- Regression, Cramér-Rao Bounds, and Hypothesis Testing          ",1611.05699,https://arxiv.org/abs/1611.05699,https://arxiv.org/pdf/1611.05699,"Authors:JohanWahlström,IsaacSkog,PatricioS.LaRosa,PeterHändel,AryeNehorai","        We develop a maximum-likelihood based method for regression in a setting where the dependent variable is a random graph and covariates are available on a graph-level. The model generalizes the well-known ββ-model for random graphs by replacing the constant model parameters with regression functions. Cramér-Rao bounds are derived for the undirected ββ-model, the directed ββ-model, and the generalized ββ-model. The corresponding maximum likelihood estimators are compared to the bounds by means of simulations. Moreover, examples are given on how to use the presented maximum likelihood estimators to test for directionality and significance. Last, the applicability of the model is demonstrated using dynamic social network data describing communication among healthcare workers.        △ Less","14 November, 2016",stat.ME,10.1109/TSP.2017.2691667 
              Variable Neighborhood Search Algorithms for the multi-depot dial-a-ride problem with heterogeneous vehicles and users          ,1611.05187,https://arxiv.org/abs/1611.05187,https://arxiv.org/pdf/1611.05187,"Authors:PaoloDetti,GaraziZabaloManriquedeLara","        In this work, a study on Variable Neighborhood Search algorithms for multi-depot dial-a-ride problems is presented. In dial-a-ride problems patients need to be transported from pre-specified pickup locations to pre-specified delivery locations, under different considerations. The addressed problem presents several constraints and features, such as heterogeneous vehicles, distributed in different depots, and heterogeneous patients. The aim is of minimizing the total routing cost, while respecting time-window, ride-time, capacity and route duration constraints. The objective of the study is of determining the best algorithm configuration in terms of initial solution, neighborhood and local search procedures. At this aim, two different procedures for the computation of an initial solution, six different type of neighborhoods and five local search procedures, where only intra-route changes are made, have been considered and compared.  We have also evaluated an ""adjusting procedure"" that aims to produce feasible solutions from infeasible solutions with small constraints violations. The different VNS algorithms have been tested on instances from literature as well as on random instances arising from a real-world healthcare application.        △ Less","16 November, 2016","cs.DM,cs.AI",
              Prognostics of Surgical Site Infections using Dynamic Health Data          ,1611.04049,https://arxiv.org/abs/1611.04049,https://arxiv.org/pdf/1611.04049,"Authors:ChuyangKe,YanJin,HeatherEvans,BillLober,XiaoningQian,JiLiu,ShuaiHuang","        Surgical Site Infection (SSI) is a national priority in healthcare research. Much research attention has been attracted to develop better SSI risk prediction models. However, most of the existing SSI risk prediction models are built on static risk factors such as comorbidities and operative factors. In this paper, we investigate the use of the dynamic wound data for SSI risk prediction. There have been emerging mobile health (mHealth) tools that can closely monitor the patients and generate continuous measurements of many wound-related variables and other evolving clinical variables. Since existing prediction models of SSI have quite limited capacity to utilize the evolving clinical data, we develop the corresponding solution to equip these mHealth tools with decision-making capabilities for SSI prediction with a seamless assembly of several machine learning models to tackle the analytic challenges arising from the spatial-temporal data. The basic idea is to exploit the low-rank property of the spatial-temporal data via the bilinear formulation, and further enhance it with automatic missing data imputation by the matrix completion technique. We derive efficient optimization algorithms to implement these models and demonstrate the superior performances of our new predictive model on a real-world dataset of SSI, compared to a range of state-of-the-art methods.        △ Less","12 November, 2016",cs.LG,
              Estimating Dynamic Treatment Regimes in Mobile Health Using V-learning          ,1611.03531,https://arxiv.org/abs/1611.03531,https://arxiv.org/pdf/1611.03531,"Authors:DanielJ.Luckett,EricB.Laber,AnnaR.Kahkoska,DavidM.Maahs,ElizabethMayer-Davis,MichaelR.Kosorok","        The vision for precision medicine is to use individual patient characteristics to inform a personalized treatment plan that leads to the best healthcare possible for each patient. Mobile technologies have an important role to play in this vision as they offer a means to monitor a patient's health status in real-time and subsequently to deliver interventions if, when, and in the dose that they are needed. Dynamic treatment regimes formalize individualized treatment plans as sequences of decision rules, one per stage of clinical intervention, that map current patient information to a recommended treatment. However, existing methods for estimating optimal dynamic treatment regimes are designed for a small number of fixed decision points occurring on a coarse time-scale. We propose a new reinforcement learning method for estimating an optimal treatment regime that is applicable to data collected using mobile technologies in an outpatient setting. The proposed method accommodates an indefinite time horizon and minute-by-minute decision making that are common in mobile health applications. We show the proposed estimators are consistent and asymptotically normal under mild conditions. The proposed methods are applied to estimate an optimal dynamic treatment regime for controlling blood glucose levels in patients with type 1 diabetes.        △ Less","14 October, 2017",stat.ML,
              Activity Recognition Based on Micro-Doppler Signature with In-Home Wi-Fi          ,1611.01801,https://arxiv.org/abs/1611.01801,https://arxiv.org/pdf/1611.01801,"Authors:QingchaoChen,BoTan,KevinChetty,KarlWoodbridge","        Device free activity recognition and monitoring has become a promising research area with increasing public interest in pattern of life monitoring and chronic health conditions. This paper proposes a novel framework for in-home Wi-Fi signal-based activity recognition in e-healthcare applications using passive micro-Doppler (m-D) signature classification. The framework includes signal modeling, Doppler extraction and m-D classification. A data collection campaign was designed to verify the framework where six m-D signatures corresponding to typical daily activities are sucessfully detected and classified using our software defined radio (SDR) demo system. Analysis of the data focussed on potential discriminative characteristics, such as maximum Doppler frequency and time duration of activity. Finally, a sparsity induced classifier is applied for adaptting the method in healthcare application scenarios and the results are compared with those from the well-known Support Vector Machine (SVM) method.        △ Less","6 November, 2016",cs.OH,
              Citation algorithms for identifying research milestones driving biomedical innovation          ,1611.01658,https://arxiv.org/abs/1611.01658,https://arxiv.org/pdf/1611.01658,"Authors:JordanA.Comins,LoetLeydesdorff","        Scientific activity plays a major role in innovation for biomedicine and healthcare. For instance, fundamental research on disease pathologies and mechanisms can generate potential targets for drug therapy. This co-evolution is punctuated by papers which provide new perspectives and open new domains. Despite the relationship between scientific discovery and biomedical advancement, identifying these research milestones that truly impact biomedical innovation can be difficult and is largely based solely on the opinions of subject matter experts. Here, we consider whether a new class of citation algorithms that identify seminal scientific works in a field, Reference Publication Year Spectroscopy (RPYS) and multi-RPYS, can identify the connections between innovation (e.g. therapeutic treatments) and the foundational research underlying them. Specifically, we assess whether the results of these analytic techniques converge with expert opinions on research milestones driving biomedical innovation in the treatment of Basal Cell Carcinoma. Our results show that these algorithms successfully identify the majority of milestone papers detailed by experts (Wong and Dlugosz 2014) thereby validating the power of these algorithms to converge on independent opinions of seminal scientific works derived by subject matter experts. These advances offer an opportunity to identify scientific activities enabling innovation in biomedicine.        △ Less","5 November, 2016",cs.DL,
"              Overview of Spintronic Sensors, Internet of Things, and Smart Living          ",1611.00317,https://arxiv.org/abs/1611.00317,https://arxiv.org/pdf/1611.00317,"Authors:X.Liu,K.H.Lam,K.Zhu,C.Zheng,X.Li,Y.Du,ChunhuaLiu,P.W.T.Pong","        Smart living is a trending lifestyle that envisions lower energy consumption, sound public services, and better quality of life for human being. The Internet of Things (IoT) is a compelling platform connecting various sensors around us to the Internet, providing great opportunities for the realization of smart living. Spintronic sensors with superb measuring ability and multiple unique advantages can be an important piece of cornerstone for IoT. In this review, we discuss successful applications of spintronic sensors in electrical current sensing, transmission and distribution lines monitoring, vehicle detection, and biodetection. Traditional monitoring systems with limited sensors and wired communication can merely collect fragmented data in the application domains. In this paper, the wireless spintronic sensor networks (WSSNs) will be proposed and illustrated to provide pervasive monitoring systems, which facilitate the intelligent surveillance and management over building, power grid, transport, and healthcare. The database of collected information will be of great use to the policy making in public services and city planning. This work provides insights for realizing smart living through the integration of IoT with spintronic sensor technology.        △ Less","29 August, 2016",cs.OH,
              On Regulatory and Organizational Constraints in Visualization Design and Evaluation          ,1610.10056,https://arxiv.org/abs/1610.10056,https://arxiv.org/pdf/1610.10056,"Authors:AnamariaCrisan,JenniferL.Gardy,TamaraMunzner","        Problem-based visualization research provides explicit guidance toward identifying and designing for the needs of users, but absent is more concrete guidance toward factors external to a user's needs that also have implications for visualization design and evaluation. This lack of more explicit guidance can leave visualization researchers and practitioners vulnerable to unforeseen constraints beyond the user's needs that can affect the validity of evaluations, or even lead to the premature termination of a project. Here we explore two types of external constraints in depth, regulatory and organizational constraints, and describe how these constraints impact visualization design and evaluation. By borrowing from techniques in software development, project management, and visualization research we recommend strategies for identifying, mitigating, and evaluating these external constraints through a design study methodology. Finally, we present an application of those recommendations in a healthcare case study. We argue that by explicitly incorporating external constraints into visualization design and evaluation, researchers and practitioners can improve the utility and validity of their visualization solution and improve the likelihood of successful collaborations with industries where external constraints are more present.        △ Less","31 October, 2016",cs.HC,10.1145/2993901.2993911 
              Sparse Hierarchical Tucker Factorization and its Application to Healthcare,1610.07722,https://arxiv.org/abs/1610.07722,https://arxiv.org/pdf/1610.07722,"Authors:IoakeimPerros,RobertChen,RichardVuduc,JimengSun","        We propose a new tensor factorization method, called the Sparse Hierarchical-Tucker (Sparse H-Tucker), for sparse and high-order data tensors. Sparse H-Tucker is inspired by its namesake, the classical Hierarchical Tucker method, which aims to compute a tree-structured factorization of an input data set that may be readily interpreted by a domain expert. However, Sparse H-Tucker uses a nested sampling technique to overcome a key scalability problem in Hierarchical Tucker, which is the creation of an unwieldy intermediate dense core tensor; the result of our approach is a faster, more space-efficient, and more accurate method. We extensively test our method on a real healthcare dataset, which is collected from 30K patients and results in an 18th order sparse data tensor. Unlike competing methods, Sparse H-Tucker can analyze the full data set on a single multi-threaded machine. It can also do so more accurately and in less time than the state-of-the-art: on a 12th order subset of the input data, Sparse H-Tucker is 18x more accurate and 7.5x faster than a previously state-of-the-art method. Even for analyzing low order tensors (e.g., 4-order), our method requires close to an order of magnitude less time and over two orders of magnitude less memory, as compared to traditional tensor factorization methods such as CP and Tucker. Moreover, we observe that Sparse H-Tucker scales nearly linearly in the number of non-zero tensor elements. The resulting model also provides an interpretable disease hierarchy, which is confirmed by a clinical expert.        △ Less","25 October, 2016","cs.LG,cs.NA",
              Sequential Learning without Feedback          ,1610.05394,https://arxiv.org/abs/1610.05394,https://arxiv.org/pdf/1610.05394,"Authors:ManjeshHanawal,CsabaSzepesvari,VenkateshSaligrama","        In many security and healthcare systems a sequence of features/sensors/tests are used for detection and diagnosis. Each test outputs a prediction of the latent state, and carries with it inherent costs. Our objective is to {\it learn} strategies for selecting tests to optimize accuracy \& costs. Unfortunately it is often impossible to acquire in-situ ground truth annotations and we are left with the problem of unsupervised sensor selection (USS). We pose USS as a version of stochastic partial monitoring problem with an {\it unusual} reward structure (even noisy annotations are unavailable). Unsurprisingly no learner can achieve sublinear regret without further assumptions. To this end we propose the notion of weak-dominance. This is a condition on the joint probability distribution of test outputs and latent state and says that whenever a test is accurate on an example, a later test in the sequence is likely to be accurate as well. We empirically verify that weak dominance holds on real datasets and prove that it is a maximal condition for achieving sublinear regret. We reduce USS to a special case of multi-armed bandit problem with side information and develop polynomial time algorithms that achieve sublinear regret.        △ Less","17 October, 2016",cs.LG,
              A Budget Feasible Mechanism for Hiring Doctors in E-Healthcare,1610.04454,https://arxiv.org/abs/1610.04454,https://arxiv.org/pdf/1610.04454,"Authors:VikashKumarSingh,SajalMukhopadhyay,FatosXhafa,AniruddhSharma","        Throughout the past decade, there has been an extensive research on scheduling the hospital resources such as the operation theatre(s) (OTs) and the experts (such as nurses, doctors etc.) inside the hospitals. With the technological growth, mainly advancement in communication media (such as smart phones, video conferencing, smart watches etc.) one may think of taking the expertise by the doctors (distributed around the globe) from outside the in-house hospitals. Earlier this interesting situation of hiring doctors from outside the hospitals has been studied from monetary (with patient having infinite budget) and non-monetary perspectives in strategic setting. In this paper, the more realistic situation is studied in terms of hiring the doctors from outside the hospital when a patient is constrained by budget. Our proposed mechanisms follow the two pass mechanism design framework each consisting of allocation rule and payment rule. Through simulations, we evaluate the performance and validate our proposed mechanisms.        △ Less","14 February, 2018",cs.GT,
              Perspectives on Surgical Data Science          ,1610.04276,https://arxiv.org/abs/1610.04276,https://arxiv.org/pdf/1610.04276,"Authors:S.SwaroopVedula,MasaruIshii,GregoryD.Hager","        The availability of large amounts of data together with advances in analytical techniques afford an opportunity to address difficult challenges in ensuring that healthcare is safe, effective, efficient, patient-centered, equitable, and timely. Surgical care and training stand to tremendously gain through surgical data science. Herein, we discuss a few perspectives on the scope and objectives for surgical data science.        △ Less","13 October, 2016",cs.CY,
              Content Based Image Retrieval (CBIR) in Remote Clinical Diagnosis and Healthcare,1610.02902,https://arxiv.org/abs/1610.02902,https://arxiv.org/pdf/1610.02902,"Authors:AlbanyE.Herrmann,VaniaVieiraEstrela","        Content-Based Image Retrieval (CBIR) locates, retrieves and displays images alike to one given as a query, using a set of features. It demands accessible data in medical archives and from medical equipment, to infer meaning after some processing. A problem similar in some sense to the target image can aid clinicians. CBIR complements text-based retrieval and improves evidence-based diagnosis, administration, teaching, and research in healthcare. It facilitates visual/automatic diagnosis and decision-making in real-time remote consultation/screening, store-and-forward tests, home care assistance and overall patient surveillance. Metrics help comparing visual data and improve diagnostic. Specially designed architectures can benefit from the application scenario. CBIR use calls for file storage standardization, querying procedures, efficient image transmission, realistic databases, global availability, access simplicity, and Internet-based structures. This chapter recommends important and complex aspects required to handle visual content in healthcare.        △ Less","10 October, 2016",cs.CV,10.4018/978-1-4666-9978-6.ch039 
              Properties of Healthcare Teaming Networks as a Function of Network Construction Algorithms          ,1610.02575,https://arxiv.org/abs/1610.02575,https://arxiv.org/pdf/1610.02575,"Authors:MartinS.Zand,MelissaTrayhan,SamirA.Farooq,ChristopherFucile,GrourabGhoshal,RobertJ.White,CarolineM.Quill,AlexanderRosenberg,HugoSerrano,HassanChafi,TimothyBoudreau","        Network models of healthcare systems can be used to examine how providers collaborate, communicate, refer patients to each other. Most healthcare service network models have been constructed from patient claims data, using billing claims to link patients with providers. The data sets can be quite large, making standard methods for network construction computationally challenging and thus requiring the use of alternate construction algorithms. While these alternate methods have seen increasing use in generating healthcare networks, there is little to no literature comparing the differences in the structural properties of the generated networks. To address this issue, we compared the properties of healthcare networks constructed using different algorithms and the 2013 Medicare Part B outpatient claims data. Three different algorithms were compared: binning, sliding frame, and trace-route. Unipartite networks linking either providers or healthcare organizations by shared patients were built using each method. We found that each algorithm produced networks with substantially different topological properties. Provider networks adhered to a power law, and organization networks to a power law with exponential cutoff. Censoring networks to exclude edges with less than 11 shared patients, a common de-identification practice for healthcare network data, markedly reduced edge numbers and greatly altered measures of vertex prominence such as the betweenness centrality. We identified patterns in the distance patients travel between network providers, and most strikingly between providers in the Northeast United States and Florida. We conclude that the choice of network construction algorithm is critical for healthcare network analysis, and discuss the implications for selecting the algorithm best suited to the type of analysis to be performed.        △ Less","8 October, 2016","cs.SI,cs.DS,physics.soc-ph",10.1371/journal.pone.0175876 
              Experimental Characterization of In Vivo Wireless Communication Channels          ,1610.01516,https://arxiv.org/abs/1610.01516,https://arxiv.org/pdf/1610.01516,"Authors:A.FatihDemir,QammerH.Abbasi,Z.EsatAnkarali,MarwaQaraqe,ErchinSerpedin,HuseyinArslan","        In vivo wireless medical devices have a critical role in healthcare technologies due to their continuous health monitoring and noninvasive surgery capabilities. In order to fully exploit the potential of such devices, it is necessary to characterize the in vivo wireless communication channel which will help to build reliable and high-performance communication systems. This paper presents preliminary results of experimental characterization for this fascinating communications medium on a human cadaver and compares the results with numerical studies.        △ Less","9 September, 2016",cs.OH,10.1109/VTCFall.2015.7390942 
              Asynchronous Multi-Task Learning          ,1609.09563,https://arxiv.org/abs/1609.09563,https://arxiv.org/pdf/1609.09563,"Authors:InciM.Baytas,MingYan,AnilK.Jain,JiayuZhou","        Many real-world machine learning applications involve several learning tasks which are inter-related. For example, in healthcare domain, we need to learn a predictive model of a certain disease for many hospitals. The models for each hospital may be different because of the inherent differences in the distributions of the patient populations. However, the models are also closely related because of the nature of the learning tasks modeling the same disease. By simultaneously learning all the tasks, multi-task learning (MTL) paradigm performs inductive knowledge transfer among tasks to improve the generalization performance. When datasets for the learning tasks are stored at different locations, it may not always be feasible to transfer the data to provide a data-centralized computing environment due to various practical issues such as high data volume and privacy. In this paper, we propose a principled MTL framework for distributed and asynchronous optimization to address the aforementioned challenges. In our framework, gradient update does not wait for collecting the gradient information from all the tasks. Therefore, the proposed method is very efficient when the communication delay is too high for some task nodes. We show that many regularized MTL formulations can benefit from this framework, including the low-rank MTL for shared subspace learning. Empirical studies on both synthetic and real-world datasets demonstrate the efficiency and effectiveness of the proposed framework.        △ Less","29 September, 2016","cs.LG,cs.DC",
              Transforming building industry and health outcomes through social data-supported design          ,1609.08778,https://arxiv.org/abs/1609.08778,https://arxiv.org/pdf/1609.08778,"Authors:MelissaMarsh,IngridErickson,JonahBleckner","        A glaring reality of American industrialized society is that people spend a tremendous amount of their waking life in their workplace and other interior environments. Despite the amount of time that we spend in them, many of our constructed environments that we inhabit are not designed for the people and communities that rely on them. From a health and wellness perspective, there is a growing body of research on the ways that our interior environments and lack of exposure to natural elements systematically impacts our health and strains our healthcare system. In short, the spaces in which we live and work are a major public health issue and should be considered in this way. In this paper, we lay out a vision for using the underleveraged social data-available through social media-to inform the architecture, developer and real estate industries. The goal is ultimately a public health initiative: to create spaces that are healthier, more responsive, equitable and human-centric through social data-supported design.        △ Less","28 September, 2016",cs.CY,
              Stabilizing Linear Prediction Models using Autoencoder          ,1609.08752,https://arxiv.org/abs/1609.08752,https://arxiv.org/pdf/1609.08752,"Authors:ShivapratapGopakumar,TruyenTran,DinhPhung,SvethaVenkatesh","        To date, the instability of prognostic predictors in a sparse high dimensional model, which hinders their clinical adoption, has received little attention. Stable prediction is often overlooked in favour of performance. Yet, stability prevails as key when adopting models in critical areas as healthcare. Our study proposes a stabilization scheme by detecting higher order feature correlations. Using a linear model as basis for prediction, we achieve feature stability by regularising latent correlation in features. Latent higher order correlation among features is modelled using an autoencoder network. Stability is enhanced by combining a recent technique that uses a feature graph, and augmenting external unlabelled data for training the autoencoder network. Our experiments are conducted on a heart failure cohort from an Australian hospital. Stability was measured using Consistency index for feature subsets and signal-to-noise ratio for model parameters. Our methods demonstrated significant improvement in feature stability and model estimation stability when compared to baselines.        △ Less","27 September, 2016",stat.ML,
              Chronodes: Interactive Multi-focus Exploration of Event Sequences          ,1609.08535,https://arxiv.org/abs/1609.08535,https://arxiv.org/pdf/1609.08535,"Authors:PeterJPolackJr,Shang-TseChen,MinsukKahng,KayadeBarbaro,MoushumiSharmin,RahulBasole,DuenHorngChau","        The advent of mobile health technologies presents new challenges that existing visualizations, interactive tools, and algorithms are not yet designed to support. In dealing with uncertainty in sensor data and high-dimensional physiological records, we must seek to improve current tools that make sense of health data from traditional perspectives in event-based trend discovery. With Chronodes, a system developed to help researchers collect, interpret, and model mobile health (mHealth) data, we posit a series of interaction techniques that enable new approaches to understanding and exploring event-based data. From numerous and discontinuous mobile health data streams, Chronodes finds and visualizes frequent event sequences that reveal common chronological patterns across participants and days. By then promoting the sequences as interactive elements, Chronodes presents opportunities for finding, defining, and comparing cohorts of participants that exhibit particular behaviors. We applied Chronodes to a real 40GB mHealth dataset capturing about 400 hours of data. Through our pilot study with 20 behavioral and biomedical health experts, we gained insights into Chronodes' efficacy, limitations, and potential applicability to a wide range of healthcare scenarios.        △ Less","27 September, 2016",cs.HC,
              Toward a Science of Autonomy for Physical Systems: Paths          ,1609.05814,https://arxiv.org/abs/1609.05814,https://arxiv.org/pdf/1609.05814,"Authors:PieterAbbeel,KenGoldberg,GregoryHager,JulieShah","        An Autonomous Physical System (APS) will be expected to reliably and independently evaluate, execute, and achieve goals while respecting surrounding rules, laws, or conventions. In doing so, an APS must rely on a broad spectrum of dynamic, complex, and often imprecise information about its surroundings, the task it is to perform, and its own sensors and actuators. For example, cleaning in a home or commercial setting requires the ability to perceive, grasp, and manipulate many physical objects, the ability to reliably perform a variety of subtasks such as washing, folding, and stacking, and knowledge about local conventions such as how objects are classified and where they should be stored. The information required for reliable autonomous operation may come from external sources and from the robot's own sensor observations or in the form of direct instruction by a trainer. Similar considerations apply across many domains - construction, manufacturing, in-home assistance, and healthcare. For example, surgeons spend many years learning about physiology and anatomy before they touch a patient. They then perform roughly 1000 surgeries under the tutelage of an expert surgeon, and they practice basic maneuvers such as suture tying thousands of times outside the operating room. All of these elements come together to achieve expertise at this task. Endowing a system with robust autonomy by traditional programming methods has thus far had limited success. Several promising new paths to acquiring and processing such data are emerging. This white paper outlines three promising research directions for enabling an APS to learn the physical and information skills necessary to perform tasks with independence and flexibility: Deep Reinforcement Learning, Human-Robot Interaction, and Cloud Robotics.        △ Less","19 September, 2016","cs.CY,cs.RO",
              Toward a Science of Autonomy for Physical Science: Healthcare,1609.05813,https://arxiv.org/abs/1609.05813,https://arxiv.org/pdf/1609.05813,"Authors:GregoryHager,EricHorvitz","        In Star Wars Episode V, we see Luke Skywalker being repaired by a surgical robot. In the context of the movie, this doesn't seem surprising or disturbing. After all, it is a long, long time ago, in a galaxy far, far away. It would never happen here. Or could it? Would we accept a robot as our doctor, our surgeon, or our in-home care specialist? Imagine walking into an operating room and no one was there. You are instructed to lie down on the operating table, and the OR system takes over. Would you feel comfortable with this possible future world?        △ Less","19 September, 2016",cs.CY,
              Calcite single crystals as hosts for atomic-scale entrapment and slow release of drugs          ,1609.05462,https://arxiv.org/abs/1609.05462,https://arxiv.org/pdf/1609.05462,"Authors:GiuliaMagnabosco,MatteoDiGiosia,IrynaPolishchuk,EvaWeber,SimonaFermani,AndreaBottoni,FrancescoZerbetto,BoazPokroy,StefaniaRapino,GiuseppeFalini,MatteoCalvaresi","        This study presents a complete structural and biological characterization of Doxorubicin CaCO3 single crystals as a pH responsive drug carrier. Using a biomimetic approach, it was demonstrated that calcite single crystals are able, during their growth in presence of doxorubicin, to entrap drug molecules inside their lattice, along specific crystallographic directions. High resolution synchrotron powder diffraction measurements allowed the determination of the lattice distortion and microstructural parameters. Confocal microscopy confirmed that doxorubicin is uniformly embedded in the crystal and that the drug is not only adsorbed on the crystal surface. A slow release of DOX is obtained that occurs preferentially in proximity of the crystals, targeting cancer cells.        △ Less","18 September, 2016",cond-mat.mtrl-sci,
              Service Rate Control For Jobs with Decaying Value          ,1609.05355,https://arxiv.org/abs/1609.05355,https://arxiv.org/pdf/1609.05355,"Authors:NealMaster,NicholasBambos","        The task of completing jobs with decaying value arises in a number of application areas including healthcare operations, communications engineering, and perishable inventory control. We consider a system in which a single server completes a finite sequence of jobs in discrete time while a controller dynamically adjusts the service rate. During service, the value of the job decays so that a greater reward is received for having shorter service times. We incorporate a non-decreasing cost for holding jobs and a non-decreasing cost on the service rate. The controller aims to minimize the total cost of servicing the set of jobs. We show that the optimal policy is non-decreasing in the number of jobs remaining -- when there are more jobs in the system the controller should use a higher service rate. The optimal policy does not necessarily vary monotonically with the residual job value, but we give algebraic conditions which can be used to determine when it does. These conditions are then simplified in the case that the reward for completion is constant when the job has positive value and zero otherwise. These algebraic conditions are interesting because they can be verified without using algorithms like value iteration and policy iteration to explicitly compute the optimal policy. We also discuss some future modeling extensions.        △ Less","17 September, 2016","math.OC,math.PR",
              Chinese Medical Device Market and The Investment Vector          ,1609.05200,https://arxiv.org/abs/1609.05200,https://arxiv.org/pdf/1609.05200,"Authors:WeifanZhang,RebeccaLiu,ChrisChatwin","        China has attracted increasing amounts of foreign investment since it opened its doors to the world and whilst many analysts have focused on foreign investment in popular areas, little has been written about medical device investment. The purpose of this article is to analyze the status of the Chinese medical device market from the perspective of the healthcare industry and its important market drivers; the study reveals that the medical device market has significant growth potential. This article aims to identify and assess the profitable sectors of medical device technologies as a guide for international companies and investors.        △ Less","7 March, 2016",q-fin.EC,
              Long-Term Trends in the Public Perception of Artificial Intelligence          ,1609.04904,https://arxiv.org/abs/1609.04904,https://arxiv.org/pdf/1609.04904,"Authors:EthanFast,EricHorvitz","        Analyses of text corpora over time can reveal trends in beliefs, interest, and sentiment about a topic. We focus on views expressed about artificial intelligence (AI) in the New York Times over a 30-year period. General interest, awareness, and discussion about AI has waxed and waned since the field was founded in 1956. We present a set of measures that captures levels of engagement, measures of pessimism and optimism, the prevalence of specific hopes and concerns, and topics that are linked to discussions about AI over decades. We find that discussion of AI has increased sharply since 2009, and that these discussions have been consistently more optimistic than pessimistic. However, when we examine specific concerns, we find that worries of loss of control of AI, ethical concerns for AI, and the negative impact of AI on work have grown in recent years. We also find that hopes for AI in healthcare and education have increased over time.        △ Less","2 December, 2016","cs.CL,cs.AI,cs.CY",
"              Integrated, reliable and cloud-based personal health record: A scoping review          ",1609.03615,https://arxiv.org/abs/1609.03615,https://arxiv.org/pdf/1609.03615,"Authors:JesúsRomero,PabloLópez,JoséLuisVázquezNoguera,CristianCappo,DiegoP.Pinto-Roa,CynthiaVillalba","        Personal Health Records (PHR) emerge as an alternative to integrate patient's health information to give a global view of patients' status. However, integration is not a trivial feature when dealing with a variety electronic health systems from healthcare centers. Access to PHR sensitive information must comply with privacy policies defined by the patient. Architecture PHR design should be in accordance to these, and take advantage of nowadays technology. Cloud computing is a current technology that provides scalability, ubiquity, and elasticity features. This paper presents a scoping review related to PHR systems that achieve three characteristics: integrated, reliable and cloud-based. We found 101 articles that addressed those characteristics. We identified four main research topics: proposal/developed systems, PHR recommendations for development, system integration and standards, and security and privacy. Integration is tackled with HL7 CDA standard. Information reliability is based in ABE security-privacy mechanism. Cloud-based technology access is achieved via SOA.        △ Less","12 September, 2016",cs.CY,10.5121/hiij.2016.5301 
              Bio-Inspired Filter Banks for SSVEP-based Brain-Computer Interfaces          ,1609.03224,https://arxiv.org/abs/1609.03224,https://arxiv.org/pdf/1609.03224,"Authors:A.FatihDemir,HuseyinArslan,IsmailUysal","        Brain-computer interfaces (BCI) have the potential to play a vital role in future healthcare technologies by providing an alternative way of communication and control. More specifically, steady-state visual evoked potential (SSVEP) based BCIs have the advantage of higher accuracy and higher information transfer rate (ITR). In order to fully exploit the capabilities of such devices, it is necessary to understand the features of SSVEP and design the system considering its biological characteristics. This paper introduces bio-inspired filter banks (BIFB) for a novel SSVEP frequency detection method. It is known that SSVEP response to a flickering visual stimulus is frequency selective and gets weaker as the frequency of the stimuli increases. In the proposed approach, the gain and bandwidth of the filters are designed and tuned based on these characteristics while also incorporating harmonic SSVEP responses. This method not only improves the accuracy but also increases the available number of commands by allowing the use of stimuli frequencies elicit weak SSVEP responses. The BIFB method achieved reliable performance when tested on datasets available online and compared with two well-known SSVEP frequency detection methods, power spectral density analysis (PSDA) and canonical correlation analysis (CCA). The results show the potential of bio-inspired design which will be extended to include further SSVEP characteristic (e.g. time-domain waveform) for future SSVEP based BCIs.        △ Less","11 September, 2016","cs.HC,q-bio.NC",10.1109/BHI.2016.7455855 
              Numerical Characterization of In Vivo Wireless Communication Channels          ,1609.02965,https://arxiv.org/abs/1609.02965,https://arxiv.org/pdf/1609.02965,"Authors:A.FatihDemir,QammerH.Abbasi,Z.EsadAnkarali,ErchinSerpedin,HuseyinArslan","        In this paper, we numerically investigated the in vivo wireless communication channel for the human male torso at 915 MHz. The results show that in vivo channel is different from the classical communication channel, and location dependency is very critical for link budget calculations. A statistical path loss model based on the angle, depth and body region is introduced for near and far field regions. Furthermore, multipath characteristics are investigated using a power delay profile as well.        △ Less","9 September, 2016",cs.IT,10.1109/IMWS-BIO.2014.7032392 
              RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism          ,1608.05745,https://arxiv.org/abs/1608.05745,https://arxiv.org/pdf/1608.05745,"Authors:EdwardChoi,MohammadTahaBahadori,JoshuaA.Kulas,AndySchuetz,WalterF.Stewart,JimengSun","        Accuracy and interpretability are two dominant features of successful predictive models. Typically, a choice must be made in favor of complex black box models such as recurrent neural networks (RNN) for accuracy versus less accurate but more interpretable traditional models such as logistic regression. This tradeoff poses challenges in medicine where both accuracy and interpretability are important. We addressed this challenge by developing the REverse Time AttentIoN model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN achieves high accuracy while remaining clinically interpretable and is based on a two-level neural attention model that detects influential past visits and significant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that recent clinical visits are likely to receive higher attention. RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K patients over an 8 year period and demonstrated predictive accuracy and computational scalability comparable to state-of-the-art methods such as RNN, and ease of interpretability comparable to traditional models.        △ Less","26 February, 2017","cs.LG,cs.AI,cs.NE",
              Medical image denoising using convolutional denoising autoencoders          ,1608.04667,https://arxiv.org/abs/1608.04667,https://arxiv.org/pdf/1608.04667,Authors:LovedeepGondara,"        Image denoising is an important pre-processing step in medical image analysis. Different algorithms have been proposed in past three decades with varying denoising performances. More recently, having outperformed all conventional methods, deep learning based models have shown a great promise. These methods are however limited for requirement of large training sample size and high computational costs. In this paper we show that using small sample size, denoising autoencoders constructed using convolutional layers can be used for efficient denoising of medical images. Heterogeneous images can be combined to boost sample size for increased denoising performance. Simplest of networks can reconstruct images with corruption levels so high that noise and signal are not differentiable to human eye.        △ Less","17 September, 2016","cs.CV,stat.ML",10.1109/ICDMW.2016.0041 
              A Pathophysiological Model-Driven Communication for Dynamic Distributed Medical Best Practice Guidance Systems          ,1608.04661,https://arxiv.org/abs/1608.04661,https://arxiv.org/pdf/1608.04661,"Authors:MohammadHosseini,YuJiang,PoliangWu,RichardB.BerlinJr.,ShangpingRen,LuiSha","        There is a great divide between rural and urban areas, particularly in medical emergency care. Although medical best practice guidelines exist in hospital handbooks, they are often lengthy and difficult to apply clinically. The challenges are exaggerated for doctors in rural areas and emergency medical technicians (EMT) during patient transport.  In this paper, we propose the concept of distributed executable medical best practice guidance systems to assist adherence to best practice from the time that a patient first presents at a rural hospital, through diagnosis and ambulance transfer to arrival and treatment at a regional tertiary hospital center. We codify complex medical knowledge in the form of simplified distributed executable disease automata, from the thin automata at rural hospitals to the rich automata in the regional center hospitals. However, a main challenge is how to efficiently and safely synchronize distributed best practice models as the communication among medical facilities, devices, and professionals generates a large number of messages. This complex problem of patient diagnosis and transport from rural to center facility is also fraught with many uncertainties and changes resulting in a high degree of dynamism. To address this situation, we propose a pathophysiological model-driven message exchange communication architecture that ensures the real-time and dynamic requirements of synchronization among distributed emergency best-practice models are met in a reliable and safe manner. Taking the signs, symptoms, and progress of stroke patients transported across a geographically distributed healthcare network as the motivating use case, we implement our communication system and apply it to our developed best practice automata using laboratory simulations. Our proof-of-concept experiments shows there is potential for the use of our system in a wide variety of domains.        △ Less","15 August, 2016",cs.CY,
              Scalable Modeling of Multivariate Longitudinal Data for Prediction of Chronic Kidney Disease Progression          ,1608.04615,https://arxiv.org/abs/1608.04615,https://arxiv.org/pdf/1608.04615,"Authors:JosephFutoma,MarkSendak,C.BlakeCameron,KatherineHeller","        Prediction of the future trajectory of a disease is an important challenge for personalized medicine and population health management. However, many complex chronic diseases exhibit large degrees of heterogeneity, and furthermore there is not always a single readily available biomarker to quantify disease severity. Even when such a clinical variable exists, there are often additional related biomarkers routinely measured for patients that may better inform the predictions of their future disease state. To this end, we propose a novel probabilistic generative model for multivariate longitudinal data that captures dependencies between multivariate trajectories. We use a Gaussian process based regression model for each individual trajectory, and build off ideas from latent class models to induce dependence between their mean functions. We fit our method using a scalable variational inference algorithm to a large dataset of longitudinal electronic patient health records, and find that it improves dynamic predictions compared to a recent state of the art method. Our local accountable care organization then uses the model predictions during chart reviews of high risk patients with chronic kidney disease.        △ Less","16 August, 2016","stat.ML,stat.AP,stat.ME",
              Conformalized density- and distance-based anomaly detection in time-series data          ,1608.04585,https://arxiv.org/abs/1608.04585,https://arxiv.org/pdf/1608.04585,"Authors:EvgenyBurnaev,VladislavIshimtsev","        Anomalies (unusual patterns) in time-series data give essential, and often actionable information in critical situations. Examples can be found in such fields as healthcare, intrusion detection, finance, security and flight safety. In this paper we propose new conformalized density- and distance-based anomaly detection algorithms for a one-dimensional time-series data. The algorithms use a combination of a feature extraction method, an approach to assess a score whether a new observation differs significantly from a previously observed data, and a probabilistic interpretation of this score based on the conformal paradigm.        △ Less","16 August, 2016","stat.AP,cs.LG,stat.ML",
              Multi-source Hierarchical Prediction Consolidation          ,1608.03344,https://arxiv.org/abs/1608.03344,https://arxiv.org/pdf/1608.03344,"Authors:ChenweiZhang,SihongXie,YaliangLi,JingGao,WeiFan,PhilipS.Yu","        In big data applications such as healthcare data mining, due to privacy concerns, it is necessary to collect predictions from multiple information sources for the same instance, with raw features being discarded or withheld when aggregating multiple predictions. Besides, crowd-sourced labels need to be aggregated to estimate the ground truth of the data. Because of the imperfect predictive models or human crowdsourcing workers, noisy and conflicting information is ubiquitous and inevitable. Although state-of-the-art aggregation methods have been proposed to handle label spaces with flat structures, as the label space is becoming more and more complicated, aggregation under a label hierarchical structure becomes necessary but has been largely ignored. These label hierarchies can be quite informative as they are usually created by domain experts to make sense of highly complex label correlations for many real-world cases like protein functionality interactions or disease relationships.  We propose a novel multi-source hierarchical prediction consolidation method to effectively exploits the complicated hierarchical label structures to resolve the noisy and conflicting information that inherently originates from multiple imperfect sources. We formulate the problem as an optimization problem with a closed-form solution. The proposed method captures the smoothness overall information sources as well as penalizing any consolidation result that violates the constraints derived from the label hierarchy. The hierarchical instance similarity, as well as the consolidation result, are inferred in a totally unsupervised, iterative fashion. Experimental results on both synthetic and real-world datasets show the effectiveness of the proposed method over existing alternatives.        △ Less","10 August, 2016","cs.DB,cs.LG",
              Deep Convolutional Neural Networks for Microscopy-Based Point of Care Diagnostics          ,1608.02989,https://arxiv.org/abs/1608.02989,https://arxiv.org/pdf/1608.02989,"Authors:JohnA.Quinn,RoseNakasi,PiusK.B.Mugagga,PatrickByanyima,WilliamLubega,AlfredAndama","        Point of care diagnostics using microscopy and computer vision methods have been applied to a number of practical problems, and are particularly relevant to low-income, high disease burden areas. However, this is subject to the limitations in sensitivity and specificity of the computer vision methods used. In general, deep learning has recently revolutionised the field of computer vision, in some cases surpassing human performance for other object recognition tasks. In this paper, we evaluate the performance of deep convolutional neural networks on three different microscopy tasks: diagnosis of malaria in thick blood smears, tuberculosis in sputum samples, and intestinal parasite eggs in stool samples. In all cases accuracy is very high and substantially better than an alternative approach more representative of traditional medical imaging techniques.        △ Less","9 August, 2016",cs.CV,
              Revisiting Causality Inference in Memory-less Transition Networks          ,1608.02658,https://arxiv.org/abs/1608.02658,https://arxiv.org/pdf/1608.02658,"Authors:AbbasShojaee,IsuruRanasinghe,AlirezaAni","        Several methods exist to infer causal networks from massive volumes of observational data. However, almost all existing methods require a considerable length of time series data to capture cause and effect relationships. In contrast, memory-less transition networks or Markov Chain data, which refers to one-step transitions to and from an event, have not been explored for causality inference even though such data is widely available. We find that causal network can be inferred from characteristics of four unique distribution zones around each event. We call this Composition of Transitions and show that cause, effect, and random events exhibit different behavior in their compositions. We applied machine learning models to learn these different behaviors and to infer causality. We name this new method Causality Inference using Composition of Transitions (CICT). To evaluate CICT, we used an administrative inpatient healthcare dataset to set up a network of patients transitions between different diagnoses. We show that CICT is highly accurate in inferring whether the transition between a pair of events is causal or random and performs well in identifying the direction of causality in a bi-directional association.        △ Less","21 December, 2016","stat.ML,cs.AI,nlin.CD,physics.data-an",
              Uncovering Voice Misuse Using Symbolic Mismatch          ,1608.02301,https://arxiv.org/abs/1608.02301,https://arxiv.org/pdf/1608.02301,"Authors:MarzyehGhassemi,ZeeshanSyed,DaryushD.Mehta,JarradH.VanStan,RobertE.Hillman,JohnV.Guttag","        Voice disorders affect an estimated 14 million working-aged Americans, and many more worldwide. We present the first large scale study of vocal misuse based on long-term ambulatory data collected by an accelerometer placed on the neck. We investigate an unsupervised data mining approach to uncovering latent information about voice misuse.  We segment signals from over 253 days of data from 22 subjects into over a hundred million single glottal pulses (closures of the vocal folds), cluster segments into symbols, and use symbolic mismatch to uncover differences between patients and matched controls, and between patients pre- and post-treatment. Our results show significant behavioral differences between patients and controls, as well as between some pre- and post-treatment patients. Our proposed approach provides an objective basis for helping diagnose behavioral voice disorders, and is a first step towards a more data-driven understanding of the impact of voice therapy.        △ Less","7 August, 2016",cs.LG,
              Deep Survival Analysis          ,1608.02158,https://arxiv.org/abs/1608.02158,https://arxiv.org/pdf/1608.02158,"Authors:RajeshRanganath,AdlerPerotte,NoémieElhadad,DavidBlei","        The electronic health record (EHR) provides an unprecedented opportunity to build actionable tools to support physicians at the point of care. In this paper, we investigate survival analysis in the context of EHR data. We introduce deep survival analysis, a hierarchical generative approach to survival analysis. It departs from previous approaches in two primary ways: (1) all observations, including covariates, are modeled jointly conditioned on a rich latent structure; and (2) the observations are aligned by their failure time, rather than by an arbitrary time zero as in traditional survival analysis. Further, it (3) scalably handles heterogeneous (continuous and discrete) data types that occur in the EHR. We validate deep survival analysis model by stratifying patients according to risk of developing coronary heart disease (CHD). Specifically, we study a dataset of 313,000 patients corresponding to 5.5 million months of observations. When compared to the clinically validated Framingham CHD risk score, deep survival analysis is significantly superior in stratifying patients according to their risk.        △ Less","18 September, 2016","stat.ML,cs.AI,stat.ME",
              Transferring Knowledge from Text to Predict Disease Onset          ,1608.02071,https://arxiv.org/abs/1608.02071,https://arxiv.org/pdf/1608.02071,"Authors:YunLiu,Kun-TaChuang,Fu-WenLiang,Huey-JenSu,CollinM.Stultz,JohnV.Guttag","        In many domains such as medicine, training data is in short supply. In such cases, external knowledge is often helpful in building predictive models. We propose a novel method to incorporate publicly available domain expertise to build accurate models. Specifically, we use word2vec models trained on a domain-specific corpus to estimate the relevance of each feature's text description to the prediction problem. We use these relevance estimates to rescale the features, causing more important features to experience weaker regularization.  We apply our method to predict the onset of five chronic diseases in the next five years in two genders and two age groups. Our rescaling approach improves the accuracy of the model, particularly when there are few positive examples. Furthermore, our method selects 60% fewer features, easing interpretation by physicians. Our method is applicable to other domains where feature and outcome descriptions are available.        △ Less","6 August, 2016","cs.LG,cs.CL",
              Mitochondria-based Renal Cell Carcinoma Subtyping: Learning from Deep vs. Flat Feature Representations          ,1608.00842,https://arxiv.org/abs/1608.00842,https://arxiv.org/pdf/1608.00842,"Authors:PeterJ.Schüffler,JudySarungbam,HassanMuhammad,EdReznik,SatishK.Tickoo,ThomasJ.Fuchs","        Accurate subtyping of renal cell carcinoma (RCC) is of crucial importance for understanding disease progression and for making informed treatment decisions. New discoveries of significant alterations to mitochondria between subtypes make immunohistochemical (IHC) staining based image classification an imperative. Until now, accurate quantification and subtyping was made impossible by huge IHC variations, the absence of cell membrane staining for cytoplasm segmentation as well as the complete lack of systems for robust and reproducible image based classification. In this paper we present a comprehensive classification framework to overcome these challenges for tissue microarrays (TMA) of RCCs. We compare and evaluate models based on domain specific hand-crafted ""flat""-features versus ""deep"" feature representations from various layers of a pre-trained convolutional neural network (CNN). The best model reaches a cross-validation accuracy of 89%, which demonstrates for the first time, that robust mitochondria-based subtyping of renal cancer is feasible        △ Less","2 August, 2016",cs.LG,
              Identifiable Phenotyping using Constrained Non-Negative Matrix Factorization          ,1608.00704,https://arxiv.org/abs/1608.00704,https://arxiv.org/pdf/1608.00704,"Authors:ShalmaliJoshi,SuriyaGunasekar,DavidSontag,JoydeepGhosh","        This work proposes a new algorithm for automated and simultaneous phenotyping of multiple co-occurring medical conditions, also referred as comorbidities, using clinical notes from the electronic health records (EHRs). A basic latent factor estimation technique of non-negative matrix factorization (NMF) is augmented with domain specific constraints to obtain sparse latent factors that are anchored to a fixed set of chronic conditions. The proposed anchoring mechanism ensures a one-to-one identifiable and interpretable mapping between the latent factors and the target comorbidities. Qualitative assessment of the empirical results by clinical experts suggests that the proposed model learns clinically interpretable phenotypes while being predictive of 30 day mortality. The proposed method can be readily adapted to any non-negative EHR data across various healthcare institutions.        △ Less","20 September, 2016","stat.ML,cs.LG",
              A Survey of Visual Analysis of Human Motion and Its Applications          ,1608.00700,https://arxiv.org/abs/1608.00700,https://arxiv.org/pdf/1608.00700,Authors:QifeiWang,"        This paper summarizes the recent progress in human motion analysis and its applications. In the beginning, we reviewed the motion capture systems and the representation model of human's motion data. Next, we sketched the advanced human motion data processing technologies, including motion data filtering, temporal alignment, and segmentation. The following parts overview the state-of-the-art approaches of action recognition and dynamics measuring since these two are the most active research areas in human motion analysis. The last part discusses some emerging applications of the human motion analysis in healthcare, human robot interaction, security surveillance, virtual reality and animation. The promising research topics of human motion analysis in the future is also summarized in the last part.        △ Less","23 August, 2016","cs.CV,cs.AI",
              Clinical Tagging with Joint Probabilistic Models          ,1608.00686,https://arxiv.org/abs/1608.00686,https://arxiv.org/pdf/1608.00686,"Authors:YoniHalpern,StevenHorng,DavidSontag","        We describe a method for parameter estimation in bipartite probabilistic graphical models for joint prediction of clinical conditions from the electronic medical record. The method does not rely on the availability of gold-standard labels, but rather uses noisy labels, called anchors, for learning. We provide a likelihood-based objective and a moments-based initialization that are effective at learning the model parameters. The learned model is evaluated in a task of assigning a heldout clinical condition to patients based on retrospective analysis of the records, and outperforms baselines which do not account for the noisiness in the labels or do not model the conditions jointly.        △ Less","21 September, 2016","stat.ML,cs.LG",
              Multi-task Prediction of Disease Onsets from Longitudinal Lab Tests          ,1608.00647,https://arxiv.org/abs/1608.00647,https://arxiv.org/pdf/1608.00647,"Authors:NargesRazavian,JakeMarcus,DavidSontag","        Disparate areas of machine learning have benefited from models that can take raw data with little preprocessing as input and learn rich representations of that raw data in order to perform well on a given prediction task. We evaluate this approach in healthcare by using longitudinal measurements of lab tests, one of the more raw signals of a patient's health state widely available in clinical data, to predict disease onsets. In particular, we train a Long Short-Term Memory (LSTM) recurrent neural network and two novel convolutional neural networks for multi-task prediction of disease onset for 133 conditions based on 18 common lab tests measured over time in a cohort of 298K patients derived from 8 years of administrative claims data. We compare the neural networks to a logistic regression with several hand-engineered, clinically relevant features. We find that the representation-based learning approaches significantly outperform this baseline. We believe that our work suggests a new avenue for patient risk stratification based solely on lab results.        △ Less","20 September, 2016",cs.LG,
              Input-Output Non-Linear Dynamical Systems applied to Physiological Condition Monitoring          ,1608.00242,https://arxiv.org/abs/1608.00242,https://arxiv.org/pdf/1608.00242,"Authors:KonstantinosGeorgatzis,ChristopherK.I.Williams,ChristopherHawthorne",        We present a non-linear dynamical system for modelling the effect of drug infusions on the vital signs of patients admitted in Intensive Care Units (ICUs). More specifically we are interested in modelling the effect of a widely used anaesthetic drug (Propofol) on a patient's monitored depth of anaesthesia and haemodynamics. We compare our approach with one from the Pharmacokinetics/Pharmacodynamics (PK/PD) literature and show that we can provide significant improvements in performance without requiring the incorporation of expert physiological knowledge in our system.        △ Less,"8 October, 2016",cs.LG,
              Learning Robust Features using Deep Learning for Automatic Seizure Detection          ,1608.00220,https://arxiv.org/abs/1608.00220,https://arxiv.org/pdf/1608.00220,"Authors:PierreThodoroff,JoellePineau,AndrewLim","        We present and evaluate the capacity of a deep neural network to learn robust features from EEG to automatically detect seizures. This is a challenging problem because seizure manifestations on EEG are extremely variable both inter- and intra-patient. By simultaneously capturing spectral, temporal and spatial information our recurrent convolutional neural network learns a general spatially invariant representation of a seizure. The proposed approach exceeds significantly previous results obtained on cross-patient classifiers both in terms of sensitivity and false positive rate. Furthermore, our model proves to be robust to missing channel and variable electrode montage.        △ Less","31 July, 2016","cs.LG,cs.CV",
              Multi-task Learning with Weak Class Labels: Leveraging iEEG to Detect Cortical Lesions in Cryptogenic Epilepsy          ,1608.00148,https://arxiv.org/abs/1608.00148,https://arxiv.org/pdf/1608.00148,"Authors:BilalAhmed,ThomasThesen,KarenE.Blackmon,RubenKuzniecky,OrrinDevinsky,JenniferG.Dy,CarlaE.Brodley","        Multi-task learning (MTL) is useful for domains in which data originates from multiple sources that are individually under-sampled. MTL methods are able to learn classification models that have higher performance as compared to learning a single model by aggregating all the data together or learning a separate model for each data source. The performance of these methods relies on label accuracy. We address the problem of simultaneously learning multiple classifiers in the MTL framework when the training data has imprecise labels. We assume that there is an additional source of information that provides a score for each instance which reflects the certainty about its label. Modeling this score as being generated by an underlying ranking function, we augment the MTL framework with an added layer of supervision. This results in new MTL methods that are able to learn accurate classifiers while preserving the domain structure provided through the rank information. We apply these methods to the task of detecting abnormal cortical regions in the MRIs of patients suffering from focal epilepsy whose MRI were read as normal by expert neuroradiologists. In addition to the noisy labels provided by the results of surgical resection, we employ the results of an invasive intracranial-EEG exam as an additional source of label information. Our proposed methods are able to successfully detect abnormal regions for all patients in our dataset and achieve a higher performance as compared to baseline methods.        △ Less","30 July, 2016",cs.CV,
              gLOP: the global and Local Penalty for Capturing Predictive Heterogeneity          ,1608.00027,https://arxiv.org/abs/1608.00027,https://arxiv.org/pdf/1608.00027,"Authors:RhiannonV.Rose,DanielJ.Lizotte","        When faced with a supervised learning problem, we hope to have rich enough data to build a model that predicts future instances well. However, in practice, problems can exhibit predictive heterogeneity: most instances might be relatively easy to predict, while others might be predictive outliers for which a model trained on the entire dataset does not perform well. Identifying these can help focus future data collection. We present gLOP, the global and Local Penalty, a framework for capturing predictive heterogeneity and identifying predictive outliers. gLOP is based on penalized regression for multitask learning, which improves learning by leveraging training signal information from related tasks. We give two optimization algorithms for gLOP, one space-efficient, and another giving the full regularization path. We also characterize uniqueness in terms of the data and tuning parameters, and present empirical results on synthetic data and on two health research problems.        △ Less","29 July, 2016","stat.ML,cs.LG",
              Preterm Birth Prediction: Deriving Stable and Interpretable Rules from High Dimensional Data          ,1607.08310,https://arxiv.org/abs/1607.08310,https://arxiv.org/pdf/1607.08310,"Authors:TruyenTran,WeiLuo,DinhPhung,JonathanMorris,KristenRickard,SvethaVenkatesh","        Preterm births occur at an alarming rate of 10-15%. Preemies have a higher risk of infant mortality, developmental retardation and long-term disabilities. Predicting preterm birth is difficult, even for the most experienced clinicians. The most well-designed clinical study thus far reaches a modest sensitivity of 18.2-24.2% at specificity of 28.6-33.3%. We take a different approach by exploiting databases of normal hospital operations. We aims are twofold: (i) to derive an easy-to-use, interpretable prediction rule with quantified uncertainties, and (ii) to construct accurate classifiers for preterm birth prediction. Our approach is to automatically generate and select from hundreds (if not thousands) of possible predictors using stability-aware techniques. Derived from a large database of 15,814 women, our simplified prediction rule with only 10 items has sensitivity of 62.3% at specificity of 81.5%.        △ Less","28 July, 2016",stat.ML,
              Diagnostic Prediction Using Discomfort Drawings with IBTM          ,1607.08206,https://arxiv.org/abs/1607.08206,https://arxiv.org/pdf/1607.08206,"Authors:ChengZhang,HedvigKjellstrom,CarlHenrikEk,BoC.Bertilson","        In this paper, we explore the possibility to apply machine learning to make diagnostic predictions using discomfort drawings. A discomfort drawing is an intuitive way for patients to express discomfort and pain related symptoms. These drawings have proven to be an effective method to collect patient data and make diagnostic decisions in real-life practice. A dataset from real-world patient cases is collected for which medical experts provide diagnostic labels. Next, we use a factorized multimodal topic model, Inter-Battery Topic Model (IBTM), to train a system that can make diagnostic predictions given an unseen discomfort drawing. The number of output diagnostic labels is determined by using mean-shift clustering on the discomfort drawing. Experimental results show reasonable predictions of diagnostic labels given an unseen discomfort drawing. Additionally, we generate synthetic discomfort drawings with IBTM given a diagnostic label, which results in typical cases of symptoms. The positive result indicates a significant potential of machine learning to be used for parts of the pain diagnostic process and to be a decision support system for physicians and other health care personnel.        △ Less","13 September, 2016",cs.LG,
              Using Kernel Methods and Model Selection for Prediction of Preterm Birth          ,1607.07959,https://arxiv.org/abs/1607.07959,https://arxiv.org/pdf/1607.07959,"Authors:IliaVovsha,AnsafSalleb-Aouissi,AnitaRaja,ThomasKoch,AlexRybchuk,AxiniaRadeva,AshwathRajan,YiwenHuang,HatimDiab,AshishTomar,RonaldWapner","        We describe an application of machine learning to the problem of predicting preterm birth. We conduct a secondary analysis on a clinical trial dataset collected by the National In- stitute of Child Health and Human Development (NICHD) while focusing our attention on predicting different classes of preterm birth. We compare three approaches for deriving predictive models: a support vector machine (SVM) approach with linear and non-linear kernels, logistic regression with different model selection along with a model based on decision rules prescribed by physician experts for prediction of preterm birth. Our approach highlights the pre-processing methods applied to handle the inherent dynamics, noise and gaps in the data and describe techniques used to handle skewed class distributions. Empirical experiments demonstrate significant improvement in predicting preterm birth compared to past work.        △ Less","5 September, 2016","cs.LG,stat.ML",
              Identifying Depression on Twitter          ,1607.07384,https://arxiv.org/abs/1607.07384,https://arxiv.org/pdf/1607.07384,Authors:MoinNadeem,"        Social media has recently emerged as a premier method to disseminate information online. Through these online networks, tens of millions of individuals communicate their thoughts, personal experiences, and social ideals. We therefore explore the potential of social media to predict, even prior to onset, Major Depressive Disorder (MDD) in online personas. We employ a crowdsourced method to compile a list of Twitter users who profess to being diagnosed with depression. Using up to a year of prior social media postings, we utilize a Bag of Words approach to quantify each tweet. Lastly, we leverage several statistical classifiers to provide estimates to the risk of depression. Our work posits a new methodology for constructing our classifier by treating social as a text-classification problem, rather than a behavioral one on social media platforms. By using a corpus of 2.5M tweets, we achieved an 81% accuracy rate in classification, with a precision score of .86. We believe that this method may be helpful in developing tools that estimate the risk of an individual being depressed, can be employed by physicians, concerned individuals, and healthcare agencies to aid in diagnosis, even possibly enabling those suffering from depression to be more proactive about recovering from their mental health.        △ Less","25 July, 2016","cs.SI,stat.ML",
              Handling Missing Data in Within-Trial Cost-Effectiveness Analysis: a Review with Future Guidelines          ,1607.06447,https://arxiv.org/abs/1607.06447,https://arxiv.org/pdf/1607.06447,"Authors:AndreaGabrio,AlexinaMason,GianlucaBaio","        Cost-Effectiveness Analyses (CEAs) alongside randomised controlled trials (RCTs) are increasingly often designed to collect resource use and preference-based health status data for the purpose of healthcare technology assessment. However, because of the way these measures are collected, they are prone to missing data, which can ultimately affect the decision of whether an intervention is good value for money. We examine how missing cost and effect outcome data are handled in RCT-based CEAs, complementing a previous review (covering 2003-2009, 88 articles) with a new systematic review (2009-2015, 81 articles) focussing on two different perspectives. First, we review the description of the missing data, the statistical methods used to deal with them, and the quality of the judgement underpinning the choice of these methods. Second, we provide guidelines on how the information about missingness and related methods should be presented to improve the reporting and handling of missing data. Our review shows that missing data in within-RCT CEAs are still often inadequately handled and the overall level of information provided to support the chosen methods is rarely satisfactory.        △ Less","21 July, 2016",stat.AP,
              Refining adverse drug reaction signals by incorporating interaction variables identified using emergent pattern mining          ,1607.05906,https://arxiv.org/abs/1607.05906,https://arxiv.org/pdf/1607.05906,"Authors:JennaM.Reps,UweAickelin,RichardB.Hubbard","        Purpose: To develop a framework for identifying and incorporating candidate confounding interaction terms into a regularised cox regression analysis to refine adverse drug reaction signals obtained via longitudinal observational data. Methods: We considered six drug families that are commonly associated with myocardial infarction in observational healthcare data, but where the causal relationship ground truth is known (adverse drug reaction or not). We applied emergent pattern mining to find itemsets of drugs and medical events that are associated with the development of myocardial infarction. These are the candidate confounding interaction terms. We then implemented a cohort study design using regularised cox regression that incorporated and accounted for the candidate confounding interaction terms. Results The methodology was able to account for signals generated due to confounding and a cox regression with elastic net regularisation correctly ranked the drug families known to be true adverse drug reactions above those.        △ Less","20 July, 2016",cs.AI,
              Juxtaposition of System Dynamics and Agent-based Simulation for a Case Study in Immunosenescence          ,1607.05888,https://arxiv.org/abs/1607.05888,https://arxiv.org/pdf/1607.05888,"Authors:GrazzielaP.Figueredo,Peer-OlafSiebers,UweAickelin,AmandaWhitbrook,JonathanM.Garibaldi","        Advances in healthcare and in the quality of life significantly increase human life expectancy. With the ageing of populations, new un-faced challenges are brought to science. The human body is naturally selected to be well-functioning until the age of reproduction to keep the species alive. However, as the lifespan extends, unseen problems due to the body deterioration emerge. There are several age-related diseases with no appropriate treatment; therefore, the complex ageing phenomena needs further understanding. Immunosenescence, the ageing of the immune system, is highly correlated to the negative effects of ageing, such as the increase of auto-inflammatory diseases and decrease in responsiveness to new diseases. Besides clinical and mathematical tools, we believe there is opportunity to further exploit simulation tools to understand immunosenescence. Compared to real-world experimentation, benefits include time and cost effectiveness due to the laborious, resource-intensiveness of the biological environment and the possibility of conducting experiments without ethic restrictions. Contrasted with mathematical models, simulation modelling is more suitable for representing complex systems and emergence. In addition, there is the belief that simulation models are easier to communicate in interdisciplinary contexts. Our work investigates the usefulness of simulations to understand immunosenescence by employing two different simulation methods, agent-based and system dynamics simulation, to a case study of immune cells depletion with age.        △ Less","20 July, 2016","cs.AI,cs.MA",
"              Shesop Healthcare: Android application to monitor heart rate variance, display influenza and stress condition using Polar H7          ",1607.04771,https://arxiv.org/abs/1607.04771,https://arxiv.org/pdf/1607.04771,"Authors:AndrienIvanderWijaya,ArySetijadiPrihatmanto,RifkiWijaya","        Shesop is an integrated system to make human lives more easily and to help people in terms of healthcare. Stress and influenza classification is a part of Shesop's application for a healthcare devices such as smartwatch, polar and fitbit. The main objective of this paper is to create a proper application to implement the stress and influenza classification. The application use Android studio, XML and Java. Also, while creating this application, all design and program is considered to be available for future updates. The application needs an android smartphone with Bluetooth Low Energy technology (bluetooth v4.0 or above). SheSop application will accommodate data entry, device picker, data gathering process, result and saving the result. In the end, we could use the polar H7 and this application to get a real-time heart rate, Heart rate variability and diagnose our stress and influenza condition.        △ Less","16 July, 2016",cs.CY,10.13140/RG.2.1.1400.4729 
              Shesop Healthcare: Stress and influenza classification using support vector machine kernel          ,1607.04770,https://arxiv.org/abs/1607.04770,https://arxiv.org/pdf/1607.04770,"Authors:AndrienIvanderWijaya,ArySetijadiPrihatmanto,RifkiWijaya","        Shesop is an integrated system to make human lives more easily and to help people in terms of healthcare. Stress and influenza classification is a part of Shesop's application for a healthcare devices such as smartwatch, polar and fitbit. The main objective of this paper is to classify a new data and inform whether you are stress, depressed, caught by influenza or not. We will use the heart rate data taken for months in Bandung, analyze the data and find the Heart rate variance that constantly related with the stress and flu level. After we found the variable, we will use the variable as an input to the support vector machine learning. We will use the lagrangian and kernel technique to transform 2D data into 3D data so we can use the linear classification in 3D space. In the end, we could use the machine learning's result to classify new data and get the final result immediately: stress or not, influenza or not.        △ Less","16 July, 2016","cs.CY,cs.LG",10.13140/RG.2.1.2449.0486 
              Cryptanalysis and Improvement of an Improved Two Factor Authentication Scheme for Telecare Medicine Information Systems          ,1607.01471,https://arxiv.org/abs/1607.01471,/search/?searchtype=author&query=Jian%2C+G,"Authors:GaopengJian,RongquanFeng",        Telecare medical information systems (TMIS) aim to provide healthcare services remotely. Efficient and secure mechanism for authentication and key agreement is required in order to guarantee the security and privacy of patients in TMIS.        △ Less,"30 December, 2016",cs.CR,
              Perfect Sampling and Gradient Simulation for Fork-Join Networks          ,1607.00748,https://arxiv.org/abs/1607.00748,https://arxiv.org/pdf/1607.00748,"Authors:XinyunChen,XianjunShi","        Fork-join network is a class of queueing networks with applications in manufactory, healthcare and computation systems. In this paper, we develop a simulation algorithm that (1) generates i.i.d. samples of the job sojourn time, jointly with the number of waiting tasks, exactly following the steady-state distribution, and (2) unbiased estimators of the derivatives of the job sojourn time with respect to the service rates of the servers in the network. The algorithm is designed based on the Coupling from the Past (CFTP) and Infinitesimal Perturbation Analysis (IPA) techniques. Two numerical examples are reported, including the special 2-station case where analytic results on the steady-state distribution is known and a 10-station network with a bottleneck.        △ Less","4 July, 2016",math.PR,
              Automated 5-year Mortality Prediction using Deep Learning and Radiomics Features from Chest Computed Tomography          ,1607.00267,https://arxiv.org/abs/1607.00267,https://arxiv.org/pdf/1607.00267,"Authors:GustavoCarneiro,LukeOakden-Rayner,AndrewP.Bradley,JacintoNascimento,LylePalmer","        We propose new methods for the prediction of 5-year mortality in elderly individuals using chest computed tomography (CT). The methods consist of a classifier that performs this prediction using a set of features extracted from the CT image and segmentation maps of multiple anatomic structures. We explore two approaches: 1) a unified framework based on deep learning, where features and classifier are automatically learned in a single optimisation process; and 2) a multi-stage framework based on the design and selection/extraction of hand-crafted radiomics features, followed by the classifier learning process. Experimental results, based on a dataset of 48 annotated chest CTs, show that the deep learning model produces a mean 5-year mortality prediction accuracy of 68.5%, while radiomics produces a mean accuracy that varies between 56% to 66% (depending on the feature selection/extraction method and classifier). The successful development of the proposed models has the potential to make a profound impact in preventive and personalised healthcare.        △ Less","1 July, 2016",cs.CV,
              A Two-Stage Patient-Focused Study Design for Rare Disease Controlled Trials          ,1607.00046,https://arxiv.org/abs/1607.00046,https://arxiv.org/pdf/1607.00046,"Authors:JianYong,SohaibH.Mohammad,YanYuan","        We developed a study design for rare disease clinical trials (RDTs) that efficiently evaluate treatments, promotes access to new treatments during treatment development, and optimizes healthcare resource utilization for future treatment allocation, development, and prioritization. Comprehensive literature review and focus group discussion were conducted. To address the multifaceted challenges facing RDTs, four key issues for RDTs must be addressed, which are 1) the opportunity to access the new treatment; 2) assessment of outcomes where clinically validated outcomes may be lacking; 3) patient heterogeneity; and 4) duration of the study and number of patients required. Our proposed study design has two stages. Stage 1 distinguishes patients who respond to the treatment from those who do not respond to the treatment after assigning them all to the experimental treatment. Stage 2 evaluates the treatment effect comparatively among patients responded in Stage 1. In addition to treatment effect evaluation, our design can greatly benefit rare disease patients and clinical practice by increasing opportunities to access experimental treatments and by providing relevant information that can be used for tailoring treatments to certain subgroups, aiding future research in treatment development, and improving healthcare resource utilization.        △ Less","30 June, 2016",stat.AP,
"              The Changing Locus of Health Data Production and Use: Patient-Generated Health Data, Observations of Daily Living, and Personal Health Information Management          ",1606.09589,https://arxiv.org/abs/1606.09589,https://arxiv.org/pdf/1606.09589,Authors:EnricoMariaPiras,"        Despite the growing attention of researcher, healthcare managers and policy makers, data gathering and information management practices are largely untheorized areas. In this work are presented and discussed some early-stage conceptualizations: Patient-Generated Health Data (PGHD), Observations of Daily Living (ODLs) and Personal Health Information Management (PHIM). As I shall try to demonstrate, these labels are not neutral rather they underpin quite different perspectives with respect to health, patient-doctor relationship, and the status of data.        △ Less","1 April, 2019",cs.CY,
              Evaluation and selection of Medical Tourism sites: A rough AHP based MABAC approach          ,1606.08962,https://arxiv.org/abs/1606.08962,https://arxiv.org/pdf/1606.08962,"Authors:JagannathRoy,KajalChatterjee,AbhirupBandhopadhyay,SamarjitKar","        In this paper, a novel multiple criteria decision making (MCDM) methodology is presented for assessing and prioritizing medical tourism destinations in uncertain environment. A systematic evaluation and assessment method is proposed by integrating rough number based AHP (Analytic Hierarchy Process) and rough number based MABAC (Multi-Attributive Border Approximation area Comparison). Rough number is used to aggregate individual judgments and preferences to deal with vagueness in decision making due to limited data. Rough AHP analyzes the relative importance of criteria based on their preferences given by experts. Rough MABAC evaluates the alternative sites based on the criteria weights. The proposed methodology is explained through a case study considering different cities for healthcare service in India. The validity of the obtained ranking for the given decision making problem is established by testing criteria proposed by Wang and Triantaphyllou (2008) along with further analysis and discussion.        △ Less","25 August, 2016",cs.AI,10.1111/exsy.12232 
              Dynamic Watermarking: Active Defense of Networked Cyber-Physical Systems          ,1606.08741,https://arxiv.org/abs/1606.08741,https://arxiv.org/pdf/1606.08741,"Authors:BharadwajSatchidanandan,P.R.Kumar","        The coming decades may see the large scale deployment of networked cyber-physical systems to address global needs in areas such as energy, water, healthcare, and transportation. However, as recent events have shown, such systems are vulnerable to cyber attacks. Being safety critical, their disruption or misbehavior can cause economic losses or injuries and loss of life. It is therefore important to secure such networked cyber-physical systems against attacks. In the absence of credible security guarantees, there will be resistance to the proliferation of cyber-physical systems, which are much needed to meet global needs in critical infrastructures and services.  This paper addresses the problem of secure control of networked cyber-physical systems. This problem is different from the problem of securing the communication network, since cyber-physical systems at their very essence need sensors and actuators that interface with the physical plant, and malicious agents may tamper with sensors or actuators, as recent attacks have shown.  We consider physical plants that are being controlled by multiple actuators and sensors communicating over a network, where some sensors could be ""malicious,"" meaning that they may not report the measurements that they observe. We address a general technique by which the actuators can detect the actions of malicious sensors in the system, and disable closed-loop control based on their information. This technique, called ""watermarking,"" employs the technique of actuators injecting private excitation into the system which will reveal malicious tampering with signals. We show how such an active defense can be used to secure networked systems of sensors and actuators.        △ Less","27 June, 2016","cs.SY,math.DS",
              Hiring Doctors in E-Healthcare With Zero Budget          ,1606.08590,https://arxiv.org/abs/1606.08590,https://arxiv.org/pdf/1606.08590,"Authors:VikashKumarSingh,SajalMukhopadhyay,RantuDas","        The doctors (or expert consultants) are the critical resources on which the success of critical medical cases are heavily dependent. With the emerging technologies (such as video conferencing, smartphone, etc.) this is no longer a dream but a fact, that for critical medical cases in a hospital, expert consultants from around the world could be hired, who may be present physically or virtually. Earlier, this interesting situation by taking the expert consultancies from outside the hospital had been studied, but under monetary perspective. In this paper, for the first time, to the best of our knowledge, we investigate the situation, where the below income group (BIG) people of the society may be served efficiently through the expert consultancy by the renowned doctors from outside of the hospital under zero budget. This will help us saving many lives which will fulfil the present day need of biomedical research. We propose three mechanisms: Random pick-assign mechanism (RanPAM), Truthful optimal allocation mechanism (TOAM), and Truthful optimal allocation mechanism for incomplete preferences (TOAM-IComP) to allocate the doctor to the patient. With theoretical analysis, we demonstrate that the TOAM is strategy-proof, and exhibits a unique core property. The mechanisms are also validated with exhaustive experiments.        △ Less","2 February, 2018",cs.GT,
              Review on Physically Flexible Nonvolatile Memory for Internet of Everything Electronics          ,1606.08404,https://arxiv.org/abs/1606.08404,https://arxiv.org/pdf/1606.08404,"Authors:MohamedT.Ghoneim,MuhammadM.Hussain","        Solid-state memory is an essential component of the digital age. With advancements in healthcare technology and the Internet of Things (IoT), the demand for ultra-dense, ultra-low-power memory is increasing. In this review, we present a comprehensive perspective on the most notable approaches to the fabrication of physically flexible memory devices. With the future goal of replacing traditional mechanical hard disks with solid-state storage devices, a fully flexible electronic system will need two basic devices: transistors and nonvolatile memory. Transistors are used for logic operations and gating memory arrays, while nonvolatile memory (NVM) devices are required for storing information in the main memory and cache storage. Since the highest density of transistors and storage structures is manifested in memories, the focus of this review is flexible NVM. Flexible NVM components are discussed in terms of their functionality, performance metrics, and reliability aspects, all of which are critical components for NVM technology to be part of mainstream consumer electronics, IoT, and advanced healthcare devices. Finally, flexible NVMs are benchmarked and future prospects are provided.        △ Less","23 June, 2016",cs.ET,10.3390/electronics4030424 
              Going Digital: A Survey on Digitalization and Large Scale Data Analytics in Healthcare,1606.08075,https://arxiv.org/abs/1606.08075,https://arxiv.org/pdf/1606.08075,"Authors:VolkerTresp,J.MarcOverhage,MarkusBundschus,ShahroozRabizadeh,PeterA.Fasching,ShipengYu","        We provide an overview of the recent trends towards digitalization and large scale data analytics in healthcare. It is expected that these trends are instrumental in the dramatic changes in the way healthcare will be organized in the future. We discuss the recent political initiatives designed to shift care delivery processes from paper to electronic, with the goals of more effective treatments with better outcomes; cost pressure is a major driver of innovation. We describe newly developed networks of healthcare providers, research organizations and commercial vendors to jointly analyze data for the development of decision support systems. We address the trend towards continuous healthcare where health is monitored by wearable and stationary devices; a related development is that patients increasingly assume responsibility for their own health data. Finally we discuss recent initiatives towards a personalized medicine, based on advances in molecular medicine, data management, and data analytics.        △ Less","19 October, 2016",cs.CY,
              Application of Wireless Sensor Networks for Indoor Temperature Regulation          ,1606.07386,https://arxiv.org/abs/1606.07386,https://arxiv.org/pdf/1606.07386,"Authors:BiljanaRisteskaStojkoska,AndrijanaPopovskaAvramova,PeriklisChatzimisios","        Wireless sensor networks take a major part in our everyday lives by enhancing systems for home automation, healthcare, temperature control, energy consumption monitoring, and so forth. In this paper we focus on a system used for temperature regulation for residential, educational, industrial, and commercial premises, and so forth. We propose a framework for indoor temperature regulation and optimization using wireless sensor networks based on ZigBee platform. This paper considers architectural design of the system, as well as implementation guidelines. The proposed system favors methods that provide energy savings by reducing the amount of data transmissions through the network. Furthermore, the framework explores techniques for localization, such that the location of the nodes can be used by algorithms that regulate temperature settings.        △ Less","23 June, 2016",cs.NI,10.1155/2014/502419 
              SMCQL: Secure Querying for Federated Databases          ,1606.06808,https://arxiv.org/abs/1606.06808,https://arxiv.org/pdf/1606.06808,"Authors:JohesBater,GregoryElliott,CraigEggen,SatyenderGoel,AbelKho,JennieRogers","        People and machines are collecting data at an unprecedented rate. Despite this newfound abundance of data, progress has been slow in sharing it for open science, business, and other data-intensive endeavors. Many such efforts are stymied by privacy concerns and regulatory compliance issues. For example, many hospitals are interested in pooling their medical records for research, but none may disclose arbitrary patient records to researchers or other healthcare providers. In this context we propose the Private Data Network (PDN), a federated database for querying over the collective data of mutually distrustful parties. In a PDN, each member database does not reveal its tuples to its peers nor to the query writer. Instead, the user submits a query to an honest broker that plans and coordinates its execution over multiple private databases using secure multiparty computation (SMC). Here, each database's query execution is oblivious, and its program counters and memory traces are agnostic to the inputs of others. We introduce a framework for executing PDN queries named SMCQL. This system translates SQL statements into SMC primitives to compute query results over the union of its source databases without revealing sensitive information about individual tuples to peer data providers or the honest broker. Only the honest broker and the querier receive the results of a PDN query. For fast, secure query evaluation, we explore a heuristics-driven optimizer that minimizes the PDN's use of secure computation and partitions its query evaluation into scalable slices.        △ Less","6 March, 2017","cs.DB,cs.CR",
              ACDC: αα-Carving Decision Chain for Risk Stratification          ,1606.05325,https://arxiv.org/abs/1606.05325,https://arxiv.org/pdf/1606.05325,"Authors:YubinPark,JoyceHo,JoydeepGhosh","        In many healthcare settings, intuitive decision rules for risk stratification can help effective hospital resource allocation. This paper introduces a novel variant of decision tree algorithms that produces a chain of decisions, not a general tree. Our algorithm, αα-Carving Decision Chain (ACDC), sequentially carves out ""pure"" subsets of the majority class examples. The resulting chain of decision rules yields a pure subset of the minority class examples. Our approach is particularly effective in exploring large and class-imbalanced health datasets. Moreover, ACDC provides an interactive interpretation in conjunction with visual performance metrics such as Receiver Operating Characteristics curve and Lift chart.        △ Less","16 June, 2016","stat.ML,cs.LG",
              Myopic Policies for Non-Preemptive Scheduling of Jobs with Decaying Value          ,1606.04136,https://arxiv.org/abs/1606.04136,https://arxiv.org/pdf/1606.04136,"Authors:NealMaster,CarriW.Chan,NicholasBambos","        In many scheduling applications, minimizing delays is of high importance. One adverse effect of such delays is that the reward for completion of a job may decay over time. Indeed in healthcare settings, delays in access to care can result in worse outcomes, such as an increase in mortality risk. Motivated by managing hospital operations in disaster scenarios, as well as other applications in perishable inventory control and information services, we consider non-preemptive scheduling of jobs whose internal value decays over time. Because solving for the optimal scheduling policy is computationally intractable, we focus our attention on the performance of three intuitive heuristics: (1) a policy which maximizes the expected immediate reward, (2) a policy which maximizes the expected immediate reward rate, and (3) a policy which prioritizes jobs with imminent deadlines. We provide performance guarantees for all three policies and show that many of these performance bounds are tight. In addition, we provide numerical experiments and simulations to compare how the policies perform in a variety of scenarios. Our theoretical and numerical results allow us to establish rules-of-thumb for applying these heuristics in a variety of situations, including patient scheduling scenarios.        △ Less","21 October, 2016","cs.SY,math.OC",
              Estimating individual treatment effect: generalization bounds and algorithms          ,1606.03976,https://arxiv.org/abs/1606.03976,https://arxiv.org/pdf/1606.03976,"Authors:UriShalit,FredrikD.Johansson,DavidSontag","        There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a ""balanced"" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.        △ Less","16 May, 2017","stat.ML,cs.AI,cs.LG",
              Social value and information quality in online health information search          ,1606.03507,https://arxiv.org/abs/1606.03507,https://arxiv.org/pdf/1606.03507,"Authors:TahirHameed,BobbySwar","        This paper extends and validates a model of value-driven online healthcare information search in online shared contexts. Perceived value is an important factor behind users' decisions concerning search, consumption and reuse of products and services. The role of utilitarian, hedonic and epistemic value of information in user satisfaction and intention to repeat online search is well recognized, but little support has been found for social value affecting user satisfaction critical for such decisions. Therefore, a value-based model of online healthcare information search was extended adding detailed information quality measures. Our survey data collected from 143 college going students from more than 10 countries studying in South Korea demonstrated two novel results. At first, unlike existing studies, strong support was found for perceived social value affecting the user satisfaction. As a second substantial finding, the significance of social value could only be appreciated by using comprehensive constructs of information quality than simpler measures. Therefore, this study shows for social and shared HIS, users consider access, representation and context quality of healthcare information to be more important from social perspective than intrinsic (or content) quality. Therefore, developers and healthcare organizations interested in traction of their online healthcare information delivery systems (HIS), especially those using internet and social media networks, should focus on enhancing information quality of their systems accordingly.        △ Less","10 June, 2016",cs.CY,
              An Energy-Efficient Compressive Sensing Framework Incorporating Online Dictionary Learning for Long-term Wireless Health Monitoring          ,1606.01557,https://arxiv.org/abs/1606.01557,https://arxiv.org/pdf/1606.01557,"Authors:KaiXu,YixingLi,FengboRen","        Wireless body area network (WBAN) is emerging in the mobile healthcare area to replace the traditional wire-connected monitoring devices. As wireless data transmission dominates power cost of sensor nodes, it is beneficial to reduce the data size without much information loss. Compressive sensing (CS) is a perfect candidate to achieve this goal compared to existing compression techniques. In this paper, we proposed a general framework that utilize CS and online dictionary learning (ODL) together. The learned dictionary carries individual characteristics of the original signal, under which the signal has an even sparser representation compared to pre-determined dictionaries. As a consequence, the compression ratio is effectively improved by 2-4x comparing to prior works. Besides, the proposed framework offloads pre-processing from sensor nodes to the server node prior to dictionary learning, providing further reduction in hardware costs. As it is data driven, the proposed framework has the potential to be used with a wide range of physiological signals.        △ Less","5 June, 2016",cs.IT,10.1109/ICASSP.2016.7471786 
              Thinking Out Loud and e-Health for Coordinated Care Lessons from User Requirements Gathering in the 4C Project          ,1606.01444,https://arxiv.org/abs/1606.01444,https://arxiv.org/pdf/1606.01444,"Authors:LeonieEllis,ColleenCheek,PaulTurner","        e-Health is a core part of Australias strategy to address rising costs and changing demands for healthcare services. With over $1bn spent and only 6% of Australians registered, the personally controlled electronic health record (PCEHR) suggests user challenges remain. While evidence confirms the benefits from involving users in systems development there is a need for more examples of how to engage effectively in healthcare settings. This research describes the use of an agile development methodology combined with the thinking out loud technique to deliver a solution that exceeded user requirements in supporting a new model of care. The 4C project solution connected Aged Care institutions with general practices, hospitals and specialist services in Tasmanias north-west region. It was underpinned by a model of Technology Mediated Social Participation (TMSP). As a trial project for the PCEHR it remains unclear why lessons learned appear not to have been deployed more explicitly in the National roll-out.        △ Less","4 June, 2016",cs.CY,
              Analysis of Research in Healthcare Data Analytics          ,1606.01354,https://arxiv.org/abs/1606.01354,https://arxiv.org/pdf/1606.01354,"Authors:MohammadAlkhatib,AmirTalaei-Khoei,AmirGhapanchi","        The main aim of this paper is to provide a deep analysis on the research field of healthcare data analytics. This paper is analyzing the previous studies and works in this research area, as well as highlighting some of guidelines and gaps. This study has used seven popular databases and selected most relevant papers, in order to conduct this paper. The paper has listed some data analytics tools and techniques that have been used to improve healthcare performance in many areas such as: medical operations, reports, decision making, and prediction and prevention system. Moreover, the systematic review has showed an interesting demographic of fields of publication, research approaches, as well as outlined some of the possible reasons and issues associated with healthcare data analytics, based on geographical distribution theme.        △ Less","4 June, 2016",cs.CY,
              Evaluating the Business Value of CPOE for Cancer Care in Australia: A Resource Based View Perspective          ,1606.00893,https://arxiv.org/abs/1606.00893,https://arxiv.org/pdf/1606.00893,"Authors:PeterHaddad,JonathanL.Schaffer,NilminiWickramasinghe","        Today, cancer is one of the leading causes of death throughout the world. This threatening disease has huge negative impacts, not only on quality of life, but also on the healthcare industry, whose resources are already scarce. Thus, finding new approaches for cancer care has been a central point of interest during the last few decades. One of these approaches is the use of computerised physician order entry (CPOE) systems. This systems have the potential to provide more effective and efficient patient-centric cancer care. This paper serves to examine the business value of an American CPOE in an Australian context. This is achieved by using our specifically designed tool to evaluate the business value of IT in the healthcare, in combination with a resource based view perspective. Our results show that the system has a number of enabling resources to generate business value subject to having other resources.        △ Less","27 May, 2016",cs.CY,
              Factors Influencing mHealth Acceptance among Elderly People in Bangladesh          ,1606.00874,https://arxiv.org/abs/1606.00874,https://arxiv.org/pdf/1606.00874,"Authors:MdHoque,GolamSorwar","        mHealth (mobile health) be the blessing of ICT and is probably one of the most prominent services with noticeable effect on the development of healthcare sector. Given the potential benefits mHealth can bring to older people, it is important to understand the users (elderly population) intention to use the technology. However, little research has been done to draw any systematic study of elderlys adoption and usage of mHealth. The aim of this study is to determine factors that influence the adoption and use of mHealth technology and services by the elderly in Bangladesh. This research will develop a theoretical model to determine the elderly behavioral intention to adopt mHealth application. It is intended to determine if there is any significant relationship between attitudinal constructs such as Performance Expectancy, Effort Expectancy, Social Influence, Facilitating Condition, Hedonic Motivation, Price Value, Habit and acceptance of mHealth services in Bangladesh. The study will also investigate the moderating role of gender & experience and their influence on the adoption of mHealth technology and services.        △ Less","28 May, 2016",cs.CY,
              iStar 2.0 Language Guide          ,1605.07767,https://arxiv.org/abs/1605.07767,https://arxiv.org/pdf/1605.07767,"Authors:FabianoDalpiaz,XavierFranch,JenniferHorkoff","        The i* modeling language was introduced to fill the gap in the spectrum of conceptual modeling languages, focusing on the intentional (why?), social (who?), and strategic (how? how else?) dimensions. i* has been applied in many areas, e.g., healthcare, security analysis, eCommerce. Although i* has seen much academic application, the diversity of extensions and variations can make it difficult for novices to learn and use it in a consistent way. This document introduces the iStar 2.0 core language, evolving the basic concepts of i* into a consistent and clear set of core concepts, upon which to build future work and to base goal-oriented teaching materials. This document was built from a set of discussions and input from various members of the i* community. It is our intention to revisit, update and expand the document after collecting examples and concrete experiences with iStar 2.0.        △ Less","16 June, 2016",cs.SE,
              Visual TASK: A Collaborative Cognitive Aid for Acute Care Resuscitation          ,1605.05224,https://arxiv.org/abs/1605.05224,https://arxiv.org/pdf/1605.05224,"Authors:MichaelJ.Gonzales,JoshuaM.Henry,AaronW.Calhoun,LaurelD.Riek","        Preventable medical errors are a severe problem in healthcare, causing over 400,000 deaths per year in the US in hospitals alone. In acute care, the branch of medicine encompassing the emergency department (ED) and intensive care units (ICU), error rates may be higher to due low situational awareness among clinicians performing resuscitation on patients. To support cognition, novice team leaders may rely on reference guides to direct and anticipate future steps. However, guides often act as a fixation point, diverting the leader's attention away from the team. To address this issue, we conducted a qualitative study that evaluates a collaborative cognitive aid co-designed with clinicians called Visual TASK. Our study explored the use of Visual TASK in three simulations employing a projected shared display with two different interaction modalities: the Microsoft Kinect and a touchscreen. Our results suggest that tools like the Kinect, while useful in other areas of acute care like the OR, are unsuitable for use in high-stress situations like resuscitation. We also observed that fixation may not be constrained to reference guides alone, and may extend to other objects in the room. We present our findings, and a discussion regarding future avenues in which collaborative cognitive aids may help in improving situational awareness in resuscitation.        △ Less","17 May, 2016",cs.HC,
              Generalized Linear Models for Aggregated Data          ,1605.04466,https://arxiv.org/abs/1605.04466,https://arxiv.org/pdf/1605.04466,"Authors:AvradeepBhowmik,JoydeepGhosh,OluwasanmiKoyejo","        Databases in domains such as healthcare are routinely released to the public in aggregated form. Unfortunately, naive modeling with aggregated data may significantly diminish the accuracy of inferences at the individual level. This paper addresses the scenario where features are provided at the individual level, but the target variables are only available as histogram aggregates or order statistics. We consider a limiting case of generalized linear modeling when the target variables are only known up to permutation, and explore how this relates to permutation testing; a standard technique for assessing statistical dependency. Based on this relationship, we propose a simple algorithm to estimate the model parameters and individual level inferences via alternating imputation and standard generalized linear model fitting. Our results suggest the effectiveness of the proposed approach when, in the original data, permutation testing accurately ascertains the veracity of the linear relationship. The framework is extended to general histogram data with larger bins - with order statistics such as the median as a limiting case. Our experimental results on simulated data and aggregated healthcare data suggest a diminishing returns property with respect to the granularity of the histogram - when a linear relationship holds in the original data, the targets can be predicted accurately given relatively coarse histograms.        △ Less","14 May, 2016","stat.ML,cs.AI,cs.LG",
              Spotting the diffusion of New Psychoactive Substances over the Internet          ,1605.03817,https://arxiv.org/abs/1605.03817,https://arxiv.org/pdf/1605.03817,"Authors:FabioDelVigna,MarcoAvvenuti,ClaraBacciu,PaoloDeluca,AndreaMarchetti,MarinellaPetrocchi,MaurizioTesconi","        Online availability and diffusion of New Psychoactive Substances (NPS) represent an emerging threat to healthcare systems. In this work, we analyse drugs forums, online shops, and Twitter. By mining the data from these sources, it is possible to understand the dynamics of drugs diffusion and their endorsement, as well as timely detecting new substances. We propose a set of visual analytics tools to support analysts in tackling NPS spreading and provide a better insight about drugs market and analysis.        △ Less","11 July, 2016","cs.CY,cs.SI",
              Learning Representations for Counterfactual Inference          ,1605.03661,https://arxiv.org/abs/1605.03661,https://arxiv.org/pdf/1605.03661,"Authors:FredrikD.Johansson,UriShalit,DavidSontag","        Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, ""Would this patient have lower blood sugar had she received a different medication?"". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.        △ Less","6 June, 2018","stat.ML,cs.AI,cs.LG",
              The large graph limit of a stochastic epidemic model on a dynamic multilayer network          ,1605.02809,https://arxiv.org/abs/1605.02809,https://arxiv.org/pdf/1605.02809,"Authors:KarlyA.Jacobsen,MarkG.Burch,JosephH.Tien,GrzegorzA.Rempała","        We consider an SIR-type (Susceptible →\to Infected →\to Recovered) stochastic epidemic process with multiple modes of transmission on a contact network. The network is given by a random graph following a multilayer configuration model where edges in different layers correspond to potentially infectious contacts of different types. We assume that the graph structure evolves in response to the epidemic via activation or deactivation of edges. We derive a large graph limit theorem that gives a system of ordinary differential equations (ODEs) describing the evolution of quantities of interest, such as the proportions of infected and susceptible vertices, as the number of nodes tends to infinity. Analysis of the limiting system elucidates how the coupling of edge activation and deactivation to infection status affects disease dynamics, as illustrated by a two-layer network example with edge types corresponding to community and healthcare contacts. Our theorem extends some earlier results deriving the deterministic limit of stochastic SIR processes on static, single-layer configuration model graphs. We also describe precisely the conditions for equivalence between our limiting ODEs and the systems obtained via pair approximation, which are widely used in the epidemiological and ecological literature to approximate disease dynamics on networks. Potential applications include modeling Ebola dynamics in West Africa, which was the motivation for this study.        △ Less","20 August, 2018","q-bio.PE,math.PR",
              2.4GHZ Class AB power Amplifier For Healthcare Application          ,1605.02455,https://arxiv.org/abs/1605.02455,https://arxiv.org/pdf/1605.02455,"Authors:WeiCai,LiangHuang,WuJieWen","        The objective of this research was to design a 2.4 GHz class AB Power Amplifier, with 0.18 um SMIC CMOS technology by using Cadence software, for health care applications. The ultimate goal for such application is to minimize the trade-offs between performance and cost, and between performance and low power consumption design. The performance of the power amplifier meets the specification requirements of the desired.        △ Less","9 May, 2016",cs.OH,
              Attack Resilience and Recovery using Physical Challenge Response Authentication for Active Sensors Under Integrity Attacks          ,1605.02062,https://arxiv.org/abs/1605.02062,https://arxiv.org/pdf/1605.02062,"Authors:YasserShoukry,PaulMartin,YairYona,SuhasDiggavi,ManiSrivastava","        Embedded sensing systems are pervasively used in life- and security-critical systems such as those found in airplanes, automobiles, and healthcare. Traditional security mechanisms for these sensors focus on data encryption and other post-processing techniques, but the sensors themselves often remain vulnerable to attacks in the physical/analog domain. If an adversary manipulates a physical/analog signal prior to digitization, no amount of digital security mechanisms after the fact can help. Fortunately, nature imposes fundamental constraints on how these analog signals can behave. This work presents PyCRA, a physical challenge-response authentication scheme designed to protect active sensing systems against physical attacks occurring in the analog domain. PyCRA provides security for active sensors by continually challenging the surrounding environment via random but deliberate physical probes. By analyzing the responses to these probes, and by using the fact that the adversary cannot change the underlying laws of physics, we provide an authentication mechanism that not only detects malicious attacks but provides resilience against them. We demonstrate the effectiveness of PyCRA through several case studies using two sensing systems: (1) magnetic sensors like those found wheel speed sensors in robotics and automotive, and (2) commercial RFID tags used in many security-critical applications. Finally, we outline methods and theoretical proofs for further enhancing the resilience of PyCRA to active attacks by means of a confusion phase---a period of low signal to noise ratio that makes it more difficult for an attacker to correctly identify and respond to PyCRA's physical challenges. In doing so, we evaluate both the robustness and the limitations of PyCRA, concluding by outlining practical considerations as well as further applications for the proposed authentication mechanism.        △ Less","12 May, 2016",cs.CR,
              The wage transition in developed countries and its implications for China          ,1605.01949,https://arxiv.org/abs/1605.01949,https://arxiv.org/pdf/1605.01949,"Authors:BelalBaaquie,BertrandM.Roehner,QinghaiWang","        The expression ""wage transition"" refers to the fact that over the past two or three decades in all developed economies wage increases have levelled off. There has been a widening divergence and decoupling between wages on the one hand and GDP per capita on the other hand. Yet, in China wages and GDP per capita climbed in sync (at least up to now). In the first part of the paper we present comparative statistical evidence which measures the extent of the wage transition effect. In a second part we consider the reasons of this phenomenon, in particular we explain how the transfers of labor from low productivity sectors (such as agriculture) to high productivity sectors (such as manufacturing) are the driver of productivity growth, particularly through their synergetic effects. Although rural flight represents only one of these effects, it is certainly the most visible because of the geographical relocation that it implies; it is also the most well-defined statistically. Moreover, it will be seen that it is a good indicator of the overall productivity and attractivity of the non-agricultural sector. Because this model accounts fairly well for the observed evolution in industrialized countries, we use it to predict the rate of Chinese economic growth in the coming decades. Our forecast for the average annual growth of real wages ranges from 4% to 6% depending on how well China will control the development of its healthcare industry.        △ Less","6 May, 2016","q-fin.GN,physics.soc-ph",10.1016/j.physa.2016.11.092 
              Recurrent Convolutional Neural Network Regression for Continuous Pain Intensity Estimation in Video          ,1605.00894,https://arxiv.org/abs/1605.00894,https://arxiv.org/pdf/1605.00894,"Authors:JingZhou,XiaopengHong,FeiSu,GuoyingZhao","        Automatic pain intensity estimation possesses a significant position in healthcare and medical field. Traditional static methods prefer to extract features from frames separately in a video, which would result in unstable changes and peaks among adjacent frames. To overcome this problem, we propose a real-time regression framework based on the recurrent convolutional neural network for automatic frame-level pain intensity estimation. Given vector sequences of AAM-warped facial images, we used a sliding-window strategy to obtain fixed-length input samples for the recurrent network. We then carefully design the architecture of the recurrent network to output continuous-valued pain intensity. The proposed end-to-end pain intensity regression framework can predict the pain intensity of each frame by considering a sufficiently large historical frames while limiting the scale of the parameters within the model. Our method achieves promising results regarding both accuracy and running speed on the published UNBC-McMaster Shoulder Pain Expression Archive Database.        △ Less","3 May, 2016",cs.CV,
              Computer keyboard interaction as an indicator of early Parkinson's disease          ,1604.08620,https://arxiv.org/abs/1604.08620,https://arxiv.org/pdf/1604.08620,"Authors:L.Giancardo,A.Sánchez-Ferro,T.Arroyo-Gallego,I.Butterworth,C.S.Mendoza,P.Montero,M.Matarazzo,A.Obeso,M.L.Gray,SanJoséEstépar","        Parkinson's disease (PD) is a slowly progressing neurodegenerative disease with early manifestation of motor signs. Objective measurements of motor signs are of vital importance for diagnosing, monitoring and developing disease modifying therapies, particularly for the early stages of the disease when putative neuroprotective treatments could stop neurodegeneration. Current medical practice has limited tools to routinely monitor PD motor signs with enough frequency and without undue burden for patients and the healthcare system. In this paper, we present data indicating that the routine interaction with computer keyboards can be used to detect motor signs in the early stages of PD. We explore a solution that measures the key hold times (the time required to press and release a key) during the normal use of a computer without any change in hardware and converts it to a PD motor index. This is achieved by the automatic discovery of patterns in the time series of key hold times using an ensemble regression algorithm. This new approach discriminated early PD groups from controls with an AUC = 0.81 (n = 42/43; mean age = 59.0/60.1; women = 43%/60%;PD/controls). The performance was comparable or better than two other quantitative motor performance tests used clinically: alternating finger tapping (AUC = 0.75) and single key tapping (AUC = 0.61).        △ Less","5 October, 2016",cs.HC,10.1038/srep34468 
              Trading-Off Cost of Deployment Versus Accuracy in Learning Predictive Models          ,1604.05819,https://arxiv.org/abs/1604.05819,https://arxiv.org/pdf/1604.05819,"Authors:DanielP.Robinson,SuchiSaria","        Predictive models are finding an increasing number of applications in many industries. As a result, a practical means for trading-off the cost of deploying a model versus its effectiveness is needed. Our work is motivated by risk prediction problems in healthcare. Cost-structures in domains such as healthcare are quite complex, posing a significant challenge to existing approaches. We propose a novel framework for designing cost-sensitive structured regularizers that is suitable for problems with complex cost dependencies. We draw upon a surprising connection to boolean circuits. In particular, we represent the problem costs as a multi-layer boolean circuit, and then use properties of boolean circuits to define an extended feature vector and a group regularizer that exactly captures the underlying cost structure. The resulting regularizer may then be combined with a fidelity function to perform model prediction, for example. For the challenging real-world application of risk prediction for sepsis in intensive care units, the use of our regularizer leads to models that are in harmony with the underlying cost structure and thus provide an excellent prediction accuracy versus cost tradeoff.        △ Less","20 April, 2016","stat.ML,cs.LG",
              Toward a Science of Autonomy for Physical Systems          ,1604.02979,https://arxiv.org/abs/1604.02979,https://arxiv.org/pdf/1604.02979,"Authors:GregoryD.Hager,DanielaRus,VijayKumar,HenrikChristensen","        Our lives have been immensely improved by decades of automation research -- we are more comfortable, more productive and safer than ever before. Just imagine a world where familiar automation technologies have failed. In that world, thermostats don't work -- you have to monitor your home heating system manually. Cruise control for your car doesn't exist. Every elevator has to have a human operator to hit the right floor, most manufactured products are assembled by hand, and you have to wash your own dishes. Who would willingly adopt that world -- the world of last century -- today? Physical systems -- elevators, cars, home appliances, manufacturing equipment -- were more troublesome, ore time consuming, less safe, and far less convenient. Now, suppose we put ourselves in the place someone 20 years in the future, a future of autonomous systems. A future where transportation is largely autonomous, more efficient, and far safer; a future where dangerous occupations like mining or disaster response are performed by autonomous systems supervised remotely by humans; a future where manufacturing and healthcare are twice as productive per person-hour by having smart monitoring and readily re-tasked autonomous physical agents; a future where the elderly and infirm have 24 hour in-home autonomous support for the basic activities, both physical and social, of daily life. In a future world where these capabilities are commonplace, why would someone come back to today's world where someone has to put their life at risk to do a menial job, we lose time to mindless activities that have no intrinsic value, or be consumed with worry that a loved one is at risk in their own home? In what follows, and in a series of associated essays, we expand on these ideas, and frame both the opportunities and challenges posed by autonomous physical systems.        △ Less","11 April, 2016",cs.CY,
              Multilevel Weighted Support Vector Machine for Classification on Healthcare Data with Missing Values          ,1604.02123,https://arxiv.org/abs/1604.02123,https://arxiv.org/pdf/1604.02123,"Authors:TalayehRazzaghi,OlegRoderick,IlyaSafro,NicholasMarko","        This work is motivated by the needs of predictive analytics on healthcare data as represented by Electronic Medical Records. Such data is invariably problematic: noisy, with missing entries, with imbalance in classes of interests, leading to serious bias in predictive modeling. Since standard data mining methods often produce poor performance measures, we argue for development of specialized techniques of data-preprocessing and classification. In this paper, we propose a new method to simultaneously classify large datasets and reduce the effects of missing values. It is based on a multilevel framework of the cost-sensitive SVM and the expected maximization imputation method for missing values, which relies on iterated regression analyses. We compare classification results of multilevel SVM-based algorithms on public benchmark datasets with imbalanced classes and missing values as well as real data in health applications, and show that our multilevel SVM-based method produces fast, and more accurate and robust classification results.        △ Less","7 April, 2016","stat.ML,cs.LG,stat.AP",10.1371/journal.pone.0155119 
              Handling missing data in large healthcare dataset: a case study of unknown trauma outcomes          ,1604.00627,https://arxiv.org/abs/1604.00627,https://arxiv.org/pdf/1604.00627,"Authors:E.M.Mirkes,T.J.Coats,J.Levesley,A.N.Gorban","        Handling of missed data is one of the main tasks in data preprocessing especially in large public service datasets. We have analysed data from the Trauma Audit and Research Network (TARN) database, the largest trauma database in Europe. For the analysis we used 165,559 trauma cases. Among them, there are 19,289 cases (13.19\%) with unknown outcome. We have demonstrated that these outcomes are not missed `completely at random' and, hence, it is impossible just to exclude these cases from analysis despite the large amount of available data. We have developed a system of non-stationary Markov models for the handling of missed outcomes and validated these models on the data of 15,437 patients which arrived into TARN hospitals later than 24 hours but within 30 days from injury. We used these Markov models for the analysis of mortality. In particular, we corrected the observed fraction of death. Two naïve approaches give 7.20\% (available case study) or 6.36\% (if we assume that all unknown outcomes are `alive'). The corrected value is 6.78\%. Following the seminal paper of Trunkey (1983) the multimodality of mortality curves has become a much discussed idea. For the whole analysed TARN dataset the coefficient of mortality monotonically decreases in time but the stratified analysis of the mortality gives a different result: for lower severities the coefficient of mortality is a non-monotonic function of the time after injury and may have maxima at the second and third weeks. The approach developed here can be applied to various healthcare datasets which experience the problem of lost patients and missed outcomes.        △ Less","18 May, 2020",stat.AP,10.1016/j.compbiomed.2016.06.004 
              Detecting weak changes in dynamic events over networks          ,1603.08981,https://arxiv.org/abs/1603.08981,https://arxiv.org/pdf/1603.08981,"Authors:ShuangLi,YaoXie,MehrdadFarajtabar,ApurvVerma,LeSong","        Large volume of networked streaming event data are becoming increasingly available in a wide variety of applications, such as social network analysis, Internet traffic monitoring and healthcare analytics. Streaming event data are discrete observation occurred in continuous time, and the precise time interval between two events carries a great deal of information about the dynamics of the underlying systems. How to promptly detect changes in these dynamic systems using these streaming event data? In this paper, we propose a novel change-point detection framework for multi-dimensional event data over networks. We cast the problem into sequential hypothesis test, and derive the likelihood ratios for point processes, which are computed efficiently via an EM-like algorithm that is parameter-free and can be computed in a distributed fashion. We derive a highly accurate theoretical characterization of the false-alarm-rate, and show that it can achieve weak signal detection by aggregating local statistics over time and networks. Finally, we demonstrate the good performance of our algorithm on numerical examples and real-world datasets from twitter and Memetracker.        △ Less","16 September, 2016","cs.LG,stat.ML",
              Big Data Spark Solution for Functional Magnetic Resonance Imaging          ,1603.07064,https://arxiv.org/abs/1603.07064,https://arxiv.org/pdf/1603.07064,"Authors:SamanSarraf,MehdiOstadhashem","        Recently, Big Data applications have rapidly expanded into different industries. Healthcare is also one the industries willing to use big data platforms so that some big data analytics tools have been adopted in this field to some extent. Medical imaging which is a pillar in diagnostic healthcare deals with high volume of data collection and processing. A huge amount of 3D and 4D images are acquired in different forms and resolutions using a variety of medical imaging modalities. Preprocessing and analyzing imaging data is currently a long process and cost and time consuming. However, not many big data platforms have been provided or redesigned for medical imaging purposes because of some restrictions such as data format. In this paper, we designed, developed and successfully tested a new pipeline for medical imaging data (especially functional magnetic resonance imaging - fMRI) using Big Data Spark / PySpark platform on a single node which allows us to read and load imaging data, convert them to Resilient Distributed Datasets in order manipulate and perform in-memory data processing in parallel and convert final results to imaging format while the pipeline provides an option to store the results in other formats such as data frame. Using this new solution and pipeline, we repeated our previous works in which we extracted brain networks from fMRI data using template matching and sum of squared differences (SSD) method. The final results revealed our Spark (PySpark) based solution improved the performance (in terms of processing time) around 4 times on a single compared to the previous work developed in Python.        △ Less","22 March, 2016","cs.DC,cs.CY",
"              A Survey of Stealth Malware: Attacks, Mitigation Measures, and Steps Toward Autonomous Open World Solutions          ",1603.06028,https://arxiv.org/abs/1603.06028,https://arxiv.org/pdf/1603.06028,"Authors:EthanM.Rudd,AndrasRozsa,ManuelGünther,TerranceE.Boult","        As our professional, social, and financial existences become increasingly digitized and as our government, healthcare, and military infrastructures rely more on computer technologies, they present larger and more lucrative targets for malware. Stealth malware in particular poses an increased threat because it is specifically designed to evade detection mechanisms, spreading dormant, in the wild for extended periods of time, gathering sensitive information or positioning itself for a high-impact zero-day attack. Policing the growing attack surface requires the development of efficient anti-malware solutions with improved generalization to detect novel types of malware and resolve these occurrences with as little burden on human experts as possible. In this paper, we survey malicious stealth technologies as well as existing solutions for detecting and categorizing these countermeasures autonomously. While machine learning offers promising potential for increasingly autonomous solutions with improved generalization to new malware types, both at the network level and at the host level, our findings suggest that several flawed assumptions inherent to most recognition algorithms prevent a direct mapping between the stealth malware recognition problem and a machine learning solution. The most notable of these flawed assumptions is the closed world assumption: that no sample belonging to a class outside of a static training set will appear at query time. We present a formalized adaptive open world framework for stealth malware recognition and relate it mathematically to research from other machine learning domains.        △ Less","2 December, 2016","cs.CR,cs.CV",
              Effective Computer Model For Recognizing Nationality From Frontal Image          ,1603.04550,https://arxiv.org/abs/1603.04550,https://arxiv.org/pdf/1603.04550,"Authors:Bat-ErdeneBatsukh,GanbatTsend","        We are introducing new effective computer model for extracting nationality from frontal image candidate using face part color, size and distances based on deep research. Determining face part size, color, and distances is depending on a variety of factors including image quality, lighting condition, rotation angle, occlusion and facial emotion. Therefore, first we need to detect a face on the image then convert an image into the real input. After that, we can determine image candidate gender, face shape, key points and face parts. Finally, we will return the result, based on the comparison of sizes and distances with the sample measurement table database. While we were measuring samples, there were big differences between images by their gender and face shapes. Input images must be the frontal face image that has smooth lighting and does not have any rotation angle. The model can be used in military, police, defense, healthcare, and technology sectors. Finally, Computer can distinguish nationality from the face image.        △ Less","15 March, 2016",cs.CV,
              Assessing Executive Function Using a Computer Game: Computational Modeling of Cognitive Processes          ,1603.03828,https://arxiv.org/abs/1603.03828,https://arxiv.org/pdf/1603.03828,"Authors:StuartHagler,HollyB.Jimison,MishaPavel","        Early and reliable detection of cognitive decline is one of the most important challenges of current healthcare. In this project we developed an approach whereby a frequently played computer game can be used to assess a variety of cognitive processes and estimate the results of the pen-and-paper Trail-Making Test (TMT) - known to measure executive function, as well as visual pattern recognition, speed of processing, working memory, and set-switching ability. We developed a computational model of the TMT based on a decomposition of the test into several independent processes, each characterized by a set of parameters that can be estimated from play of a computer game designed to resemble the TMT. An empirical evaluation of the model suggests that it is possible to use the game data to estimate the parameters of the underlying cognitive processes and using the values of the parameters to estimate the TMT performance. Cognitive measures and trends in these measures can be used to identify individuals for further assessment, to provide a mechanism for improving the early detection of neurological problems, and to provide feedback and monitoring for cognitive interventions in the home.        △ Less","11 March, 2016","q-bio.QM,q-bio.NC",10.1109/JBHI.2014.2299793 
              Learning from Imbalanced Multiclass Sequential Data Streams Using Dynamically Weighted Conditional Random Fields          ,1603.03627,https://arxiv.org/abs/1603.03627,https://arxiv.org/pdf/1603.03627,"Authors:RobertoL.ShinmotoTorres,DamithC.Ranasinghe,QinfengShi,AntonvandenHengel","        The present study introduces a method for improving the classification performance of imbalanced multiclass data streams from wireless body worn sensors. Data imbalance is an inherent problem in activity recognition caused by the irregular time distribution of activities, which are sequential and dependent on previous movements. We use conditional random fields (CRF), a graphical model for structured classification, to take advantage of dependencies between activities in a sequence. However, CRFs do not consider the negative effects of class imbalance during training. We propose a class-wise dynamically weighted CRF (dWCRF) where weights are automatically determined during training by maximizing the expected overall F-score. Our results based on three case studies from a healthcare application using a batteryless body worn sensor, demonstrate that our method, in general, improves overall and minority class F-score when compared to other CRF based classifiers and achieves similar or better overall and class-wise performance when compared to SVM based classifiers under conditions of limited training data. We also confirm the performance of our approach using an additional battery powered body worn sensor dataset, achieving similar results in cases of high class imbalance.        △ Less","11 March, 2016",cs.LG,
              A Markovian-based Approach for Daily Living Activities Recognition          ,1603.03251,https://arxiv.org/abs/1603.03251,https://arxiv.org/pdf/1603.03251,"Authors:ZainebLiouane,TayebLemlouma,PhilippeRoose,FrédéricWeis,MessaoudHassani","        Recognizing the activities of daily living plays an important role in healthcare. It is necessary to use an adapted model to simulate the human behavior in a domestic space to monitor the patient harmonically and to intervene in the necessary time. In this paper, we tackle this problem using the hierarchical hidden Markov model for representing and recognizing complex indoor activities. We propose a new grammar, called ""Home By Room Activities Language"", to facilitate the complexity of human scenarios and consider the abnormal activities.        △ Less","10 March, 2016","cs.HC,cs.AI,cs.CY",
              A Framework for Extracting and Modeling HIPAA Privacy Rules for Healthcare Applications          ,1603.02964,https://arxiv.org/abs/1603.02964,https://arxiv.org/pdf/1603.02964,"Authors:TariqAlshugran,JuliusDichter","        Some organizations use software applications to manage their customers' personal, medical, or financial information. In the United States, those software applications are obligated to preserve users' privacy and to comply with the United States federal privacy laws and regulations. To formally guarantee compliance with those regulations, it is essential to extract and model the privacy rules from the text of the law using a formal framework. In this work we propose a goal-oriented framework for modeling and extracting the privacy requirements from regulatory text using natural language processing techniques.        △ Less","9 March, 2016","cs.CY,cs.CR",10.5121/hiij.2016.5101 
              Uncovering Longitudinal Healthcare Utilization from Patient-Level Medical Claims Data          ,1603.00896,https://arxiv.org/abs/1603.00896,https://arxiv.org/pdf/1603.00896,"Authors:RossP.Hilton,NicoletaSerban,RichardY.Zheng","        The objective of this study is to introduce methodology for studying longitudinal claims data observed at the patient level, with inference on the heterogeneity of healthcare utilization behaviors within large healthcare systems such as Medicaid. The proposed approach is model-based, allowing for visualization of longitudinal utilization behaviors using simple stochastic graphical networks. The approach is general, providing a framework for the study of other chronic conditions wherever longitudinal healthcare utilization data are available. Our methods are inspired by and applied to patient-level Medicaid claims for asthma-diagnosed children diagnosed observed over a period of five years, with a comparison of two neighboring states, Georgia and North Carolina.        △ Less","2 March, 2016",stat.AP,
              The SPHERE Challenge: Activity Recognition with Multimodal Sensor Data          ,1603.00797,https://arxiv.org/abs/1603.00797,https://arxiv.org/pdf/1603.00797,"Authors:NiallTwomey,TomDiethe,MeelisKull,HaoSong,MassimoCamplani,SionHannuna,XenofonFafoutis,NiZhu,PeteWoznowski,PeterFlach,IanCraddock","        This paper outlines the Sensor Platform for HEalthcare in Residential Environment (SPHERE) project and details the SPHERE challenge that will take place in conjunction with European Conference on Machine Learning and Principles and Practice of Knowledge Discovery (ECML-PKDD) between March and July 2016. The SPHERE challenge is an activity recognition competition where predictions are made from video, accelerometer and environmental sensors. Monetary prizes will be awarded to the top three entrants, with Euro 1,000 being awarded to the winner, Euro 600 being awarded to the first runner up, and Euro 400 being awarded to the second runner up.        △ Less","17 March, 2016","cs.CY,cs.HC",
              Dynamic Channel Allocation for Interference Mitigation in Relay-assisted Wireless Body Networks          ,1602.09039,https://arxiv.org/abs/1602.09039,https://arxiv.org/pdf/1602.09039,"Authors:MohamadJaafarAli,HassineMoungla,AhmedMehaoua,YongXu","        We focus on interference mitigation and energy conservation within a single wireless body area network (WBAN). We adopt two-hop communication scheme supported by the the IEEE 802.15.6 standard (2012). In this paper, we propose a dynamic channel allocation scheme, namely DCAIM to mitigate node-level interference amongst the coexisting regions of a WBAN. At the time, the sensors are in the radius communication of a relay, they form a relay region (RG) coordinated by that relay using time division multiple access (TDMA). In the proposed scheme, each RG creates a table consisting of interfering sensors which it broadcasts to its neighboring sensors. This broadcast allows each pair of RGs to create an interference set (IS). Thus, the members of IS are assigned orthogonal sub-channels whereas other sonsors that do not belong to IS can transmit using the same time slots. Experimental results show that our proposal mitigates node-level interference and improves node and WBAN energy savings. These results are then compared to the results of other schemes. As a result, our scheme outperforms in all cases. Node-level signal to interference and noise ratio (SINR) improved by 11dB whilst, the energy consumption decreased significantly. We further present a probabilistic method and analytically show the outage probability can be effectively reduced to the minimal.        △ Less","2 November, 2016","cs.NI,cs.IT",
              Machine Agency in Human-Machine Networks; Impacts and Trust Implications          ,1602.08237,https://arxiv.org/abs/1602.08237,https://arxiv.org/pdf/1602.08237,"Authors:VegardEngen,J.BrianPickering,PaulWalland","        We live in an emerging hyper-connected era in which people are in contact and interacting with an increasing number of other people and devices. Increasingly, modern IT systems form networks of humans and machines that interact with one another. As machines take a more active role in such networks, they exert an in-creasing level of influence on other participants. We review the existing literature on agency and propose a definition of agency that is practical for describing the capabilities and impact human and machine actors may have in a human-machine network. On this basis, we discuss and demonstrate the impact and trust implica-tions for machine actors in human-machine networks for emergency decision support, healthcare and future smart homes. We maintain that machine agency not only facilitates human to machine trust, but also interpersonal trust; and that trust must develop to be able to seize the full potential of future technology.        △ Less","26 February, 2016","cs.HC,cs.CY,cs.SI",
              Big Data For Development: Applications and Techniques          ,1602.07810,https://arxiv.org/abs/1602.07810,https://arxiv.org/pdf/1602.07810,"Authors:AnwaarAli,JunaidQadir,RaihanurRasool,ArjunaSathiaseelan,AndrejZwitter","        With the explosion of social media sites and proliferation of digital computing devices and Internet access, massive amounts of public data is being generated on a daily basis. Efficient techniques/ algorithms to analyze this massive amount of data can provide near real-time information about emerging trends and provide early warning in case of an imminent emergency (such as the outbreak of a viral disease). In addition, careful mining of these data can reveal many useful indicators of socioeconomic and political events, which can help in establishing effective public policies. The focus of this study is to review the application of big data analytics for the purpose of human development. The emerging ability to use big data techniques for development (BD4D) promises to revolutionalize healthcare, education, and agriculture; facilitate the alleviation of poverty; and help to deal with humanitarian crises and violent conflicts. Besides all the benefits, the large-scale deployment of BD4D is beset with several challenges due to the massive size, fast-changing and diverse nature of big data. The most pressing concerns relate to efficient data acquisition and sharing, establishing of context (e.g., geolocation and time) and veracity of a dataset, and ensuring appropriate privacy. In this study, we provide a review of existing BD4D work to study the impact of big data on the development of society. In addition to reviewing the important works, we also highlight important challenges and open issues.        △ Less","25 February, 2016",cs.CY,
              FLASH: Fast Bayesian Optimization for Data Analytic Pipelines          ,1602.06468,https://arxiv.org/abs/1602.06468,https://arxiv.org/pdf/1602.06468,"Authors:YuyuZhang,MohammadTahaBahadori,HangSu,JimengSun","        Modern data science relies on data analytic pipelines to organize interdependent computational steps. Such analytic pipelines often involve different algorithms across multiple steps, each with its own hyperparameters. To achieve the best performance, it is often critical to select optimal algorithms and to set appropriate hyperparameters, which requires large computational efforts. Bayesian optimization provides a principled way for searching optimal hyperparameters for a single algorithm. However, many challenges remain in solving pipeline optimization problems with high-dimensional and highly conditional search space. In this work, we propose Fast LineAr SearcH (FLASH), an efficient method for tuning analytic pipelines. FLASH is a two-layer Bayesian optimization framework, which firstly uses a parametric model to select promising algorithms, then computes a nonparametric model to fine-tune hyperparameters of the promising algorithms. FLASH also includes an effective caching algorithm which can further accelerate the search process. Extensive experiments on a number of benchmark datasets have demonstrated that FLASH significantly outperforms previous state-of-the-art methods in both search speed and accuracy. Using 50% of the time budget, FLASH achieves up to 20% improvement on test error rate compared to the baselines. FLASH also yields state-of-the-art performance on a real-world application for healthcare predictive modeling.        △ Less","23 June, 2016",cs.LG,
              Human-Data Interaction in Healthcare,1602.05751,https://arxiv.org/abs/1602.05751,https://arxiv.org/pdf/1602.05751,"Authors:FedericoCabitza,AngelaLocoro","        In this paper, we focus on an emerging strand of IT-oriented research, namely Human-Data Interaction (HDI) and how this can be applied to healthcare. HDI regards both how humans create and use data by means of interactive systems, which can both assist and constrain them, as well as to passively collect and proactively generate data. Healthcare provides a challenging arena to test the potential of HDI to provide a new, user-centered perspective on how data work should be supported and assessed, especially in the light of the fact that data are becoming increasingly big and that many tools are now available for the lay people, including doctors and nurses, to interact with health-related data.        △ Less","13 May, 2016",cs.HC,
              Multi-layer Representation Learning for Medical Concepts          ,1602.05568,https://arxiv.org/abs/1602.05568,https://arxiv.org/pdf/1602.05568,"Authors:EdwardChoi,MohammadTahaBahadori,ElizabethSearles,CatherineCoffey,JimengSun","        Learning efficient representations for concepts has been proven to be an important basis for many applications such as machine translation or document classification. Proper representations of medical concepts such as diagnosis, medication, procedure codes and visits will have broad applications in healthcare analytics. However, in Electronic Health Records (EHR) the visit sequences of patients include multiple concepts (diagnosis, procedure, and medication codes) per visit. This structure provides two types of relational information, namely sequential order of visits and co-occurrence of the codes within each visit. In this work, we propose Med2Vec, which not only learns distributed representations for both medical codes and visits from a large EHR dataset with over 3 million visits, but also allows us to interpret the learned representations confirmed positively by clinical experts. In the experiments, Med2Vec displays significant improvement in key medical applications compared to popular baselines such as Skip-gram, GloVe and stacked autoencoder, while providing clinically meaningful interpretation.        △ Less","17 February, 2016",cs.LG,
              Encoding Distortion Modeling For DWT-Based Wireless EEG Monitoring System          ,1602.04974,https://arxiv.org/abs/1602.04974,https://arxiv.org/pdf/1602.04974,"Authors:AlaaAwad,MedhatH.M.Elsayed,AmrMohamed","        Recent advances in wireless body area sensor net- works leverage wireless and mobile communication technologies to facilitate development of innovative medical applications that can significantly enhance healthcare services and improve quality of life. Specifically, Electroencephalography (EEG)-based applications lie at the heart of these promising technologies. However, the design and operation of such applications is challenging. Power consumption requirements of the sensor nodes may turn some of these applications impractical. Hence, implementing efficient encoding schemes are essential to reduce power consumption in such applications. In this paper, we propose an analytical distortion model for the EEG-based encoding systems. Using this model, the encoder can effectively reconfigure its complexity by adjusting its control parameters to satisfy application constraints while maintaining reconstruction accuracy at the receiver side. The simulation results illustrate that the main parameters that affect the distortion are compression ratio and filter length of the considered DWT-based encoder. Furthermore, it is found that the wireless channel variations have a significant influence on the estimated distortion at the receiver side.        △ Less","16 February, 2016",cs.OH,
              Identifying Diabetic Patients with High Risk of Readmission          ,1602.04257,https://arxiv.org/abs/1602.04257,https://arxiv.org/pdf/1602.04257,"Authors:MalladihalliSBhuvan,AnkitKumar,AdilZafar,VinithKishore","        Hospital readmissions are expensive and reflect the inadequacies in healthcare system. In the United States alone, treatment of readmitted diabetic patients exceeds 250 million dollars per year. Early identification of patients facing a high risk of readmission can enable healthcare providers to to conduct additional investigations and possibly prevent future readmissions. This not only improves the quality of care but also reduces the medical expenses on readmission. Machine learning methods have been leveraged on public health data to build a system for identifying diabetic patients facing a high risk of future readmission. Number of inpatient visits, discharge disposition and admission type were identified as strong predictors of readmission. Further, it was found that the number of laboratory tests and discharge disposition together predict whether the patient will be readmitted shortly after being discharged from the hospital (i.e. <30 days) or after a longer period of time (i.e. >30 days). These insights can help healthcare providers to improve inpatient diabetic care. Finally, the cost analysis suggests that $252.76 million can be saved across 98,053 diabetic patient encounters by incorporating the proposed cost sensitive analysis model.        △ Less","12 February, 2016","cs.AI,cs.CY",
              Data-Driven Online Decision Making with Costly Information Acquisition          ,1602.03600,https://arxiv.org/abs/1602.03600,https://arxiv.org/pdf/1602.03600,"Authors:OnurAtan,MihaelavanderSchaar","        In most real-world settings such as recommender systems, finance, and healthcare, collecting useful information is costly and requires an active choice on the part of the decision maker. The decision-maker needs to learn simultaneously what observations to make and what actions to take. This paper incorporates the information acquisition decision into an online learning framework. We propose two different algorithms for this dual learning problem: Sim-OOS and Seq-OOS where observations are made simultaneously and sequentially, respectively. We prove that both algorithms achieve a regret that is sublinear in time. The developed framework and algorithms can be used in many applications including medical informatics, recommender systems and actionable intelligence in transportation, finance, cyber-security etc., in which collecting information prior to making decisions is costly. We validate our algorithms in a breast cancer example setting in which we show substantial performance gains for our proposed algorithms.        △ Less","20 October, 2017","stat.ML,cs.LG",
              Eye-CU: Sleep Pose Classification for Healthcare using Multimodal Multiview Data          ,1602.02343,https://arxiv.org/abs/1602.02343,https://arxiv.org/pdf/1602.02343,"Authors:CarlosTorres,VictorFragoso,ScottD.Hammond,JeffreyC.Fried,B.S.Manjunath","        Manual analysis of body poses of bed-ridden patients requires staff to continuously track and record patient poses. Two limitations in the dissemination of pose-related therapies are scarce human resources and unreliable automated systems. This work addresses these issues by introducing a new method and a new system for robust automated classification of sleep poses in an Intensive Care Unit (ICU) environment. The new method, coupled-constrained Least-Squares (cc-LS), uses multimodal and multiview (MM) data and finds the set of modality trust values that minimizes the difference between expected and estimated labels. The new system, Eye-CU, is an affordable multi-sensor modular system for unobtrusive data collection and analysis in healthcare. Experimental results indicate that the performance of cc-LS matches the performance of existing methods in ideal scenarios. This method outperforms the latest techniques in challenging scenarios by 13% for those with poor illumination and by 70% for those with both poor illumination and occlusions. Results also show that a reduced Eye-CU configuration can classify poses without pressure information with only a slight drop in its performance.        △ Less","22 February, 2016",cs.CV,
              BANZKP: a Secure Authentication Scheme Using Zero Knowledge Proof for WBANs          ,1602.00895,https://arxiv.org/abs/1602.00895,https://arxiv.org/pdf/1602.00895,"Authors:NesrineKhernane,MariaPotop-Butucaru,ClaudeChaudet","        -Wireless body area network(WBAN) has shown great potential in improving healthcare quality not only for patients but also for medical staff. However, security and privacy are still an important issue in WBANs especially in multi-hop architectures. In this paper, we propose and present the design and the evaluation of a secure lightweight and energy efficient authentication scheme BANZKP based on an efficient cryptographic protocol, Zero Knowledge Proof (ZKP) and a commitment scheme. ZKP is used to confirm the identify of the sensor nodes, with small computational requirement, which is favorable for body sensors given their limited resources, while the commitment scheme is used to deal with replay attacks and hence the injection attacks by committing a message and revealing the key later. Our scheme reduces the memory requirement by 56.13 % compared to TinyZKP [13], the comparable alternative so far for Body Area Networks, and uses 10 % less energy.        △ Less","2 February, 2016","cs.CR,cs.NI",
              DeepCare: A Deep Dynamic Memory Model for Predictive Medicine          ,1602.00357,https://arxiv.org/abs/1602.00357,https://arxiv.org/pdf/1602.00357,"Authors:TrangPham,TruyenTran,DinhPhung,SvethaVenkatesh","        Personalized predictive medicine necessitates the modeling of patient illness and care processes, which inherently have long-term temporal dependencies. Healthcare observations, recorded in electronic medical records, are episodic and irregular in time. We introduce DeepCare, an end-to-end deep dynamic neural network that reads medical records, stores previous illness history, infers current illness states and predicts future medical outcomes. At the data level, DeepCare represents care episodes as vectors in space, models patient health state trajectories through explicit memory of historical records. Built on Long Short-Term Memory (LSTM), DeepCare introduces time parameterizations to handle irregular timed events by moderating the forgetting and consolidation of memory cells. DeepCare also incorporates medical interventions that change the course of illness and shape future medical risk. Moving up to the health state level, historical and present health states are then aggregated through multiscale temporal pooling, before passing through a neural network that estimates future outcomes. We demonstrate the efficacy of DeepCare for disease progression modeling, intervention recommendation, and future risk prediction. On two important cohorts with heavy social and economic burden -- diabetes and mental health -- the results show improved modeling and risk prediction accuracy.        △ Less","10 April, 2017","stat.ML,cs.LG",
              Active Particles in Complex and Crowded Environments          ,1602.00081,https://arxiv.org/abs/1602.00081,https://arxiv.org/pdf/1602.00081,"Authors:ClemensBechinger,RobertoDiLeonardo,HartmutLöwen,CharlesReichhardt,GiorgioVolpe,GiovanniVolpe","        Differently from passive Brownian particles, active particles, also known as self-propelled Brownian particles or microswimmers and nanoswimmers, are capable of taking up energy from their environment and converting it into directed motion. Because of this constant flow of energy, their behavior can only be explained and understood within the framework of nonequilibrium physics. In the biological realm, many cells perform directed motion, for example, as a way to browse for nutrients or to avoid toxins. Inspired by these motile microorganisms, researchers have been developing artificial particles that feature similar swimming behaviors based on different mechanisms; these manmade micro- and nanomachines hold a great potential as autonomous agents for healthcare, sustainability, and security applications. With a focus on the basic physical features of the interactions of self-propelled Brownian particles with a crowded and complex environment, this comprehensive review will put the reader at the very forefront of the field, providing a guided tour through its basic principles, the development of artificial self-propelling micro- and nanoparticles, and their application to the study of nonequilibrium phenomena, as well as the open challenges that the field is currently facing.        △ Less","29 December, 2016",cond-mat.soft,10.1103/RevModPhys.88.045006 
              Your Activities of Daily Living (YADL): An Image-based Survey Technique for Patients with Arthritis          ,1601.03278,https://arxiv.org/abs/1601.03278,https://arxiv.org/pdf/1601.03278,"Authors:LongqiYang,DianaFreed,AlexWu,JudyWu,JPPollak,DeborahEstrin","Healthcare professionals use Activities of Daily Living (ADL) to characterize a patient's functional status and to evaluate the effectiveness of treatment plans. ADLs are traditionally measured using standardized text-based questionnaires and the only form of personalization is in the form of question branching logic. Pervasive smartphone adoption makes it feasible to consider more frequent patient-reporting on ADLs. However, asking generic sets of questions repeatedly introduces user burden and fatigue that threatens to interfere with their utility. We introduce an approach called YADL (Your Activities of Daily Living) which uses images of ADLs and personalization to improve survey efficiency and the patient-experience. It offers several potential benefits: wider coverage of ADLs, improved engagement, and accurate capture of individual health situations. In this paper, we discuss our system design and the wide applicability of the design process for survey tools in healthcare and beyond. Interactions with with a small number of patients with Arthritis throughout the design process have been promising and we share detailed insights.        △ Less","13 January, 2016","cs.CY,cs.HC",
              A new Bayesian regression model for counts in medicine          ,1601.02820,https://arxiv.org/abs/1601.02820,https://arxiv.org/pdf/1601.02820,"Authors:HamedHaselimashhadi,VeronicaVinciotti,KemingYu","        Discrete data are collected in many application areas and are often characterised by highly skewed and power-lawlike distributions. An example of this, which is considered in this paper, is the number of visits to a specialist, often taken as a measure of demand in healthcare. A discrete Weibull regression model was recently proposed for regression problems with a discrete response and it was shown to possess two important features: the ability to capture over and under-dispersion simultaneously and a closed-form analytical expression of the quantiles of the conditional distribution. In this paper, we propose the first Bayesian implementation of a discrete Weibull regression model. The implementation considers a novel parameterization, where both parameters of the discrete Weibull distribution can be made dependent on the predictors. In addition, prior distributions can be imposed that encourage parameter shrinkage and that lead to variable selection. As with Bayesian procedures, the full posterior distribution of the parameters is returned, from which credible intervals can be readily calculated. A simulation study and the analysis of four real datasets of medical records show promises for the wide applicability of this approach to the analysis of count data. The method is implemented in the R package BDWreg.        △ Less","12 January, 2016",stat.ME,
              Discovering topic structures of a temporally evolving document corpus          ,1512.08008,https://arxiv.org/abs/1512.08008,https://arxiv.org/pdf/1512.08008,"Authors:AdhamBeykikhoshk,OgnjenArandjelovic,DinhPhung,SvethaVenkatesh","        In this paper we describe a novel framework for the discovery of the topical content of a data corpus, and the tracking of its complex structural changes across the temporal dimension. In contrast to previous work our model does not impose a prior on the rate at which documents are added to the corpus nor does it adopt the Markovian assumption which overly restricts the type of changes that the model can capture. Our key technical contribution is a framework based on (i) discretization of time into epochs, (ii) epoch-wise topic discovery using a hierarchical Dirichlet process-based model, and (iii) a temporal similarity graph which allows for the modelling of complex topic changes: emergence and disappearance, evolution, splitting, and merging. The power of the proposed framework is demonstrated on two medical literature corpora concerned with the autism spectrum disorder (ASD) and the metabolic syndrome (MetS) -- both increasingly important research subjects with significant social and healthcare consequences. In addition to the collected ASD and metabolic syndrome literature corpora which we made freely available, our contribution also includes an extensive empirical analysis of the proposed framework. We describe a detailed and careful examination of the effects that our algorithms's free parameters have on its output, and discuss the significance of the findings both in the context of the practical application of our algorithm as well as in the context of the existing body of work on temporal topic analysis. Our quantitative analysis is followed by several qualitative case studies highly relevant to the current research on ASD and MetS, on which our algorithm is shown to capture well the actual developments in these fields.        △ Less","25 December, 2015","cs.IR,cs.LG",
              Flexible Attribute-Based Encryption Applicable to Secure E-Healthcare Records          ,1512.06578,https://arxiv.org/abs/1512.06578,https://arxiv.org/pdf/1512.06578,"Authors:BoQin,HuaDeng,QianhongWu,JosepDomingo-Ferrer,DavidNaccache,YunyaZhou","        In e-healthcare record systems (EHRS), attribute-based encryption (ABE) appears as a natural way to achieve fine-grained access control on health records. Some proposals exploit key-policy ABE (KP-ABE) to protect privacy in such a way that all users are associated with specific access policies and only the ciphertexts matching the users' access policies can be decrypted. An issue with KP-ABE is that it requires an a priori formulation of access policies during key generation, which is not always practicable in EHRS because the policies to access health records are sometimes determined after key generation. In this paper, we revisit KPABE and propose a dynamic ABE paradigm, referred to as access policy redefinable ABE (APR-ABE). To address the above issue, APR-ABE allows users to redefine their access policies and delegate keys for the redefined ones; hence a priori precise policies are no longer mandatory. We construct an APR-ABE scheme with short ciphertexts and prove its full security in the standard model under several static assumptions.        △ Less","21 December, 2015",cs.CR,10.1007/s10207-014-0272-7 
              Remote Health Coaching System and Human Motion Data Analysis for Physical Therapy with Microsoft Kinect          ,1512.06492,https://arxiv.org/abs/1512.06492,https://arxiv.org/pdf/1512.06492,"Authors:QifeiWang,GregorijKurillo,FerdaOfli,RuzenaBajcsy","        This paper summarizes the recent progress we have made for the computer vision technologies in physical therapy with the accessible and affordable devices. We first introduce the remote health coaching system we build with Microsoft Kinect. Since the motion data captured by Kinect is noisy, we investigate the data accuracy of Kinect with respect to the high accuracy motion capture system. We also propose an outlier data removal algorithm based on the data distribution. In order to generate the kinematic parameter from the noisy data captured by Kinect, we propose a kinematic filtering algorithm based on Unscented Kalman Filter and the kinematic model of human skeleton. The proposed algorithm can obtain smooth kinematic parameter with reduced noise compared to the kinematic parameter generated from the raw motion data from Kinect.        △ Less","20 December, 2015","cs.CV,cs.AI",
              Can Pretrained Neural Networks Detect Anatomy?          ,1512.05986,https://arxiv.org/abs/1512.05986,https://arxiv.org/pdf/1512.05986,"Authors:VladoMenkovski,ZharkoAleksovski,AxelSaalbach,HannesNickisch","        Convolutional neural networks demonstrated outstanding empirical results in computer vision and speech recognition tasks where labeled training data is abundant. In medical imaging, there is a huge variety of possible imaging modalities and contrasts, where annotated data is usually very scarce. We present two approaches to deal with this challenge. A network pretrained in a different domain with abundant data is used as a feature extractor, while a subsequent classifier is trained on a small target dataset; and a deep architecture trained with heavy augmentation and equipped with sophisticated regularization methods. We test the approaches on a corpus of X-ray images to design an anatomy detection system.        △ Less","18 December, 2015","cs.CV,cs.AI,cs.NE",
              Exploring Controversy in Twitter          ,1512.05550,https://arxiv.org/abs/1512.05550,https://arxiv.org/pdf/1512.05550,"Authors:KiranGarimella,GianmarcoDeFrancisciMorales,AristidesGionis,MichaelMathioudakis","        Among the topics discussed on social media, some spark more heated debate than others. For example, experience suggests that major political events, such as a vote for healthcare law in the US, would spark more debate between opposing sides than other events, such as a concert of a popular music band. Exploring the topics of discussion on Twitter and understanding which ones are controversial is extremely useful for a variety of purposes, such as for journalists to understand what issues divide the public, or for social scientists to understand how controversy is manifested in social interactions.  The system we present processes the daily trending topics discussed on the platform, and assigns to each topic a controversy score, which is computed based on the interactions among Twitter users, and a visualization of these interactions, which provides an intuitive visual cue regarding the controversy of the topic. The system also allows users to explore the messages (tweets) associated with each topic, and sort and explore the topics by different criteria (e.g., by controversy score, time, or related keywords).        △ Less","17 December, 2015",cs.SI,
              Evaluation of Pose Tracking Accuracy in the First and Second Generations of Microsoft Kinect          ,1512.04134,https://arxiv.org/abs/1512.04134,https://arxiv.org/pdf/1512.04134,"Authors:QifeiWang,GregorijKurillo,FerdaOfli,RuzenaBajcsy","        Microsoft Kinect camera and its skeletal tracking capabilities have been embraced by many researchers and commercial developers in various applications of real-time human movement analysis. In this paper, we evaluate the accuracy of the human kinematic motion data in the first and second generation of the Kinect system, and compare the results with an optical motion capture system. We collected motion data in 12 exercises for 10 different subjects and from three different viewpoints. We report on the accuracy of the joint localization and bone length estimation of Kinect skeletons in comparison to the motion capture. We also analyze the distribution of the joint localization offsets by fitting a mixture of Gaussian and uniform distribution models to determine the outliers in the Kinect motion data. Our analysis shows that overall Kinect 2 has more robust and more accurate tracking of human pose as compared to Kinect 1.        △ Less","13 December, 2015","cs.CV,cs.AI",10.1109/ICHI.2015.54 
              Distilling Knowledge from Deep Networks with Applications to Healthcare Domain          ,1512.03542,https://arxiv.org/abs/1512.03542,https://arxiv.org/pdf/1512.03542,"Authors:ZhengpingChe,SanjayPurushotham,RobinderKhemani,YanLiu","        Exponential growth in Electronic Healthcare Records (EHR) has resulted in new opportunities and urgent needs for discovery of meaningful data-driven representations and patterns of diseases in Computational Phenotyping research. Deep Learning models have shown superior performance for robust prediction in computational phenotyping tasks, but suffer from the issue of model interpretability which is crucial for clinicians involved in decision-making. In this paper, we introduce a novel knowledge-distillation approach called Interpretable Mimic Learning, to learn interpretable phenotype features for making robust prediction while mimicking the performance of deep learning models. Our framework uses Gradient Boosting Trees to learn interpretable features from deep learning models such as Stacked Denoising Autoencoder and Long Short-Term Memory. Exhaustive experiments on a real-world clinical time-series dataset show that our method obtains similar or better performance than the deep learning models, and it provides interpretable phenotypes for clinical decision making.        △ Less","11 December, 2015","stat.ML,cs.LG",
Healthcare IT: Is your Information at Risk?          ,1512.01731,https://arxiv.org/abs/1512.01731,https://arxiv.org/pdf/1512.01731,"Authors:KimmarieDonahue,ShawonRahman","Healthcare Information Technology (IT) has made great advances over the past few years and while these advances have enable healthcare professionals to provide higher quality healthcare to a larger number of individuals it also provides the criminal element more opportunities to access sensitive information, such as patient protected health information (PHI) and Personal identification Information (PII). Having an Information Assurance (IA) programallows for the protection of information and information systems and ensures the organization is in compliance with all requires regulations, laws and directive is essential. While most organizations have such a policy in place, often it is inadequate to ensure the proper protection to prevent security breaches. The increase of data breaches in the last few years demonstrates the importance of an effective IA program. To ensure an effective IA policy, the policy must manage the operational risk, including identifying risks, assessment and mitigation of identified risks and ongoing monitoring to ensure compliance        △ Less","30 November, 2015","cs.CR,cs.CY",10.5121/ijnsa.2012.4508 
              A Shapley Value Solution to Game Theoretic-based Feature Reduction in False Alarm Detection          ,1512.01680,https://arxiv.org/abs/1512.01680,https://arxiv.org/pdf/1512.01680,"Authors:FatemehAfghah,AbolfazlRazi,KayvanNajarian","        False alarm is one of the main concerns in intensive care units and can result in care disruption, sleep deprivation, and insensitivity of care-givers to alarms. Several methods have been proposed to suppress the false alarm rate through improving the quality of physiological signals by filtering, and developing more accurate sensors. However, significant intrinsic correlation among the extracted features limits the performance of most currently available data mining techniques, as they often discard the predictors with low individual impact that may potentially have strong discriminatory power when grouped with others. We propose a model based on coalition game theory that considers the inter-features dependencies in determining the salient predictors in respect to false alarm, which results in improved classification accuracy. The superior performance of this method compared to current methods is shown in simulation results using PhysionNet's MIMIC II database.        △ Less","5 December, 2015",cs.CV,
              Position paper: a general framework for applying machine learning techniques in operating room          ,1511.09099,https://arxiv.org/abs/1511.09099,https://arxiv.org/pdf/1511.09099,"Authors:FilippoMariaBianchi,EnricoDeSantis,HediehMontazeri,ParisaNaraei,AlirezaSadeghian","        In this position paper we describe a general framework for applying machine learning and pattern recognition techniques in healthcare. In particular, we are interested in providing an automated tool for monitoring and incrementing the level of awareness in the operating room and for identifying human errors which occur during the laparoscopy surgical operation. The framework that we present is divided in three different layers: each layer implements algorithms which have an increasing level of complexity and which perform functionality with an higher degree of abstraction. In the first layer, raw data collected from sensors in the operating room during surgical operation, they are pre-processed and aggregated. The results of this initial phase are transferred to a second layer, which implements pattern recognition techniques and extract relevant features from the data. Finally, in the last layer, expert systems are employed to take high level decisions, which represent the final output of the system.        △ Less","29 November, 2015","cs.CY,cs.LG",
              Temporal Convolutional Neural Networks for Diagnosis from Lab Tests          ,1511.07938,https://arxiv.org/abs/1511.07938,https://arxiv.org/pdf/1511.07938,"Authors:NargesRazavian,DavidSontag","        Early diagnosis of treatable diseases is essential for improving healthcare, and many diseases' onsets are predictable from annual lab tests and their temporal trends. We introduce a multi-resolution convolutional neural network for early detection of multiple diseases from irregularly measured sparse lab values. Our novel architecture takes as input both an imputed version of the data and a binary observation matrix. For imputing the temporal sparse observations, we develop a flexible, fast to train method for differentiable multivariate kernel regression. Our experiments on data from 298K individuals over 8 years, 18 common lab measurements, and 171 diseases show that the temporal signatures learned via convolution are significantly more predictive than baselines commonly used for early disease diagnosis.        △ Less","10 March, 2016",cs.LG,
              Bioinspired interfacial materials with enhanced drop mobility: From fundamentals to multifunctional applications          ,1511.07707,https://arxiv.org/abs/1511.07707,https://arxiv.org/pdf/1511.07707,"Authors:ChongleiHao,YahuaLiu,XuemeiChen,JingLi,MeiZhang,YanhuaZhao,ZuankaiWang","        The development of bio-inspired interfacial materials with enhanced drop mobility that mimic the innate functionalities of nature will have significant impact on the energy, environment and global healthcare. In spite of extensive progress, the state of the art of interfacial materials have not reached the level of maturity sufficient for industrial applications in terms of scalability, stability and reliability, which are complicated by their operating environments and lack of facile approaches to exquisitely control the local structural texture and chemical composition at multiple length scales. In this review, we focus on the recent advances in the fundamental understanding as well as practical applications of bio-inspired interfacial materials, with an emphasis on the drop impact induced bouncing and coalescence induced jumping behaviors. We also suggest our own perspectives on how to catalyze new discoveries and to foster technological adoption to move this exciting area forward.        △ Less","24 November, 2015",cond-mat.mtrl-sci,
              Doctor AI: Predicting Clinical Events via Recurrent Neural Networks          ,1511.05942,https://arxiv.org/abs/1511.05942,https://arxiv.org/pdf/1511.05942,"Authors:EdwardChoi,MohammadTahaBahadori,AndySchuetz,WalterF.Stewart,JimengSun","        Leveraging large historical data in electronic health record (EHR), we developed Doctor AI, a generic predictive model that covers observed medical conditions and medication uses. Doctor AI is a temporal model using recurrent neural networks (RNN) and was developed and applied to longitudinal time stamped EHR data from 260K patients over 8 years. Encounter records (e.g. diagnosis codes, medication codes or procedure codes) were input to RNN to predict (all) the diagnosis and medication categories for a subsequent visit. Doctor AI assesses the history of patients to make multilabel predictions (one label for each diagnosis or medication category). Based on separate blind test set evaluation, Doctor AI can perform differential diagnosis with up to 79% recall@30, significantly higher than several baselines. Moreover, we demonstrate great generalizability of Doctor AI by adapting the resulting models from one institution to another without losing substantial accuracy.        △ Less","28 September, 2016",cs.LG,
              Improving Efficiency of Hospitals and Healthcare Centres          ,1511.03204,https://arxiv.org/abs/1511.03204,https://arxiv.org/pdf/1511.03204,Authors:PrachiKariya,"        The Project aims at improving the efficiency of hospitals and healthcare centres using Big Data Analytics to evaluate identified KPIs (Key Performance Indicators) of its various functions. The Dashboards designed using computer technology serves as an interactive and dynamic tool for various stakeholders, which helps in optimising performance of various functions and more so maximise the financial returns. The Project entails improving performance of patient servicing, operations and OPD departments, finance function, procurement function, HR function, etc. I developed KPIs and drilldown KPIs for various functions and assisted in designing and developing interactive Dashboards with dynamic charts.        △ Less","10 November, 2015",cs.CY,
              Learning Linguistic Biomarkers for Predicting Mild Cognitive Impairment using Compound Skip-grams          ,1511.02436,https://arxiv.org/abs/1511.02436,https://arxiv.org/pdf/1511.02436,"Authors:SylvesterOluboluOrimaye,KahYeeTai,JojoSze-MengWong,CheePiauWong","        Predicting Mild Cognitive Impairment (MCI) is currently a challenge as existing diagnostic criteria rely on neuropsychological examinations. Automated Machine Learning (ML) models that are trained on verbal utterances of MCI patients can aid diagnosis. Using a combination of skip-gram features, our model learned several linguistic biomarkers to distinguish between 19 patients with MCI and 19 healthy control individuals from the DementiaBank language transcript clinical dataset. Results show that a model with compound of skip-grams has better AUC and could help ML prediction on small MCI data sample.        △ Less","9 December, 2015","cs.CL,cs.AI",
              AIDE: An Automated Sample-based Approach for Interactive Data Exploration          ,1510.08897,https://arxiv.org/abs/1510.08897,https://arxiv.org/pdf/1510.08897,"Authors:KyriakiDimitriadou,OlgaPapaemmanouil,YanleiDiao","        In this paper, we argue that database systems be augmented with an automated data exploration service that methodically steers users through the data in a meaningful way. Such an automated system is crucial for deriving insights from complex datasets found in many big data applications such as scientific and healthcare applications as well as for reducing the human effort of data exploration. Towards this end, we present AIDE, an Automatic Interactive Data Exploration framework that assists users in discovering new interesting data patterns and eliminate expensive ad-hoc exploratory queries.  AIDE relies on a seamless integration of classification algorithms and data management optimization techniques that collectively strive to accurately learn the user interests based on his relevance feedback on strategically collected samples. We present a number of exploration techniques as well as optimizations that minimize the number of samples presented to the user while offering interactive performance. AIDE can deliver highly accurate query predictions for very common conjunctive queries with small user effort while, given a reasonable number of samples, it can predict with high accuracy complex disjunctive queries. It provides interactive performance as it limits the user wait time per iteration of exploration to less than a few seconds.        △ Less","29 October, 2015","cs.DB,cs.IR",
              There is Individualized Treatment. Why Not Individualized Inference?          ,1510.08539,https://arxiv.org/abs/1510.08539,https://arxiv.org/pdf/1510.08539,"Authors:KeliLiu,Xiao-LiMeng","        Doctors use statistics to advance medical knowledge; we use a medical analogy to introduce statistical inference ""from scratch"" and to highlight an improvement. Your doctor, perhaps implicitly, predicts the effectiveness of a treatment for you based on its performance in a clinical trial; the trial patients serve as controls for you. The same logic underpins statistical inference: to identify the best statistical procedure to use for a problem, we simulate a set of control problems and evaluate candidate procedures on the controls. Now for the improvement: recent interest in personalized/individualized medicine stems from the recognition that some clinical trial patients are better controls for you than others. Therefore, treatment decisions for you should depend only on a subset of relevant patients. Individualized statistical inference implements this idea for control problems (rather than patients). Its potential for improving data analysis matches personalized medicine's for improving healthcare. The central issue--for both individualized medicine and individualized inference--is how to make the right relevance robustness trade-off: if we exercise too much judgement in determining which controls are relevant, our inferences will not be robust. How much is too much? We argue that the unknown answer is the Holy Grail of statistical inference.        △ Less","28 October, 2015","stat.ME,math.ST",
              Economics of Internet of Things (IoT): An Information Market Approach          ,1510.06837,https://arxiv.org/abs/1510.06837,https://arxiv.org/pdf/1510.06837,"Authors:D.Niyato,X.Lu,P.Wang,D.I.Kim,Z.Han","        Internet of things (IoT) has been proposed to be a new paradigm of connecting devices and providing services to various applications, e.g., transportation, energy, smart city, and healthcare. In this paper, we focus on an important issue, i.e., economics of IoT, that can have a great impact to the success of IoT applications. In particular, we adopt and present the information economics approach with its applications in IoT. We first review existing economic models developed for IoT services. Then, we outline two important topics of information economics which are pertinent to IoT, i.e., the value of information and information good pricing. Finally, we propose a game theoretic model to study the price competition of IoT sensing services. Some outlooks on future research directions of applying information economics to IoT are discussed.        △ Less","23 October, 2015",cs.NI,
              Application of Machine Learning Techniques in Human Activity Recognition          ,1510.05577,https://arxiv.org/abs/1510.05577,https://arxiv.org/pdf/1510.05577,"Authors:JitenkumarBabubhaiRana,RashmiShetty,TanyaJha",        Human activity detection has seen a tremendous growth in the last decade playing a major role in the field of pervasive computing. This emerging popularity can be attributed to its myriad of real-life applications primarily dealing with human-centric problems like healthcare and elder care. Many research attempts with data mining and machine learning techniques have been undergoing to accurately detect human activities for e-health systems. This paper reviews some of the predictive data mining algorithms and compares the accuracy and performances of these models. A discussion on the future research directions is subsequently offered.        △ Less,"19 October, 2015",cs.LG,
              A Privacy Preserving Improvement for SRTA in Telecare Systems          ,1510.04197,https://arxiv.org/abs/1510.04197,https://arxiv.org/pdf/1510.04197,"Authors:SeyedSalmanSajjadiGhaemmaghami,MahtabMirmohseni,AfroozHaghbin","        Radio Frequency Identification (RFID) is a modern communication technology, which provides authentication and identification through a nonphysical contact. Recently, the use of this technology is almost developed in healthcare environments. Although RFID technology can prepare sagacity in systems, privacy and security issues ought to be considered before. Recently, in 2015, Li et al. proposed SRTA, a hash-based RFID authentication protocol in medication verification for healthcare. In this paper, we study this protocol and show that SRTA protocol is vulnerable to traceability, impersonation and Dos attacks. So it does not provide the privacy and security of RFID end users. Therefore, we propose an improved secure and efficient RFID authentication protocol to enhance the performance of Li et al. method. Our analyze show that the existing weaknesses of SRTA protocol are eliminated in our proposed protocol.        △ Less","14 October, 2015",cs.CR,
              Inferring disease correlation from healthcare data          ,1510.03051,https://arxiv.org/abs/1510.03051,https://arxiv.org/pdf/1510.03051,"Authors:GargiPriyadarshini,AshishAnand","        Electronic Health Records maintained in health care settings are a potential source of substantial clinical knowledge. The massive volume of data, unstructured nature of records and obligatory requirement of domain acquaintance together pose a challenge in knowledge extraction from it. The aim of this study is to overcome this challenge with a methodical analysis, abstraction and summarization of such data. This is an attempt to explain clinical observations through bio-medical and genomic data. Discharge summaries of obesity patients were processed to extract coherent patterns. This was supported by Machine Learning and Natural Language Processing based technologies and concept mapping tool along with biomedical, clinical and genomic knowledge bases. Semantic relations between diseases were extracted and filtered through Chi square test to remove spurious relations. The remaining relations were validated against biomedical literature and gene interaction networks. A collection of binary relations of diseases was derived from the data. One set implied co-morbidity while the other set contained diseases which are risk factors of others. Validation against bio-medical literature increased the prospect of correlation between diseases. Gene interaction network revealed that the diseases are related and their corresponding genes are in close proximity. Conclusion: This study focuses on deducing meaningful relations between diseases from discharge summaries. For analytical purpose, the scope has been limited to a few common, well-researched diseases. It can be extended to incorporate relatively unknown, complex diseases and discover new traits to help in clinical assessments.        △ Less","11 October, 2015","cs.IR,cs.CY",
              Repeated transcranial direct current stimulation (tDCS) in nondepressed reduces levels of reported negative affects from daily stressors          ,1510.02261,https://arxiv.org/abs/1510.02261,https://arxiv.org/pdf/1510.02261,"Authors:AdelaideAustin,GabrielaJiga-Boy,SaraRea,SimonNewstead,SianRoderick,NickJDavis,R.MarcClement,FredericBoy","        Negative emotional responses to the daily life stresses have cumulative effects which, in turn, impose wide-ranging negative constraints on emotional well being and neurocognitive performance (Kalueff:2007cp, Charles:2013eq, Nadler:2010hk). Crucial cognitive functions such as memory and problem solving, as well more short term emotional responses (e.g., anticipation of- and response to- monetary rewards or losses) are influenced by mood. The negative impact of these behavioural responses is felt at the individual level, but it also imposes major economic burden on modern healthcare systems. Although much research have been undertaken to understand the underlying mechanisms of depressed mood and design efficient treatment pathways, comparatively little was done to characterize mood modulations that remain within the boundaries of a healthy mental functioning. In one placebo-controlled experiments, we applied daily prefrontal transcranial Direct Current Stimulation (tDCS) at five points in time, and found reliable improvements on self-reported mood evaluation. We replicated this finding in an independent double-blinded placebo-controlled experiment and showed that stimulation over a shorter period of time (3 days) is sufficient to create detectable mood improvements. Taken together, our data show that repeated bilateral prefrontal tDCS can reduce psychological distress in nondepressed individuals.        △ Less","10 February, 2016",q-bio.NC,
              A Social Network Framework to Explore Healthcare Collaboration          ,1509.07578,https://arxiv.org/abs/1509.07578,https://arxiv.org/pdf/1509.07578,"Authors:UmaSrinivasan,ShahadatUddin","        A patient-centric approach to healthcare leads to an informal social network among medical professionals. This chapter presents a research framework to: identify the collaboration structure among physicians that is effective and efficient for patients, discover effective structural attributes of a collaboration network that evolves during the course of providing care, and explore the impact of socio-demographic characteristics of healthcare professionals, patients, and hospitals on collaboration structures, from the point of view of measurable outcomes such as cost and quality of care. The framework uses illustrative examples drawn from a data set of patients undergoing hip replacement surgery.        △ Less","23 September, 2015","cs.SI,cs.CY",10.4018/978-1-4666-6316-9 
              CRDT: Correlation Ratio Based Decision Tree Model for Healthcare Data Mining          ,1509.07266,https://arxiv.org/abs/1509.07266,https://arxiv.org/pdf/1509.07266,"Authors:SmitaRoy,SamratMondal,AsifEkbal","        The phenomenal growth in the healthcare data has inspired us in investigating robust and scalable models for data mining. For classification problems Information Gain(IG) based Decision Tree is one of the popular choices. However, depending upon the nature of the dataset, IG based Decision Tree may not always perform well as it prefers the attribute with more number of distinct values as the splitting attribute. Healthcare datasets generally have many attributes and each attribute generally has many distinct values. In this paper, we have tried to focus on this characteristics of the datasets while analysing the performance of our proposed approach which is a variant of Decision Tree model and uses the concept of Correlation Ratio(CR). Unlike IG based approach, this CR based approach has no biasness towards the attribute with more number of distinct values. We have applied our model on some benchmark healthcare datasets to show the effectiveness of the proposed technique.        △ Less","24 September, 2015","cs.AI,cs.DB",
              Models and Representations for Fractal Social Organizations          ,1509.05112,https://arxiv.org/abs/1509.05112,https://arxiv.org/pdf/1509.05112,Authors:ArianitPajaziti,"        Our subject is oriented towards investigation of potential ways of societal organization, that allow for collective intelligent organization and management of resources. The main objective of such organizations is the exploration of the social energy from the existing societies. We conjecture that an organizational model that fulfills the mentioned requirements is the Fractal Social Organization (FSO). Our goal is to prove and verify the effectiveness of this model by performing various simulations using the NetLogo environment, a tool that allows agent-based rapid prototyping. We begin by simulating trivial real life activities that demonstrate the main properties of the core unit of the FSO, namely the SoC. Further, more complex scenarios involving various nested SoCs are simulated. Two main simulation models are presented, allowing us to obtain preliminary results using the FSO concepts as a potential solution. In the first simulation model we demonstrate that by the use of FSO properties the individuals may benefit by receiving more qualitative healthcare services, while in the second simulation model we show how it might be possible to improve the fall detection systems by the use of FSO mechanism.        △ Less","16 September, 2015",cs.MA,
              Coverage-Driven Verification - An approach to verify code for robots that directly interact with humans          ,1509.04852,https://arxiv.org/abs/1509.04852,https://arxiv.org/pdf/1509.04852,"Authors:DejaniraAraiza-Illan,DavidWestern,AnthonyPipe,KerstinEder","        Collaborative robots could transform several industries, such as manufacturing and healthcare, but they present a significant challenge to verification. The complex nature of their working environment necessitates testing in realistic detail under a broad range of circumstances. We propose the use of Coverage-Driven Verification (CDV) to meet this challenge. By automating the simulation-based testing process as far as possible, CDV provides an efficient route to coverage closure. We discuss the need, practical considerations, and potential benefits of transferring this approach from microelectronic design verification to the field of human-robot interaction. We demonstrate the validity and feasibility of the proposed approach by constructing a custom CDV testbench and applying it to the verification of an object handover task.        △ Less","16 September, 2015",cs.RO,10.1007/978-3-319-26287-1_5 
              Estimating heterogeneous graphical models for discrete data with an application to roll call voting          ,1509.04828,https://arxiv.org/abs/1509.04828,https://arxiv.org/pdf/1509.04828,"Authors:JianGuo,JieCheng,ElizavetaLevina,GeorgeMichailidis,JiZhu","        We consider the problem of jointly estimating a collection of graphical models for discrete data, corresponding to several categories that share some common structure. An example for such a setting is voting records of legislators on different issues, such as defense, energy, and healthcare. We develop a Markov graphical model to characterize the heterogeneous dependence structures arising from such data. The model is fitted via a joint estimation method that preserves the underlying common graph structure, but also allows for differences between the networks. The method employs a group penalty that targets the common zero interaction effects across all the networks. We apply the method to describe the internal networks of the U.S. Senate on several important issues. Our analysis reveals individual structure for each issue, distinct from the underlying well-known bipartisan structure common to all categories which we are able to extract separately. We also establish consistency of the proposed method both for parameter estimation and model selection, and evaluate its numerical performance on a number of simulated examples.        △ Less","16 September, 2015",stat.AP,10.1214/13-AOAS700 
              Simple Empirical Model for Identifying Rheological Properties of Soft Biological Tissues          ,1509.02021,https://arxiv.org/abs/1509.02021,https://arxiv.org/pdf/1509.02021,"Authors:YoKobayashi,MarikoTsukune,TomoyukiMiyashita,MasakatsuG.Fujie",        Understanding the rheological properties of soft biological tissue is a key issue for mechanical systems used in the healthcare field. We propose a simple empirical model using Fractional Dynamics and Exponential Nonlinearity (FDEN) to identify the rheological properties of soft biological tissue. The model is derived from detailed material measurements using samples isolated from porcine liver. We conducted dynamic viscoelastic and creep tests on liver samples using a rheometer. The experimental results indicated that biological tissue has specific properties: i) power law increases in storage elastic modulus and loss elastic modulus with the same slope; ii) power law gain decrease and constant phase delay in the frequency domain over two decades; iii) log-log scale linearity between time and strain relationships under constant force; and iv) linear and log scale linearity between strain and stress relationships. Our simple FDEN model uses only three dependent parameters and represents the specific properties of soft biological tissue.        △ Less,"7 September, 2015","physics.bio-ph,cond-mat.soft",10.1103/PhysRevE.95.022418 
              Anticipatory Mobile Digital Health: Towards Personalised Proactive Therapies and Prevention Strategies          ,1508.03722,https://arxiv.org/abs/1508.03722,https://arxiv.org/pdf/1508.03722,"Authors:VeljkoPejovic,AbhinavMehrotra,MircoMusolesi","        The last two centuries saw groundbreaking advances in the field of healthcare: from the invention of the vaccine to organ transplant, and eradication of numerous deadly diseases. Yet, these breakthroughs have only illuminated the role that individual traits and behaviours play in the health state of a person. Continuous patient monitoring and individually-tailored therapies can help in early detection and efficient tackling of health issues. However, even the most developed nations cannot afford proactive personalised healthcare at scale. Mobile computing devices, nowadays equipped with an array of sensors, high-performance computing power, and carried by their owners at all time, promise to revolutionise modern healthcare. These devices can enable continuous patient monitoring, and, with the help of machine learning, can build predictive models of patient's health and behaviour. Finally, through their close integration with a user's lifestyle mobiles can be used to deliver personalised proactive therapies. In this article, we develop the concept of anticipatory mobile-based healthcare - anticipatory mobile digital health - and examine the opportunities and challenges associated with its practical realisation.        △ Less","15 August, 2015","cs.CY,cs.HC",
              HMIoT: A New Healthcare Model Based on Internet of Things          ,1507.08569,https://arxiv.org/abs/1507.08569,https://arxiv.org/pdf/1507.08569,"Authors:MohsenYaghoubiSuraki,MortezaYaghoubiSuraki,LeilaSourakiAzad","        In recent century, with developing of equipment, using of the internet and things connected to the internet is growing. Therefore, the need for informing in the process of expanding the scope of its application is very necessary and important. These days, using intelligent and autonomous devices in our daily lives has become commonplace and the Internet is the most important part of the relationship between these tools and even at close distances also. Things connected to the Internet that are currently in use and can be inclusive of all the sciences as a step to develop and coordinate of them. In this paper we investigate application and using of Internet of things from the perspective of various sciences. We show that how this phenomenon can influence on future health of people.        △ Less","28 July, 2015",cs.CY,
              Towards Storytelling from Visual Lifelogging: An Overview          ,1507.06120,https://arxiv.org/abs/1507.06120,https://arxiv.org/pdf/1507.06120,"Authors:MarcBolaños,MariellaDimiccoli,PetiaRadeva","        Visual lifelogging consists of acquiring images that capture the daily experiences of the user by wearing a camera over a long period of time. The pictures taken offer considerable potential for knowledge mining concerning how people live their lives, hence, they open up new opportunities for many potential applications in fields including healthcare, security, leisure and the quantified self. However, automatically building a story from a huge collection of unstructured egocentric data presents major challenges. This paper provides a thorough review of advances made so far in egocentric data analysis, and in view of the current state of the art, indicates new lines of research to move us towards storytelling from visual lifelogging.        △ Less","20 July, 2016",cs.CV,10.1109/THMS.2016.2616296 
              A Study of the Management of Electronic Medical Records in Fijian Hospitals          ,1507.03659,https://arxiv.org/abs/1507.03659,https://arxiv.org/pdf/1507.03659,"Authors:SwaranS.Ravindra,RohitashChandra,VirallikatturS.Dhenesh","        Despite having a number of benefits for healthcare settings, the successful implementation of health information systems (HIS) continues to be a challenge in many developing countries. This paper examines the current state of health information systems in government hospitals in Fiji. It also investigates if the general public as well as medical practitioners in Fiji have interest in having web based electronic medical records systems that allow patients to access their medical reports and make online bookings for their appointments. Nausori Health Centre was used as a case study to examine the information systems in a government hospital in Fiji.        △ Less","2 August, 2017",cs.CY,
              Revisiting Large Scale Distributed Machine Learning          ,1507.01461,https://arxiv.org/abs/1507.01461,https://arxiv.org/pdf/1507.01461,Authors:RaduCristianIonescu,"        Nowadays, with the widespread of smartphones and other portable gadgets equipped with a variety of sensors, data is ubiquitous available and the focus of machine learning has shifted from being able to infer from small training samples to dealing with large scale high-dimensional data. In domains such as personal healthcare applications, which motivates this survey, distributed machine learning is a promising line of research, both for scaling up learning algorithms, but mostly for dealing with data which is inherently produced at different locations. This report offers a thorough overview of and state-of-the-art algorithms for distributed machine learning, for both supervised and unsupervised learning, ranging from simple linear logistic regression to graphical models and clustering. We propose future directions for most categories, specific to the potential personal healthcare applications. With this in mind, the report focuses on how security and low communication overhead can be assured in the specific case of a strictly client-server architectural model. As particular directions we provides an exhaustive presentation of an empirical clustering algorithm, k-windows, and proposed an asynchronous distributed machine learning algorithm that would scale well and also would be computationally cheap and easy to implement.        △ Less","6 July, 2015","cs.DC,cs.LG",
              Business Models for e-Health: Evidence from Ten Case Studies          ,1507.00553,https://arxiv.org/abs/1507.00553,https://arxiv.org/pdf/1507.00553,Authors:ChrisKimble,"        An increasingly aging population and spiraling healthcare costs have made the search for financially viable healthcare models an imperative of this century. The careful and creative application of information technology can play a significant role in meeting that challenge. Valuable lessons can be learned from an analysis of ten innovative telemedicine and e-health initiatives. Having proven their effectiveness in addressing a variety of medical needs, they have progressed beyond small-scale implementations to become an established part of healthcare delivery systems around the world.        △ Less","2 July, 2015",cs.CY,10.1002/joe.21611 
              High quality monolayer graphene synthesized by resistive heating cold wall chemical vapour deposition          ,1506.08569,https://arxiv.org/abs/1506.08569,https://arxiv.org/pdf/1506.08569,"Authors:ThomasH.Bointon,MatthewD.Barnes,SaverioRusso,MonicaF.Craciun","        Emerging flexible and wearable technologies such as healthcare electronics and energy-harvest devices could be transformed by the unique properties of graphene. The vision for a graphene-driven industrial revolution is motivating intensive research on the synthesis of (1) high quality and (2) low cost graphene. Hot-wall chemical vapour deposition (CVD) is one of the most competitive growth methods, but its long processing times are incompatible with production lines. Here we demonstrate the growth of high quality monolayer graphene using a technique that is 100 times faster than standard hot-wall CVD, resulting in 99% reduction in production costs. A thorough complementary study of Raman spectroscopy, atomic force microscopy, scanning electron microscopy and electrical magneto-transport measurements shows that our cold wall CVD-grown graphene is of comparable quality to that of natural graphene. Finally, we demonstrate the first transparent and flexible graphene capacitive touch-sensor that could enable the development of artificial skin for robots.        △ Less","29 June, 2015",cond-mat.mtrl-sci,10.1002/adma.201501600 
              Gamification of Quantum Mechanics Teaching          ,1506.08128,https://arxiv.org/abs/1506.08128,https://arxiv.org/pdf/1506.08128,"Authors:OleEggersBjælde,MadsKockPedersen,JacobSherson","        In this small scale study we demonstrate how a gamified teaching setup can be used effectively to support student learning in a quantum mechanics course. The quantum mechanics games were research games, which were played during lectures and the learning was measured with a pretest/posttest method with promising results. The study works as a pilot study to guide the planning of quantum mechanics courses in the future at Aarhus University in Denmark.        △ Less","26 June, 2015","physics.ed-ph,quant-ph",
              wHealth - Transforming Telehealth Services          ,1506.05543,https://arxiv.org/abs/1506.05543,https://arxiv.org/pdf/1506.05543,"Authors:RajibRana,MargeeHume,JohnReilly,JeffreySoar","        A worldwide increase in proportions of older people in the population poses the challenge of managing their increasing healthcare needs within limited resources. To achieve this many countries are interested in adopting telehealth technology. Several shortcomings of state-of-the-art telehealth technology constrain widespread adoption of telehealth services. We present an ensemble-sensing framework - wHealth (short form of wireless health) for effective delivery of telehealth services. It extracts personal health information using sensors embedded in everyday devices and allows effective and seamless communication between patients and clinicians. Due to the non-stigmatizing design, ease of maintenance, simplistic interaction and seamless intervention, our wHealth platform has the potential to enable widespread adoption of telehealth services for managing elderly healthcare. We discuss the key barriers and potential solutions to make the wHealth platform a reality.        △ Less","17 June, 2015",cs.CY,
              Medical Synonym Extraction with Concept Space Models          ,1506.00528,https://arxiv.org/abs/1506.00528,https://arxiv.org/pdf/1506.00528,"Authors:ChangWang,LiangliangCao,BowenZhou","        In this paper, we present a novel approach for medical synonym extraction. We aim to integrate the term embedding with the medical domain knowledge for healthcare applications. One advantage of our method is that it is very scalable. Experiments on a dataset with more than 1M term pairs show that the proposed approach outperforms the baseline approaches by a large margin.        △ Less","1 June, 2015",cs.CL,
              Variable subset selection via GA and information complexity in mixtures of Poisson and negative binomial regression models          ,1505.05229,https://arxiv.org/abs/1505.05229,https://arxiv.org/pdf/1505.05229,"Authors:T.J.Massaro,H.Bozdogan","        Count data, for example the number of observed cases of a disease in a city, often arise in the fields of healthcare analytics and epidemiology. In this paper, we consider performing regression on multivariate data in which our outcome is a count. Specifically, we derive log-likelihood functions for finite mixtures of regression models involving counts that come from a Poisson distribution, as well as a negative binomial distribution when the counts are significantly overdispersed. Within our proposed modeling framework, we carry out optimal component selection using the information criteria scores AIC, BIC, CAIC, and ICOMP. We demonstrate applications of our approach on simulated data, as well as on a real data set of HIV cases in Tennessee counties from the year 2010. Finally, using a genetic algorithm within our framework, we perform variable subset selection to determine the covariates that are most responsible for categorizing Tennessee counties. This leads to some interesting insights into the traits of counties that have high HIV counts.        △ Less","19 May, 2015","stat.ML,stat.ME",
              Spatial database implementation of fuzzy region connection calculus for analysing the relationship of diseases          ,1505.04746,https://arxiv.org/abs/1505.04746,https://arxiv.org/pdf/1505.04746,"Authors:SomayeDavari,NasserGhadiri","        Analyzing huge amounts of spatial data plays an important role in many emerging analysis and decision-making domains such as healthcare, urban planning, agriculture and so on. For extracting meaningful knowledge from geographical data, the relationships between spatial data objects need to be analyzed. An important class of such relationships are topological relations like the connectedness or overlap between regions. While real-world geographical regions such as lakes or forests do not have exact boundaries and are fuzzy, most of the existing analysis methods neglect this inherent feature of topological relations. In this paper, we propose a method for handling the topological relations in spatial databases based on fuzzy region connection calculus (RCC). The proposed method is implemented in PostGIS spatial database and evaluated in analyzing the relationship of diseases as an important application domain. We also used our fuzzy RCC implementation for fuzzification of the skyline operator in spatial databases. The results of the evaluation show that our method provides a more realistic view of spatial relationships and gives more flexibility to the data analyst to extract meaningful and accurate results in comparison with the existing methods.        △ Less","10 May, 2016","cs.DB,cs.AI,cs.CG",10.1109/IranianCEE.2015.7146310 
              On the Control of Fork-Join Networks          ,1505.04470,https://arxiv.org/abs/1505.04470,https://arxiv.org/pdf/1505.04470,"Authors:ErhunÖzkan,AmyR.Ward","        Networks in which the processing of jobs occurs both sequentially and in parallel are prevalent in many application domains, such as computer systems, healthcare, manufacturing, and project management. The parallel processing of jobs gives rise to synchronization constraints that can be a main reason for job delay. In comparison with feedforward queueing networks that have only sequential processing of jobs, the approximation and control of networks that have synchronization constraints is less understood. One well-known modeling framework in which synchronization constraints are prominent is the fork-join processing network.  Our objective is to find scheduling rules for fork-join processing networks with multiple job types in which there is first a fork operation, then activities that can be performed in parallel, and then a join operation. The difficulty is that some of the activities that can be performed in parallel require a shared resource. We solve the scheduling problem for that shared server (that is, which type of job to prioritize each time the server becomes available) when that server is in heavy traffic and prove an asymptotic optimality result.        △ Less","13 June, 2016",math.PR,
"              How Resilient Are Our Societies? Analyses, Models, and Preliminary Results          ",1505.02759,https://arxiv.org/abs/1505.02759,https://arxiv.org/pdf/1505.02759,"Authors:VincenzoDeFlorio,ArianitPajaziti","        Traditional social organizations such as those for the management of healthcare and civil defence are the result of designs and realizations that matched well with an operational context considerably different from the one we are experiencing today: A simpler world, characterized by a greater amount of resources to match less users producing lower peaks of requests. The new context reveals all the fragility of our societies: unmanageability is just around the corner unless we do not complement the ""old recipes"" with smarter forms of social organization. Here we analyze this problem and propose a refinement to our fractal social organizations as a model for resilient cyber-physical societies. Evidence to our claims is provided by simulating our model in terms of multi-agent systems.        △ Less","8 May, 2015","cs.OH,cs.MA",
              Nonlinear Markov Processes in Big Networks          ,1504.07974,https://arxiv.org/abs/1504.07974,https://arxiv.org/pdf/1504.07974,Authors:Quan-LinLi,"        Big networks express various large-scale networks in many practical areas such as computer networks, internet of things, cloud computation, manufacturing systems, transportation networks, and healthcare systems. This paper analyzes such big networks, and applies the mean-field theory and the nonlinear Markov processes to set up a broad class of nonlinear continuous-time block-structured Markov processes, which can be applied to deal with many practical stochastic systems. Firstly, a nonlinear Markov process is derived from a large number of interacting big networks with symmetric interactions, each of which is described as a continuous-time block-structured Markov process. Secondly, some effective algorithms are given for computing the fixed points of the nonlinear Markov process by means of the UL-type RG-factorization. Finally, the Birkhoff center, the Lyapunov functions and the relative entropy are used to analyze stability or metastability of the big network, and several interesting open problems are proposed with detailed interpretation. We believe that the results given in this paper can be useful and effective in the study of big networks.        △ Less","5 April, 2016",cs.SY,
              A multi-class approach for ranking graph nodes: models and experiments with incomplete data          ,1504.07766,https://arxiv.org/abs/1504.07766,https://arxiv.org/pdf/1504.07766,"Authors:GiannaM.DelCorso,FrancescoRomani","        After the phenomenal success of the PageRank algorithm, many researchers have extended the PageRank approach to ranking graphs with richer structures beside the simple linkage structure. In some scenarios we have to deal with multi-parameters data where each node has additional features and there are relationships between such features.  This paper stems from the need of a systematic approach when dealing with multi-parameter data. We propose models and ranking algorithms which can be used with little adjustments for a large variety of networks (bibliographic data, patent data, twitter and social data, healthcare data). In this paper we focus on several aspects which have not been addressed in the literature: (1) we propose different models for ranking multi-parameters data and a class of numerical algorithms for efficiently computing the ranking score of such models, (2) by analyzing the stability and convergence properties of the numerical schemes we tune a fast and stable technique for the ranking problem, (3) we consider the issue of the robustness of our models when data are incomplete. The comparison of the rank on the incomplete data with the rank on the full structure shows that our models compute consistent rankings whose correlation is up to 60% when just 10% of the links of the attributes are maintained suggesting the suitability of our model also when the data are incomplete.        △ Less","29 April, 2015","math.NA,cs.IR,physics.soc-ph",10.1016/j.ins.2015.09.046 
              Gender Issues & Information Communication Technology for Development (ICT4D): Prospects and Challenges for Women in Nigeria          ,1504.04644,https://arxiv.org/abs/1504.04644,https://arxiv.org/pdf/1504.04644,"Authors:KwetisheJoroDanjuma,BayoMohammedOnimode,OchedikwuJonahOnche","        Information and Communication Technology is a compendium of interrelated applications, products and services that can either deepen or alter gender equality. It has successfully transformed education, businesses, healthcare, entertainment, politics and good governance within the Global North; providing equitable access to developmental framework driven by ICTs. However, in drafting the core developmental objectives of sustainable development, there has been considerable gender digital divide limiting women access to resources based on their gender, ethnicity, socio-cultural bias and the rights to utilize such resources for development. In realization of the United Nation Millennium Development Goals within the Global South specifically Nigeria, women are often marginalized or excluded from ICT policy drafting and imbalance associated with ICT. This paper identifies and evaluates gender issues and information communication technology, with focus on the challenges and prospects for women empowerment in Nigeria. The study critically examined research literatures and conducted research survey on the prospects and challenges of promoting gender equality and women empowerment through ICTs; and identifies policy implication for Nigeria. The research survey used a random sampling technique with a target sample size of eighty respondents. Data gathered from the questionnaire was analyzed using Statistical Package for Social Science version 19, and the result was presented using ANOVA, and descriptive analysis. The study reveal gender inclusiveness in policy drafting as a key driver for socio-economic development, improved healthcare and women empowerment in Nigeria. We recommend a deliberate ICT policy that attract and encourages women participation in ICT developmental framework.        △ Less","17 April, 2015",cs.CY,
              Preprint Clinical Feedback and Technology Selection of Game Based Dysphonic Rehabilitation Tool          ,1504.04309,https://arxiv.org/abs/1504.04309,https://arxiv.org/pdf/1504.04309,"Authors:ZhihanLv,ChantalEsteve,JavierChirivella,PabloGagliardo","        This is the preprint version of our paper on 2015 9th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth2015). An assistive training tool software for rehabilitation of dysphonic patients is evaluated according to the practical clinical feedback from the treatments. One stroke sufferer and one parkinson sufferer have provided earnest suggestions for the improvement of our tool software. The assistive tool employs a serious game as the attractive logic part, and running on the tablet with normal microphone as input device. Seven pitch estimation algorithms have been evaluated and compared with selected patients voice database. A series of benchmarks have been generated during the evaluation process for technology selection.        △ Less","29 July, 2015",cs.HC,
              On mining complex sequential data by means of FCA and pattern structures          ,1504.02255,https://arxiv.org/abs/1504.02255,https://arxiv.org/pdf/1504.02255,"Authors:AlekseyBuzmakov,EliasEgho,NicolasJay,SergeiO.Kuznetsov,AmedeoNapoli,ChedyRaïssi","        Nowadays data sets are available in very complex and heterogeneous ways. Mining of such data collections is essential to support many real-world applications ranging from healthcare to marketing. In this work, we focus on the analysis of ""complex"" sequential data by means of interesting sequential patterns. We approach the problem using the elegant mathematical framework of Formal Concept Analysis (FCA) and its extension based on ""pattern structures"". Pattern structures are used for mining complex data (such as sequences or graphs) and are based on a subsumption operation, which in our case is defined with respect to the partial order on sequences. We show how pattern structures along with projections (i.e., a data reduction of sequential structures), are able to enumerate more meaningful patterns and increase the computing efficiency of the approach. Finally, we show the applicability of the presented method for discovering and analyzing interesting patient patterns from a French healthcare data set on cancer. The quantitative and qualitative results (with annotations and analysis from a physician) are reported in this use case which is the main motivation for this work.  Keywords: data mining; formal concept analysis; pattern structures; projections; sequences; sequential data.        △ Less","9 April, 2015","cs.AI,cs.DB",
              Towards the fast and robust optimal design of Wireless Body Area Networks          ,1504.01356,https://arxiv.org/abs/1504.01356,https://arxiv.org/pdf/1504.01356,"Authors:FabioD'Andreagiovanni,AntonellaNardin","        Wireless body area networks are wireless sensor networks whose adoption has recently emerged and spread in important healthcare applications, such as the remote monitoring of health conditions of patients. A major issue associated with the deployment of such networks is represented by energy consumption: in general, the batteries of the sensors cannot be easily replaced and recharged, so containing the usage of energy by a rational design of the network and of the routing is crucial. Another issue is represented by traffic uncertainty: body sensors may produce data at a variable rate that is not exactly known in advance, for example because the generation of data is event-driven. Neglecting traffic uncertainty may lead to wrong design and routing decisions, which may compromise the functionality of the network and have very bad effects on the health of the patients. In order to address these issues, in this work we propose the first robust optimization model for jointly optimizing the topology and the routing in body area networks under traffic uncertainty. Since the problem may result challenging even for a state-of-the-art optimization solver, we propose an original optimization algorithm that exploits suitable linear relaxations to guide a randomized fixing of the variables, supported by an exact large variable neighborhood search. Experiments on realistic instances indicate that our algorithm performs better than a state-of-the-art solver, fast producing solutions associated with improved optimality gaps.        △ Less","24 June, 2015","math.OC,cs.NI",10.1016/j.asoc.2015.04.037 
              Fast Imbalanced Classification of Healthcare Data with Missing Values          ,1503.06250,https://arxiv.org/abs/1503.06250,https://arxiv.org/pdf/1503.06250,"Authors:TalayehRazzaghi,OlegRoderick,IlyaSafro,NickMarko","        In medical domain, data features often contain missing values. This can create serious bias in the predictive modeling. Typical standard data mining methods often produce poor performance measures. In this paper, we propose a new method to simultaneously classify large datasets and reduce the effects of missing values. The proposed method is based on a multilevel framework of the cost-sensitive SVM and the expected maximization imputation method for missing values, which relies on iterated regression analyses. We compare classification results of multilevel SVM-based algorithms on public benchmark datasets with imbalanced classes and missing values as well as real data in health applications, and show that our multilevel SVM-based method produces fast, and more accurate and robust classification results.        △ Less","20 March, 2015","stat.ML,cs.LG",
              Relationship-Based Access Control for OpenMRS          ,1503.06154,https://arxiv.org/abs/1503.06154,https://arxiv.org/pdf/1503.06154,"Authors:SyedZainRizvi,PhilipW.L.Fong,JasonCrampton,JamesSellwood","        Inspired by the access control models of social network systems, Relationship-Based Access Control (ReBAC) was recently proposed as a general-purpose access control paradigm for application domains in which authorization must take into account the relationship between the access requestor and the resource owner. The healthcare domain is envisioned to be an archetypical application domain in which ReBAC is sorely needed: e.g., my patient record should be accessible only by my family doctor, but not by all doctors.  In this work, we demonstrate for the first time that ReBAC can be incorporated into a production-scale medical records system, OpenMRS, with backward compatibility to the legacy RBAC mechanism. Specifically, we extend the access control mechanism of OpenMRS to enforce ReBAC policies. Our extensions incorporate and extend advanced ReBAC features recently proposed by Crampton and Sellwood. In addition, we designed and implemented the first administrative model for ReBAC. In this paper, we describe our ReBAC implementation, discuss the system engineering lessons learnt as a result, and evaluate the experimental work we have undertaken. In particular, we compare the performance of the various authorization schemes we implemented, thereby demonstrating the feasibility of ReBAC.        △ Less","20 March, 2015",cs.CR,
              Prediction Using Note Text: Synthetic Feature Creation with word2vec          ,1503.05123,https://arxiv.org/abs/1503.05123,https://arxiv.org/pdf/1503.05123,"Authors:ManuelAmunategui,TristanMarkwell,YelenaRozenfeld","        word2vec affords a simple yet powerful approach of extracting quantitative variables from unstructured textual data. Over half of healthcare data is unstructured and therefore hard to model without involved expertise in data engineering and natural language processing. word2vec can serve as a bridge to quickly gather intelligence from such data sources.  In this study, we ran 650 megabytes of unstructured, medical chart notes from the Providence Health & Services electronic medical record through word2vec. We used two different approaches in creating predictive variables and tested them on the risk of readmission for patients with COPD (Chronic Obstructive Lung Disease). As a comparative benchmark, we ran the same test using the LACE risk model (a single score based on length of stay, acuity, comorbid conditions, and emergency department visits).  Using only free text and mathematical might, we found word2vec comparable to LACE in predicting the risk of readmission of COPD patients.        △ Less","17 March, 2015",cs.CL,
              Poker Cash Game: a Thermodynamic Description          ,1503.01418,https://arxiv.org/abs/1503.01418,https://arxiv.org/pdf/1503.01418,Authors:MarcoAlbertoJavarone,"        Poker is one of the most popular card games, whose rational investigation represents also one of the major challenges in several scientific areas, spanning from information theory and artificial intelligence to game theory and statistical physics. In principle, several variants of Poker can be identified, although all of them make use of money to make the challenge meaningful and, moreover, can be played in two different formats: tournament and cash game. An important issue when dealing with Poker is its classification, i.e., as a `skill game' or as gambling. Nowadays, its classification still represents an open question, having a long list of implications (e.g., legal and healthcare) that vary from country to country. In this study, we analyze Poker challenges, considering the cash game format, in terms of thermodynamics systems. Notably, we propose a framework to represent a cash game Poker challenge that, although based on a simplified scenario, allows both to obtain useful information for rounders (i.e., Poker players), and to evaluate the role of Poker room in this context. Finally, starting from a model based on thermodynamics, we show the evolution of a Poker challenge, making a direct connection with the probability theory underlying its dynamics and finding that, even if we consider these games as `skill games', to take a real profit from Poker is really hard.        △ Less","4 March, 2015","physics.soc-ph,cond-mat.stat-mech",
              Development of an Android Application for an Electronic Medical Record System in an Outpatient Environment for Healthcare in Fiji          ,1503.00810,https://arxiv.org/abs/1503.00810,https://arxiv.org/pdf/1503.00810,"Authors:DarylAbel,BulouGavidi,NicholasRollings,RohitashChandra",        The outpatients department in a developing country is typically understaffed and inadequately equipped to handle a large numbers of patients filing through on an average day. The use of electronic medical record (EMR) systems can resolve some of the longstanding medical inefficiencies common in developing countries. This paper presents the design and implementation of a proposed outpatient management system that enables efficient management of a patient's medical details. We present a system to create appointments with medical practitioners by integrating a proposed Android-based mobile application with a selected open source EMR system. The application allows both the patient and the medical practitioners to manage appointments and make use of the electronic messaging facility to send reminders when the appointed time is approaching in real-time. A mobile application prototype is developed and the road map for implementation is also discussed.        △ Less,"2 March, 2015",cs.CY,
              Refining Adverse Drug Reactions using Association Rule Mining for Electronic Healthcare Data          ,1502.05943,https://arxiv.org/abs/1502.05943,https://arxiv.org/pdf/1502.05943,"Authors:JennaM.Reps,UweAickelin,JiangangMa,YanchunZhang","        Side effects of prescribed medications are a common occurrence. Electronic healthcare databases present the opportunity to identify new side effects efficiently but currently the methods are limited due to confounding (i.e. when an association between two variables is identified due to them both being associated to a third variable).  In this paper we propose a proof of concept method that learns common associations and uses this knowledge to automatically refine side effect signals (i.e. exposure-outcome associations) by removing instances of the exposure-outcome associations that are caused by confounding. This leaves the signal instances that are most likely to correspond to true side effect occurrences. We then calculate a novel measure termed the confounding-adjusted risk value, a more accurate absolute risk value of a patient experiencing the outcome within 60 days of the exposure.  Tentative results suggest that the method works. For the four signals (i.e. exposure-outcome associations) investigated we are able to correctly filter the majority of exposure-outcome instances that were unlikely to correspond to true side effects. The method is likely to improve when tuning the association rule mining parameters for specific health outcomes.  This paper shows that it may be possible to filter signals at a patient level based on association rules learned from considering patients' medical histories. However, additional work is required to develop a way to automate the tuning of the method's parameters.        △ Less","20 February, 2015","cs.DB,cs.CE,cs.LG",10.1109/ICDMW.2014.53 
              Incorporating Spontaneous Reporting System Data to Aid Causal Inference in Longitudinal Healthcare Data          ,1502.05938,https://arxiv.org/abs/1502.05938,https://arxiv.org/pdf/1502.05938,"Authors:JennaReps,UweAickelin","        Inferring causality using longitudinal observational databases is challenging due to the passive way the data are collected. The majority of associations found within longitudinal observational data are often non-causal and occur due to confounding.  The focus of this paper is to investigate incorporating information from additional databases to complement the longitudinal observational database analysis. We investigate the detection of prescription drug side effects as this is an example of a causal relationship. In previous work a framework was proposed for detecting side effects only using longitudinal data. In this paper we combine a measure of association derived from mining a spontaneous reporting system database to previously proposed analysis that extracts domain expertise features for causal analysis of a UK general practice longitudinal database.  The results show that there is a significant improvement to the performance of detecting prescription drug side effects when the longitudinal observation data analysis is complemented by incorporating additional drug safety sources into the framework. The area under the receiver operating characteristic curve (AUC) for correctly classifying a side effect when other data were considered was 0.967, whereas without it the AUC was 0.923 However, the results of this paper may be biased by the evaluation and future work should overcome this by developing an unbiased reference set.        △ Less","20 February, 2015",cs.CE,10.1109/ICDMW.2014.54 
              Diagnosis of diabetes using classification mining techniques          ,1502.03774,https://arxiv.org/abs/1502.03774,https://arxiv.org/pdf/1502.03774,"Authors:AiswaryaIyer,S.Jeyalatha,RonakSumbaly","        Diabetes has affected over 246 million people worldwide with a majority of them being women. According to the WHO report, by 2025 this number is expected to rise to over 380 million. The disease has been named the fifth deadliest disease in the United States with no imminent cure in sight. With the rise of information technology and its continued advent into the medical and healthcare sector, the cases of diabetes as well as their symptoms are well documented. This paper aims at finding solutions to diagnose the disease by analyzing the patterns found in the data through classification analysis by employing Decision Tree and Naïve Bayes algorithms. The research hopes to propose a quicker and more efficient technique of diagnosing the disease, leading to timely treatment of the patients.        △ Less","12 February, 2015",cs.CE,10.5121/ijdkp.2015.5101 
              The snowball effect of customer slowdown in critical many-server systems          ,1502.02856,https://arxiv.org/abs/1502.02856,https://arxiv.org/pdf/1502.02856,"Authors:JoriSelen,IvoAdan,VidyadharKulkarni,JohanvanLeeuwaarden","        Customer slowdown describes the phenomenon that a customer's service requirement increases with experienced delay. In healthcare settings, there is substantial empirical evidence for slowdown, particularly when a patient's delay exceeds a certain threshold. For such threshold slowdown situations, we design and analyze a many-server system that leads to a two-dimensional Markov process. Analysis of this system leads to insights into the potentially detrimental effects of slowdown, especially in heavy-traffic conditions. We quantify the consequences of underprovisioning due to neglecting slowdown, demonstrate the presence of a subtle bistable system behavior, and discuss in detail the snowball effect: A delayed customer has an increased service requirement, causing longer delays for other customers, who in turn due to slowdown might require longer service times.        △ Less","31 March, 2016",math.PR,10.1080/15326349.2015.1136221 
              Hierarchical Dirichlet process for tracking complex topical structure evolution and its application to autism research literature          ,1502.02233,https://arxiv.org/abs/1502.02233,https://arxiv.org/pdf/1502.02233,"Authors:AdhamBeykikhoshk,OgnjenArandjelovic,DinhPhung,SvethaVenkatesh","        In this paper we describe a novel framework for the discovery of the topical content of a data corpus, and the tracking of its complex structural changes across the temporal dimension. In contrast to previous work our model does not impose a prior on the rate at which documents are added to the corpus nor does it adopt the Markovian assumption which overly restricts the type of changes that the model can capture. Our key technical contribution is a framework based on (i) discretization of time into epochs, (ii) epoch-wise topic discovery using a hierarchical Dirichlet process-based model, and (iii) a temporal similarity graph which allows for the modelling of complex topic changes: emergence and disappearance, evolution, and splitting and merging. The power of the proposed framework is demonstrated on the medical literature corpus concerned with the autism spectrum disorder (ASD) - an increasingly important research subject of significant social and healthcare importance. In addition to the collected ASD literature corpus which we will make freely available, our contributions also include two free online tools we built as aids to ASD researchers. These can be used for semantically meaningful navigation and searching, as well as knowledge discovery from this large and rapidly growing corpus of literature.        △ Less","8 February, 2015","cs.IR,cs.CL",
              Facilitating Evolution during Design and Implementation          ,1502.01514,https://arxiv.org/abs/1502.01514,https://arxiv.org/pdf/1502.01514,Authors:R.McClatchey,"        The volumes and complexity of data that companies need to handle are increasing at an accelerating rate. In order to compete effectively and ensure their commercial sustainability, it is becoming crucial for them to achieve robust traceability in both their data and the evolving designs of their systems. This is addressed by the CRISTAL software which was originally developed at CERN by UWE, Bristol, for one of the particle detectors at the Large Hadron Collider, and has been subsequently transferred into the commercial world. Companies have been able to demonstrate increased agility, generate additional revenue, and improve the efficiency and cost-effectiveness with which they develop and implement systems in various areas, including business process management (BPM), healthcare and accounting applications. CRISTALs ability to manage data and its provenance at the terabyte scale, with full traceability over extended timescales, together with its description-driven approach, has provided the flexible adaptability required to future proof dynamically evolving software for these businesses.        △ Less","5 February, 2015",cs.SE,10.1007/s13218-014-0324-1 
              Using temporal abduction for biosignal interpretation: A case study on QRS detection          ,1502.01497,https://arxiv.org/abs/1502.01497,https://arxiv.org/pdf/1502.01497,"Authors:TomásTeijeiro,PauloFélix,JesúsPresedo","        In this work, we propose an abductive framework for biosignal interpretation, based on the concept of Temporal Abstraction Patterns. A temporal abstraction pattern defines an abstraction relation between an observation hypothesis and a set of observations constituting its evidence support. New observations are generated abductively from any subset of the evidence of a pattern, building an abstraction hierarchy of observations in which higher levels contain those observations with greater interpretative value of the physiological processes underlying a given signal. Non-monotonic reasoning techniques have been applied to this model in order to find the best interpretation of a set of initial observations, permitting even to correct these observations by removing, adding or modifying them in order to make them consistent with the available domain knowledge. Some preliminary experiments have been conducted to apply this framework to a well known and bounded problem: the QRS detection on ECG signals. The objective is not to provide a new better QRS detector, but to test the validity of an abductive paradigm. These experiments show that a knowledge base comprising just a few very simple rhythm abstraction patterns can enhance the results of a state of the art algorithm by significantly improving its detection F1-score, besides proving the ability of the abductive framework to correct both sensitivity and specificity failures.        △ Less","5 February, 2015",cs.AI,10.1109/ICHI.2014.52 
              Static Analysis of File-Processing Programs using File Format Specifications          ,1501.04730,https://arxiv.org/abs/1501.04730,https://arxiv.org/pdf/1501.04730,"Authors:RaveendraKumarMedicherla,RaghavanKomondoor,S.Narendran","        Programs that process data that reside in files are widely used in varied domains, such as banking, healthcare, and web-traffic analysis. Precise static analysis of these programs in the context of software verification and transformation tasks is a challenging problem. Our key insight is that static analysis of file-processing programs can be made more useful if knowledge of the input file formats of these programs is made available to the analysis. We propose a generic framework that is able to perform any given underlying abstract interpretation on the program, while restricting the attention of the analysis to program paths that are potentially feasible when the program's input conforms to the given file format specification. We describe an implementation of our approach, and present empirical results using real and realistic programs that show how our approach enables novel verification and transformation tasks, and also improves the precision of standard analysis problems.        △ Less","3 April, 2015",cs.PL,
              Editorial: Spatial accessibility of pediatric primary healthcare: Measurement and inference          ,1501.03887,https://arxiv.org/abs/1501.03887,https://arxiv.org/pdf/1501.03887,Authors:SusanM.Paddock,"        Improving access to health care has long been at the forefront of policy debates in the U.S. There are multiple determinants of healthcare utilization: predisposing characteristics that explain individuals' propensities to use healthcare; enabling characteristics that describe the resources individuals have to use healthcare; and perceived or actual need for healthcare [Aday and Andersen (1974)]. Nobles, Serban and Swann (2014) illustrate the complexity involved with developing an understanding of one determinant of healthcare utilization. They examine spatial accessibility - an enabling characteristic under the aforementioned framework - to primary care pediatricians in Georgia. The authors encounter challenges that arise in many public policy applications, namely, the limitations of the available data, the need to conduct analyses that reflect system constraints, model selection and uncertainty quantification. The Area Editors featured this paper, along with contributions from three discussants, in an AoAS invited session at the 2014 Joint Statistical Meetings and are including the paper and those discussions in this issue because the paper showcases the type of research AoAS aims to publish: analyses requiring innovative statistical thinking to address questions of practical importance.        △ Less","16 January, 2015",stat.AP,10.1214/14-AOAS728ED 
"              Rejoinder: ""Spatial accessibility of pediatric primary healthcare: Measurement and inference""          ",1501.03886,https://arxiv.org/abs/1501.03886,https://arxiv.org/pdf/1501.03886,"Authors:MalloryNobles,NicoletaSerban,JulieSwann","        Rejoinder of ""Spatial accessibility of pediatric primary healthcare: Measurement and inference"" by Mallory Nobles, Nicoleta Serban and Julie Swann [arXiv:1501.03626].        △ Less","16 January, 2015",stat.AP,10.1214/14-AOAS728R 
"              Discussion of ""Spatial accessibility of pediatric primary healthcare: Measurement and inference""          ",1501.03885,https://arxiv.org/abs/1501.03885,https://arxiv.org/pdf/1501.03885,Authors:LanceA.Waller,"        Discussion of ""Spatial accessibility of pediatric primary healthcare: Measurement and inference"" by Mallory Nobles, Nicoleta Serban and Julie Swann [arXiv:1501.03626].        △ Less","16 January, 2015",stat.AP,10.1214/14-AOAS728C 
"              Discussion of ""Spatial accessibility of pediatric primary healthcare: Measurement and inference""          ",1501.03884,https://arxiv.org/abs/1501.03884,https://arxiv.org/pdf/1501.03884,Authors:AmeliaM.Haviland,"        Discussion of ""Spatial accessibility of pediatric primary healthcare: Measurement and inference"" by Mallory Nobles, Nicoleta Serban and Julie Swann [arXiv:1501.03626].        △ Less","16 January, 2015",stat.AP,10.1214/14-AOAS728B 
"              Discussion of ""Spatial accessibility of pediatric primary healthcare: Measurement and inference""          ",1501.03883,https://arxiv.org/abs/1501.03883,https://arxiv.org/pdf/1501.03883,Authors:LauraA.Hatfield,"        Discussion of ""Spatial accessibility of pediatric primary healthcare: Measurement and inference"" by Mallory Nobles, Nicoleta Serban and Julie Swann [arXiv:1501.03626].        △ Less","16 January, 2015",stat.AP,10.1214/14-AOAS728A 
              Computer-assisted polyp matching between optical colonoscopy and CT colonography: a phantom study          ,1501.03779,https://arxiv.org/abs/1501.03779,https://arxiv.org/pdf/1501.03779,"Authors:HolgerR.Roth,ThomasE.Hampshire,EmmaHelbren,MingxingHu,RoserVega,SteveHalligan,DavidJ.Hawkes","        Potentially precancerous polyps detected with CT colonography (CTC) need to be removed subsequently, using an optical colonoscope (OC). Due to large colonic deformations induced by the colonoscope, even very experienced colonoscopists find it difficult to pinpoint the exact location of the colonoscope tip in relation to polyps reported on CTC. This can cause unduly prolonged OC examinations that are stressful for the patient, colonoscopist and supporting staff.  We developed a method, based on monocular 3D reconstruction from OC images, that automatically matches polyps observed in OC with polyps reported on prior CTC. A matching cost is computed, using rigid point-based registration between surface point clouds extracted from both modalities. A 3D printed and painted phantom of a 25 cm long transverse colon segment was used to validate the method on two medium sized polyps. Results indicate that the matching cost is smaller at the correct corresponding polyp between OC and CTC: the value is 3.9 times higher at the incorrect polyp, comparing the correct match between polyps to the incorrect match. Furthermore, we evaluate the matching of the reconstructed polyp from OC with other colonic endoluminal surface structures such as haustral folds and show that there is a minimum at the correct polyp from CTC.  Automated matching between polyps observed at OC and prior CTC would facilitate the biopsy or removal of true-positive pathology or exclusion of false-positive CTC findings, and would reduce colonoscopy false-negative (missed) polyps. Ultimately, such a method might reduce healthcare costs, patient inconvenience and discomfort.        △ Less","15 January, 2015",cs.CV,10.1117/12.2042860 
              Spatial accessibility of pediatric primary healthcare: Measurement and inference          ,1501.03626,https://arxiv.org/abs/1501.03626,https://arxiv.org/pdf/1501.03626,"Authors:MalloryNobles,NicoletaSerban,JulieSwann","        Although improving financial access is in the spotlight of the current U.S. health policy agenda, this alone does not address universal and comprehensive healthcare. Affordability is one barrier to healthcare, but others such as availability and accessibility, together defined as spatial accessibility, are equally important. In this paper, we develop a measurement and modeling framework that can be used to infer the impact of policy changes on disparities in spatial accessibility within and across different population groups. The underlying model for measuring spatial accessibility is optimization-based and accounts for constraints in the healthcare delivery system. The measurement method is complemented by statistical modeling and inference on the impact of various potential contributing factors to disparities in spatial accessibility. The emphasis of this study is on children's accessibility to primary care pediatricians, piloted for the state of Georgia. We focus on disparities in accessibility between and within two populations: children insured by Medicaid and other children. We find that disparities in spatial accessibility to pediatric primary care in Georgia are significant, and resistant to many policy interventions, suggesting the need for major changes to the structure of Georgia's pediatric healthcare provider network.        △ Less","15 January, 2015",stat.AP,10.1214/14-AOAS728 
              A Pilot Study on Coupling CT and MRI through Use of Semiconductor Nanoparticles          ,1412.7554,https://arxiv.org/abs/1412.7554,https://arxiv.org/pdf/1412.7554,"Authors:MatthewGetzin,LarsGjesteby,Yen-JunChuang,ScottMcCallum,WenxiangCong,ChaoWang,ZhengweiPan,GuohaoDai,GeWang","        CT and MRI are the two most widely used imaging modalities in healthcare, each with its own merits and drawbacks. Combining these techniques in one machine could provide unprecedented resolution and sensitivity in a single scan, and serve as an ideal platform to explore physical coupling of x-ray excitation and magnetic resonance. Molecular probes such as functionalized nanophosphors present an opportunity to demonstrate a synergy between these modalities. However, a simultaneous CT-MRI scanner does not exist at this moment. As a pilot study, here we propose a mechanism in which water solutions containing LiGa5O8:Cr3+ nanophosphors can be excited with x-rays to store energy, and these excited particles may subsequently influence the T2 relaxation times of the solutions so that a difference in T2 can be measured by MRI before and after x-ray excitation. The trends seen in our study suggest that a measurable effect may exist from x-ray excitation of the nanophosphors. However, there are several experimental conditions that hinder the clarity of the results to be statistically significant up to a commonly accepted level (p=0.05), including insoluble nanoparticles and inter-scan variability. Nevertheless, the initial results from our experiments seem a consistent and inspiring story that x-rays modify MRI T2 values around nanophosphors. Upon availability of soluble nanophosphors, we will repeat our experiments to confirm these observations.        △ Less","23 December, 2014",physics.med-ph,
              A systematic literature review of cloud computing in eHealth          ,1412.2494,https://arxiv.org/abs/1412.2494,https://arxiv.org/pdf/1412.2494,"Authors:YanHu,GuohuaBai","        Cloud computing in eHealth is an emerging area for only few years. There needs to identify the state of the art and pinpoint challenges and possible directions for researchers and applications developers. Based on this need, we have conducted a systematic review of cloud computing in eHealth. We searched ACM Digital Library, IEEE Xplore, Inspec, ISI Web of Science and Springer as well as relevant open-access journals for relevant articles. A total of 237 studies were first searched, of which 44 papers met the Include Criteria. The studies identified three types of studied areas about cloud computing in eHealth, namely (1) cloud-based eHealth framework design (n=13); (2) applications of cloud computing (n=17); and (3) security or privacy control mechanisms of healthcare data in the cloud (n=14). Most of the studies in the review were about designs and concept-proof. Only very few studies have evaluated their research in the real world, which may indicate that the application of cloud computing in eHealth is still very immature. However, our presented review could pinpoint that a hybrid cloud platform with mixed access control and security protection mechanisms will be a main research area for developing citizen centred home-based healthcare applications.        △ Less","8 December, 2014",cs.CY,
              Fuzzy human motion analysis: A review          ,1412.0439,https://arxiv.org/abs/1412.0439,https://arxiv.org/pdf/1412.0439,"Authors:ChernHongLim,EktaVats,CheeSengChan","        Human Motion Analysis (HMA) is currently one of the most popularly active research domains as such significant research interests are motivated by a number of real world applications such as video surveillance, sports analysis, healthcare monitoring and so on. However, most of these real world applications face high levels of uncertainties that can affect the operations of such applications. Hence, the fuzzy set theory has been applied and showed great success in the recent past. In this paper, we aim at reviewing the fuzzy set oriented approaches for HMA, individuating how the fuzzy set may improve the HMA, envisaging and delineating the future perspectives. To the best of our knowledge, there is not found a single survey in the current literature that has discussed and reviewed fuzzy approaches towards the HMA. For ease of understanding, we conceptually classify the human motion into three broad levels: Low-Level (LoL), Mid-Level (MiL), and High-Level (HiL) HMA.        △ Less","2 December, 2014","cs.CV,cs.AI",10.1016/j.patcog.2014.11.016 
              Falling Rule Lists          ,1411.5899,https://arxiv.org/abs/1411.5899,https://arxiv.org/pdf/1411.5899,"Authors:FultonWang,CynthiaRudin","        Falling rule lists are classification models consisting of an ordered list of if-then rules, where (i) the order of rules determines which example should be classified by each rule, and (ii) the estimated probability of success decreases monotonically down the list. These kinds of rule lists are inspired by healthcare applications where patients would be stratified into risk sets and the highest at-risk patients should be considered first. We provide a Bayesian framework for learning falling rule lists that does not rely on traditional greedy decision tree learning methods.        △ Less","1 February, 2015","cs.AI,cs.LG",
              Pervasive Healthcare-A Comprehensive Survey of Tools and Techniques          ,1411.1821,https://arxiv.org/abs/1411.1821,https://arxiv.org/pdf/1411.1821,"Authors:DeepakUniyal,VaskarRaychoudhury","        Pervasive healthcare is an emerging technology that aims to provide round-the-clock monitoring of several vital signs of patients using various health sensors, specialized communication protocols, and intelligent context-aware applications. Pervasive healthcare applications proactively contact the caregiver provided any abnormality arises in the health condition of a monitored patient. It has been a boon to the patients suffering from different diseases and requiring continuous monitoring and care, such as, disabled individuals, elderly and weak persons living alone, children of different ages, and adults who are susceptible to near-fatal falls or sudden increases in blood pressure, heart rates, stress level, etc. Existing surveys on pervasive healthcare cover generic techniques or a particular application, like fall detection. In this paper, we carry out a comprehensive coverage of several common disorders addressed by pervasive healthcare in recent years. We roughly classify different diseases by age groups of patients and then discuss various hardware and software tools and techniques to detect or treat them. We have also included detailed tabular classification of a large selection of significant research articles in pervasive healthcare.        △ Less","6 November, 2014",cs.CY,
              Why are public health authorities not concerned about Ebola in the US? Part I. Fat tailed distributions          ,1411.1472,https://arxiv.org/abs/1411.1472,https://arxiv.org/pdf/1411.1472,Authors:YaneerBar-Yam,"        US public health authorities claim imposing quarantines on healthcare workers returning from West Africa is incorrect according to science. Their positions rely upon a set of studies and experience about outbreaks and transmission mechanisms in Africa as well as assumptions about what those studies imply about outbreaks in the US. According to this view the probability of a single infection is low and that of a major outbreak is non-existent. In a series of brief reports we will provide insight into why properties of networks of contagion that are not considered in traditional statistics suggest that risks are higher than those assumptions suggest. We begin with the difference between thin and fat tailed distributions applied to the number of infected individuals that can arise from a single one. Traditional epidemiological models consider the contagion process as described by R0R_0, the average number of new infected individuals arising from a single case. However, in a complex interdependent society it is possible for the actual number due to a single individual to dramatically differ from the average number, with severe consequences for the ability to contain an outbreak when it is just beginning. Our analysis raises doubts about the scientific validity of policy recommendations of public health authorities. We also point out that existing CDC public health policies and actions are inconsistent with their claims.        △ Less","5 November, 2014","physics.soc-ph,physics.data-an,physics.med-ph,q-bio.PE,stat.AP",
              The Potential Impact of Increased Hospital Capacity to Contain and Control Ebola in Liberia          ,1410.8207,https://arxiv.org/abs/1410.8207,https://arxiv.org/pdf/1410.8207,"Authors:EricT.Lofgren,CaitlinM.Rivers,MadhavV.Marathe,StephenG.Eubank,BryanL.Lewis","        West Africa is currently experiencing a severe outbreak of Ebola virus disease (EVD). As part of the international effort to address this outbreak, the United States has committed to building specialized Ebola treatment facilities with 1700 beds. However, the effectiveness of this increase in the available healthcare facilities to treat Ebola is unclear, especially in light of the rapidly increasing number of cases. Adapting a previously validated mathematical model of Ebola in West Africa, we examine the potential impact of an increase in hospital capacity to mitigate the impact of Ebola under several scenarios, ranging from the planned scenario of 1700 beds in 10 weeks to a considerably more aggressive approach of twice the number of beds in 5 weeks. We find that even for the most aggressive scenarios, while increasing the availability of healthcare reduces the number of Ebola cases and slows the outbreak, it is not sufficient to stop the epidemic within the next three months. We find that only a combination of increased hospital beds and a dramatic decrease in the rate of transmission within the community can bring the epidemic under control within the near future.        △ Less","29 October, 2014",q-bio.PE,
              Health Information Search Behavior on the Web: A Pilot Study          ,1410.8068,https://arxiv.org/abs/1410.8068,https://arxiv.org/pdf/1410.8068,"Authors:ShanuSushmita,Si-ChiChin","        Searching health information on web has become an integral part of today's world, and many people turn to the Web for healthcare information and healthcare assessment. Our pilot study investigates users' preferences for the type of search results (image, news, video, etc.), and investigates users' ability to accurately interpret online health information for the purpose of self diagnosis. The preliminary results reveal that blog and news articles are most sought by users when searching online information and there exist challenges in the use of online health information for self-diagnosis.        △ Less","27 October, 2014",cs.IR,
              Fully Automated Myocardial Infarction Classification using Ordinary Differential Equations          ,1410.6984,https://arxiv.org/abs/1410.6984,https://arxiv.org/pdf/1410.6984,"Authors:GetieZewdie,MomiaoXiong","        Portable, Wearable and Wireless electrocardiogram (ECG) Systems have the potential to be used as point-of-care for cardiovascular disease diagnostic systems. Such wearable and wireless ECG systems require automatic detection of cardiovascular disease. Even in the primary care, automation of ECG diagnostic systems will improve efficiency of ECG diagnosis and reduce the minimal training requirement of local healthcare workers. However, few fully automatic myocardial infarction (MI) disease detection algorithms have well been developed. This paper presents a novel automatic MI classification algorithm using second order ordinary differential equation (ODE) with time varying coefficients, which simultaneously captures morphological and dynamic feature of highly correlated ECG signals. By effectively estimating the unobserved state variables and the parameters of the second order ODE, the accuracy of the classification was significantly improved. The estimated time varying coefficients of the second order ODE were used as an input to the support vector machine (SVM) for the MI classification. The proposed method was applied to the PTB diagnostic ECG database within Physionet. The overall sensitivity, specificity, and classification accuracy of 12 lead ECGs for MI binary classifications were 98.7%, 96.4% and 98.3%, respectively. We also found that even using one lead ECG signals, we can reach accuracy as high as 97%. Multiclass MI classification is a challenging task but the developed ODE approach for 12 lead ECGs coupled with multiclass SVM reached 96.4% accuracy for classifying 5 subgroups of MI and healthy controls.        △ Less","26 October, 2014",stat.ML,
              OHMF: A Query Based Optimal Healthcare Medication Framework          ,1410.5815,https://arxiv.org/abs/1410.5815,https://arxiv.org/pdf/1410.5815,"Authors:SantoshKumarMajhi,PadmalochanBera","        Today cloud computing infrastructure is largely being deployed in healthcare to access various healthcare services easily over the Internet on an as needed basis. The main advantage of healthcare cloud is that it can be used as a tool for patients, medical professionals and insurance providers, to query and coordinate among medical departments, organizations and other healthcare related hubs. Although healthcare cloud services can enable better medication process with high responsiveness, but the privacy and other requirements of the patients need to be ensured in the process. Patients medical data may be required by the medical professionals, hospitals, diagnostic centers for analysis and diagnosis. However, data privacy and service quality cannot be compromised. In other words, there may exist various service providers corresponding to a specific healthcare service. The main challenge is to find the appropriate providers that comply best with patients requirement. In this paper, we propose a query based optimal medication framework to support the patients healthcare service accessibility comprehensively with considerable response time. The framework accepts related healthcare queries in natural language through a comprehensive user-interface and then processes the input query through a first order logic based evaluation engine and finds all possible services satisfying the requirements. First order logic is used for modeling of user requirements and queries. The query evaluation engine is built using zChaff, a Boolean logic satisfiability solver. The efficacy and usability of the framework is evaluated with initial case studies on synthetic and real life healthcare cloud.        △ Less","21 October, 2014","cs.CY,cs.IR,cs.MA,cs.NI",
              Ensemble Sensing on Smart Werables for a better Telehealth System          ,1409.6415,https://arxiv.org/abs/1409.6415,/search/?searchtype=author&query=Rana%2C+R,"Authors:RajibRana,MargeeHume","        Telehealth offers interesting avenues for improving healthcare access in vulnerable populations through use of electronic devices in the patient's home that monitor and assess for early complications. However, complication of operation and poor reliability hinders the wide acceptability of telehealth services. We propose ensemble sensing on everyday wearable devices, which does not impose the burden of carrying wearable sensors, yet offers a seamless and simple platform to deliver telehealth services.        △ Less","29 September, 2014",cs.CY,
              Comparison of algorithms that detect drug side effects using electronic healthcare databases          ,1409.0748,https://arxiv.org/abs/1409.0748,https://arxiv.org/pdf/1409.0748,"Authors:JennaReps,JonathanM.Garibaldi,UweAickelin,DanieleSoria,JackGibson,RichardHubbard","        The electronic healthcare databases are starting to become more readily available and are thought to have excellent potential for generating adverse drug reaction signals. The Health Improvement Network (THIN) database is an electronic healthcare database containing medical information on over 11 million patients that has excellent potential for detecting ADRs. In this paper we apply four existing electronic healthcare database signal detecting algorithms (MUTARA, HUNT, Temporal Pattern Discovery and modified ROR) on the THIN database for a selection of drugs from six chosen drug families. This is the first comparison of ADR signalling algorithms that includes MUTARA and HUNT and enabled us to set a benchmark for the adverse drug reaction signalling ability of the THIN database. The drugs were selectively chosen to enable a comparison with previous work and for variety. It was found that no algorithm was generally superior and the algorithms' natural thresholds act at variable stringencies. Furthermore, none of the algorithms perform well at detecting rare ADRs.        △ Less","2 September, 2014","cs.LG,cs.CE",
              Sensors for healthcare: Would you want them in your home?          ,1408.2476,https://arxiv.org/abs/1408.2476,https://arxiv.org/pdf/1408.2476,"Authors:AlisonBurrows,RachelGooberman-Hill,IanCraddock,DavidCoyle","        This paper describes some of the challenges set within SPHERE, a large-scale Interdisciplinary Research Collaboration that aims to develop sensor systems to monitor people's health and wellbeing in the home. In particular we discuss the dual task facing the User- Centered Design research group, to ensure the development of inclusive and desirable domestic healthcare technology. On the one hand, we seek to gain a rich understanding of the many envisaged users of the SPHERE system. On the other hand, for the user experience requirements to be translated into tangible outputs, it is crucial that we effectively communicate these findings to the broader team of SPHERE engineers and computer scientists.        △ Less","11 August, 2014","cs.HC,cs.CY",
              Wireless Sensor Networks Attacks and Solutions          ,1407.6290,https://arxiv.org/abs/1407.6290,https://arxiv.org/pdf/1407.6290,Authors:NaserAlajmi,"        A few years ago, wireless sensor networks (WSNs) used by only military. Now, we have seen many of organizations use WSNs for some purposes such as weather, pollution, traffic control, and healthcare. Security is becoming on these days a major concern for wireless sensor network. In this paper I focus on the security types of attacks and their detection. This paper anatomizes the security requirements and security attacks in wireless sensor networks. Also, indicate to the benchmarks for the security in WSNs.        △ Less","23 July, 2014",cs.CR,
              Stabilizing Sparse Cox Model using Clinical Structures in Electronic Medical Records          ,1407.6094,https://arxiv.org/abs/1407.6094,https://arxiv.org/pdf/1407.6094,"Authors:ShivapratapGopakumar,TruyenTran,DinhPhung,SvethaVenkatesh","        Stability in clinical prediction models is crucial for transferability between studies, yet has received little attention. The problem is paramount in high dimensional data which invites sparse models with feature selection capability. We introduce an effective method to stabilize sparse Cox model of time-to-events using clinical structures inherent in Electronic Medical Records. Model estimation is stabilized using a feature graph derived from two types of EMR structures: temporal structure of disease and intervention recurrences, and hierarchical structure of medical knowledge and practices. We demonstrate the efficacy of the method in predicting time-to-readmission of heart failure patients. On two stability measures - the Jaccard index and the Consistency index - the use of clinical structures significantly increased feature stability without hurting discriminative power. Our model reported a competitive AUC of 0.64 (95% CIs: [0.58,0.69]) for 6 months prediction.        △ Less","22 July, 2014","stat.ML,cs.LG",
              Improving energy efficiency in MANET's for healthcare environments          ,1407.2747,https://arxiv.org/abs/1407.2747,https://arxiv.org/pdf/1407.2747,"Authors:SohailAbid,ImranShafi,ShahidAbid","        Now a day ad hoc mobile networks (MANETs) have lots of routing protocols, but no one can meet maximum performance. Some are good in a small network; some are suitable in large networks, and some give better performance in location or global networks. Today modern and innovative applications for health care environments based on a wireless network are being developed in the commercial sectors. The emerging wireless networks are rapidly becoming a fundamental part of every single field of life. Our proposed DEERP framework gives a better performance as compared to other routing protocol.        △ Less","10 July, 2014",cs.NI,
"              A Coordinated MDP Approach to Multi-Agent Planning for Resource Allocation, with Applications to Healthcare",1407.1584,https://arxiv.org/abs/1407.1584,https://arxiv.org/pdf/1407.1584,"Authors:HadiHosseini,JesseHoey,RobinCohen","        This paper considers a novel approach to scalable multiagent resource allocation in dynamic settings. We propose an approximate solution in which each resource consumer is represented by an independent MDP-based agent that models expected utility using an average model of its expected access to resources given only limited information about all other agents. A global auction-based mechanism is proposed for allocations based on expected regret. We assume truthful bidding and a cooperative coordination mechanism, as we are considering healthcare scenarios. We illustrate the performance of our coordinated MDP approach against a Monte-Carlo based planning algorithm intended for large-scale applications, as well as other approaches suitable for allocating medical resources. The evaluations show that the global utility value across all consumer agents is closer to optimal when using our algorithms under certain time constraints, with low computational cost. As such, we offer a promising approach for addressing complex resource allocation problems that arise in healthcare settings.        △ Less","7 July, 2014","cs.AI,cs.MA",
"              Security Requirements, Counterattacks and Projects in Healthcare Applications Using WSNs - A Review          ",1406.1795,https://arxiv.org/abs/1406.1795,https://arxiv.org/pdf/1406.1795,"Authors:NusratFatema,RemusBrad","Healthcare applications are well thought-out as interesting fields for WSN where patients can be examine using wireless medical sensor networks. Inside the hospital or extensive care surroundings there is a tempting need for steady monitoring of essential body functions and support for patient mobility. Recent research cantered on patient reliable communication, mobility, and energy-efficient routing. Yet deploying new expertise in healthcare applications presents some understandable security concerns which are the important concern in the inclusive deployment of wireless patient monitoring systems. This manuscript presents a survey of the security features, its counter attacks in healthcare applications including some proposed projects which have been done recently.        △ Less","6 June, 2014","cs.CR,cs.CY",
              Direct determination of radiation dose in human blood          ,1405.5386,https://arxiv.org/abs/1405.5386,https://arxiv.org/pdf/1405.5386,"Authors:AyseGunesTanir,OzgeGulec,ErenSahiner,MustafaHicabiBolukdemir,KemalKoc,NiyaziMeric,SuleKayaKelec","        Our purpose is to measure the internal radiation dose (ID) using human blood sample. In the literature, there is no process that allows the direct measurement of ID received by a person. This study has shown that it is possible to determine ID in human blood exposed to internal or external ionizing radiation treatment both directly and retrospectively. OSL technique was used to measure the total dose from the blood sample. OSL counts from the waste blood of the patient injected with a radiopharmaceutical for diagnostic or treatment purposes and from a blood sample having a laboratory-injected radiation dose were both used for measurements. The decay and dose-response curves (DRC) were plotted for different doses. The doses received by different blood aliquots have been determined by interpolating the natural luminescence counts to DRC. In addition, OSL counts from a healthy blood sample exposed to an external radiation source were measured. The blood aliquots were given different 0-200Gy beta doses and their decay and dose-response curves were plotted. The internal dose received by the blood aliquots injected with radioisotope was determined by interpolating the natural luminescence counts to DRC. The internal dose values were found as 0.46Gy and 0.51Gy for different dose range. The blood aliquots were exposed to different external laboratory doses. The internal dose values corresponding to 10Gy laboratory dose from the aliquots exposed to external radiation were found as 10.94Gy for Disc3 and ~10.79Gy for Disc1.This study shows that the dose received by a person can be measured directly, simply and retrospectively by using only a very small amount of blood sample. The results will have important ramifications for the medicine and healthcare fields in particular.        △ Less","21 May, 2014",physics.med-ph,
              Electronic Health Records: Cure-all or Chronic Condition?          ,1405.2088,https://arxiv.org/abs/1405.2088,https://arxiv.org/pdf/1405.2088,Authors:ChrisKimble,"        Computer-based information systems feature in almost every aspect of our lives, and yet most of us receive handwritten prescriptions when we visit our doctors and rely on paper-based medical records in our healthcare. Although electronic health record (EHR) systems have long been promoted as a cost-effective and efficient alternative to this situation, clear-cut evidence of their success has not been forthcoming. An examination of some of the underlying problems that prevent EHR systems from delivering the benefits that their proponents tout identifies four broad objectives - reducing cost, reducing errors, improving coordination and improving adherence to standards - and shows that they are not always met. The three possible causes for this failure to deliver involve problems with the codification of knowledge, group and tacit knowledge, and coordination and communication. There is, however, reason to be optimistic that EHR systems can fulfil a healthy part, if not all, of their potential.        △ Less","29 April, 2014",cs.CY,10.1002/joe.21554 
"              A Naive Bayes machine learning approach to risk prediction using censored, time-to-event data          ",1404.2124,https://arxiv.org/abs/1404.2124,https://arxiv.org/pdf/1404.2124,"Authors:JulianWolfson,SunayanBandyopadhyay,MohamedElidrisi,GabrielaVazquez-Benitez,DonaldMusgrove,GediminasAdomavicius,PaulJohnson,PatrickO'Connor","        Predicting an individual's risk of experiencing a future clinical outcome is a statistical task with important consequences for both practicing clinicians and public health experts. Modern observational databases such as electronic health records (EHRs) provide an alternative to the longitudinal cohort studies traditionally used to construct risk models, bringing with them both opportunities and challenges. Large sample sizes and detailed covariate histories enable the use of sophisticated machine learning techniques to uncover complex associations and interactions, but observational databases are often ``messy,'' with high levels of missing data and incomplete patient follow-up. In this paper, we propose an adaptation of the well-known Naive Bayes (NB) machine learning approach for classification to time-to-event outcomes subject to censoring. We compare the predictive performance of our method to the Cox proportional hazards model which is commonly used for risk prediction in healthcare populations, and illustrate its application to prediction of cardiovascular risk using an EHR dataset from a large Midwest integrated healthcare system.        △ Less","8 April, 2014",stat.ML,
              Development of Wearable Systems for Ubiquitous Healthcare Service Provisioning          ,1404.0158,https://arxiv.org/abs/1404.0158,https://arxiv.org/pdf/1404.0158,"Authors:O.O.Ogunduyile,O.O.Olugbara,M.Lall","        This paper reports on the development of a wearable system using wireless biomedical sensors for ubiquitous healthcare service provisioning. The prototype system is developed to address current healthcare challenges such as increasing cost of services, inability to access diverse services, low quality services and increasing population of elderly as experienced globally. The biomedical sensors proactively collect physiological data of remote patients to recommend diagnostic services. The prototype system is designed to monitor oxygen saturation level (SpO2), Heart Rate (HR), activity and location of the elderly. Physiological data collected are uploaded to a Health Server (HS) via GPRS/Internet for analysis.        △ Less","1 April, 2014",cs.OH,
              Task & Resource Self-adaptive Embedded Real-time Operating System Microkernel for Wireless Sensor Nodes          ,1403.5010,https://arxiv.org/abs/1403.5010,https://arxiv.org/pdf/1403.5010,"Authors:KexingXing,DechengZuo,HaiyingZhou,HouKun-Mean","        Wireless Sensor Networks (WSNs) are used in many application fields, such as military, healthcare, environment surveillance, etc. The WSN OS based on event-driven model doesn't support real-time and multi-task application types and the OSs based on thread-driven model consume much energy because of frequent context switch. Due to the high-dense and large-scale deployment of sensor nodes, it is very difficult to collect sensor nodes to update their software. Furthermore, the sensor nodes are vulnerable to security attacks because of the characteristics of broadcast communication and unattended application. This paper presents a task and resource self-adaptive embedded real-time microkernel, which proposes hybrid programming model and offers a two-level scheduling strategy to support real-time multi-task correspondingly. A communication scheme, which takes the ""tuple"" space and ""IN/OUT"" primitives from ""LINDA"", is proposed to support some collaborative and distributed tasks. In addition, this kernel implements a run-time over-the-air updating mechanism and provides a security policy to avoid the attacks and ensure the reliable operation of nodes. The performance evaluation is proposed and the experiential results show this kernel is task-oriented and resource-aware and can be used for the applications of event-driven and real-time multi-task.        △ Less","19 March, 2014",cs.OS,
              Cancer Prognosis Prediction Using Balanced Stratified Sampling          ,1403.2950,https://arxiv.org/abs/1403.2950,https://arxiv.org/pdf/1403.2950,"Authors:JSSaleema,NBhagawathi,SMonica,PDeepaShenoy,KRVenugopal,LMPatnaik","        High accuracy in cancer prediction is important to improve the quality of the treatment and to improve the rate of survivability of patients. As the data volume is increasing rapidly in the healthcare research, the analytical challenge exists in double. The use of effective sampling technique in classification algorithms always yields good prediction accuracy. The SEER public use cancer database provides various prominent class labels for prognosis prediction. The main objective of this paper is to find the effect of sampling techniques in classifying the prognosis variable and propose an ideal sampling method based on the outcome of the experimentation. In the first phase of this work the traditional random sampling and stratified sampling techniques have been used. At the next level the balanced stratified sampling with variations as per the choice of the prognosis class labels have been tested. Much of the initial time has been focused on performing the pre_processing of the SEER data set. The classification model for experimentation has been built using the breast cancer, respiratory cancer and mixed cancer data sets with three traditional classifiers namely Decision Tree, Naive Bayes and K-Nearest Neighbor. The three prognosis factors survival, stage and metastasis have been used as class labels for experimental comparisons. The results shows a steady increase in the prediction accuracy of balanced stratified model as the sample size increases, but the traditional approach fluctuates before the optimum results.        △ Less","12 March, 2014",cs.LG,10.5121/ijscai.2014.3102 
              Effect of Social Media on Website Popularity: Differences between Public and Private Universities in Indonesia          ,1403.1956,https://arxiv.org/abs/1403.1956,https://arxiv.org/pdf/1403.1956,"Authors:HanumPutriPermatasari,SilviaHarlena,DonnyErlangga,RezaChandra","        Social media has become something that is important to enhance social networking and sharing of information through the website. Social media have not only changed social networking, they provide a valuable tool for social organization, activism, political, healthcare and even academic relations in the university. The researchers conducted present study with objectives to a). examine the academic use of social media by universities, b). measure the popularity and visibility of social media owned by universities. This study was delimited to the universities in Indonesia. The population of the study consisted both on public and private universities. The sample size comprised totally of 264 universities that their ranks included both in Webometrics and 4ICU in July 2012 edition. The social media which was examined included Facebook, Twitter, Flicker, LinkedIn, Youtube, Wikipeda, Blogs, social network community owned by the university and Open Course Ware. This study used an approach for data collection and measurement: by using Alexa and Majestic SEO. Data analysis using the Pearson Chi-square for social media ownership that using data ordinal and independent t test for examining effects of social media on website popularity. The study revealed that majority of the social media users used Facebook, then followed by Twitter. There are also most significant differences for result of popularity by Alexa Rank and visibility by Majestic SEO in universities whether used social media or no.        △ Less","8 March, 2014","cs.CY,cs.SI",
              A multivariate hierarchical Bayesian framework for healthcare predictions with application to medical home study in the Department of Veteran Affairs          ,1403.0674,https://arxiv.org/abs/1403.0674,https://arxiv.org/pdf/1403.0674,"Authors:IssacShams,SaeedeAjorlou,KaiYang","        Recently the patient centered medical home (PCMH) model has become a popular approach to deliver better care to patients. Current research shows that the most important key for succession of this method is to make balance between healthcare supply and demand. Without such balance in clinical supply and demand, issues such as excessive under and over utilization of physicians, long waiting time for receiving the appropriate treatment, and non continuity of care will eliminate many advantages of the medical home strategy. To reach this end we need to have information about both supply and demand in healthcare system. Healthcare supply can be calculated easily based on head counts and available hours which is offered by professionals for a specific time period while healthcare demand is not easy to calculate, and it is affected by some healthcare, diagnostic and demographic attributes. In this paper, by extending the hierarchical generalized linear model to include multivariate responses, we develop a clinical workload prediction model for care portfolio demands in a Bayesian framework. Our analyses of a recent data from Veteran Health Administration indicate that our prediction model works for clinical data with high performance.        △ Less","3 March, 2014",stat.AP,
              I Know Why You Went to the Clinic: Risks and Realization of HTTPS Traffic Analysis          ,1403.0297,https://arxiv.org/abs/1403.0297,https://arxiv.org/pdf/1403.0297,"Authors:BradMiller,LingHuang,A.D.Joseph,J.D.Tygar","        Revelations of large scale electronic surveillance and data mining by governments and corporations have fueled increased adoption of HTTPS. We present a traffic analysis attack against over 6000 webpages spanning the HTTPS deployments of 10 widely used, industry-leading websites in areas such as healthcare, finance, legal services and streaming video. Our attack identifies individual pages in the same website with 89% accuracy, exposing personal details including medical conditions, financial and legal affairs and sexual orientation. We examine evaluation methodology and reveal accuracy variations as large as 18% caused by assumptions affecting caching and cookies. We present a novel defense reducing attack accuracy to 27% with a 9% traffic increase, and demonstrate significantly increased effectiveness of prior defenses in our evaluation context, inclusive of enabled caching, user-specific cookies and pages within the same website.        △ Less","2 March, 2014",cs.CR,
              A predictive analytics approach to reducing avoidable hospital readmission          ,1402.5991,https://arxiv.org/abs/1402.5991,https://arxiv.org/pdf/1402.5991,"Authors:IssacShams,SaeedeAjorlou,KaiYang","        Hospital readmission has become a critical metric of quality and cost of healthcare. Medicare anticipates that nearly $17 billion is paid out on the 20% of patients who are readmitted within 30 days of discharge. Although several interventions such as transition care management and discharge reengineering have been practiced in recent years, the effectiveness and sustainability depends on how well they can identify and target patients at high risk of rehospitalization. Based on the literature, most current risk prediction models fail to reach an acceptable accuracy level; none of them considers patient's history of readmission and impacts of patient attribute changes over time; and they often do not discriminate between planned and unnecessary readmissions. Tackling such drawbacks, we develop a new readmission metric based on administrative data that can identify potentially avoidable readmissions from all other types of readmission. We further propose a tree based classification method to estimate the predicted probability of readmission that can directly incorporate patient's history of readmission and risk factors changes over time. The proposed methods are validated with 2011-12 Veterans Health Administration data from inpatients hospitalized for heart failure, acute myocardial infarction, pneumonia, or chronic obstructive pulmonary disease in the State of Michigan. Results shows improved discrimination power compared to the literature (c-statistics>80%) and good calibration.        △ Less","12 March, 2014","stat.AP,cs.AI",
              On Learning from Label Proportions          ,1402.5902,https://arxiv.org/abs/1402.5902,https://arxiv.org/pdf/1402.5902,"Authors:FelixX.Yu,KrzysztofChoromanski,SanjivKumar,TonyJebara,Shih-FuChang","        Learning from Label Proportions (LLP) is a learning setting, where the training data is provided in groups, or ""bags"", and only the proportion of each class in each bag is known. The task is to learn a model to predict the class labels of the individual instances. LLP has broad applications in political science, marketing, healthcare, and computer vision. This work answers the fundamental question, when and why LLP is possible, by introducing a general framework, Empirical Proportion Risk Minimization (EPRM). EPRM learns an instance label classifier to match the given label proportions on the training data. Our result is based on a two-step analysis. First, we provide a VC bound on the generalization error of the bag proportions. We show that the bag sample complexity is only mildly sensitive to the bag size. Second, we show that under some mild assumptions, good bag proportion prediction guarantees good instance label prediction. The results together provide a formal guarantee that the individual labels can indeed be learned in the LLP setting. We discuss applications of the analysis, including justification of LLP algorithms, learning with population proportions, and a paradigm for learning algorithms with privacy guarantees. We also demonstrate the feasibility of LLP based on a case study in real-world setting: predicting income based on census data.        △ Less","11 February, 2015","stat.ML,cs.LG",
              Data Management Challenges in Paediatric Information Systems          ,1402.5773,https://arxiv.org/abs/1402.5773,https://arxiv.org/pdf/1402.5773,Authors:RichardMcClatchey,"        There is a compelling demand for the data integration and exploitation of heterogeneous biomedical information for improved clinical practice, medical research, and personalised healthcare across the EU. The area of paediatric information integration is particularly challenging since the patients physiology changes with growth and different aspects of health being regularly monitored over extended periods of time. Paediatricians require access to heterogeneous data sets, often collected in different locations with different apparatus and over extended timescales. Using a Grid platform originally developed for physics at CERN and a novel integrated semantic data model the Health-e-Child project has developed an integrated healthcare platform for European paediatrics, providing seamless integration of traditional and emerging sources of biomedical data. The long-term goal of the project was to provide uninhibited access to universal biomedical knowledge repositories for personalised and preventive healthcare, large-scale information-based biomedical research and training, and informed policy making. The project built a Grid-enabled european network of leading clinical centres that can share and annotate paediatric data, can validate systems clinically, and diffuse clinical excellence across Europe by setting up new technologies, clinical workflows, and standards. The Health-e-Child project highlights data management challenges for the future of European paediatric healthcare and is the subject of this chapter.        △ Less","24 February, 2014","cs.DB,cs.CY",
              Classification Tree Diagrams in Health Informatics Applications          ,1402.1947,https://arxiv.org/abs/1402.1947,https://arxiv.org/pdf/1402.1947,Authors:FarrukhArslan,"        Health informatics deal with the methods used to optimize the acquisition, storage and retrieval of medical data, and classify information in healthcare applications. Healthcare analysts are particularly interested in various computer informatics areas such as; knowledge representation from data, anomaly detection, outbreak detection methods and syndromic surveillance applications. Although various parametric and non-parametric approaches are being proposed to classify information from data, classification tree diagrams provide an interactive visualization to analysts as compared to other methods. In this work we discuss application of classification tree diagrams to classify information from medical data in healthcare applications.        △ Less","9 February, 2014","cs.IR,cs.CV,cs.LG",
              Study of Cloud Computing in HealthCare Industry          ,1402.1841,https://arxiv.org/abs/1402.1841,https://arxiv.org/pdf/1402.1841,"Authors:G.NikhitaReddy,G.J.UganderReddy",        In Todays real world technology has become a domiant crucial component in every industry including healthcare industry. The benefits of storing electronically the records of patients have increased the productivity of patient care and easy accessibility and usage. The recent technological innovations in the health care is the invention of cloud based Technology. But many fears and security measures regarding patient records storing remotely is a concern for many in health care industry. One needs to understand the benefits and fears of implementation of cloud computing its advantages and disadvantages of this new technology.        △ Less,"8 February, 2014","cs.CY,cs.DC",
              SPHERE: Meaningful and Inclusive Sensor-Based Home Healthcare,1402.0200,https://arxiv.org/abs/1402.0200,https://arxiv.org/pdf/1402.0200,"Authors:AlisonBurrows,RachelGooberman-Hill,IanCraddock,DavidCoyle","        Given current demographic and health trends, and their economic implications, home healthcare technology has become a fertile area for research and development. Motivated by the need for a radical reform of healthcare provision, SPHERE is a large-scale Interdisciplinary Research Collaboration that aims to develop home sensor systems to monitor people's health and wellbeing in the home. This paper outlines the unique circumstances of designing healthcare technology for the home environment, with a particular focus on how to ensure future systems are meaningful to and desirable for the intended users.        △ Less","2 February, 2014","cs.HC,cs.CY",
"              Mobile Services and ICT4D, To the Network Economy - Bridging the Digital Divide, Ethiopia's Case          ",1401.7435,https://arxiv.org/abs/1401.7435,https://arxiv.org/pdf/1401.7435,"Authors:NaodDugaJebessa,HenokGetachewAlemayehu","        This paper presents a development paradigm for Ethiopia, based on appropriate services and innovative use of mobile communications technologies via applications tailored for sectors like business, finance, healthcare, governance, education and infotainment. The experience of other developing countries like India and Kenya is cited so as to adapt those to the Ethiopian context. Notable application areas in the aforementioned sectors have been outlined. The ETC 'next generation network' is taken into consideration, with an emphasis on mobile service offering by the Telco itself and/or third party service providers. In addition, enabling technologies like mobile internet, location-based systems, open interfaces to large telecom networks, specifically service-oriented architecture (SOA), Parlay/JAIN and the like are discussed. The paper points out possible endeavors by such stakeholders like: telecom agencies and network operators; businesses, government and NGOs; entrepreneurs and innovators; technology companies and professionals; as well as researchers and academic institutions. ICT4D through mobile services and their role in bridging the digital divide by building a virtual 'network economy' is presented.        △ Less","29 January, 2014",cs.CY,
              Direct determination of external radiation dose in human blood          ,1401.6856,https://arxiv.org/abs/1401.6856,/search/?searchtype=author&query=Tanir%2C+A,"Authors:AGTanir,OGulec,ESahiner,MHBolukdemir,KKoc,NMeric,SKKeles,OKucuk","        In this study it was shown that it is possible to determine radiation doses from external beam therapy both directly and retrospectively from a human blood sample. To the best of our knowledge no other studies exist on the direct measurement of doses received by a person from external beam therapy. Optically stimulated luminescence counts from a healthy blood sample exposed to an external radiation source were measured. Blood aliquots were given 0, 1, 2, 3, 4, 5, 10, 15, 20, 25, 50, 100 and 200Gy beta doses and their decay and dose-response curves were plotted. While the luminescence intensities were found to be relatively low for the doses smaller than 10Gy, they were measured considerably higher for doses greater than 10Gy. The dose received by the blood aliquots was determined by interpolating the luminescence counts of 10Gy to the dose-response curve. This study has important ramifications for healthcare, medicine and radiation protection        △ Less","14 January, 2016",physics.med-ph,
              Bayesian Inference from Non-Ignorable Network Sampling Designs          ,1401.4718,https://arxiv.org/abs/1401.4718,https://arxiv.org/pdf/1401.4718,"Authors:SimonLunagomez,EdoardoAiroldi","        Consider a population of individuals and a network that encodes social connections among them. We are interested in making inference on finite population and super-population estimands that are a function of both individuals' responses and of the network, from a sample. Neither the sampling frame nor the network are available. However, the sampling mechanism implicitly leverages the network to recruit individuals, thus partially revealing social interactions among the individuals in the sample, as well as their responses. This is a common setting that arises, for instance, in epidemiology and healthcare, where samples from hard-to-reach populations are collected using link-tracing mechanisms, including respondent-driven sampling. In this paper, we study statistical properties of popular network sampling mechanisms. We formulate the estimation problem in terms of Rubin's inferential framework to explicitly account for social network structure. We then identify key modeling elements that lead to inferences with good frequentist properties when dealing with data collected through non-ignorable network sampling mechanisms. We demonstrate these methods on a study of the incidence of HIV in Brazil        △ Less","9 December, 2016",stat.ME,
              Attacks And Counterattacks On Wireless Sensor Networks          ,1401.4443,https://arxiv.org/abs/1401.4443,https://arxiv.org/pdf/1401.4443,"Authors:NusratFatema,RemusBrad","        WSN is formed by autonomous nodes with partial memory, communication range, power, and bandwidth. Their occupation depends on inspecting corporal and environmental conditions and communing through a system and performing data processing. The application field is vast, comprising military, ecology, healthcare, home or commercial and require a highly secured communication. The paper analyses different types of attacks and counterattacks and provides solutions for the WSN threats        △ Less","17 January, 2014",cs.NI,
              Direct determination of internal radiation dose in human blood          ,1401.3140,https://arxiv.org/abs/1401.3140,/search/?searchtype=author&query=Tan%C4%B1r%2C+A+G,"Authors:AyseGüneşTanır,ÖzgeGüleç","        The purpose of this study is to measure the internal radiation dose using a human blood sample. In the literature, there is no process that allows the direct measurement of the internal radiation dose received by a person. The luminescence counts from a blood sample having a laboratory-injected radiation dose and the waste blood of the patient injected with a radiopharmaceutical for diagnostic purposes were both measured. The decay and dose-response curves were plotted for the different doses. The doses received by the different blood aliquots can be determined by interpolating the luminescence counts to the dose-response curve. This study shows that the dose received by a person can be measured directly, simply and retrospectively by using only a very small amount of blood sample. The results will have important ramifications for the medicine and healthcare fields in particular. This will also be very important in cases of suspicion of radiation poisoning, malpractice and so on.        △ Less","14 January, 2016",physics.med-ph,
              Participant: A New Concept for Optimally Assisting the Elder People          ,1401.2782,https://arxiv.org/abs/1401.2782,https://arxiv.org/pdf/1401.2782,"Authors:HongSun,VincenzoDeFlorio,NingGui,ChrisBlondia","        Elder people are becoming a predominant aspect of our societies. As such, solutions both efficacious and cost-effective need to be sought. The approach pursued so far to solve this problem used to increase the number of people working in the health sector, e.g. doctors, nurses, etc. This increases the costs, which is becoming a big burden for countries. In this paper we propose a new concept in the health management of elder people, which we name as ""participant"". We propose the ""participant"" concept to encourage elder people to participate in those group activities that they are able to. Their roles in these activities are not passively requesting help, but actively participating to some healthcare processes. Characteristics of the participant approach are that medical resources are efficiently spared with this model, and the social network of the elder people is kept. A ""virtual community"" for mutual assistance is set up in this paper, and the simulations demonstrate that the ""participant"" model could fully utilize the community resources. Furthermore, the psychological health of the elder people will be improved.        △ Less","13 January, 2014",cs.CY,10.1109/CBMS.2007.82 
              Association Rules Mining Based Clinical Observations          ,1401.2571,https://arxiv.org/abs/1401.2571,https://arxiv.org/pdf/1401.2571,"Authors:MahmoodA.Rashid,MdTamjidulHoque,AbdulSattar","Healthcare institutes enrich the repository of patients' disease related information in an increasing manner which could have been more useful by carrying out relational analysis. Data mining algorithms are proven to be quite useful in exploring useful correlations from larger data repositories. In this paper we have implemented Association Rules mining based a novel idea for finding co-occurrences of diseases carried by a patient using the healthcare repository. We have developed a system-prototype for Clinical State Correlation Prediction (CSCP) which extracts data from patients' healthcare database, transforms the OLTP data into a Data Warehouse by generating association rules. The CSCP system helps reveal relations among the diseases. The CSCP system predicts the correlation(s) among primary disease (the disease for which the patient visits the doctor) and secondary disease/s (which is/are other associated disease/s carried by the same patient having the primary disease).        △ Less","11 January, 2014","cs.DB,cs.CE",
              Black Box Variational Inference          ,1401.0118,https://arxiv.org/abs/1401.0118,https://arxiv.org/pdf/1401.0118,"Authors:RajeshRanganath,SeanGerrish,DavidM.Blei","        Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a ""black box"" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.        △ Less","31 December, 2013","stat.ML,cs.LG,stat.CO,stat.ME",
              Architectural Pattern of Health Care System Using GSM Networks          ,1312.2323,https://arxiv.org/abs/1312.2323,https://arxiv.org/pdf/1312.2323,"Authors:Meiappane.A,Dr.V.PrasannaVenkatesan,SelvaMurugan.S,Arun.A,Ramachandran.A","        Large-scale networked environments, such as the Internet, possess the characteristics of centralised data, centralised access and centralised control; this gives the user a powerful mechanism for building and integrating large repositories of centralised information from diverse resources set. However, a centralised network system with GSM Networks development for a hospital information systems or a health care information portal is still in its infancy. The shortcomings of the currently available tools have made the use of mobile devices more appealing. In mobile computing, the issues such as low bandwidth, high latency wireless Networks, loss or degradation of wireless connections, and network errors or failures need to be dealt with. Other issues to be addressed include system adaptability, reliability, robustness, extensibility, flexibility, and maintainability. GSM approach has emerged as the most viable approach for development of intelligent software applications for wireless mobile devices in a centralized environment, which gives higher band width of 900 MHz for transmission. The e-healthcare system that we have developed provides support for physicians, nurses, pharmacists and other healthcare professionals, as well as for patients and medical devices used to monitor patients. In this paper, we present the architecture and the demonstration prototype.        △ Less","9 December, 2013","cs.SE,cs.CY",
              Optimal Provision-After-Wait in Healthcare,1312.1955,https://arxiv.org/abs/1312.1955,https://arxiv.org/pdf/1312.1955,"Authors:MarkBraverman,JingChen,SampathKannan","        We investigate computational and mechanism design aspects of scarce resource allocation, where the primary rationing mechanism is through waiting times. Specifically we consider allocating medical treatments to a population of patients. Each patient needs exactly one treatment, and can choose from kk hospitals. Hospitals have different costs, which are fully paid by a third party ---the ""payer"". The payer has a fixed budget BB, and each hospital will have its own waiting time. At equilibrium, each patient will choose his most preferred hospital given his intrinsic preferences and the waiting times. The payer thus computes the waiting times so that at equilibrium the budget constraint is satisfied and the social welfare is maximized.  We first show that the optimization problem is NP-hard, yet if the budget can be relaxed to (1+ε)B(1+ε)B for an arbitrarily small εε, then the optimum under budget BB can be approximated efficiently. Next, we study the endogenous emergence of waiting time from the dynamics between hospitals and patients, and show that there is no need for the payer to explicitly enforce the optimal waiting times. Under certain conditions, all he need is to enforce the amount of money he wants to pay to each hospital. The dynamics will always converge to the desired waiting times in finite time.  We then go beyond equilibrium solutions and investigate the optimization problem over a much larger class of mechanisms containing the equilibrium ones as special cases. With two hospitals, we show that under a natural assumption on the patients' preference profiles, optimal welfare is in fact attained by the randomized assignment mechanism, which allocates patients to hospitals at random subject to the budget constraint, but avoids waiting times.  Finally, we discuss potential policy implications of our results, as well as follow-up directions and open problems.        △ Less","6 December, 2013",cs.GT,
              Theoretical Foundation for Research in Communication using Information and Communication Technology Devices in Healthcare: An Interdisciplinary Scoping Review          ,1312.0520,https://arxiv.org/abs/1312.0520,https://arxiv.org/pdf/1312.0520,"Authors:ArunKeepanasseril,KathleenAnnMcKibbon,AlfonsoIorio","        Faulty communication between team members is one of the most important factors preventing substantial improvement in patient safety. Aviation, nuclear power and defense have been able to improve their safety record by adopting theory and model based solutions. In contrast, healthcare's thrust towards modern communication devices is largely devoid of theoretical foundation. The objective of this scoping review is to compile communication theories, frameworks, and models used by high risk organizations outside healthcare to study and resolve workplace communication issues. The healthcare databases searched included Medline, CINAHL, EMBASE, and PsycInfo. In addition, we searched engineering and science literature to include articles in the fields of information sciences, computer sciences, nuclear power generation, aviation, the military and other domains such as sociology that address the science and theory of communication. Comprehensive searching was also done in the communication studies literature. We also reviewed conference proceedings and grey literature and conducted citation tracking. Our initial systematic search yielded 15,365 articles. Hand searching and reviewing references resulted in a set of 181 articles. 144 full text articles were read and 40 of them were selected to be included in the review. We were able to identify 14 theories and 12 models which could be applied in hospital communication research. However, it must be noted that most of them have not yet been applied in biomedical research in hospital communication and as such their applicability can only be suggested-a gap which future research may be able to address. Formulation of a custom model representing the unique features and complexities of communication within hospitals is recommended.        △ Less","2 December, 2013",cs.OH,
              Sparse Linear Dynamical System with Its Application in Multivariate Clinical Time Series          ,1311.7071,https://arxiv.org/abs/1311.7071,https://arxiv.org/pdf/1311.7071,"Authors:ZitaoLiu,MilosHauskrecht","        Linear Dynamical System (LDS) is an elegant mathematical framework for modeling and learning multivariate time series. However, in general, it is difficult to set the dimension of its hidden state space. A small number of hidden states may not be able to model the complexities of a time series, while a large number of hidden states can lead to overfitting. In this paper, we study methods that impose an ℓ1\ell_1 regularization on the transition matrix of an LDS model to alleviate the problem of choosing the optimal number of hidden states. We incorporate a generalized gradient descent method into the Maximum a Posteriori (MAP) framework and use Expectation Maximization (EM) to iteratively achieve sparsity on the transition matrix of an LDS model. We show that our Sparse Linear Dynamical System (SLDS) improves the predictive performance when compared to ordinary LDS on a multivariate clinical time series dataset.        △ Less","3 December, 2013","cs.AI,cs.LG,stat.ML",
              A customized flocking algorithm for swarms of sensors tracking a swarm of targets          ,1311.6981,https://arxiv.org/abs/1311.6981,https://arxiv.org/pdf/1311.6981,"Authors:AnupamShukla,GauravOjha,SachinAcharya,ShubhamJain","        Wireless mobile sensor networks (WMSNs) are groups of mobile sensing agents with multi-modal sensing capabilities that communicate over wireless networks. WMSNs have more flexibility in terms of deployment and exploration abilities over static sensor networks. Sensor networks have a wide range of applications in security and surveillance systems, environmental monitoring, data gathering for network-centric healthcare systems, monitoring seismic activities and atmospheric events, tracking traffic congestion and air pollution levels, localization of autonomous vehicles in intelligent transportation systems, and detecting failures of sensing, storage, and switching components of smart grids. The above applications require target tracking for processes and events of interest occurring in an environment. Various methods and approaches have been proposed in order to track one or more targets in a pre-defined area. Usually, this turns out to be a complicated job involving higher order mathematics coupled with artificial intelligence due to the dynamic nature of the targets. To optimize the resources we need to have an approach that works in a more straightforward manner while resulting in fairly satisfactory data. In this paper we have discussed the various cases that might arise while flocking a group of sensors to track targets in a given environment. The approach has been developed from scratch although some basic assumptions have been made keeping in mind some previous theories. This paper outlines a customized approach for feasibly tracking swarms of targets in a specific area so as to minimize the resources and optimize tracking efficiency.        △ Less","12 November, 2013","cs.OH,cs.NI",
              Universal Scaling Law to Predict the Efficiency of Magnetic Nanoparticles as MRI T2-Contrast Agents          ,1311.6022,https://arxiv.org/abs/1311.6022,https://arxiv.org/pdf/1311.6022,"Authors:Q.L.Vuong,J.-F.Berret,J.Fresnais,Y.Gossuin,O.Sandre","        Magnetic particles are very efficient Magnetic Resonance Imaging (MRI) contrast agents. In the recent years, chemists have unleashed their imagination to design multi-functional nanoprobes for biomedical applications including MRI contrast enhancement. This study is focused on the direct relationship between the size and magnetization of the particles and their nuclear magnetic resonance relaxation properties, which condition their efficiency. Experimental relaxation results with maghemite particles exhibiting a wide range of sizes and magnetizations are compared to previously published data and to well-established relaxation theories with a good agreement. This allows deriving the experimental master curve of the transverse relaxivity versus particle size and to predict the MRI contrast efficiency of any type of magnetic nanoparticles. This prediction only requires the knowledge of the size of the particles impermeable to water protons and the saturation magnetization of the corresponding volume. To predict the T2 relaxation efficiency of magnetic single crystals, the crystal size and magnetization obtained through a single Langevin fit of a magnetization curve is the only information needed. For contrast agents made of several magnetic cores assembled into various geometries (dilute fractal aggregates, dense spherical clusters, core-shell micelles, hollow vesicles), one needs to know a third parameter, namely the intra-aggregate volume fraction occupied by the magnetic materials relatively to the whole (hydrodynamic) sphere. Finally a calculation of the maximum achievable relaxation effect and the size needed to reach this maximum is performed for different cases: maghemite single crystals and dense clusters, core-shell particles (oxide layer around a metallic core) and zinc manganese ferrite crystals.        △ Less","23 November, 2013",cond-mat.mtrl-sci,10.1002/adhm.201200078 
              Towards a New Science of a Clinical Data Intelligence          ,1311.4180,https://arxiv.org/abs/1311.4180,https://arxiv.org/pdf/1311.4180,"Authors:VolkerTresp,SonjaZillner,MariaJ.Costa,YiHuang,AlexanderCavallaro,PeterA.Fasching,AndreReis,MartinSedlmayr,ThomasGanslandt,KlemensBudde,CarlHinrichs,DaniloSchmidt,PhilippDaumke,DanielSonntag,ThomasWittenberg,PatriciaG.Oppelt,DenisKrompass","        In this paper we define Clinical Data Intelligence as the analysis of data generated in the clinical routine with the goal of improving patient care. We define a science of a Clinical Data Intelligence as a data analysis that permits the derivation of scientific, i.e., generalizable and reliable results. We argue that a science of a Clinical Data Intelligence is sensible in the context of a Big Data analysis, i.e., with data from many patients and with complete patient information. We discuss that Clinical Data Intelligence requires the joint efforts of knowledge engineering, information extraction (from textual and other unstructured data), and statistics and statistical machine learning. We describe some of our main results as conjectures and relate them to a recently funded research project involving two major German university hospitals.        △ Less","30 December, 2013","cs.CY,cs.AI",
              A hybrid decision support system : application on healthcare,1311.4086,https://arxiv.org/abs/1311.4086,https://arxiv.org/pdf/1311.4086,"Authors:AbdelhakMansoul,BaghdadAtmani,SofiaBenbelkacem","        Many systems based on knowledge, especially expert systems for medical decision support have been developed. Only systems are based on production rules, and cannot learn and evolve only by updating them. In addition, taking into account several criteria induces an exorbitant number of rules to be injected into the system. It becomes difficult to translate medical knowledge or a support decision as a simple rule. Moreover, reasoning based on generic cases became classic and can even reduce the range of possible solutions. To remedy that, we propose an approach based on using a multi-criteria decision guided by a case-based reasoning (CBR) approach.        △ Less","16 November, 2013","cs.AI,cs.LG",
              Escaping the poverty trap: modeling the interplay between economic growth and the ecology of infectious disease          ,1311.4079,https://arxiv.org/abs/1311.4079,https://arxiv.org/pdf/1311.4079,"Authors:GeorgM.Goerg,OscarPatterson-Lomba,LaurentHébert-Dufresne,BenjaminM.Althouse","        The dynamics of economies and infectious disease are inexorably linked: economic well-being influences health (sanitation, nutrition, treatment capacity, etc.) and health influences economic well-being (labor productivity lost to sickness and disease). Often societies are locked into ""poverty traps"" of poor health and poor economy. Here, using a simplified coupled disease-economic model with endogenous capital growth we demonstrate the formation of poverty traps, as well as ways to escape them. We suggest two possible mechanisms of escape both motivated by empirical data: one, through an influx of capital (development aid), and another through changing the percentage of GDP spent on healthcare. We find that a large influx of capital is successful in escaping the poverty trap, but increasing health spending alone is not. Our results demonstrate that escape from a poverty trap may be possible, and carry important policy implications in the world-wide distribution of aid and within-country healthcare spending.        △ Less","5 March, 2014","physics.soc-ph,q-bio.PE",
              Infrastructure Logicielle d un Environnement Hospitalier Intelligent          ,1311.3231,https://arxiv.org/abs/1311.3231,https://arxiv.org/pdf/1311.3231,"Authors:YoussefGahi,MeryemLamrani,MouhcineGuennoun,KhalilEl-Khatib","        The impact of new technologies in the field of healthcare has been proven to be satisfactory in improving the quality of care and services for patients. The hospital environment is undoubtedly one of the environments where human or hardware errors can cause harmful damage. It is therefore very useful for the sector to be at the forefront of technology and use the strongest and most efficient means since it is supposed to be an environment of reliability, trust and is legally responsible for ensuring the confidentiality of all patient information. In this project we have proposed and developed a smart hospital enviornement that allows both, detecting patien fall and send alerts to the suitable parties, and securing patien stored data through RFID technology.        △ Less","12 November, 2013",cs.CY,
              Motion and audio analysis in mobile devices for remote monitoring of physical activities and user authentication          ,1311.1132,https://arxiv.org/abs/1311.1132,https://arxiv.org/pdf/1311.1132,"Authors:HamedKetabdar,JalaluddinQureshi,PanHui","        In this article we propose the use of accelerometer embedded by default in smartphone as a cost-effective, reliable and efficient way to provide remote physical activity monitoring for the elderly and people requiring healthcare service. Mobile phones are regularly carried by users during their day-to-day work routine, physical movement information can be captured by the mobile phone accelerometer, processed and sent to a remote server for monitoring. The acceleration pattern can deliver information related to the pattern of physical activities the user is engaged in. We further show how this technique can be extended to provide implicit real-time security by analysing unexpected movements captured by the phone accelerometer, and automatically locking the phone in such situation to prevent unauthorised access. This technique is also shown to provide implicit continuous user authentication, by capturing regular user movements such as walking, and requesting for re-authentication whenever it detects a non-regular movement.        △ Less","5 November, 2013","cs.HC,cs.CV",10.1080/17489725.2011.644331 
"              Smartphone as a Personal, Pervasive Health Informatics Services Platform: Literature Review          ",1310.7965,https://arxiv.org/abs/1310.7965,https://arxiv.org/pdf/1310.7965,Authors:KatarzynaWac,"        Objectives: The article provides an overview of current trends in personal sensor, signal and imaging informatics, that are based on emerging mobile computing and communications technologies enclosed in a smartphone and enabling the provision of personal, pervasive health informatics services.  Methods: The article reviews examples of these trends from the PubMed and Google scholar literature search engines, which, by no means claim to be complete, as the field is evolving and some recent advances may not be documented yet.  Results: There exist critical technological advances in the surveyed smartphone technologies, employed in provision and improvement of diagnosis, acute and chronic treatment and rehabilitation health services, as well as in education and training of healthcare practitioners. However, the most emerging trend relates to a routine application of these technologies in a prevention/wellness sector, helping its users in self-care to stay healthy.  Conclusions: Smartphone-based personal health informatics services exist, but still have a long way to go to become an everyday, personalized healthcare-provisioning tool in the medical field and in a clinical practice. Key main challenge for their widespread adoption involve lack of user acceptance striving from variable credibility and reliability of applications and solutions as they a) lack evidence-based approach; b) have low levels of medical professional involvement in their design and content; c) are provided in an unreliable way, influencing negatively its usability; and, in some cases, d) being industry-driven, hence exposing bias in information provided, for example towards particular types of treatment or intervention procedures.        △ Less","6 September, 2013","cs.CY,cs.HC",
              Efficient Opportunistic Sensing using Mobile Collaborative Platform MOSDEN          ,1310.4052,https://arxiv.org/abs/1310.4052,https://arxiv.org/pdf/1310.4052,"Authors:PremPrakashJayaraman,CharithPerera,DimitriosGeorgakopoulos,ArkadyZaslavsky","        Mobile devices are rapidly becoming the primary computing device in people's lives. Application delivery platforms like Google Play, Apple App Store have transformed mobile phones into intelligent computing devices by the means of applications that can be downloaded and installed instantly. Many of these applications take advantage of the plethora of sensors installed on the mobile device to deliver enhanced user experience. The sensors on the smartphone provide the opportunity to develop innovative mobile opportunistic sensing applications in many sectors including healthcare, environmental monitoring and transportation. In this paper, we present a collaborative mobile sensing framework namely Mobile Sensor Data EngiNe (MOSDEN) that can operate on smartphones capturing and sharing sensed data between multiple distributed applications and users. MOSDEN follows a component-based design philosophy promoting reuse for easy and quick opportunistic sensing application deployments. MOSDEN separates the application-specific processing from the sensing, storing and sharing. MOSDEN is scalable and requires minimal development effort from the application developer. We have implemented our framework on Android-based mobile platforms and evaluate its performance to validate the feasibility and efficiency of MOSDEN to operate collaboratively in mobile opportunistic sensing applications. Experimental outcomes and lessons learnt conclude the paper.        △ Less","15 October, 2013","cs.NI,cs.DC",
              Teaching Wireless Sensor Networks: An Holistic Approach Bridging Theory and Practice at the Master Level          ,1310.2488,https://arxiv.org/abs/1310.2488,https://arxiv.org/pdf/1310.2488,Authors:CarloFischione,"        Wireless Sensor Networks (WSNs) are a new technology that has received a substantial attention from several academic research fields in the last years. There are many applications of WSNs, including environmental monitoring, industrial automation, intelligent transportation systems, healthcare and wellbeing, smart energy, to mention a few. Courses have been introduced both at the PhD and at the Master levels. However, these existing courses focus on particular aspects of WSNs (Networking, or Signal Processing, or Embedded Software), whereas WSNs encompass disciplines traditionally separated in Electrical Engineering and Computer Sciences. This paper gives two original contributions: the essential knowledge that should be brought in a WSNs course is characterized, and a course structure with an harmonious holistic approach is proposed. A method based on both theory and experiments is illustrated for the design of this course, whereby the students have hands-on to implement, understand, and develop in practice the implications of theoretical concepts. Theory and applications are thus considered all together. Ultimately, the objective of this paper is to design a new course, to use innovative hands-on experiments to illustrate the theoretical concepts in the course, to show that theoretical aspects are essential for the solution of real-life engineering WSNs problems, and finally to create a fun and interesting teaching and learning environments for WSNs.        △ Less","9 October, 2013","cs.CY,cs.NI",
              Security Enhancement of Biometric Authentication Scheme for Telecare Medicine Information Systems with Nonce          ,1309.4690,https://arxiv.org/abs/1309.4690,/search/?searchtype=author&query=Mishra%2C+D,"Authors:DheerendraMishra,SouravMukhopadhyay",        Telecare medicine information systems (TMIS) present the platform to deliver clinical service door to door. The technological advances in mobile computing are enhancing the quality of healthcare and a user can access these services using its mobile device. Existing authentication schemes for TMIS are either vulnerable to attacks or they have higher computational cost. We propose a biometric based efficient authentication scheme for TMIS which only requires the computation of the hash and XOR functions.        △ Less,"2 May, 2014",cs.CR,
              Compression via Compressive Sensing : A Low-Power Framework for the Telemonitoring of Multi-Channel Physiological Signals          ,1309.4136,https://arxiv.org/abs/1309.4136,https://arxiv.org/pdf/1309.4136,"Authors:BenyuanLiu,ZhilinZhang,HongqiFan,QiangFu","        Telehealth and wearable equipment can deliver personal healthcare and necessary treatment remotely. One major challenge is transmitting large amount of biosignals through wireless networks. The limited battery life calls for low-power data compressors. Compressive Sensing (CS) has proved to be a low-power compressor. In this study, we apply CS on the compression of multichannel biosignals. We firstly develop an efficient CS algorithm from the Block Sparse Bayesian Learning (BSBL) framework. It is based on a combination of the block sparse model and multiple measurement vector model. Experiments on real-life Fetal ECGs showed that the proposed algorithm has high fidelity and efficiency. Implemented in hardware, the proposed algorithm was compared to a Discrete Wavelet Transform (DWT) based algorithm, verifying the proposed one has low power consumption and occupies less computational resources.        △ Less","16 November, 2013",cs.IT,
              ACTORS: A Goal-driven Approach for Capturing and Managing Consent in e-Health Systems          ,1309.2869,https://arxiv.org/abs/1309.2869,https://arxiv.org/pdf/1309.2869,"Authors:MuhammadRizwanAsghar,GiovanniRussello","        The notion of patient's consent plays a major role in granting access to medical data. In typical healthcare systems, consent is captured by a form that the patient has to fill in and sign. In e-Health systems, the paper-form consent is being replaced by the integration of the notion of consent in the mechanisms that regulate the access to the medical data. This helps in empowering the patient with the capability of granting and revoking consent in a more effective manner. However, the process of granting and revoking consent greatly varies according to the situation in which the patient is. Our main argument is that such a level of detail is very difficult and error-prone to capture as a set of authorisation policies. In this paper, we present ACTORS, a goal-driven approach to manage consent. The main idea behind ACTORS is to leverage the goal-driven approach of Teleo-Reactive (TR) programming for managing consent that takes into account changes regarding the domains and contexts in which the patient is providing her consent.        △ Less","11 September, 2013",cs.CY,
"              Ubiquitous healthcare monitoring system using integrated triaxial accelerometer,spo2 and location sensors          ",1309.1542,https://arxiv.org/abs/1309.1542,https://arxiv.org/pdf/1309.1542,"Authors:O.O.Ogunduyile,K.Zuva,O.A.Randle,T.Zuva","        Ubiquitous healthcare has become one of the prominent areas of research inorder to address the challenges encountered in healthcare environment. In contribution to this area, this study developed a system prototype that recommends diagonostic services based on physiological data collected in real time from a distant patient. The prototype uses WBAN body sensors to be worn by the individual and an android smart phone as a personal server. Physiological data is collected and uploaded to a Medical Health Server (MHS) via GPRS/internet to be analysed. Our implemented prototype monitors the activity, location and physiological data such as SpO2 and Heart Rate (HR) of the elderly and patients in rehabilitation. The uploaded information can be accessed in real time by medical practitioners through a web application.        △ Less","6 September, 2013",cs.NI,10.5121/iju.2013.4201 
              Enhanced Tumor Accumulation of Sub-2 nm Gold Nanoclusters for Cancer Radiation Therapy          ,1308.6737,https://arxiv.org/abs/1308.6737,https://arxiv.org/pdf/1308.6737,"Authors:Xiao-DongZhang,JieChen,ZhentaoLuo,DiWu,XiuShen,Sha-ShaSong,Yuan-MingSun,Pei-XunLiu,JingZhao,ShuaidongHuo,SaijunFan,FeiyueFan,Xing-JieLiang,JianpingXie","        A new type of metabolizable and efficient radiosensitizer for cancer radiotherapy is presented in this study by combining ultrasmall Au nanoclusters (NCs, <2 nm) with biocompatible coating ligands (glutathione, GSH). The new nano-construct (GSH-coated Au25 NCs) inherits attractive features of both the Au core (strong radiosensitizing effect) and GSH shell (good biocompatibility). It can preferentially accumulate in tumor via the improved EPR effect, which leads to strong enhancement for cancer radiotherapy. After the treatment, the small-sized GSH-Au25 NCs can be efficiently cleared by the kidney, minimizing any potential side effects due to the accumulation of Au25 NCs in the body.        △ Less","30 August, 2013","physics.bio-ph,physics.med-ph",10.1002/adhm.201300189 
              PANAS-t: A Pychometric Scale for Measuring Sentiments on Twitter          ,1308.1857,https://arxiv.org/abs/1308.1857,https://arxiv.org/pdf/1308.1857,"Authors:PollyannaGonçalves,FabrícioBenevenuto,MeeyoungCha","        Online social networks have become a major communication platform, where people share their thoughts and opinions about any topic real-time. The short text updates people post in these network contain emotions and moods, which when measured collectively can unveil the public mood at population level and have exciting implications for businesses, governments, and societies. Therefore, there is an urgent need for developing solid methods for accurately measuring moods from large-scale social media data. In this paper, we propose PANAS-t, which measures sentiments from short text updates in Twitter based on a well-established psychometric scale, PANAS (Positive and Negative Affect Schedule). We test the efficacy of PANAS-t over 10 real notable events drawn from 1.8 billion tweets and demonstrate that it can efficiently capture the expected sentiments of a wide variety of issues spanning tragedies, technology releases, political debates, and healthcare.        △ Less","8 August, 2013","cs.SI,physics.soc-ph",
              Evaluating a healthcare data warehouse for cancer diseases          ,1307.3448,https://arxiv.org/abs/1307.3448,https://arxiv.org/pdf/1307.3448,"Authors:Dr.OsamaE.Sheta,AhmedNourEldeen",        This paper presents the evaluation of the architecture of healthcare data warehouse specific to cancer diseases. This data warehouse containing relevant cancer medical information and patient data. The data warehouse provides the source for all current and historical health data to help executive manager and doctors to improve the decision making process for cancer patients. The evaluation model based on Bill Inmon's definition of data warehouse is proposed to evaluate the Cancer data warehouse.        △ Less,"12 July, 2013",cs.DB,
              Forecasts of Cancer and Chronic Patients: Big Data Metrics of Population Health          ,1307.3434,https://arxiv.org/abs/1307.3434,https://arxiv.org/pdf/1307.3434,"Authors:JacobKuriyan,NathanielCobb","        Chronic diseases and cancer account for over 75 percent of healthcare costs in the US. Increased prevention services and improved primary care are thought to decrease costs. Current models for detecting changes in the health of populations are cumbersome and expensive, and are not sensitive in the short term. In this paper we model population health as a dynamical system to predict the time evolution of the new diagnosis of chronic diseases and cancer. This provides a reliable forecasting tool and a means of measuring short-term changes in the health status of the population resulting from preventive care programs. Twelve month forecasts of cancer and chronic populations were accurate with errors lying between 3 percent and 6 percent. We confirmed what other studies have demonstrated that diabetes patients are at increased cancer risk but, interestingly, we also discovered that all of the studied chronic conditions increased cancer risk just as diabetes did, and by a similar amount. The model(i)yields a new metric for measuring performance of preventive and clinical care programs that can provide timely feedback for quality improvement programs;(ii)helps understand ""savings"" in the context of preventive care programs and explains how they can be calculated in the short term, even though they materialize only in the long term and(iii)provides an analytic tool and metrics to infer correlations and derive insights on the effect of changes in socio-economic factors affecting population health on improving health and lowering costs of populations.        △ Less","12 July, 2013",q-bio.PE,
              Engaging with mental health: a global challenge          ,1307.3174,https://arxiv.org/abs/1307.3174,https://arxiv.org/pdf/1307.3174,"Authors:DavidCoyle,MarkMatthews,GavinDoherty,JohnSharry","        Using the metrics of the World Health Organisation, the Global Burden of Disease Study has found that mental health difficulties are currently the leading cause of disability in developed countries [1]. Projections also indicate that the global burden of mental health difficulties will continue to rise in the coming decades. The human and economic costs of this trend will be substantial. In this paper we discuss how effectively designed interactive systems, developed through collaborative, interdisciplinary efforts, can play a significant role in helping to address this challenge. Our discussion is grounded in a description of four exploratory systems, each of which has undergone initial clinical evaluations. Directions for future research on mental health technologies are also identified.        △ Less","11 July, 2013","cs.HC,cs.CY",
              The technology of using a data warehouse to support decision-making in health care          ,1307.3061,https://arxiv.org/abs/1307.3061,https://arxiv.org/pdf/1307.3061,"Authors:Dr.OsamaE.Sheta,AhmedNourEldeen","        This paper describes the technology of data warehouse in healthcare decision-making and tools for support of these technologies, which is used to cancer diseases. The healthcare executive managers and doctors needs information about and insight into the existing health data, so as to make decision more efficiently without interrupting the daily work of an On-Line Transaction Processing (OLTP) system. This is a complex problem during the healthcare decision-making process. To solve this problem, the building a healthcare data warehouse seems to be efficient. First in this paper we explain the concepts of the data warehouse, On-Line Analysis Processing (OLAP). Changing the data in the data warehouse into a multidimensional data cube is then shown. Finally, an application example is given to illustrate the use of the healthcare data warehouse specific to cancer diseases developed in this study. The executive managers and doctors can view data from more than one perspective with reduced query time, thus making decisions faster and more comprehensive.        △ Less","11 July, 2013",cs.DB,10.5121/ijdms.2013.5305 
              RFID Technology Based Attendance Management System          ,1306.5381,https://arxiv.org/abs/1306.5381,https://arxiv.org/pdf/1306.5381,"Authors:SumitaNainan,RominParekh,TanviShah","        RFID is a nascent technology, deeply rooted by its early developments in using radar1 as a harbinger of adversary planes during World War II. A plethora of industries have leveraged the benefits of RFID technology for enhancements in sectors like military, sports, security, airline, animal farms, healthcare and other areas. Industry specific key applications of this technology include vehicle tracking, automated inventory management, animal monitoring, secure store checkouts, supply chain management, automatic payment, sport timing technologies, etc. This paper introduces the distinctive components of RFID technology and focuses on its core competencies: scalability and security. It will be then supplemented by a detailed synopsis of an investigation conducted to test the feasibility and practicality of RFID technology.        △ Less","23 June, 2013",cs.ET,
              Clinical Relationships Extraction Techniques from Patient Narratives          ,1306.5170,https://arxiv.org/abs/1306.5170,https://arxiv.org/pdf/1306.5170,"Authors:WafaaTawfikAbdel-moneim,MohamedHashemAbdel-Aziz,MohamedMonierHassan","        The Clinical E-Science Framework (CLEF) project was used to extract important information from medical texts by building a system for the purpose of clinical research, evidence-based healthcare and genotype-meets-phenotype informatics. The system is divided into two parts, one part concerns with the identification of relationships between clinically important entities in the text. The full parses and domain-specific grammars had been used to apply many approaches to extract the relationship. In the second part of the system, statistical machine learning (ML) approaches are applied to extract relationship. A corpus of oncology narratives that hand annotated with clinical relationships can be used to train and test a system that has been designed and implemented by supervised machine learning (ML) approaches. Many features can be extracted from these texts that are used to build a model by the classifier. Multiple supervised machine learning algorithms can be applied for relationship extraction. Effects of adding the features, changing the size of the corpus, and changing the type of the algorithm on relationship extraction are examined. Keywords: Text mining; information extraction; NLP; entities; and relations.        △ Less","21 June, 2013","cs.IR,cs.CL",
              An Exploratory Ethnographic Study of Issues and Concerns with Whole Genome Sequencing          ,1306.4962,https://arxiv.org/abs/1306.4962,https://arxiv.org/pdf/1306.4962,Authors:EmilianoDeCristofaro,"        Progress in Whole Genome Sequencing (WGS) will soon allow a large number of individuals to have their genome fully sequenced. This lays the foundations to improve modern healthcare, enabling a new era of personalized medicine where diagnosis and treatment is tailored to the patient's genetic makeup. It also allows individuals motivated by personal curiosity to have access to their genetic information, and use it, e.g., to trace their ancestry. However, the very same progress also amplifies a number of ethical and privacy concerns, that stem from the unprecedented sensitivity of genomic information and that are not well studied. This paper presents an exploratory ethnographic study of users' perception of privacy and ethical issues with WGS, as well as their attitude toward different WGS programs. We report on a series of semi-structured interviews, involving 16 participants, and analyze the results both quantitatively and qualitatively. Our analysis shows that users exhibit common trust concerns and fear of discrimination, and demand to retain strict control over their genetic information. Finally, we highlight the need for further research in the area and follow-up studies that build on our initial findings.        △ Less","31 January, 2014","cs.CR,cs.CY,q-bio.GN",
              Energy efficient routing in mobile ad-hoc networks for Healthcare Environments          ,1306.4082,https://arxiv.org/abs/1306.4082,https://arxiv.org/pdf/1306.4082,"Authors:SohailAbid,ImranShafi,ShahidAbid","        The modern and innovative medical applications based on wireless network are being developed in the commercial sectors as well as in research. The emerging wireless networks are rapidly becoming a fundamental part of medical solutions due to increasing accessibility for healthcare professionals/patients reducing healthcare costs. Discovering the routes among hosts that are energy efficient without compromise on smooth communication is desirable. This work investigates energy efficiency of some selected proactive and reactive routing protocols in wireless network for healthcare environments. After simulation and analysis we found that DSR is best energy efficient routing protocol among DSR, DSDV and AODV, because DSR has maximum remaining energy.        △ Less","18 June, 2013",cs.NI,
              The Appliance Pervasive of Internet of Things in Healthcare Systems          ,1306.3953,https://arxiv.org/abs/1306.3953,https://arxiv.org/pdf/1306.3953,Authors:MirSajjadHussainTalpur,"        In fact, information systems are the foundation of new productivity sources, medical organizational forms, and erection of a global economy. IoT based healthcare systems play a significant role in ICT and have contribution in growth of medical information systems, which are underpinning of recent medical and economic development strategies. However, to take advantages of IoT, it is essential that medical enterprises and community should trust the IoT systems in terms of performance, security, privacy, reliability and return-on-investment, which are open challenges of current IoT systems. For heightening of healthcare system; tracking, tracing and monitoring of patients and medical objects are more essential. But due to the inadequate healthcare situation, medical environment, medical technologies and the unique requirements of some healthcare applications, the obtainable tools cannot meet them accurately. The tracking, tracing and monitoring of patients and healthcare actors activities in healthcare system are challenging research directions for IoT researchers. State-of-the-art IoT based healthcare system should be developed which ensure the safety of patients and other healthcare activities. With this manuscript, we elaborate the essential role of IoT in healthcare systems; immense prospects of Internet of things in healthcare systems; extensive aspect of the use of IoT is dissimilar among different healthcare components and finally the participation of IoT between the useful research and present realistic applications. IoT and few other modern technologies are still in underpinning stage; mainly in the healthcare system.        △ Less","17 June, 2013",cs.SY,
              The Chills and Thrills of Whole Genome Sequencing          ,1306.1264,https://arxiv.org/abs/1306.1264,https://arxiv.org/pdf/1306.1264,"Authors:ErmanAyday,EmilianoDeCristofaro,Jean-PierreHubaux,GeneTsudik","        In recent years, Whole Genome Sequencing (WGS) evolved from a futuristic-sounding research project to an increasingly affordable technology for determining complete genome sequences of complex organisms, including humans. This prompts a wide range of revolutionary applications, as WGS promises to improve modern healthcare and provide a better understanding of the human genome -- in particular, its relation to diseases and response to treatments. However, this progress raises worrisome privacy and ethical issues, since, besides uniquely identifying its owner, the genome contains a treasure trove of highly personal and sensitive information. In this article, after summarizing recent advances in genomics, we discuss some important privacy issues associated with human genomic information and identify a number of particularly relevant research challenges.        △ Less","16 February, 2015",cs.CR,
              Shifting Role of Customer from Recipient To Partner of Care In Healthcare Organization          ,1304.5297,https://arxiv.org/abs/1304.5297,https://arxiv.org/pdf/1304.5297,"Authors:MuhammadAnshari,MohammadNabilAlmunawar","        Most recent e-health initiatives perceive customers (patients) as recipients of medical care where they do not have a significant role in the process of health decision making. However, the advancement of Web 2.0 offers patients to have a greater role in the decision making process related to their health as they can be empowered with the ability to access and control information that fits with their personalized needs. However, providing patient empowerment in e-health through Web 2.0 is challenging task because the complexity nature of healthcare business processes. Empowerment closely relates to the concept of Customer Relationship Management (CRM) in managing good relationships with the customers. The adoption of Web 2.0 in CRM systems is known as Social CRM or CRM 2.0. Social CRM emerges to accommodate dynamic means of interaction between patients with their healthcare providers. The aim of this paper is to present a model that embeds empowerment of patient through Social CRM intervention that may extend the role of the patient as an individual health actor, a social health agent, and a medical care partner. A survey has been conducted to gain a feedback from customers regarding the proposed model. A prototype derived from the model namely Clinic 2.0 has also been developed. Using the prototype we measure its impact towards customer satisfaction and health literacy. The results show that the system intervention through Clinic 2.0 improves the level of satisfaction and health literacy of participants.        △ Less","18 April, 2013",cs.CY,
              System Architecture of HatterHealthConnect: An Integration of Body Sensor Networks and Social Networks to Improve Health Awareness          ,1304.4548,https://arxiv.org/abs/1304.4548,https://arxiv.org/pdf/1304.4548,"Authors:HalaElAarag,DavidBauschlicher,StevenBauschlicher","        Over the last decade, the demand for efficient healthcare monitoring has increased and forced the health and wellness industry to embrace modern technological advances. Body Sensor Networks, or BSNs, can remotely collect users data and upload vital statistics to servers over the Internet. Advances in wireless technologies such as cellular devices and Bluetooth increase the mobility users experience while wearing a body sensor network. When connected by the proper framework, BSNs can efficiently monitor and record data while minimizing the energy expenditure of nodes in the BSN. Social networking sites play a large role in the aggregation and sharing of data between many users. Connecting a BSN to a social network creates the unique ability to share health related data with other users through social interaction. In this research, we present an integration of BSNs and social networks to establish a community promoting well being and great social awareness. We present the system architecture; both hardware and software, of a prototype implementation using Zephyr HxM heart monitor, Intel-Shimmer EMG senor and a Samsung Captivate smart phone. We provide implementation details for the design on the base station, the database server and the Facebook application. We illustrate how the Android application was designed with both functionality and user perspective in mind that resulted in an easy to use system. This prototype can be used in multiple health related applications based on the type of sensors used.        △ Less","16 April, 2013",cs.NI,
              Simulation of Obstruction Avoidance Generously Mobility (OAGM) Model using Graph-theory Technique          ,1304.3560,https://arxiv.org/abs/1304.3560,https://arxiv.org/pdf/1304.3560,"Authors:V.Vasanthi,M.Hemalatha","        An Obstruction Avoidance Generously Mobility (OAGM) model has been introduced for controlling ad-hoc sensor networks and thereby operating emerging fields like military and healthcare services. According to this model, the ability to send a message to a group of users simultaneously, based solely on their geographic location, is desirable by using Mission Critical Mobility model that assumes the obstacle shapes like rectangle or square in the simulation terrain. The OAGM model is developed by grasping the critical situations of military and healthcare services by incorporating the node movement model, hierarchical node organization, placement of obstacle that affect the movement of nodes and also signal propagation. Graph theory technique is used to find the shortest path of the node movement process. The varying number of parameter sets with DSR protocol is analyzed for MCM and OAGM mobility model. The results show OAGM performance is better than MCM.        △ Less","12 April, 2013",cs.NI,
              Ubiquitous HealthCare in Wireless Body Area Networks - A Survey          ,1303.2062,https://arxiv.org/abs/1303.2062,https://arxiv.org/pdf/1303.2062,"Authors:N.Javaid,N.A.Khan,M.Shakir,M.A.Khan,S.H.Bouk,Z.A.Khan","        Advances in wireless communication, system on chip and low power sensor nodes allowed realization of Wireless Body Area Network (WBAN). WBAN comprised of tiny sensors, which collect information of patient's vital signs and provide a real time feedback. In addition, WBAN also supports many applications including Ubiquitous HealthCare (UHC), entertainment, gaming, military, etc. UHC is required by elderly people to facilitate them with instant monitoring anywhere they move around. In this paper, different standards used in WBAN were also discussed briefly. Path loss in WBAN and its impact on communication was presented with the help of simulations, which were performed for different models of In-Body communication and different factors (such as, attenuation, frequency, distance etc) influencing path loss in On-Body communications.        △ Less","8 March, 2013",cs.NI,
              Risk Prediction of a Multiple Sclerosis Diagnosis          ,1303.1170,https://arxiv.org/abs/1303.1170,https://arxiv.org/pdf/1303.1170,"Authors:JoyceC.Ho,JoydeepGhosh,KPUnnikrishnan","        Multiple sclerosis (MS) is a chronic autoimmune disease that affects the central nervous system. The progression and severity of MS varies by individual, but it is generally a disabling disease. Although medications have been developed to slow the disease progression and help manage symptoms, MS research has yet to result in a cure. Early diagnosis and treatment of the disease have been shown to be effective at slowing the development of disabilities. However, early MS diagnosis is difficult because symptoms are intermittent and shared with other diseases. Thus most previous works have focused on uncovering the risk factors associated with MS and predicting the progression of disease after a diagnosis rather than disease prediction. This paper investigates the use of data available in electronic medical records (EMRs) to create a risk prediction model; thereby helping clinicians perform the difficult task of diagnosing an MS patient. Our results demonstrate that even given a limited time window of patient data, one can achieve reasonable classification with an area under the receiver operating characteristic curve of 0.724. By restricting our features to common EMR components, the developed models also generalize to other healthcare systems.        △ Less","5 March, 2013","stat.AP,q-bio.QM",
              Automatic symmetry based cluster approach for anomalous brain identification in PET scan image : An Analysis          ,1303.0644,https://arxiv.org/abs/1303.0644,https://arxiv.org/pdf/1303.0644,"Authors:A.Meena,K.Raja","        Medical image segmentation is referred to the segmentation of known anatomic structures from different medical images. Normally, the medical data researches are more complicated and an exclusive structures. This computer aided diagnosis is used for assisting doctors in evaluating medical imagery or in recognizing abnormal findings in a medical image. To integrate the specialized knowledge for medical data processing is helpful to form a real useful healthcare decision making system. This paper studies the different symmetry based distances applied in clustering algorithms and analyzes symmetry approach for Positron Emission Tomography (PET) scan image segmentation. Unlike CT and MRI, the PET scan identifies the structure of blood flow to and from organs. PET scan also helps in early diagnosis of cancer and heart, brain and gastro intestinal ailments and to detect the progress of treatment. In this paper, the scope diagnostic task expands for PET image in various brain functions.        △ Less","4 March, 2013",cs.CV,
"              Cloud Forensics: A Meta-Study of Challenges, Approaches, and Open Problems          ",1302.6312,https://arxiv.org/abs/1302.6312,https://arxiv.org/pdf/1302.6312,"Authors:ShamsZawoad,RagibHasan","        In recent years, cloud computing has become popular as a cost-effective and efficient computing paradigm. Unfortunately, today's cloud computing architectures are not designed for security and forensics. To date, very little research has been done to develop the theory and practice of cloud forensics. Many factors complicate forensic investigations in a cloud environment. First, the storage system is no longer local. Therefore, even with a subpoena, law enforcement agents cannot confiscate the suspect's computer and get access to the suspect's files. Second, each cloud server contains files from many users. Hence, it is not feasible to seize servers from a data center without violating the privacy of many other users. Third, even if the data belonging to a particular suspect is identified, separating it from other users' data is difficult. Moreover, other than the cloud provider's word, there is usually no evidence that links a given data file to a particular suspect. For such challenges, clouds cannot be used to store healthcare, business, or national security related data, which require audit and regulatory compliance. In this paper, we systematically examine the cloud forensics problem and explore the challenges and issues in cloud forensics. We then discuss existing research projects and finally, we highlight the open problems and future directions in cloud forensics research area. We posit that our systematic approach towards understanding the nature and challenges of cloud forensics will allow us to examine possible secure solution approaches, leading to increased trust on and adoption of cloud computing, especially in business, healthcare, and national security. This in turn will lead to lower cost and long-term benefit to our society as a whole.        △ Less","25 February, 2013","cs.DC,cs.CR",
              A user profile based access control model and architecture          ,1302.1667,https://arxiv.org/abs/1302.1667,https://arxiv.org/pdf/1302.1667,"Authors:MeriemZerkouk,AbdallahMhamed,BelhadriMessabih","        Personalization and adaptation to the user profile capability are the hottest issues to ensure ambient assisted living and context awareness in nowadays environments. With the growing healthcare and wellbeing context aware applications, modeling security policies becomes an important issue in the design of future access control models. This requires rich semantics using ontology modeling for the management of services provided to dependant people. However, current access control models remain unsuitable due to lack of personalization, adaptability and smartness to the handicap situation. In this paper, we propose a novel adaptable access control model and its related architecture in which the security policy is based on the handicap situation analyzed from the monitoring of userss behavior in order to grant a service using any assistive device within intelligent environment. the design of our model is an ontology-learning and evolving security policy for predicting the future actions of dependent people. This is reached by reasoning about historical data, contextual data and user behavior according to the access rules that are used in the inference engine to provide the right service according to the users needs.        △ Less","10 February, 2013","cs.CR,cs.CY",10.5121/ijcnc.2013.5112 
              Artificial Intelligence Framework for Simulating Clinical Decision-Making: A Markov Decision Process Approach          ,1301.2158,https://arxiv.org/abs/1301.2158,https://arxiv.org/pdf/1301.2158,"Authors:CaseyC.Bennett,KrisHauser","        In the modern healthcare system, rapidly expanding costs/complexity, the growing myriad of treatment options, and exploding information streams that often do not effectively reach the front lines hinder the ability to choose optimal treatment decisions over time. The goal in this paper is to develop a general purpose (non-disease-specific) computational/artificial intelligence (AI) framework to address these challenges. This serves two potential functions: 1) a simulation environment for exploring various healthcare policies, payment methodologies, etc., and 2) the basis for clinical artificial intelligence - an AI that can think like a doctor. This approach combines Markov decision processes and dynamic decision networks to learn from clinical data and develop complex plans via simulation of alternative sequential decision paths while capturing the sometimes conflicting, sometimes synergistic interactions of various components in the healthcare system. It can operate in partially observable environments (in the case of missing observations or data) by maintaining belief states about patient health status and functions as an online agent that plans and re-plans. This framework was evaluated using real patient data from an electronic health record. Such an AI framework easily outperforms the current treatment-as-usual (TAU) case-rate/fee-for-service models of healthcare (Cost per Unit Change: 189vs.189 vs. 497) while obtaining a 30-35% increase in patient outcomes. Tweaking certain model parameters further enhances this advantage, obtaining roughly 50% more improvement for roughly half the costs. Given careful design and problem formulation, an AI simulation framework can approximate optimal decisions even in complex and uncertain environments. Future work is described that outlines potential lines of research and integration of machine learning algorithms for personalized medicine.        △ Less","10 January, 2013","cs.AI,stat.ML",10.1016/j.artmed.2012.12.003 
              Adaptation of Web services to the context based on workflow: Approach for self-adaptation of service-oriented architectures to the context          ,1211.4867,https://arxiv.org/abs/1211.4867,https://arxiv.org/pdf/1211.4867,"Authors:FaîçalFelhi,JalelAkaichi","        The emergence of Web services in the information space, as well as the advanced technology of SOA, give tremendous opportunities for users in an ambient space or distant, empowerment and organizations in various fields application, such as geolocation, E-learning, healthcare, digital government, etc.. In fact, Web services are a solution for the integration of distributed information systems, autonomous, heterogeneous and self-adaptable to the context. However, as Web services can evolve in a dynamic environment in a well-defined context and according to events automatically, such as time, temperature, location, authentication, etc.. We are interested in improving their SOA to empower the Web services to be self adaptive contexts. In this paper, we propose a new trend of self adaptability of Web services context. Then applying these requirements in the architecture of the platform of adaptability to context WComp, by integrating the workflow. Our work is illustrated by a case study of authentication.        △ Less","20 November, 2012",cs.SE,10.5121/ijwest.2012.3401 
              Building a health care data warehouse for cancer diseases          ,1211.4371,https://arxiv.org/abs/1211.4371,https://arxiv.org/pdf/1211.4371,"Authors:OsamaEl-SayedSheta,AhmedNourEldeen","        This paper presents architecture for health care data warehouse specific to cancer diseases which could be used by executive managers, doctors, physicians and other health professionals to support the healthcare process. The data today existing in multi-sources with different formats makes it necessary to have some techniques for data integration. Executive managers need access to Information so that decision makers can react in real time to changing needs. Information is one of the most factors to an organization success that executive managers or physicians would need to base their decisions on, during decision making. A health care data warehouse is therefore necessary to integrate the different data sources into a central data repository and analysis this data.        △ Less","19 November, 2012",cs.DB,10.5121/ijdms.2012.4503 
              Dynamic Decision Support System Based on Bayesian Networks Application to fight against the Nosocomial Infections          ,1211.2126,https://arxiv.org/abs/1211.2126,https://arxiv.org/pdf/1211.2126,"Authors:HelaLtifi,GhadaTrabelsi,MounirBenAyed,AdelM.Alimi","        The improvement of medical care quality is a significant interest for the future years. The fight against nosocomial infections (NI) in the intensive care units (ICU) is a good example. We will focus on a set of observations which reflect the dynamic aspect of the decision, result of the application of a Medical Decision Support System (MDSS). This system has to make dynamic decision on temporal data. We use dynamic Bayesian network (DBN) to model this dynamic process. It is a temporal reasoning within a real-time environment; we are interested in the Dynamic Decision Support Systems in healthcare domain (MDDSS).        △ Less","9 November, 2012","cs.AI,cs.DB",
              An ICT Enhanced Life Quality for the Elderly in Developing Countries: Analysis Study Applied to Sri Lanka          ,1211.2033,https://arxiv.org/abs/1211.2033,https://arxiv.org/pdf/1211.2033,"Authors:MohamedFirdhous,P.M.Karunaratne","        In the recent years, the entire world has seen a tremendous increase in the elderly population. Even though countries differ in the numerical criterion for defining the old age, the UN has agreed that the cutoff of 60+ years refers to the older population. When the percentage of older population increases in a country, the country faces new challenges in the form of economical as well as social impacts. The economic impacts are felt in the form of a shortfall in labor supply, income, household savings while an increase in the payment of retirement benefits and healthcare expenditures. On the social side, the elderly population feels continuously isolated due to the changes in the value system. The United Nations has identified five quality of life characteristics for the elderly in its 1991 resolution named the United Nations Principles for Older Persons. In this paper the authors initially look at the trends in the increase of aged population in Asia in general and Sri Lanka in particular. Then they discuss how the quality of life characteristics can be achieved in a cost effective way through the use of Information and Communication Technology (ICT). The authors take an in depth look at the recent developments in field of ICT and its penetration in developing countries especially Sri Lanka with special emphasis to constraints and challenges in adopting ICT for the improving the quality of life of elderly.        △ Less","8 November, 2012",cs.CY,
              An Adaptive parameter free data mining approach for healthcare application          ,1211.1788,https://arxiv.org/abs/1211.1788,https://arxiv.org/pdf/1211.1788,"Authors:DiptiPatil,Dr.VijayM.Wadhai,MayuriGund,RichaBiyani,SnehalAndhalkar,BhagyashreeAgrawal","        In today's world, healthcare is the most important factor affecting human life. Due to heavy work load it is not possible for personal healthcare. The proposed system acts as a preventive measure for determining whether a person is fit or unfit based on person's historical and real time data by applying clustering algorithms like K-means and D-stream. The Density-based clustering algorithm i.e. the D-stream algorithm overcomes drawbacks of K-Means algorithm. By calculating their performance measures we finally find out effectiveness and efficiency of both the algorithms. Both clustering algorithms are applied on patient's bio-medical historical database. To check the correctness of both the algorithms, we apply them on patient's current bio-medical data.        △ Less","8 November, 2012",cs.DB,
              An Automated Petri-Net Based Approach for Change Management in Distributed Telemedicine Environment          ,1210.6076,https://arxiv.org/abs/1210.6076,https://arxiv.org/pdf/1210.6076,"Authors:SabriMtibaa,MoncefTagina","        The worldwide healthcare industry is facing a number of daunting challenges which are forcing healthcare systems worldwide to adapt and transform, and will ultimately completely redefine the way they do business and deliver care for patients. In this paper, we present a distributed telemedicine environement reaping from both the benefits of Service Oriented Approach (SOA) and the strong telecoms capabilities. We propose an automated approach to handle changes in a distributed telemedicine environement. A combined Petri nets model to handle changes and Reconfigurable Petri nets model to react to these changes are used to fulfill telemedicine functional and non functional requirements.        △ Less","19 October, 2012","cs.SE,cs.SY",
              Managing Changes in Citizen-Centric Healthcare Service Platform using High Level Petri Net          ,1210.5516,https://arxiv.org/abs/1210.5516,https://arxiv.org/pdf/1210.5516,"Authors:SabriMtibaa,MoncefTagina","        The healthcare organizations are facing a number of daunting challenges pushing systems to deal with requirements changes and benefit from modern technologies and telecom capabilities. Systems evolution through extension of the existing information technology infrastructure becomes one of the most challenging aspects of healthcare and the adaptation to changes is a must. The paper presents a change management framework for a citizen-centric healthcare service platform. A combination between Petri nets model to handle changes and reconfigurable Petri nets model to react to these changes are introduced to fulfill healthcare goals. Thanks to this management framework model, consistency and correctness of a healthcare processes in the presence of frequent changes can be checked and guaranteed.        △ Less","19 October, 2012","cs.SE,cs.SY",
              Timing Constraints Support on Petri-Net Model for Healthcare System Design          ,1210.5374,https://arxiv.org/abs/1210.5374,https://arxiv.org/pdf/1210.5374,"Authors:SabriMtibaa,MoncefTagina","        The worldwide healthcare organizations are facing a number of daunting challenges forcing systems to benefit from modern technologies and telecom capabilities. Hence, systems evolution through extension of the existing information technology infrastructure becomes one of the most challenging aspects of healthcare. In this paper, we present a newly architecture for evolving healthcare systems towards a service-oriented architecture. Since healthcare process exists in temporal context, timing constraints satisfiability verification techniques are growing to enable designers to test and repair design errors. Thanks to Hierarchical Timed Predicate Petri-Net based conceptual framework, desirable properties such as deadlock free and safe as well as timing constraints satisfiability can be easily checked by designer.        △ Less","19 October, 2012","cs.SE,cs.SY",
              Whole Genome Sequencing: Innovation Dream or Privacy Nightmare?          ,1210.4820,https://arxiv.org/abs/1210.4820,/search/?searchtype=author&query=De+Cristofaro%2C+E,Authors:EmilianoDeCristofaro,"        Over the past several years, DNA sequencing has emerged as one of the driving forces in life-sciences, paving the way for affordable and accurate whole genome sequencing. As genomes represent the entirety of an organism's hereditary information, the availability of complete human genomes prompts a wide range of revolutionary applications. The hope for improving modern healthcare and better understanding the human genome propels many interesting and challenging research frontiers. Unfortunately, however, the proliferation of human genomes amplifies worrisome privacy concerns, since a genome represents a treasure trove of highly personal and sensitive information. In this article, we provide an overview of positive results and biomedical advances in the field, and discuss privacy issues associated with human genomic information. Finally, we survey available privacy-enhancing technologies and list a number of open research challenges.        △ Less","16 February, 2015","cs.CR,cs.ET,q-bio.GN",
              Semantic integration and analysis of clinical data          ,1210.4405,https://arxiv.org/abs/1210.4405,https://arxiv.org/pdf/1210.4405,"Authors:HongSun,KristofDepraetere,JosDeRoo,BorisDeVloed,GiovanniMels,DirkColaert","        There is a growing need to semantically process and integrate clinical data from different sources for Clinical Data Management and Clinical Decision Support in the healthcare IT industry. In the clinical practice domain, the semantic gap between clinical information systems and domain ontologies is quite often difficult to bridge in one step. In this paper, we report our experience in using a two-step formalization approach to formalize clinical data, i.e. from database schemas to local formalisms and from local formalisms to domain (unifying) formalisms. We use N3 rules to explicitly and formally state the mapping from local ontologies to domain ontologies. The resulting data expressed in domain formalisms can be integrated and analyzed, though originating from very distinct sources. Practices of applying the two-step approach in the infectious disorders and cancer domains are introduced.        △ Less","24 October, 2012",cs.DB,
              On the Privacy of Optimization Approaches          ,1210.3283,https://arxiv.org/abs/1210.3283,https://arxiv.org/pdf/1210.3283,"Authors:PradeepChathurangaWeeraddana,GeorgeAthanasiou,MartinJakobsson,CarloFischione,JohnS.Baras","        Ensuring privacy of sensitive data is essential in many contexts, such as healthcare data, banks, e-commerce, wireless sensor networks, and social networks. It is common that different entities coordinate or want to rely on a third party to solve a specific problem. At the same time, no entity wants to publish its problem data during the solution procedure unless there is a privacy guarantee. Unlike cryptography and differential privacy based approaches, the methods based on optimization lack a quantification of the privacy they can provide. The main contribution of this paper is to provide a mechanism to quantify the privacy of a broad class of optimization approaches. In particular, we formally define a one-to-many relation, which relates a given adversarial observed message to an uncertainty set of the problem data. This relation quantifies the potential ambiguity on problem data due to the employed optimization approaches. The privacy definitions are then formalized based on the uncertainty sets. The properties of the proposed privacy measure is analyzed. The key ideas are illustrated with examples, including localization, average consensus, among others.        △ Less","13 June, 2014","cs.CR,cs.DC",
              The Enemy Within: The Emerging Threats to Healthcare from Malicious Mobile Devices          ,1210.2149,https://arxiv.org/abs/1210.2149,https://arxiv.org/pdf/1210.2149,"Authors:ShamsZawoad,RagibHasan","        With the proliferation of wireless networks, mobile devices and medical devices are increasingly being equipped with wireless interfaces, such as Bluetooth and WiFi to allow easy access to and control of the medical devices. Unfortunately, the very presence and usage of such interfaces also expose the medical devices to novel attacks from malicious parties. The emerging threat from malicious mobile devices is significant and severe, since attackers can steal confidential data from a patient's medical device. Also, attackers can compromise the medical device and either feed doctors bad data from it or issue potentially fatal commands to the device, which may even result in the death of the patient. As the mobile devices are often at close proximity to the patient (either in the hospital or home settings), attacks from such devices are hard to prevent. In this paper, we present a systematic analysis of this new threat from mobile devices on medical devices and healthcare infrastructure. We also perform a thorough security analysis of a major hospital and uncover potential vulnerabilities. Finally, we propose a set of potential solutions and defenses against such attacks.        △ Less","8 October, 2012","cs.CR,cs.NI",
              Needle-free injection into skin and soft matter with highly focused microjets          ,1210.1907,https://arxiv.org/abs/1210.1907,https://arxiv.org/pdf/1210.1907,"Authors:YoshiyukiTagawa,NikolaiOudalov,A.ElGhalbzouri,ChaoSun,DetlefLohse","        The development of needle-free drug injection systems is of great importance to global healthcare. However, in spite of its great potential and research history over many decades, these systems are not commonly used. One of the main problems is that existing methods use diffusive jets, which result in scattered penetration and severe deceleration of the jets, causing frequent pain and insufficient penetration. Another longstanding challenge is the development of accurate small volume injections. In this paper we employ a novel method of needle-free drug injection, using highly-focused high speed microjets, which aims to solve these challenges. We experimentally demonstrate that these unique jets are able to penetrate human skin: the focused nature of these microjets creates an injection spot smaller than a mosquito's proboscis and guarantees a high percentage of the liquid being injected. The liquid substances can be delivered to a much larger depth than conventional methods, and create a well-controlled dispersion pattern. Thanks to the excellent controllability of the microjet, small volume injections become feasible. Furthermore, the penetration dynamics is studied through experiments performed on gelatin mixtures (human soft tissue equivalent) and human skin, agreeing well with a viscous stress model which we develop. This model predicts the depth of the penetration into both human skin and soft tissue. The results presented here take needle-free injections a step closer to widespread use.        △ Less","15 October, 2012",physics.flu-dyn,
              Empowered Customers in E-Health Business Process          ,1208.2620,https://arxiv.org/abs/1208.2620,https://arxiv.org/pdf/1208.2620,"Authors:MuhammadAnshari,MohammadNabilAlmunawar","        E-health innovations support empowered customers. It offers the ability for customers to have greater control and ready access applications of health information, clinical information, and social interaction between interested groups. However, providing empowerment in any state of interaction levels to customers (patients) in a healthcare organization is challenging tasks. Customers are empowered in the sense of controlling the process of interaction between a firm with its customers, and among customers themselves. This paper discusses dimension of customers' empowerment in e-health business process. We propose reference model of Personal Health Cycle (PHC) as a holistic view of healthcare business process. The PHC is used to define and distinct electronic health record (EHR) from electronic medical record (EMR) and customers empowerment.        △ Less","7 August, 2012",cs.OH,
              Framework of Social Customer Relationship Management in E-Health Services          ,1207.6179,https://arxiv.org/abs/1207.6179,https://arxiv.org/pdf/1207.6179,"Authors:MuhammadAnshari,MohammadNabilAlmunawar","Healthcare organization is implementing Customer Relationship Management (CRM) as a strategy for managing interactions with patients involving technology to organize, automate, and coordinate business processes. Web-based CRM provides healthcare organization with the ability to broaden service beyond its usual practices in achieving a complex patient care goal, and this paper discusses and demonstrates how a new approach in CRM based on Web 2.0 or Social CRM helps healthcare organizations to improve their customer support, and at the same time avoiding possible conflicts, and promoting better healthcare to patients. A conceptual framework of the new approach will be proposed and highlighted. The framework includes some important features of Social CRM such as customer's empowerment, social interactivity between healthcare organization-patients, and patients-patients. The framework offers new perspective in building relationships between healthcare organizations and customers and among customers in e-health scenario. It is developed based on the latest development of CRM literatures and case studies analysis. In addition, customer service paradigm in social network's era, the important of online health education, and empowerment in healthcare organization will be taken into consideration.        △ Less","26 July, 2012",cs.CY,10.5171/2012.766268 
              Customer Empowerment in Healthcare Organisations Through CRM 2.0: Survey Results from Brunei Tracking a Future Path in E-Health Research          ,1207.6164,https://arxiv.org/abs/1207.6164,https://arxiv.org/pdf/1207.6164,"Authors:MuhammadAnshari,MohammadN.Almunawar,PatrickK.C.Low,ZawWint","        Customer Relationship Management (CRM) with the Web technology provides healthcare organizations the ability to broaden services beyond its usual practices, and thus provides a particular advantageous environment to achieve complex e-health goals. This paper discusses and demonstrates how a new approach in CRM based on Web 2.0 namely CRM 2.0 will help customers to have greater control in the sense of controlling the process of interaction (empowerment) between healthcare organizations with its customers, and among customers themselves. A survey was conducted to gather preliminary requirements and expectations on empowerment in Brunei. The survey revealed that there is a high demand for empowering customers in Brunei through the Web. Regardless of the limitations of the survey, the general public has responded with a great support for the capabilities of empowerment listed from the questionnaires. The data were analyzed to provide initial ideas and recommendation to a future direction on research for customers' empowerment in e-health services.        △ Less","26 July, 2012","cs.CY,cs.SI",
              Ubiquitous HealthCare in Wireless Body Area Networks          ,1207.2240,https://arxiv.org/abs/1207.2240,https://arxiv.org/pdf/1207.2240,"Authors:N.A.Khan,N.Javaid,Z.A.Khan,M.Jaffar,U.Rafiq,A.Bibi","        Recent advances in wireless communications, system on chip and low power sensor nodes allow realization of Wireless Body Area Networks (WBANs).WBANs comprise of tiny sensors, which collect information of a patient's vital signs and provide a real time feedback. In addition,WBANs also support many applications including ubiquitous healthcare, entertainment, gaming, military, etc. Ubiquitous healthcare is required by elderly people to facilitate them with instant monitoring anywhere they move around. In this paper, we provide a survey on different architectures used in WBANs for ubiquitous healthcare monitoring. Different standards and devices used in these architectures are also discussed in this paper. Finally, path loss in WBANs and its impact on communication is presented with the help of simulations performed for different models of In-Body communication and different factors (such as, attenuation, frequency, distance etc) influencing path loss in On-Body communications.        △ Less","10 July, 2012",cs.NI,
              A Complex Systems Science Approach to Healthcare Costs and Quality          ,1207.0034,https://arxiv.org/abs/1207.0034,https://arxiv.org/pdf/1207.0034,"Authors:YaneerBar-YamwithShlomiyaBar-Yam,KarlaZ.Bertrand,NancyCohen,AlexanderS.Gard-Murray,HelenP.Harte,LuciLeykum","        There is a mounting crisis in delivering affordable healthcare in the US. For decades, key decision makers in the public and private sectors have considered cost-effectiveness in healthcare a top priority. Their actions have focused on putting a limit on fees, services, or care options. However, they have met with limited success as costs have increased rapidly while the quality isn't commensurate with the high costs. A new approach is needed. Here we provide eight scientifically-based steps for improving the healthcare system. The core of the approach is promoting the best use of resources by matching the people and organization to the tasks they are good at, and providing the right incentive structure. Harnessing costs need not mean sacrificing quality. Quality service and low costs can be achieved by making sure the right people and the right organizations deliver services. As an example, the frequent use of emergency rooms for non-emergency care demonstrates the waste of resources of highly capable individuals and facilities resulting in high costs and ineffective care. Neither free markets nor managed care guarantees the best use of resources. A different oversight system is needed to promote the right incentives. Unlike managed care, effective oversight must not interfere with the performance of care. Otherwise, cost control only makes care more cumbersome. The eight steps we propose are designed to dramatically improve the effectiveness of the healthcare system, both for those who receive services and those who provide them.        △ Less","29 June, 2012","physics.soc-ph,cs.CY",
              Opportunities in Delivery of Preventive Services in Retail Settings          ,1207.0029,https://arxiv.org/abs/1207.0029,https://arxiv.org/pdf/1207.0029,"Authors:YaneerBar-Yam,DionHarmon,KeithNesbitt,MayLim,SuzanneSmith,BradleyA.Perkins","        Recommended clinical preventive services are not being delivered despite well-documented benefits. Here we show that transferring simple and repetitive preventive services to nurse-staffed retail clinics provides an opportunity for dramatically improving their delivery. For each of 35 high-benefit, cost-effective preventive services, we identify required training, number of repetitions, and time and cost for full coverage in the US. We determine that full delivery through physician-based practices would require an unrealistic 400,000 full-time personnel. We estimate the efficiency gains from implementation at nurse-staffed clinics at retail locations for 28 services. Widespread adoption would result in a five-fold reduction in variable costs and three-fold reduction in personnel. By elevating the benefit-to-cost ratio, retail implementation can expedite widespread prevention coverage and help transform US healthcare.        △ Less","29 June, 2012","physics.soc-ph,physics.med-ph",
              Usage Management of Personal Health Records          ,1206.5451,https://arxiv.org/abs/1206.5451,https://arxiv.org/pdf/1206.5451,"Authors:ChristopherC.Lamb,GregoryL.Heileman,PramodA.Jamkhedkar","        Personal health record (PHR) management is under new scrutiny as private companies move into the market and government agencies actively address perceived health care distribution inequalities and inefficiencies. Current systems are coarse-grained and provide consumers very little actual control over their data. Herein, we propose an alternative system for managing the use of healthcare information. This novel system is finer grained, allows for data mining and repackaging, and gives users more control over their data, allowing it to be distributed to their specifications. In this paper, we outline the characteristics of such a system in different contexts, present relevant background information and research leading to the system design, and cover specific usage scenarios supported by this system that are difficult to control using simpler access control strategies.        △ Less","23 June, 2012",cs.CY,
              Home Healthcare Process: Challenges and Open Issues          ,1206.5430,https://arxiv.org/abs/1206.5430,https://arxiv.org/pdf/1206.5430,"Authors:SelmaArbaoui,N.Cislo,N.Smith-Guerin","        Home healthcare is part of the most critical research and development healthcare areas. The objective is to decentralize healthcare, leading to a shift from in-hospital care to more advanced home healthcare, while improving efficiency, individualisation, equity and quality of healthcare delivery and limiting financial resources. In this paper, we adopt a process approach to tackle the home healthcare domain in order to highlight the importance of organisational aspects in the success of an ICT-home healthcare project. Such projects should be supported by an automated system, called in this paper, Home Healthcare support system. We examine HH processes from two selected perspectives (complexity and dynamics) to illustrate the requirements of a HH support system. We advocate that satisfying these requirements is part of the most important challenges in the home healthcare research domain and we propose first track of solutions by attempting to benefit from past experiences in 3 process research communities.        △ Less","23 June, 2012",cs.SE,
              A Study on Internet of Things based Applications          ,1206.3891,https://arxiv.org/abs/1206.3891,https://arxiv.org/pdf/1206.3891,"Authors:DeekshaJain,P.VenkataKrishna,V.Saritha","        This paper gives a detail analysis of various applications based on Internet of Thing (IoT)s. This explains about how internet of things evolved from mobile computing and ubiquitous computing. It emphasises the fact that objects are connected over the internet rather than people. The properties of Internet of Things (IOT) are product information, electronic tag, standard expressed and uploading information. It utilises the Radio Frequency Identification (RFID) technology and wireless sensor networks (WSN). IOT applications are used in domains such as healthcare, supply chain management, defence and agriculture. Lastly the paper focuses on issues involved in IOT. Though it is a boon, IOT faces certain crucial issues like privacy and security.        △ Less","18 June, 2012",cs.NI,
              Clinical Productivity System - A Decision Support Model          ,1206.0021,https://arxiv.org/abs/1206.0021,https://arxiv.org/pdf/1206.0021,Authors:CaseyC.Bennett,"        Purpose: This goal of this study was to evaluate the effects of a data-driven clinical productivity system that leverages Electronic Health Record (EHR) data to provide productivity decision support functionality in a real-world clinical setting. The system was implemented for a large behavioral health care provider seeing over 75,000 distinct clients a year. Design/methodology/approach: The key metric in this system is a ""VPU"", which simultaneously optimizes multiple aspects of clinical care. The resulting mathematical value of clinical productivity was hypothesized to tightly link the organization's performance to its expectations and, through transparency and decision support tools at the clinician level, affect significant changes in productivity, quality, and consistency relative to traditional models of clinical productivity. Findings: In only 3 months, every single variable integrated into the VPU system showed significant improvement, including a 30% rise in revenue, 10% rise in clinical percentage, a 25% rise in treatment plan completion, a 20% rise in case rate eligibility, along with similar improvements in compliance/audit issues, outcomes collection, access, etc. Practical implications: A data-driven clinical productivity system employing decision support functionality is effective because of the impact on clinician behavior relative to traditional clinical productivity systems. Critically, the model is also extensible to integration with outcomes-based productivity. Originality/Value: EHR's are only a first step - the problem is turning that data into useful information. Technology can leverage the data in order to produce actionable information that can inform clinical practice and decision-making. Without additional technology, EHR's are essentially just copies of paper-based records stored in electronic form.        △ Less","31 May, 2012",cs.DB,10.1108/17410401111112014 
              Restructuring the Italian NHS: a case study of the regional hospital network          ,1205.3519,https://arxiv.org/abs/1205.3519,https://arxiv.org/pdf/1205.3519,Authors:CarloCastellana,"        One of the main issues affecting the Italian NHS is the healthcare deficit: according to current agreements between the Italian State and its Regions, public funding of regional NHS is now limited to the amount of regional deficit and is subject to previous assessment of strict adherence to constraint on regional healthcare balance sheet. Many Regions with previously uncontrolled healthcare deficit have now to plan their ""Piano di Rientro"" (PdR) and submit it for the approval of the Italian Ministry of Economy and Finances. Those Regions that will fail to comply to deficit constraints will suffer cuts on their public NHS financing. A smart Health Planning can make sure health spending is managed appropriately. Indeed a restructuring of the Italian healthcare system has recently been enforced in order to cope for the clumsy regional healthcare balance sheets. Half of total Italian healthcare expenditure is accounted by hospital services which therefore configure as one of the main restructuring targets. This paper provides a general framework for planning a re-engineering of a hospital network. This framework is made of economic, legal and healthcare constraints. We apply the general framework to the particular case of Puglia region and explore a set of re-engineered solutions which to different extent could help solve the difficult dilemma: cutting costs without worsening the delivery of public healthcare services.        △ Less","15 May, 2012",q-fin.GN,
              Impact of the economic crisis on the Italian public healthcare expenditure          ,1205.2863,https://arxiv.org/abs/1205.2863,https://arxiv.org/pdf/1205.2863,Authors:CarloCastellana,"        The global financial crisis, beginning in 2008, took an historic toll on national economies around the world. Following equity market crashes, unemployment rates rose significantly in many countries: Italy was among those. What will be the impact of such large shocks on Italian healthcare finances? An empirical model for estimating the impact of the crisis on Italian public healthcare expenditure is presented. Based on data from epidemiological studies related to past economic crisis, the financial impact is estimated to be comparable to the healthcare deficit of Italian Regions (EUR 3-5 bn). According to current agreements between the Italian State and its Regions, public funding of regional National Health Services (NHSs) is limited to the amount of regional deficit and is subject to previous assessment of strict adherence to constraint on regional healthcare balance-sheet. Those Regions that will fail to comply to balance-sheet constraints will suffer cuts on their public NHS financing with foreseeable bad consequences for the health of their regional population. The current crisis could be a good timing for a large-scale re-engineering of the Italian NHS, probably the only way for self-sustainability of the public system.        △ Less","13 May, 2012",q-fin.GN,
"              EHRs Connect Research and Practice: Where Predictive Modeling, Artificial Intelligence, and Clinical Decision Support Intersect          ",1204.4927,https://arxiv.org/abs/1204.4927,https://arxiv.org/pdf/1204.4927,"Authors:CaseyBennett,TomDoub,RebeccaSelove","        Objectives: Electronic health records (EHRs) are only a first step in capturing and utilizing health-related data - the challenge is turning that data into useful information. Furthermore, EHRs are increasingly likely to include data relating to patient outcomes, functionality such as clinical decision support, and genetic information as well, and, as such, can be seen as repositories of increasingly valuable information about patients' health conditions and responses to treatment over time. Methods: We describe a case study of 423 patients treated by Centerstone within Tennessee and Indiana in which we utilized electronic health record data to generate predictive algorithms of individual patient treatment response. Multiple models were constructed using predictor variables derived from clinical, financial and geographic data. Results: For the 423 patients, 101 deteriorated, 223 improved and in 99 there was no change in clinical condition. Based on modeling of various clinical indicators at baseline, the highest accuracy in predicting individual patient response ranged from 70-72% within the models tested. In terms of individual predictors, the Centerstone Assessment of Recovery Level - Adult (CARLA) baseline score was most significant in predicting outcome over time (odds ratio 4.1 + 2.27). Other variables with consistently significant impact on outcome included payer, diagnostic category, location and provision of case management services. Conclusions: This approach represents a promising avenue toward reducing the current gap between research and practice across healthcare, developing data-driven clinical decision support based on real-world populations, and serving as a component of embedded clinical artificial intelligences that ""learn"" over time.        △ Less","22 April, 2012","cs.AI,cs.DB,stat.ML",10.1016/j.hlpt.2012.03.001 
              Energy-Delay Tradeoff and Dynamic Sleep Switching for Bluetooth-Like Body-Area Sensor Networks          ,1204.4840,https://arxiv.org/abs/1204.4840,https://arxiv.org/pdf/1204.4840,"Authors:EricRebeiz,GiuseppeCaire,AndreasF.Molisch","        Wireless technology enables novel approaches to healthcare, in particular the remote monitoring of vital signs and other parameters indicative of people's health. This paper considers a system scenario relevant to such applications, where a smart-phone acts as a data-collecting hub, gathering data from a number of wireless-capable body sensors, and relaying them to a healthcare provider host through standard existing cellular networks. Delay of critical data and sensors' energy efficiency are both relevant and conflicting issues. Therefore, it is important to operate the wireless body-area sensor network at some desired point close to the optimal energy-delay tradeoff curve. This tradeoff curve is a function of the employed physical-layer protocol: in particular, it depends on the multiple-access scheme and on the coding and modulation schemes available. In this work, we consider a protocol closely inspired by the widely-used Bluetooth standard. First, we consider the calculation of the minimum energy function, i.e., the minimum sum energy per symbol that guarantees the stability of all transmission queues in the network. Then, we apply the general theory developed by Neely to develop a dynamic scheduling policy that approaches the optimal energy-delay tradeoff for the network at hand. Finally, we examine the queue dynamics and propose a novel policy that adaptively switches between connected and disconnected (sleeping) modes. We demonstrate that the proposed policy can achieve significant gains in the realistic case where the control ""NULL"" packets necessary to maintain the connection alive, have a non-zero energy cost, and the data arrival statistics corresponding to the sensed physical process are bursty.        △ Less","21 April, 2012","cs.IT,cs.NI",10.1109/TCOMM.2012.12.110143A 
              Utilizing RxNorm to Support Practical Computing Applications: Capturing Medication History in Live Electronic Health Records          ,1204.4093,https://arxiv.org/abs/1204.4093,https://arxiv.org/pdf/1204.4093,Authors:CaseyBennett,"        RxNorm was utilized as the basis for direct-capture of medication history data in a live EHR system deployed in a large, multi-state outpatient behavioral healthcare provider in the United States serving over 75,000 distinct patients each year across 130 clinical locations. This tool incorporated auto-complete search functionality for medications and proper dosage identification assistance. The overarching goal was to understand if and how standardized terminologies like RxNorm can be used to support practical computing applications in live EHR systems. We describe the stages of implementation, approaches used to adapt RxNorm's data structure for the intended EHR application, and the challenges faced. We evaluate the implementation using a four-factor framework addressing flexibility, speed, data integrity, and medication coverage. RxNorm proved to be functional for the intended application, given appropriate adaptations to address high-speed input/output (I/O) requirements of a live EHR and the flexibility required for data entry in multiple potential clinical scenarios. Future research around search optimization for medication entry, user profiling, and linking RxNorm to drug classification schemes holds great potential for improving the user experience and utility of medication data in EHRs.        △ Less","15 August, 2012","cs.DB,cs.HC",10.1016/j.jbi.2012.02.011 
              E-Health Initiative and Customer's Expectation: Case Brunei          ,1204.3691,https://arxiv.org/abs/1204.3691,https://arxiv.org/pdf/1204.3691,"Authors:MohammadNabilAlmunawar,ZawWint,PatrickKimChengLow,MuhammadAnshari","        This paper is to determine the dimension of e-health services in Brunei Darussalam (Brunei) from customers' perspective. It is to identify, understand, analyze and evaluate public's expectation on e-health in Brunei. A questionnaire was designed to gather quantitative and qualitative data to survey patients, patient's family, and health practitioners at hospitals, clinics, or home care centers in Brunei starting from February to March, 2011. A 25-item Likert-type survey instrument was specifically developed for this study and administered to a sample of 366 patients. The data were analyzed to provide initial ideas and recommendation to policy makers on how to move forward with the e-health initiative as a mean to improve healthcare services. The survey revealed that there exists a high demand and expectation from people in Brunei to have better healthcare services accessible through an e-health system in order to improve health literacy as well as quality and efficiency of healthcare. Regardless of the limitations of the survey, the general public has responded with a great support for the capabilities of an e-health system listed from the questionnaires. The results of the survey provide a solid foundation for our on going research project to proceed further to develop a model of e-health and subsequently develop a system prototype that incorporate expectations from the people.        △ Less","16 April, 2012",cs.OH,
              Evaluating CRM Implementation in Healthcare Organization          ,1204.3689,https://arxiv.org/abs/1204.3689,https://arxiv.org/pdf/1204.3689,"Authors:MuhammadAnshari,MohammadNabilAlmunawar","        Recently, many healthcare organizations are adopting CRM as a strategy, which involves using technology to organize, automate, and coordinate business processes, in managing interactions with their patients. CRM with the Web technology provides healthcare providers the ability to broaden their services beyond usual practices, and thus offers suitable environment using latest technology to achieve superb patient care. This paper discusses and demonstrates how a new approach in CRM based on Web 2.0 will help the healthcare providers improving their customer support, avoiding conflict, and promoting better health to patient. With this new approach patients will benefit from the customized personal service with full information access to perform self managed their own health. It also helps healthcare providers retaining the right customer. A conceptual framework of the new approach will be discussed.        △ Less","16 April, 2012",cs.SE,
              Improving Customer Service in Healthcare with CRM 2.0          ,1204.3685,https://arxiv.org/abs/1204.3685,https://arxiv.org/pdf/1204.3685,"Authors:MohammadNabilAlmunawar,MuhammadAnshari","        The Healthcare industry is undergoing a paradigm shift from healthcare institution-centred care to a citizen-centred care that emphasises on continuity of care from prevention to rehabilitation. The recent development of Information and Communication Technology (ICT), especially the Internet and its related technologies has become the main driver of the paradigm shift. Managing relationship with customers (patients) is becoming more important in the new paradigm. The paper discusses Customer Relationship Management (CRM) in healthcare and proposes a Social CRM or CRM 2.0 model to take advantage of the multi-way relationships created by Web 2.0 and its widespread use in improving customer services for mutual benefits between healthcare providers and their customers.        △ Less","16 April, 2012",cs.OH,
              Intra-bodyhybrid communication scheme for healthcare systems          ,1204.1420,https://arxiv.org/abs/1204.1420,https://arxiv.org/pdf/1204.1420,"Authors:AbdullahAlshehab,ChiuTungWu,NaoKobayashi,SikiengSok,ShigeruShimamoto","        Intra-body communication (IBC) is a type of Body Area Network (BAN)that utilizes human body as the medium for data transmission. Thelow power requirements of intra-body communication (IBC) as compared to near field electromagnetic waves showed that it can be a suitable solution for Medical Body Area Networks (MBANs) in a mobile health care system.In this paper, we investigate the transmission characteristics of the human body as a conductor of signals by considering different data transmission rates of multi-point to point network in order to reduce overall power consumption of the BAN.Furthermore, we utilize IBC and propose a new scheme to combines Slotted ALOHA, TDMA, and Reservation ALOHA together to increase the throughput and decrease the delay. By using our new hybrid scheme with the movable boundary designed for health status monitoring, we are able to increase the efficiency of data transmission by prioritizing the more critical data from the sensors.        △ Less","6 April, 2012",cs.ET,10.5121/ijbb.2012.21011 
              CRM 2.0 within E-Health Systems: Towards Achieving Health Literacy & Customer Satisfaction          ,1203.4309,https://arxiv.org/abs/1203.4309,https://arxiv.org/pdf/1203.4309,"Authors:MuhammadAnshari,MohammadNabilAlmunawar,PatrickKimChengLow","        Customer Relationship Management (CRM) within healthcare organization can be viewed as a strategy to attract new customers and retaining them throughout their entire lifetime of relationships. At the same time, the advancement of Web technology known as Web 2.0 plays a significant part in the CRM transition which drives social change that impacts all institutions including business and healthcare organizations. This new paradigm has been named as Social CRM or CRM 2.0 because it is based on Web 2.0. We conducted survey to examine the features of CRM 2.0 in healthcare scenario to the customer in Brunei Darussalam. We draw the conclusion that the CRM 2.0 in healthcare technologies has brought a possibility to extend the services of e-health by enabling patients, patient's families, and community at large to participate more actively in the process of health education; it helps improve health literacy through empowerment, social networking process, and online health educator. This paper is based on our works presented at ICID 2011.        △ Less","31 July, 2012",cs.OH,
              Health Information Systems (HIS): Concept and Technology          ,1203.3923,https://arxiv.org/abs/1203.3923,https://arxiv.org/pdf/1203.3923,"Authors:MohammadNabilAlmunawar,MuhammadAnshari","        A health information system (HIS) is the intersection of between healthcare's business process, and information systems to deliver better healthcare services. The nature of healthcare industry, which is highly influenced by economic, social, politic, and technological factors, has changed over time. This paper will address some important concepts of healthcare and related terminologies to provide a holistic view for HIS. Related technological milestones and major events are briefly summarized. The trends and rapid development of health information technologies are also discussed.        △ Less","18 March, 2012",cs.OH,
              Building Healthcare - Patient Relationship with CRM 2.0: Lesson Learnt from Prita Mulyasari's Case          ,1203.3919,https://arxiv.org/abs/1203.3919,https://arxiv.org/pdf/1203.3919,"Authors:MuhammadAnshari,MohammadNabilAlmunawar","Healthcare is implementing CRM as a strategy for managing interactions and communication with patients which involves using Information and Communication Technology (ICT) to organize, automate, and coordinate business processes. CRM with the Web technology provides healthcare the ability to broaden service beyond its usual practices, and thus provides a particular advantageous environment for them that want to use ICT to achieve complex healthcare goal. This paper we will discuss and demonstrate how a new approach in CRM will help the healthcare increasing their customer support, and promoting better health to patient. The patients benefited from the customized personal service so that they have full information access to perform self managed their own health and the healthcare provider will have a loyal and retains the right customer. A conceptual framework of approach will be highlighted. Customer centric paradigm in social network's era and value creation of healthcare's business process will be taken into consideration.        △ Less","31 July, 2012",cs.OH,
              Geometric Pricing: How Low Dimensionality Helps in Approximability          ,1202.2840,https://arxiv.org/abs/1202.2840,https://arxiv.org/pdf/1202.2840,"Authors:ParinyaChalermsook,KhaledElbassioni,DanuponNanongkai,HeSun","        Consider the following toy problem. There are mm rectangles and nn points on the plane. Each rectangle RR is a consumer with budget BRB_R, who is interested in purchasing the cheapest item (point) inside R, given that she has enough budget. Our job is to price the items to maximize the revenue. This problem can also be defined on higher dimensions. We call this problem the geometric pricing problem.  In this paper, we study a new class of problems arising from a geometric aspect of the pricing problem. It intuitively captures typical real-world assumptions that have been widely studied in marketing research, healthcare economics, etc. It also helps classify other well-known pricing problems, such as the highway pricing problem and the graph vertex pricing problem on planar and bipartite graphs. Moreover, this problem turns out to have close connections to other natural geometric problems such as the geometric versions of the unique coverage and maximum feasible subsystem problems.  We show that the low dimensionality arising in this pricing problem does lead to improved approximation ratios, by presenting sublinear-approximation algorithms for two central versions of the problem: unit-demand uniform-budget min-buying and single-minded pricing problems. Our algorithm is obtained by combining algorithmic pricing and geometric techniques. These results suggest that considering geometric aspect might be a promising research direction in obtaining improved approximation algorithms for such pricing problems. To the best of our knowledge, this is one of very few problems in the intersection between geometry and algorithmic pricing areas. Thus its study may lead to new algorithmic techniques that could benefit both areas.        △ Less","24 July, 2012","cs.GT,cs.CG,cs.DS",
              Multi databases in Health Care Networks          ,1112.4099,https://arxiv.org/abs/1112.4099,https://arxiv.org/pdf/1112.4099,"Authors:NadirK.Salih,TianyiZang,MingruiSun","        E-Health is a relatively recent term for healthcare practice supported by electronic processes and communication, dating back to at least 1999. E-Health is greatly impacting on information distribution and availability within the health services, hospitals and to the public. E-health was introduced as the death of telemedicine, because - in the context of a broad availability of medical information systems that can interconnect and communicate - telemedicine will no longer exist as a specific field. The same could also be said for any other traditional field in medical informatics, including information systems and electronic patient records. E-health presents itself as a common name for all such technological fields. In this paper we focuses in multi database by determined some sites and distributed it in Homogenous way. This will be followed by an illustrative example as related works. Finally, the paper concludes with general remarks and a statement of further work.        △ Less","17 December, 2011",cs.OH,
              Ontology-Based Emergency Management System in a Social Cloud          ,1112.2067,https://arxiv.org/abs/1112.2067,https://arxiv.org/pdf/1112.2067,"Authors:Bhuvaneswari.A,Dr.G.R.Karpagam","        The need for Emergency Management continually grows as the population and exposure to catastrophic failures increase. The ability to offer appropriate services at these emergency situations can be tackled through group communication mechanisms. The entities involved in the group communication include people, organizations, events, locations and essential services. Cloud computing is a ""as a service"" style of computing that enables on-demand network access to a shared pool of resources. So this work focuses on proposing a social cloud constituting group communication entities using an open source platform, Eucalyptus. The services are exposed as semantic web services, since the availability of machine-readable metadata (Ontology) will enable the access of these services more intelligently. The objective of this paper is to propose an Ontology-based Emergency Management System in a social cloud and demonstrate the same using emergency healthcare domain.        △ Less","9 December, 2011",cs.SI,
              On the Long-term Health Care Crisis. A Possible Eradication Scenario          ,1112.1814,https://arxiv.org/abs/1112.1814,https://arxiv.org/pdf/1112.1814,"Authors:RaulIsea,ErW.Bai,KarlE.Lonngren","        The purpose of the present essay is to suggest a possible model to describe the worldwide healthcare crisis, where diseases that have been considered to be eradicated or under our control are re-emerging today.        △ Less","8 December, 2011","q-bio.PE,cs.CY",
              Data Mining Session-Based Patient Reported Outcomes (PROs) in a Mental Health Setting: Toward Data-Driven Clinical Decision Support and Personalized Treatment          ,1112.1670,https://arxiv.org/abs/1112.1670,https://arxiv.org/pdf/1112.1670,"Authors:CaseyBennett,ThomasDoub,AprilBragg,JasonLuellen,ChristinaVanRegenmorter,JenniferLockman,RandallReiserer","        The CDOI outcome measure - a patient-reported outcome (PRO) instrument utilizing direct client feedback - was implemented in a large, real-world behavioral healthcare setting in order to evaluate previous findings from smaller controlled studies. PROs provide an alternative window into treatment effectiveness based on client perception and facilitate detection of problems/symptoms for which there is no discernible measure (e.g. pain). The principal focus of the study was to evaluate the utility of the CDOI for predictive modeling of outcomes in a live clinical setting. Implementation factors were also addressed within the framework of the Theory of Planned Behavior by linking adoption rates to implementation practices and clinician perceptions. The results showed that the CDOI does contain significant capacity to predict outcome delta over time based on baseline and early change scores in a large, real-world clinical setting, as suggested in previous research. The implementation analysis revealed a number of critical factors affecting successful implementation and adoption of the CDOI outcome measure, though there was a notable disconnect between clinician intentions and actual behavior. Most importantly, the predictive capacity of the CDOI underscores the utility of direct client feedback measures such as PROs and their potential use as the basis for next generation clinical decision support tools and personalized treatment approaches.        △ Less","7 December, 2011","cs.AI,cs.GL",10.1109/HISB.2011.20 
              Data Mining and Electronic Health Records: Selecting Optimal Clinical Treatments in Practice          ,1112.1668,https://arxiv.org/abs/1112.1668,https://arxiv.org/pdf/1112.1668,"Authors:CaseyBennett,ThomasDoub","        Electronic health records (EHR's) are only a first step in capturing and utilizing health-related data - the problem is turning that data into useful information. Models produced via data mining and predictive analysis profile inherited risks and environmental/behavioral factors associated with patient disorders, which can be utilized to generate predictions about treatment outcomes. This can form the backbone of clinical decision support systems driven by live data based on the actual population. The advantage of such an approach based on the actual population is that it is ""adaptive"". Here, we evaluate the predictive capacity of a clinical EHR of a large mental healthcare provider (~75,000 distinct clients a year) to provide decision support information in a real-world clinical setting. Initial research has achieved a 70% success rate in predicting treatment outcomes using these methods.        △ Less","7 December, 2011",cs.DB,
              The CHRONIOUS Ontology-Driven Search Tool: Enabling Access to Focused and Up-to-Date Healthcare Literature          ,1110.2400,https://arxiv.org/abs/1110.2400,https://arxiv.org/pdf/1110.2400,"Authors:StephanKiefer,JochenRauch,RiccardoAlbertoni,MarcoAttene,FrancaGiannini,SimoneMarini,LucSchneider,CarlosMesquita,XinXing,MichaelLawo","        This paper presents an advanced search engine prototype for bibliography retrieval developed within the CHRONIOUS European IP project of the seventh Framework Program (FP7). This search engine is specifically targeted to clinicians and healthcare practitioners searching for documents related to Chronic Obstructive Pulmonary Disease (COPD) and Chronic Kidney Disease (CKD). To this aim, the presented tool exploits two pathology-specific ontologies that allow focused document indexing and retrieval. These ontologies have been developed on the top of the Middle Layer Ontology for Clinical Care (MLOCC), which provides a link with the Basic Formal Ontology, a foundational ontology used in the Open Biological and Biomedical Ontologies (OBO) Foundry. In addition link with the terms of the MeSH (Medical Subject Heading) thesaurus has been provided to guarantee the coverage with the general certified medical terms and multilingual capabilities.        △ Less","11 October, 2011",cs.DL,
              Higher Order Programming to Mine Knowledge for a Modern Medical Expert System          ,1107.4651,https://arxiv.org/abs/1107.4651,https://arxiv.org/pdf/1107.4651,"Authors:NittayaKerdprasop,KittisakKerdprasop","        Knowledge mining is the process of deriving new and useful knowledge from vast volumes of data and background knowledge. Modern healthcare organizations regularly generate huge amount of electronic data stored in the databases. These data are a valuable resource for mining useful knowledge to help medical practitioners making appropriate and accurate decision on the diagnosis and treatment of diseases. In this paper, we propose the design of a novel medical expert system based on a logic-programming framework. The proposed system includes a knowledge-mining component as a repertoire of tools for discovering useful knowledge. The implementation of classification and association mining tools based on the higher order and meta-level programming schemes using Prolog has been presented to express the power of logic-based language. Such language also provides a pattern matching facility, which is an essential function for the development of knowledge-intensive tasks. Besides the major goal of medical decision support, the knowledge discovered by our logic-based knowledge-mining component can also be deployed as background knowledge to pre-treatment data from other sources as well as to guard the data repositories against constraint violation. A framework for knowledge deployment is also presented.        △ Less","22 July, 2011","cs.LO,cs.AI",
              A compact analytical formalism for current transients in electrochemical systems          ,1106.4464,https://arxiv.org/abs/1106.4464,https://arxiv.org/pdf/1106.4464,"Authors:PradeepR.Nair,MuhammadA.Alam","        Micro and nanostructured electrodes form an integral part of a wide variety of electrochemical systems for biomolecule detection, batteries, solar cells, scanning electrochemical microscopy, etc. Given the complexity of the electrode structures, the Butler-Volmer formalism of redox reactions, and the diffusion transport of redox species, it is hardly surprising that only a few problems are amenable to closed form, compact analytical solutions. While numerical solutions are widely used, it is often difficult to integrate the insights gained to the design and optimization of electrochemical systems. In this article, we develop a comprehensive analytical formalism for current transients that not only anticipate the response of complex electrode structures to complicated voltammetry measurements, but also intuitively interpret diverse experiments such as redox detection of molecules at nanogap electrodes, scanning electrochemical microscopy, etc. The results from the analytical model, well supported through detailed numerical simulations and experimental data from literature, have broad implications for the design and optimization of nanostructured electrodes for healthcare and energy storage applications.        △ Less","22 June, 2011","physics.chem-ph,cond-mat.mes-hall,physics.bio-ph",
              Web services synchronization health care application          ,1104.5139,https://arxiv.org/abs/1104.5139,https://arxiv.org/pdf/1104.5139,"Authors:HelaLimam,JalelAkaichi","        With the advance of Web Services technologies and the emergence of Web Services into the information space, tremendous opportunities for empowering users and organizations appear in various application domains including electronic commerce, travel, intelligence information gathering and analysis, health care, digital government, etc. In fact, Web services appear to be s solution for integrating distributed, autonomous and heterogeneous information sources. However, as Web services evolve in a dynamic environment which is the Internet many changes can occur and affect them. A Web service is affected when one or more of its associated information sources is affected by schema changes. Changes can alter the information sources contents but also their schemas which may render Web services partially or totally undefined. In this paper, we propose a solution for integrating information sources into Web services. Then we tackle the Web service synchronization problem by substituting the affected information sources. Our work is illustrated with a healthcare case study.        △ Less","27 April, 2011",cs.DB,10.5121/ijwest.2011.2204 
              A Study of IEEE 802.15.4 Security Framework for Wireless Body Area Network          ,1102.0682,https://arxiv.org/abs/1102.0682,https://arxiv.org/pdf/1102.0682,"Authors:ShahnazSaleem,SanaUllah,KyungSupKwak","        A Wireless Body Area Network (WBAN) is a collection of low-power and lightweight wireless sensor nodes that are used to monitor the human body functions and the surrounding environment. It supports a number of innovative and interesting applications, including ubiquitous healthcare and Consumer Electronics (CE) applications. Since WBAN nodes are used to collect sensitive (life-critical) information and may operate in hostile environments, they require strict security mechanisms to prevent malicious interaction with the system. In this paper, we first highlight major security requirements and Denial of Service (DoS) attacks in WBAN at Physical, Medium Access Control (MAC), Network, and Transport layers. Then we discuss the IEEE 802.15.4 security framework and identify the security vulnerabilities and major attacks in the context of WBAN. Different types of attacks on the Contention Access Period (CAP) and Contention Free Period (CFP) parts of the superframe are analyzed and discussed. It is observed that a smart attacker can successfully corrupt an increasing number of GTS slots in the CFP period and can considerably affect the Quality of Service (QoS) in WBAN (since most of the data is carried in CFP period). As we increase the number of smart attackers the corrupted GTS slots are eventually increased, which prevents the legitimate nodes to utilize the bandwidth efficiently. This means that the direct adaptation of IEEE 802.15.4 security framework for WBAN is not totally secure for certain WBAN applications. New solutions are required to integrate high level security in WBAN.        △ Less","3 February, 2011",cs.NI,10.3390/s110201383 
              Multiparty Symmetric Sum Types          ,1011.6436,https://arxiv.org/abs/1011.6436,https://arxiv.org/pdf/1011.6436,"Authors:LasseNielsen,NobukoYoshida,KoheiHonda","        This paper introduces a new theory of multiparty session types based on symmetric sum types, by which we can type non-deterministic orchestration choice behaviours.  While the original branching type in session types can represent a choice made by a single participant and accepted by others determining how the session proceeds, the symmetric sum type represents a choice made by agreement among all the participants of a session. Such behaviour can be found in many practical systems, including collaborative workflow in healthcare systems for clinical practice guidelines (CPGs). Processes using the symmetric sums can be embedded into the original branching types using conductor processes.  We show that this type-driven embedding preserves typability, satisfies semantic soundness and completeness, and meets the encodability criteria adapted to the typed setting.  The theory leads to an efficient implementation of a prototypical tool for CPGs which automatically translates the original CPG specifications from a representation called the Process Matrix to symmetric sum types, type checks programs and executes them.        △ Less","29 November, 2010","cs.DC,cs.PL",10.4204/EPTCS.41.9 
              Will the US Economy Recover in 2010? A Minimal Spanning Tree Study          ,1009.5800,https://arxiv.org/abs/1009.5800,https://arxiv.org/pdf/1009.5800,"Authors:YitingZhang,GladysHuiTingLee,JianChengWong,JunLiangKok,ManamohanPrusty,SiewAnnCheong","        We calculated the cross correlations between the half-hourly times series of the ten Dow Jones US economic sectors over the period February 2000 to August 2008, the two-year intervals 2002--2003, 2004--2005, 2008--2009, and also over 11 segments within the present financial crisis, to construct minimal spanning trees (MSTs) of the US economy at the sector level. In all MSTs, a core-fringe structure is found, with consumer goods, consumer services, and the industrials consistently making up the core, and basic materials, oil and gas, healthcare, telecommunications, and utilities residing predominantly on the fringe. More importantly, we find that the MSTs can be classified into two distinct, statistically robust, topologies: (i) star-like, with the industrials at the center, associated with low-volatility economic growth; and (ii) chain-like, associated with high-volatility economic crisis. Finally, we present statistical evidence, based on the emergence of a star-like MST in Sep 2009, and the MST staying robustly star-like throughout the Greek Debt Crisis, that the US economy is on track to a recovery.        △ Less","20 December, 2010","q-fin.ST,q-fin.RM",10.1016/j.physa.2011.01.020 
              DynaChanAl: Dynamic Channel Allocation with Minimal End-to-end Delay for Wireless Sensor Networks          ,1009.1604,https://arxiv.org/abs/1009.1604,https://arxiv.org/pdf/1009.1604,"Authors:JeongGilKo,AmitabhMishra","        With recent advances in wireless communication, networking, and low power sensor technology, wireless sensor network (WSN) systems have begun to take significant roles in various applications ranging from environmental sensing to mobile healthcare sensing. While some WSN applications only require a lim- ited amount of bandwidth, new emerging applications operate with a notice- ably large amount of data transfers. One way to deal with such applications is to maximize the available capacity by utilizing the use of multiple wireless channels. This work proposes DynaChannAl, a distributed dynamic wireless channel algorithm with the goal of effectively distributing nodes on multiple wireless channels in WSN systems. Specifically, DynaChannAl targets applica- tions where mobile nodes connect to a pre-existing wireless backbone and takes the expected end-to-end queuing delay as its core metric. We use the link qual- ity indicator (LQI) values provided by IEEE 802.15.4 radios white-list potential links with good link quality and evaluate such links with the aggregated packet transmission latency at each hop. Our approach is useful for applications that require minimal end-to-end delay (i.e., healthcare applications). DynaChannAl is a light weight and highly adoptable scheme that can be easily incorporated with various pre-developed components and pre-deployed applications. We eval- uate DynaChannAl in on a 45 node WSN testbed. As the first study to consider end-to-end latency as the core metric for channel allocation in WSN systems, the experimental results indicate that DynaChannAl successfully distributes multi- ple (mobile) source nodes on different wireless channels and enables the nodes to select wireless channel and links that can minimize the end-to-end latency.        △ Less","8 September, 2010",cs.NI,
              A Novel Chronic Disease Policy Model          ,1009.0405,https://arxiv.org/abs/1009.0405,https://arxiv.org/pdf/1009.0405,"Authors:NathanGreen,DuncanSmith,MatthewSperrin,IainBuchan","        We develop a simulation tool to support policy-decisions about healthcare for chronic diseases in defined populations. Incident disease-cases are generated in-silico from an age-sex characterised general population using standard epidemiological approaches. A novel disease-treatment model then simulates continuous life courses for each patient using discrete event simulation. Ideally, the discrete event simulation model would be inferred from complete longitudinal healthcare data via a likelihood or Bayesian approach. Such data is seldom available for relevant populations, therefore an innovative approach to evidence synthesis is required. We propose a novel entropy-based approach to fit survival densities. This method provides a fully flexible way to incorporate the available information, which can be derived from arbitrary sources. Discrete event simulation then takes place on the fitted model using a competing hazards framework. The output is then used to help evaluate the potential impacts of policy options for a given population.        △ Less","2 September, 2010",stat.AP,
              Using Integrated Nested Laplace Approximation for Modeling Spatial Healthcare Utilization          ,1006.3764,https://arxiv.org/abs/1006.3764,https://arxiv.org/pdf/1006.3764,"Authors:ErikA.Sauleau,ValentinaMameli,MonicaMusio","        In recent years, spatial and spatio-temporal modeling have become an important area of research in many fields (epidemiology, environmental studies, disease mapping). In this work we propose different spatial models to study hospital recruitment, including some potentially explicative variables. Interest is on the distribution per geographical unit of the ratio between the number of patients living in this geographical unit and the population in the same unit. Models considered are within the framework of Bayesian Latent Gaussian models. Our response variable is assumed to follow a binomial distribution, with logit link, whose parameters are the population in the geographical unit and the corresponding relative risk. The structured additive predictor accounts for effects of various covariates in an additive way, including smoothing functions of the covariates (for example spatial effect), linear effect of covariates. To approximate posterior marginals, which not available in closed form, we use integrated nested Laplace approximations (INLA), recently proposed for approximate Bayesian inference in latent Gaussian models. INLA has the advantage of giving very accurate approximations and being faster than McMC methods when the number of parameters does not exceed 6 (as it is in our case). Model comparisons are assessed using DIC criterion.        △ Less","18 June, 2010","stat.AP,stat.ME",
              Holographic Projection Technology: The World is Changing          ,1006.0846,https://arxiv.org/abs/1006.0846,https://arxiv.org/pdf/1006.0846,Authors:AhmedElmorshidy,"        This research papers examines the new technology of Holographic Projections. It highlights the importance and need of this technology and how it represents the new wave in the future of technology and communications, the different application of the technology, the fields of life it will dramatically affect including business, education, telecommunication and healthcare. The paper also discusses the future of holographic technology and how it will prevail in the coming years highlighting how it will also affect and reshape many other fields of life, technologies and businesses.        △ Less","4 June, 2010",cs.CY,
              Radio Frequency Identifiers: What are the Possibilities?          ,1005.5129,https://arxiv.org/abs/1005.5129,https://arxiv.org/pdf/1005.5129,Authors:AhmedElmorshidy,"        This paper defines the components of radio frequency identifiers (RFID). It also explores the different areas and sectors where RFID can be beneficial. The paper discusses the uses and advantages of RFID in deference, consumer packaged goods (CPG), healthcare, logistics, manufacturing, and retail.        △ Less","27 May, 2010",cs.OH,
              A General Simulation Framework for Supply Chain Modeling: State of the Art and Case Study          ,1004.3271,https://arxiv.org/abs/1004.3271,https://arxiv.org/pdf/1004.3271,"Authors:AntonioCimino,FrancescoLongo,GiovanniMirabelli","        Nowadays there is a large availability of discrete event simulation software that can be easily used in different domains: from industry to supply chain, from healthcare to business management, from training to complex systems design. Simulation engines of commercial discrete event simulation software use specific rules and logics for simulation time and events management. Difficulties and limitations come up when commercial discrete event simulation software are used for modeling complex real world-systems (i.e. supply chains, industrial plants). The objective of this paper is twofold: first a state of the art on commercial discrete event simulation software and an overview on discrete event simulation models development by using general purpose programming languages are presented; then a Supply Chain Order Performance Simulator (SCOPS, developed in C++) for investigating the inventory management problem along the supply chain under different supply chain scenarios is proposed to readers.        △ Less","19 April, 2010",cs.OH,
              Lessons from the Failure and Subsequent Success of a Complex Healthcare Sector IT Project          ,1003.3880,https://arxiv.org/abs/1003.3880,https://arxiv.org/pdf/1003.3880,"Authors:DavidGreenwood,AliKhajeh-Hosseini,IanSommerville","        This paper argues that IT failures diagnosed as errors at the technical or project management level are often mistakenly pointing to symptoms of failure rather than a project's underlying socio-complexity (complexity resulting from the interactions of people and groups) which is usually the actual source of failure. We propose a novel method, Stakeholder Impact Analysis, that can be used to identify risks associated with socio-complexity as it is grounded in insights from the social sciences, psychology and management science. This paper demonstrates the effectiveness of Stakeholder Impact Analysis by using the 1992 London Ambulance Service Computer Aided Dispatch project as a case study, and shows that had our method been used to identify the risks and had they been mitigated, it would have reduced the risk of project failure. This paper's original contribution comprises expanding upon existing accounts of failure by examining failures at a level of granularity not seen elsewhere that enables the underlying socio-complexity sources of risk to be identified.        △ Less","14 May, 2010",cs.SE,
              RFID Applications: An Introductory and Exploratory Study          ,1002.1179,https://arxiv.org/abs/1002.1179,https://arxiv.org/pdf/1002.1179,"Authors:KamranAhsan,HanifaShah,PaulKingston","        RFID is not a new technology and has passed through many decades of use in military, airline, library, security, healthcare, sports, animal farms and other areas. Industries use RFID for various applications such as personal/vehicle access control, departmental store security, equipment tracking, baggage, fast food establishments, logistics, etc. The enhancement in RFID technology has brought advantages that are related to resource optimization, increased efficiency within business processes, and enhanced customer care, overall improvements in business operations and healthcare. Our research is part of a big project; its aim is to produce a model for mobile technology implementation of hospital patients' movement process. However, the focus of this paper is to explore the main RFID components, i.e. the tag, antenna and reader. The results of the investigations conducted on the three RFID components will be used to develop our research model.        △ Less","5 February, 2010",cs.NI,
              A Review of Wireless Body Area Networks for Medical Applications          ,1001.0831,https://arxiv.org/abs/1001.0831,https://arxiv.org/pdf/1001.0831,"Authors:SanaUllah,PervezKhan,NiamatUllah,ShahnazSaleem,HenryHiggins,KyungSupKwak","        Recent advances in Micro-Electro-Mechanical Systems (MEMS) technology, integrated circuits, and wireless communication have allowed the realization of Wireless Body Area Networks (WBANs). WBANs promise unobtrusive ambulatory health monitoring for a long period of time and provide real-time updates of the patient's status to the physician. They are widely used for ubiquitous healthcare, entertainment, and military applications. This paper reviews the key aspects of WBANs for numerous applications. We present a WBAN infrastructure that provides solutions to on-demand, emergency, and normal traffic. We further discuss in-body antenna design and low-power MAC protocol for WBAN. In addition, we briefly outline some of the WBAN applications with examples. Our discussion realizes a need for new power-efficient solutions towards in-body and on-body sensor networks.        △ Less","3 August, 2010",cs.NI,10.4236/ijcns.2009.28093 
              Prediction-Based Data Transmission for Energy Conservation in Wireless Body Sensors          ,0912.2430,https://arxiv.org/abs/0912.2430,https://arxiv.org/pdf/0912.2430,"Authors:FengXia,ZhenzhenXu,LinYao,WeifengSun,MingchuLi","        Wireless body sensors are becoming popular in healthcare applications. Since they are either worn or implanted into human body, these sensors must be very small in size and light in weight. The energy consequently becomes an extremely scarce resource, and energy conservation turns into a first class design issue for body sensor networks (BSNs). This paper deals with this issue by taking into account the unique characteristics of BSNs in contrast to conventional wireless sensor networks (WSNs) for e.g. environment monitoring. A prediction-based data transmission approach suitable for BSNs is presented, which combines a dual prediction framework and a low-complexity prediction algorithm that takes advantage of PID (proportional-integral-derivative) control. Both the framework and the algorithm are generic, making the proposed approach widely applicable. The effectiveness of the approach is verified through simulations using real-world health monitoring datasets.        △ Less","12 December, 2009","cs.NI,cs.DC",
              A Study of Implanted and Wearable Body Sensor Networks          ,0911.1546,https://arxiv.org/abs/0911.1546,https://arxiv.org/pdf/0911.1546,"Authors:SanaUllah,HenryHiggins,M.ArifSiddiqui,KyungSupKwak","        Recent advances in intelligent sensors, microelectronics and integrated circuit, system-on-chip design and low power wireless communication introduced the development of miniaturised and autonomous sensor nodes. These tiny sensor nodes can be deployed to develop a proactive Body Sensor Network (BSN). The rapid advancement in ultra low-power RF (radio frequency) technology enables invasive and non-invasive devices to communicate with a remote station. This communication revolutionizes healthcare system by enabling long term health monitoring of a patient and providing real time feedback to the medical experts. In this paper, we present In-body and On-body communication networks with a special focus on the methodologies of wireless communication between implanted medical devices with external monitoring equipment and recent technological growth in both areas. We also discuss open issues and challenges in a BSN.        △ Less","29 October, 2018",cs.NI,10.1007/978-3-540-78582-8_47 
              Proceedings of 1st International Workshop on Collaborative Information Seeking          ,0908.0583,https://arxiv.org/abs/0908.0583,/search/?searchtype=author&query=Pickens%2C+J,"Authors:JeremyPickens,GeneGolovchinsky,MeredithRingelMorris","        The goal of the workshop is to bring together researchers interested in various aspects of small-team collaborative search to share ideas, to stimulate research in the area, and to increase the visibility of this emerging area. We expect to identify promising directions for further exploration and to establish collaborative links among research groups. The workshop took place on June 20, 2008 in Pittsburgh, Pennsylvania, USA, in conjunction with the JCDL 2008 conference.  The workshop was organized around three themes: practices, models, and evaluation. We started with a discussion (still ongoing) about terminology, about how to situate our work in the existing research space. We also wanted to motivate our modeling and design discussions with real-world examples of collaboration. We discussed examples from the healthcare domain, students, faculty members, the military, and businesses such as pharmaceutical companies that conduct research.  We discussed several models of collaborative information seeking, including sense-making, communication, and information seeking theory based on Marcia Bates' Berrypicking theory.  Finally, presenters described several systems that implement various aspects of collaboration, including using search paths, simulations of user behavior to model system performance, and characterizing properties of groups that lead to more effective collaboration.        △ Less","7 August, 2009","cs.IR,cs.HC",
              Adaptive Process Management in Highly Dynamic and Pervasive Scenarios          ,0906.4149,https://arxiv.org/abs/0906.4149,https://arxiv.org/pdf/0906.4149,Authors:MassimilianodeLeoni,"        Process Management Systems (PMSs) are currently more and more used as a supporting tool for cooperative processes in pervasive and highly dynamic situations, such as emergency situations, pervasive healthcare or domotics/home automation. But in all such situations, designed processes can be easily invalidated since the execution environment may change continuously due to frequent unforeseeable events. This paper aims at illustrating the theoretical framework and the concrete implementation of SmartPM, a PMS that features a set of sound and complete techniques to automatically cope with unplanned exceptions. PMS SmartPM is based on a general framework which adopts the Situation Calculus and Indigolog.        △ Less","22 June, 2009",cs.SE,10.4204/EPTCS.2.7 
              ICTD for Healthcare in Ghana: Two Parallel Case Studies          ,0905.0203,https://arxiv.org/abs/0905.0203,https://arxiv.org/pdf/0905.0203,"Authors:RowenaLuk,MateiZaharia,MelissaHo,BrianLevine,PaulM.Aoki","          This paper examines two parallel case studies to promote remote medical consultation in Ghana. These projects, initiated independently by different researchers in different organizations, both deployed ICT solutions in the same medical community in the same year. The Ghana Consultation Network currently has over 125 users running a Web-based application over a delay-tolerant network of servers. OneTouch MedicareLine is currently providing 1700 doctors in Ghana with free mobile phone calls and text messages to other members of the medical community. We present the consequences of (1) the institutional context and identity of the investigators, as well as specific decisions made with respect to (2) partnerships formed, (3) perceptions of technological infrastructure, and (4) high-level design decisions. In concluding, we discuss lessons learned and high-level implications for future ICTD research agendas.        △ Less","2 May, 2009",cs.HC,10.1109/ICTD.2009.5426714 
              Hospital Acquired Infections: Advantages of a Computerized Surveillance          ,0903.0962,https://arxiv.org/abs/0903.0962,https://arxiv.org/pdf/0903.0962,Authors:LeonardMada,        To asses the advantages of a computerized surveillance system to detect Healthcare-Associated Infections (HAI). All HAI reported to the Timis County branch of the Romanian National Health Insurance and the Public Health Authority during the year 2007 were collected and assessed for validity        △ Less,"5 March, 2009",cs.DM,
              A List of Household Objects for Robotic Retrieval Prioritized by People with ALS (Version 092008)          ,0902.2186,https://arxiv.org/abs/0902.2186,https://arxiv.org/pdf/0902.2186,"Authors:YoungSangChoi,TravisDeyle,CharlesC.Kemp","        This technical report is designed to serve as a citable reference for the original prioritized object list that the Healthcare Robotics Lab at Georgia Tech released on its website in September of 2008. It is also expected to serve as the primary citable reference for the research associated with this list until the publication of a detailed, peer-reviewed paper.  The original prioritized list of object classes resulted from a needs assessment involving 8 motor-impaired patients with amyotrophic lateral sclerosis (ALS) and targeted, in-person interviews of 15 motor-impaired ALS patients. All of these participants were drawn from the Emory ALS Center.  The prioritized object list consists of 43 object classes ranked by how important the participants considered each class to be for retrieval by an assistive robot. We intend for this list to be used by researchers to inform the design and benchmarking of robotic systems, especially research related to autonomous mobile manipulation.        △ Less","12 February, 2009","cs.RO,cs.HC",
              A Data Model for Integrating Heterogeneous Medical Data in the Health-e-Child Project          ,0812.2874,https://arxiv.org/abs/0812.2874,https://arxiv.org/pdf/0812.2874,"Authors:AndrewBranson,TamasHauer,RichardMcClatchey,DmitryRogulin,JetendrShamdasani","        There has been much research activity in recent times about providing the data infrastructures needed for the provision of personalised healthcare. In particular the requirement of integrating multiple, potentially distributed, heterogeneous data sources in the medical domain for the use of clinicians has set challenging goals for the healthgrid community. The approach advocated in this paper surrounds the provision of an Integrated Data Model plus links to/from ontologies to homogenize biomedical (from genomic, through cellular, disease, patient and population-related) data in the context of the EC Framework 6 Health-e-Child project. Clinical requirements are identified, the design approach in constructing the model is detailed and the integrated model described in the context of examples taken from that project. Pointers are given to future work relating the model to medical ontologies and challenges to the use of fully integrated models and ontologies are identified.        △ Less","15 December, 2008",cs.DB,
"              A Complex Data Warehouse for Personalized, Anticipative Medicine          ",0809.2688,https://arxiv.org/abs/0809.2688,https://arxiv.org/pdf/0809.2688,"Authors:JérômeDarmont,EmersonOlivier","        With the growing use of new technologies, healthcare is nowadays undergoing significant changes. Information-based medicine has to exploit medical decision-support systems and requires the analysis of various, heterogeneous data, such as patient records, medical images, biological analysis results, etc. In this paper, we present the design of the complex data warehouse relating to high-level athletes. It is original in two ways. First, it is aimed at storing complex medical data. Second, it is designed to allow innovative and quite different kinds of analyses to support: (1) personalized and anticipative medicine (in opposition to curative medicine) for well-identified patients; (2) broad-band statistical studies over a given population of patients. Furthermore, the system includes data relating to several medical fields. It is also designed to be evolutionary to take into account future advances in medical research.        △ Less","16 September, 2008",cs.DB,
              Measuring Value in Healthcare,0806.2397,https://arxiv.org/abs/0806.2397,https://arxiv.org/pdf/0806.2397,Authors:ChristopherGardner,"        A statistical description and model of individual healthcare expenditures in the US has been developed for measuring value in healthcare. We find evidence that healthcare expenditures are quantifiable as an infusion-diffusion process, which can be thought of intuitively as a steady change in the intensity of treatment superimposed on a random process reflecting variations in the efficiency and effectiveness of treatment. The arithmetic mean represents the net average annual cost of healthcare; and when multiplied by the arithmetic standard deviation, which represents the effective risk, the result is a measure of healthcare cost control. Policymakers, providers, payors, or patients that decrease these parameters are generating value in healthcare. The model has an average absolute prediction error of approximately 10-12% across the range of expenditures which spans 6 orders of magnitude over a nearly 10-year period. For the top 1% of the population with the largest expenditures, representing 20%-30% of total spending on healthcare, a power-law relationship emerges. This relationship also applies to the most expensive medical conditions in the US. A fundamental connection between healthcare expenditures and mathematical finance is found by showing that the process healthcare expenditures follow is similar to a widely used model for managing financial assets, leading to the conclusion that a combination of these two fields may yield useful results.        △ Less","14 June, 2008","physics.soc-ph,physics.med-ph,q-fin.ST",
              Securing U-Healthcare Sensor Networks using Public Key Based Scheme          ,0804.1184,https://arxiv.org/abs/0804.1184,https://arxiv.org/pdf/0804.1184,"Authors:Md.MokammelHaque,Al-SakibKhanPathan,ChoongSeonHong","        Recent emergence of electronic culture uplifts healthcare facilities to a new era with the aid of wireless sensor network (WSN) technology. Due to the sensitiveness of medical data, austere privacy and security are inevitable for all parts of healthcare systems. However, the constantly evolving nature and constrained resources of sensors in WSN inflict unavailability of a lucid line of defense to ensure perfect security. In order to provide holistic security, protections must be incorporated in every component of healthcare sensor networks. This paper proposes an efficient security scheme for healthcare applications of WSN which uses the notion of public key cryptosystem. Our entire security scheme comprises basically of two parts; a key handshaking scheme based on simple linear operations and the derivation of decryption key by a receiver node for a particular sender in the network. Our architecture allows both base station to node or node to base station secure communications, and node-to-node secure communications. We consider both the issues of stringent security and network performance to propose our security scheme.        △ Less","8 April, 2008",cs.CR,
              Controlling nosocomial infection based on structure of hospital social networks          ,0803.1879,https://arxiv.org/abs/0803.1879,https://arxiv.org/pdf/0803.1879,"Authors:TaroUeno,NaokiMasuda","        Nosocomial infection raises a serious public health problem, as implied by the existence of pathogens characteristic to healthcare and hospital-mediated outbreaks of influenza and SARS. We simulate stochastic SIR dynamics on social networks, which are based on observations in a hospital in Tokyo, to explore effective containment strategies against nosocomial infection. The observed networks have hierarchical and modular structure. We show that healthcare workers, particularly medical doctors, are main vectors of diseases on these networks. Intervention methods that restrict interaction between medical doctors and their visits to different wards shrink the final epidemic size more than intervention methods that directly protect patients, such as isolating patients in single rooms. By the same token, vaccinating doctors with priority rather than patients or nurses is more effective. Finally, vaccinating individuals with large betweenness centrality is superior to vaccinating ones with large connectedness to others or randomly chosen individuals, as suggested by previous model studies. [The abstract of the manuscript has more information.]        △ Less","19 October, 2008",q-bio.PE,
              Hospital Case Cost Estimates Modelling - Algorithm Comparison          ,0802.4126,https://arxiv.org/abs/0802.4126,https://arxiv.org/pdf/0802.4126,"Authors:PeterAndru,AlexeiBotchkarev","        Ontario (Canada) Health System stakeholders support the idea and necessity of the integrated source of data that would include both clinical (e.g. diagnosis, intervention, length of stay, case mix group) and financial (e.g. cost per weighted case, cost per diem) characteristics of the Ontario healthcare system activities at the patient-specific level. At present, the actual patient-level case costs in the explicit form are not available in the financial databases for all hospitals. The goal of this research effort is to develop financial models that will assign each clinical case in the patient-specific data warehouse a dollar value, representing the cost incurred by the Ontario health care facility which treated the patient. Five mathematical models have been developed and verified using real dataset. All models can be classified into two groups based on their underlying method: 1. Models based on using relative intensity weights of the cases, and 2. Models based on using cost per diem.        △ Less","27 February, 2008","cs.CE,cs.DB",
"              Balancing transparency, efficiency and security in pervasive systems          ",0801.3102,https://arxiv.org/abs/0801.3102,https://arxiv.org/pdf/0801.3102,"Authors:MarkWenstrom,EloisaBentivegna,AliHurson","        This chapter will survey pervasive computing with a look at how its constraint for transparency affects issues of resource management and security. The goal of pervasive computing is to render computing transparent, such that computing resources are ubiquitously offered to the user and services are proactively performed for a user without his or her intervention. The task of integrating computing infrastructure into everyday life without making it excessively invasive brings about tradeoffs between flexibility and robustness, efficiency and effectiveness, as well as autonomy and reliability. As the feasibility of ubiquitous computing and its real potential for mass applications are still a matter of controversy, this chapter will look into the underlying issues of resource management and authentication to discover how these can be handled in a least invasive fashion. The discussion will be closed by an overview of the solutions proposed by current pervasive computing efforts, both in the area of generic platforms and for dedicated applications such as pervasive education and healthcare.        △ Less","20 January, 2008","cs.HC,cs.IR",
              On Frequency Optimisation for Power Saving in WSNs: Finding Optimum Hardware Timers Frequencies          ,0709.2618,https://arxiv.org/abs/0709.2618,https://arxiv.org/pdf/0709.2618,"Authors:AndreeaPicu,AntoineFraboulet,EricFleury","        Wireless Sensor Networks research and demand are now in full expansion, since people came to understand these are the key to a large number of issues in industry, commerce, home automation, healthcare, agriculture and environment, monitoring, public safety etc. One of the most challenging research problems in sensor networks research is power awareness and power-saving techniques. In this master's thesis, we have studied one particular power-saving technique, i.e. frequency scaling. In particular, we analysed the close relationship between clock frequencies in a microcontroller and several types of constraints imposed on these frequencies, e.g. by other components of the microcontroller, by protocol specifications, by external factors etc. Among these constraints, we were especially interested in the ones imposed by the timer service and by the serial ports' transmission rates. Our efforts resulted in a microcontroller configuration management tool which aims at assisting application programmers in choosing microcontroller configurations, in function of the particular needs and constraints of their application.        △ Less","27 February, 2012",cs.NI,
              The Management and Integration of Biomedical Knowledge: Application in the Health-e-Child Project (Position Paper)          ,cs/0609144,https://arxiv.org/abs/cs/0609144,https://arxiv.org/pdf/cs/0609144,"Authors:E.Jimenez-Ruiz,R.Berlanga,I.Sanz,R.McClatchey,R.Danger,D.Manset,J.Paraire,A.Rios","        The Health-e-Child project aims to develop an integrated healthcare platform for European paediatrics. In order to achieve a comprehensive view of childrens health, a complex integration of biomedical data, information, and knowledge is necessary. Ontologies will be used to formally define this domain knowledge and will form the basis for the medical knowledge management system. This paper introduces an innovative methodology for the vertical integration of biomedical knowledge. This approach will be largely clinician-centered and will enable the definition of ontology fragments, connections between them (semantic bridges) and enriched ontology fragments (views). The strategy for the specification and capture of fragments, bridges and views is outlined with preliminary examples demonstrated in the collection of biomedical information from hospital databases, biomedical ontologies, and biomedical public databases.        △ Less","26 September, 2006",cs.DB,
              Towards grid-enabled telemedicine in Africa          ,physics/0605104,https://arxiv.org/abs/physics/0605104,https://arxiv.org/pdf/physics/0605104,"Authors:F.Jacq,F.Bacin,N.Meda,D.Donnarieix,J.Salzemann,V.Vayssiere,N.Jacq,M.Renaud,F.Traore,G.Meda,R.Nikiema,V.Breton",        Telemedicine services are very relevant tools to train local physicians and to improve diagnosis by exchanging medical data. Telemedicine networks allow these exchanges but the set-up of multipoint dynamic telemedicine requires moving towards GRID technologies. A healthgrid is an environment where data of medical interest can be stored and is easily available between the different actors of healthcare. Two telemedicine applications were developed to link physicians from Burkina Faso and France with the perspective of setting up a grid infrastructure between the participating medical sites. A web site to exchange diagnosis on diabetic retinopathy was developed in PHP and another application using web services was developed to exchange patient information between two databases.        △ Less,"12 May, 2006",physics.ins-det,
"              Feedback Medicine: Control Systems Concepts in Personalised, Predictive Medicine and Combinatorial Intervention          ",q-bio/0603032,https://arxiv.org/abs/q-bio/0603032,https://arxiv.org/pdf/q-bio/0603032,"Authors:PeterWellstead,RickMiddleton,OlafWolkenhauer","        In its broadest definition, systems biology is the application of a `systems' way of thinking about and doing cell biology. By implication, this also invites us to consider a systems approach in the context of medicine and the treatment of disease. In particular, the idea that systems biology can form the basis of a personalised, predictive medicine will require that much closer attention is paid to the analytic properties of the feedback loops which will be set up by a personalised approach to healthcare. To emphasize the role that feedback theory will play in understanding personalised medicine, we use the term feedback medicine to describe the issues outlined.In these notes we consider feedback and control systems concepts applied to two important themes in medical systems biology - personalised medicine and combinatorial intervention. In particular, we formulate a feedback control interpretation for the administration of medicine, and relate them to various forms of medical treatment.        △ Less","27 March, 2006","q-bio.TO,q-bio.QM",
              Health-e-Child : An Integrated Biomedical Platform for Grid-Based Paediatric Applications          ,cs/0603036,https://arxiv.org/abs/cs/0603036,https://arxiv.org/pdf/cs/0603036,"Authors:JoergFreund,DorinComaniciu,YannisIoannis,PeiyaLiu,RichardMcClatchey,EdwinMorley-Fletcher,XavierPennec,GiacomoPongiglione,Xiang,ZHOU","        There is a compelling demand for the integration and exploitation of heterogeneous biomedical information for improved clinical practice, medical research, and personalised healthcare across the EU. The Health-e-Child project aims at developing an integrated healthcare platform for European Paediatrics, providing seamless integration of traditional and emerging sources of biomedical information. The long-term goal of the project is to provide uninhibited access to universal biomedical knowledge repositories for personalised and preventive healthcare, large-scale information-based biomedical research and training, and informed policy making. The project focus will be on individualised disease prevention, screening, early diagnosis, therapy and follow-up of paediatric heart diseases, inflammatory diseases, and brain tumours. The project will build a Grid-enabled European network of leading clinical centres that will share and annotate biomedical data, validate systems clinically, and diffuse clinical excellence across Europe by setting up new technologies, clinical workflows, and standards. This paper outlines the design approach being adopted in Health-e-Child to enable the delivery of an integrated biomedical information platform.        △ Less","11 March, 2006",cs.DC,
              The contact network of patients in a regional healthcare system          ,q-bio/0505020,https://arxiv.org/abs/q-bio/0505020,https://arxiv.org/pdf/q-bio/0505020,"Authors:FredrikLiljeros,PetterHolme,JohanGiesecke","        Yet in spite of advances in hospital treatment, hospitals continue to be a breeding ground for several airborne diseases and for diseases that are transmitted through close contacts like SARS, methicillin-resistant Staphylococcus aureus (MRSA), norovirus infections and tuberculosis (TB). Here we extract contact networks for up to 295,108 inpatients for durations up to two years from a database used for administrating a local public healthcare system serving a population of 1.9 million individuals. Structural and dynamical properties of the network of importance for the transmission of contagious diseases are then analyzed by methods from network epidemiology. The contact networks are found to be very much determined by an extreme (age independent) variation in duration of hospital stays and the hospital structure. We find that that the structure of contacts between in-patients exhibit structural properties, such as a high level of transitivity, assortativity and variation in number of contacts, that are likely to be of importance for the transmission of less contagious diseases. If these properties are considered when designing prevention programs the risk for and the effect of epidemic outbreaks may be decreased.        △ Less","10 May, 2005","q-bio.OT,physics.soc-ph,q-bio.PE",10.1080/08898480701612899 
              Should Cyberspace Chat Rooms be closed to protect Children?          ,cs/0409021,https://arxiv.org/abs/cs/0409021,https://arxiv.org/pdf/cs/0409021,Authors:VitaHinze-Hoare,"        The explosion of people networking in cyberspace, disseminating terabytes of information, is being promoted through the use of broadband, bluetooth technology, and wireless mobile computing facilities. New communities within such venues as virtual chat rooms discussion groups, newsgroups etc are being created daily and even hourly. This is raising issues of cyberethics concerning privacy,security, crime, human needs, e-business, e-healthcare, e-government and intellectual property among others that need to be evaluated and reflected upon. With this new freedom come new moral and ethical responsibilities, which raise questions as to whether anything can be published or whether there should be restrictions. This paper addresses one specific area, that has come into the public eye, the closure by Microsoft of all of its free chat rooms.        △ Less","11 September, 2004",cs.CY,
              A Grid Information Infrastructure for Medical Image Analysis          ,cs/0405087,https://arxiv.org/abs/cs/0405087,https://arxiv.org/pdf/cs/0405087,"Authors:DRogulin,FEstrella,THauer,RMcClatchey,TSolomonides","        The storage and manipulation of digital images and the analysis of the information held in those images are essential requirements for next-generation medical information systems. The medical community has been exploring collaborative approaches for managing image data and exchanging knowledge and Grid technology [1] is a promising approach to enabling distributed analysis across medical institutions and for developing new collaborative and cooperative approaches for image analysis without the necessity for clinicians to co-locate. The EU-funded MammoGrid project [2] is one example of this and it aims to develop a Europe-wide database of mammograms to support effective co-working between healthcare professionals across the EU. The MammoGrid prototype comprises a high-quality clinician visualization workstation (for data acquisition and inspection), a DICOM-compliant interface to a set of medical services (annotation, security, image analysis, data storage and querying services) residing on a so-called Grid-box and secure access to a network of other Grid-boxes connected through Grid middleware. One of the main deliverables of the project is a Grid-enabled infrastructure that manages federated mammogram databases across Europe. This paper outlines the MammoGrid Information Infrastructure (MII) for meta-data analysis and knowledge discovery in the medical imaging domain.        △ Less","24 May, 2004","cs.DB,cs.DC",
              A perspective on the Healthgrid initiative          ,cs/0402025,https://arxiv.org/abs/cs/0402025,https://arxiv.org/pdf/cs/0402025,"Authors:V.Breton,A.E.Solomonides,R.H.McClatchey","        This paper presents a perspective on the Healthgrid initiative which involves European projects deploying pioneering applications of grid technology in the health sector. In the last couple of years, several grid projects have been funded on health related issues at national and European levels. A crucial issue is to maximize their cross fertilization in the context of an environment where data of medical interest can be stored and made easily available to the different actors in healthcare, physicians, healthcare centres and administrations, and of course the citizens. The Healthgrid initiative, represented by the Healthgrid association (http://www.healthgrid.org), was initiated to bring the necessary long term continuity, to reinforce and promote awareness of the possibilities and advantages linked to the deployment of GRID technologies in health. Technologies to address the specific requirements for medical applications are under development. Results from the DataGrid and other projects are given as examples of early applications.        △ Less","12 February, 2004","cs.DB,cs.SE",10.1109/CCGrid.2004.1336598 
              MammoGrid: Large-Scale Distributed Mammogram Analysis          ,cs/0402006,https://arxiv.org/abs/cs/0402006,https://arxiv.org/pdf/cs/0402006,"Authors:S.RobertoAmendolia,MichaelBrady,RichardMcClatchey,MiguelMulet-Parada,MohammedOdeh,TonySolomonides","        Breast cancer as a medical condition and mammograms as images exhibit many dimensions of variability across the population. Similarly, the way diagnostic systems are used and maintained by clinicians varies between imaging centres and breast screening programmes, and so does the appearance of the mammograms generated. A distributed database that reflects the spread of pathologies across the population is an invaluable tool for the epidemiologist and the understanding of the variation in image acquisition protocols is essential to a radiologist in a screening programme. Exploiting emerging grid technology, the aim of the MammoGrid [1] project is to develop a Europe-wide database of mammograms that will be used to investigate a set of important healthcare applications and to explore the potential of the grid to support effective co-working between healthcare professionals.        △ Less","2 February, 2004",cs.SE,
              The MammoGrid Project Grids Architecture          ,cs/0306095,https://arxiv.org/abs/cs/0306095,https://arxiv.org/pdf/cs/0306095,"Authors:RichardMcClatchey,PredragBuncic,DavidManset,TamasHauer,FloridaEstrella,PabloSaiz,DmitriRogulin","        The aim of the recently EU-funded MammoGrid project is, in the light of emerging Grid technology, to develop a European-wide database of mammograms that will be used to develop a set of important healthcare applications and investigate the potential of this Grid to support effective co-working between healthcare professionals throughout the EU. The MammoGrid consortium intends to use a Grid model to enable distributed computing that spans national borders. This Grid infrastructure will be used for deploying novel algorithms as software directly developed or enhanced within the project. Using the MammoGrid clinicians will be able to harness the use of massive amounts of medical image data to perform epidemiological studies, advanced image processing, radiographic education and ultimately, tele-diagnosis over communities of medical ""virtual organisations"". This is achieved through the use of Grid-compliant services [1] for managing (versions of) massively distributed files of mammograms, for handling the distributed execution of mammograms analysis software, for the development of Grid-aware algorithms and for the sharing of resources between multiple collaborating medical centres. All this is delivered via a novel software and hardware information infrastructure that, in addition guarantees the integrity and security of the medical data. The MammoGrid implementation is based on AliEn, a Grid framework developed by the ALICE Collaboration. AliEn provides a virtual file catalogue that allows transparent access to distributed data-sets and provides top to bottom implementation of a lightweight Grid applicable to cases when handling of a large number of files is required. This paper details the architecture that will be implemented by the MammoGrid project.        △ Less","16 June, 2003","cs.DC,cs.DB",
              Fuzzy data: XML may handle it          ,cs/0007017,https://arxiv.org/abs/cs/0007017,https://arxiv.org/pdf/cs/0007017,"Authors:R.Schweiger,S.Hoelzer,J.Dudeck","        Data modeling is one of the most difficult tasks in application engineering. The engineer must be aware of the use cases and the required application services and at a certain point of time he has to fix the data model which forms the base for the application services. However, once the data model has been fixed it is difficult to consider changing needs. This might be a problem in specific domains, which are as dynamic as the healthcare domain. With fuzzy data we address all those data that are difficult to organize in a single database. In this paper we discuss a gradual and pragmatic approach that uses the XML technology to conquer more model flexibility. XML may provide the clue between unstructured text data and structured database solutions and shift the paradigm from ""organizing the data along a given model"" towards ""organizing the data along user requirements"".        △ Less","13 July, 2000",cs.IR,
              Natural Language Generation in Healthcare: Brief Review          ,cmp-lg/9708002,https://arxiv.org/abs/cmp-lg/9708002,https://arxiv.org/pdf/cmp-lg/9708002,"Authors:AlisonJ.Cawsey,BonnieL.Webber,RayB.Jones","        Good communication is vital in healthcare, both among healthcare professionals, and between healthcare professionals and their patients. And well-written documents, describing and/or explaining the information in structured databases may be easier to comprehend, more edifying and even more convincing, than the structured data, even when presented in tabular or graphic form. Documents may be automatically generated from structured data, using techniques from the field of natural language generation. These techniques are concerned with how the content, organisation and language used in a document can be dynamically selected, depending on the audience and context. They have been used to generate health education materials, explanations and critiques in decision support systems, and medical reports and progress notes.        △ Less","7 August, 1997",cs.CL,
